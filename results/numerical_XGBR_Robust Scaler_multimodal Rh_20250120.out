
------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n01>
Subject: Job 292168: <numerical_XGBR_polymer_size_feats_on_multimodal Rh_all_num_20250120> in cluster <Hazel> Exited

Job <numerical_XGBR_polymer_size_feats_on_multimodal Rh_all_num_20250120> was submitted from host <c031n04> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 11:55:31 2025
Job was executed on host(s) <6*c039n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 11:55:33 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 11:55:33 2025
Terminated at Mon Jan 20 11:55:51 2025
Results reported at Mon Jan 20 11:55:51 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 25:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "numerical_XGBR_polymer_size_feats_on_multimodal Rh_all_num_20250120"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR_Robust Scaler_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR_Robust Scaler_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "multimodal Rh"                                     --regressor_type "XGBR"                                     --transform_type "Robust Scaler"                                     --numerical_feats 'Mn (g/mol)' 'PDI' "solvent dP" "solvent dD" "solvent dH" "polymer dP" "polymer dD" "polymer dH"                                     --columns_to_impute "PDI"
                                    --special_impute 'Mw (g/mol)' #                                   --imputer mean


conda deactivate


------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   1.76 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   33 sec.
    Turnaround time :                            20 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR_Robust Scaler_multimodal Rh_20250120.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n01>
Subject: Job 292175: <numerical_XGBR_polymer_size_feats_on_multimodal Rh_all_num_20250120> in cluster <Hazel> Done

Job <numerical_XGBR_polymer_size_feats_on_multimodal Rh_all_num_20250120> was submitted from host <c031n04> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 11:56:56 2025
Job was executed on host(s) <6*c039n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 11:56:58 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 11:56:58 2025
Terminated at Mon Jan 20 11:57:47 2025
Results reported at Mon Jan 20 11:57:47 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 25:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "numerical_XGBR_polymer_size_feats_on_multimodal Rh_all_num_20250120"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR_Robust Scaler_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR_Robust Scaler_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "multimodal Rh"                                     --regressor_type "XGBR"                                     --transform_type "Robust Scaler"                                     --numerical_feats 'Mn (g/mol)' 'PDI' 'Mw (g/mol)' "solvent dP" "solvent dD" "solvent dH" "polymer dP" "polymer dD" "polymer dH"                                     --columns_to_impute "PDI"                                     --special_impute 'Mw (g/mol)' #                                   --imputer mean


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   6.53 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   59 sec.
    Turnaround time :                            51 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR_Robust Scaler_multimodal Rh_20250120.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02197492131603932), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.021723169447991613), ('regressor__regressor__estimator__max_depth', 1230), ('regressor__regressor__estimator__n_estimators', 65), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.007365436263358182), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 137), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.023843831586658776), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.025701461050069208), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.006825336264293421), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 105), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00341035322931715), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 306), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.010498599669788057), ('regressor__regressor__estimator__max_depth', 23), ('regressor__regressor__estimator__n_estimators', 101), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.006126535835527631), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 188), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02166680099146942), ('regressor__regressor__estimator__max_depth', 25), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0018529093925428416), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 626), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.019234315898403652), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.017540423029366144), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.016583801360190567), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01962239743535831), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.011301419861704928), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 73), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.012598346009955488), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 85), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0024214277806018413), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 330), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002479891999546227), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 474), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0010905582661193069), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 988), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005890775484528729), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 153), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.011326812748507682), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 93), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.011849714869199033), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 74), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.025755348844466033), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 66), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00965453895275351), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 83), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01346773908289801), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 77), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0191861545626102), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01305512170368334), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 72), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.001), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 1226), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0018645784526166392), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 528), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005125203529850082), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 158), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.008497418703563863), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 105), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02021921555063376), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.004355601817305349), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 162), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0031191257309464357), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 341), ('regressor__regressor__estimator__n_jobs', -2)])


Average scores:	 r2: [ 0.139  0.228 -0.041]Â±[0.111 0.103 0.022]
[array([ 0.13863379,  0.22761346, -0.04051206]), array([8.06843110e+00, 1.38863932e+02, 9.38185439e+04]), array([5.60283671e+00, 8.94344321e+01, 1.41204826e+04])]
scaler
Filename: (PDI-Mw-solvent dP-solvent dD-solvent dH-polymer dP-polymer dD-polymer dH)_XGBR_mean_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/scaler/(PDI-Mw-solvent dP-solvent dD-solvent dH-polymer dP-polymer dD-polymer dH)_XGBR_mean_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/scaler/(PDI-Mw-solvent dP-solvent dD-solvent dH-polymer dP-polymer dD-polymer dH)_XGBR_mean_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/scaler/(PDI-Mw-solvent dP-solvent dD-solvent dH-polymer dP-polymer dD-polymer dH)_XGBR_mean_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n01>
Subject: Job 292196: <numerical_XGBR_polymer_size_feats_on_multimodal Rh_all_num_20250120> in cluster <Hazel> Done

Job <numerical_XGBR_polymer_size_feats_on_multimodal Rh_all_num_20250120> was submitted from host <c031n04> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:03:43 2025
Job was executed on host(s) <6*c039n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:03:45 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:03:45 2025
Terminated at Mon Jan 20 12:48:04 2025
Results reported at Mon Jan 20 12:48:04 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 25:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "numerical_XGBR_polymer_size_feats_on_multimodal Rh_all_num_20250120"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR_Robust Scaler_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR_Robust Scaler_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "multimodal Rh"                                     --regressor_type "XGBR"                                     --transform_type "Robust Scaler"                                     --numerical_feats 'Mn (g/mol)' 'PDI' 'Mw (g/mol)' "solvent dP" "solvent dD" "solvent dH" "polymer dP" "polymer dD" "polymer dH"                                     --columns_to_impute "PDI"                                     --special_impute 'Mw (g/mol)'                                     --imputer mean


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   7908.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             8.78 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               7.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   2686 sec.
    Turnaround time :                            2661 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR_Robust Scaler_multimodal Rh_20250120.err> for stderr output of this job.

