
------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n03>
Subject: Job 330387: <numerical_MLP_with_feats_on_log Rg (nm)_20250520> in cluster <Hazel> Done

Job <numerical_MLP_with_feats_on_log Rg (nm)_20250520> was submitted from host <c201n02> by user <sdehgha2> in cluster <Hazel> at Tue May 20 17:18:10 2025
Job was executed on host(s) <6*c203n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue May 20 17:18:10 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue May 20 17:18:10 2025
Terminated at Tue May 20 17:18:19 2025
Results reported at Tue May 20 17:18:19 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 8:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_MLP_with_feats_on_log Rg (nm)_20250520"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "MLP"                                   --numerical_feats 'Xn' 'Mw (g/mol)' 'PDI' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' "polymer dP" "polymer dD" "polymer dH" 'solvent dP' 'solvent dD' 'solvent dH' 

conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4.27 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   26 sec.
    Turnaround time :                            9 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.err> for stderr output of this job.

Average scores:	 r: 0.77±0.08	 r2: 0.57±0.14
Filename: (Xn-Mw-PDI-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_MLP_hypOFF_Standard
{6: {'fit_time': array([0.30548739, 0.24710989, 0.24414849, 0.24782562, 0.23983169]), 'score_time': array([0.01007795, 0.01003885, 0.00999427, 0.0100317 , 0.00998497]), 'test_pearson_r': array([0.80784317, 0.78612751, 0.67961195, 0.78527721, 0.75043198]), 'test_spearman_r': array([0.83782747, 0.77769003, 0.72249499, 0.7861062 , 0.7572631 ]), 'test_rmse': array([-0.39260747, -0.39616759, -0.4852421 , -0.39829046, -0.36860264]), 'test_mae': array([-0.24317143, -0.27847756, -0.28219564, -0.25321899, -0.2556382 ]), 'test_r2': array([0.64685604, 0.60778049, 0.4023161 , 0.59605142, 0.55485526])}, 13: {'fit_time': array([0.21767831, 0.2039063 , 0.2097857 , 0.20820022, 0.20806742]), 'score_time': array([0.00995827, 0.00961041, 0.00989962, 0.00959945, 0.00962901]), 'test_pearson_r': array([0.66960584, 0.86496534, 0.58221559, 0.74988086, 0.81493366]), 'test_spearman_r': array([0.82196461, 0.89273593, 0.67146305, 0.78696016, 0.8004254 ]), 'test_rmse': array([-0.49174026, -0.3363793 , -0.55584259, -0.42988891, -0.34314229]), 'test_mae': array([-0.29549694, -0.25611194, -0.31800545, -0.28102419, -0.25787603]), 'test_r2': array([0.38530935, 0.73233679, 0.20722149, 0.55421908, 0.63223257])}, 42: {'fit_time': array([0.21661735, 0.16978645, 0.20527983, 0.20778108, 0.20296311]), 'score_time': array([0.00919032, 0.00916052, 0.0088973 , 0.00888395, 0.00894308]), 'test_pearson_r': array([0.81049488, 0.6139718 , 0.73633132, 0.91436193, 0.74075515]), 'test_spearman_r': array([0.80542214, 0.6836286 , 0.74281201, 0.90832248, 0.7314193 ]), 'test_rmse': array([-0.40231745, -0.54139782, -0.34826043, -0.26686625, -0.44718061]), 'test_mae': array([-0.31356933, -0.32891571, -0.24427676, -0.20803741, -0.25128844]), 'test_r2': array([0.6353691 , 0.31973856, 0.51135858, 0.82715273, 0.52028222])}, 69: {'fit_time': array([0.20551205, 0.20782876, 0.21605682, 0.21432304, 0.21736193]), 'score_time': array([0.00901222, 0.00894547, 0.00929856, 0.00924277, 0.00948238]), 'test_pearson_r': array([0.8686223 , 0.70346116, 0.80367434, 0.68132751, 0.84813364]), 'test_spearman_r': array([0.87001537, 0.71420046, 0.83579541, 0.73276837, 0.87121589]), 'test_rmse': array([-0.32407282, -0.4270794 , -0.37378803, -0.4429403 , -0.39143933]), 'test_mae': array([-0.23584305, -0.26803888, -0.25022018, -0.2679122 , -0.2739296 ]), 'test_r2': array([0.72775637, 0.48117138, 0.64401362, 0.44618026, 0.67426893])}, 420: {'fit_time': array([0.20890999, 0.20157766, 0.2045958 , 0.21022534, 0.21414638]), 'score_time': array([0.00888824, 0.00895667, 0.00944495, 0.00917721, 0.00886798]), 'test_pearson_r': array([0.85581089, 0.74790252, 0.83774182, 0.75951984, 0.704924  ]), 'test_spearman_r': array([0.80849074, 0.80309078, 0.80288638, 0.76355952, 0.72579587]), 'test_rmse': array([-0.31407606, -0.41826233, -0.31311793, -0.43764619, -0.46392364]), 'test_mae': array([-0.22689691, -0.29408857, -0.22647111, -0.28709781, -0.28573305]), 'test_r2': array([0.72896558, 0.55171566, 0.69632005, 0.55753472, 0.49627613])}, 1234567890: {'fit_time': array([0.21333885, 0.20537806, 0.21269941, 0.2077992 , 0.21842027]), 'score_time': array([0.00933146, 0.00862455, 0.00855398, 0.00863576, 0.00955153]), 'test_pearson_r': array([0.84483994, 0.88877772, 0.77290055, 0.7491378 , 0.62696303]), 'test_spearman_r': array([0.83048026, 0.86295611, 0.79136783, 0.76659339, 0.68829088]), 'test_rmse': array([-0.30963144, -0.26600254, -0.40741653, -0.42637917, -0.54572845]), 'test_mae': array([-0.21624775, -0.18956021, -0.27824127, -0.28885958, -0.32442526]), 'test_r2': array([0.70931948, 0.78606683, 0.59076415, 0.55312611, 0.37466037])}, 473129: {'fit_time': array([0.20058584, 0.20491171, 0.20742106, 0.20802498, 0.20859432]), 'score_time': array([0.00902343, 0.00909901, 0.00929999, 0.00918937, 0.00922894]), 'test_pearson_r': array([0.60765879, 0.78666651, 0.83677548, 0.85042364, 0.78404976]), 'test_spearman_r': array([0.72113441, 0.81498721, 0.81601025, 0.82595332, 0.76932127]), 'test_rmse': array([-0.53228568, -0.37822361, -0.32668094, -0.34640154, -0.39983194]), 'test_mae': array([-0.29405198, -0.25439802, -0.22668445, -0.25432801, -0.31278754]), 'test_r2': array([0.36013575, 0.55661047, 0.6911337 , 0.71102665, 0.57645848])}, 'pearson_r_avg': 0.7676034117163957, 'pearson_r_stdev': 0.08285711579841884, 'spearman_r_avg': 0.786898548499544, 'spearman_r_stdev': 0.05853923855230339, 'rmse_avg': 0.40139583076231095, 'rmse_stdev': 0.0748258583781406, 'mae_avg': 0.2663748413422705, 'mae_stdev': 0.03320037792580905, 'r2_avg': 0.5727595572635009, 'r2_stdev': 0.138009788847169}
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n03>
Subject: Job 330459: <numerical_MLP_with_feats_on_log Rg (nm)_20250520> in cluster <Hazel> Done

Job <numerical_MLP_with_feats_on_log Rg (nm)_20250520> was submitted from host <c201n02> by user <sdehgha2> in cluster <Hazel> at Tue May 20 17:20:52 2025
Job was executed on host(s) <6*c203n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue May 20 17:20:52 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue May 20 17:20:52 2025
Terminated at Tue May 20 17:21:13 2025
Results reported at Tue May 20 17:21:13 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 8:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_MLP_with_feats_on_log Rg (nm)_20250520"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "MLP"                                   --numerical_feats 'Xn' 'Mw (g/mol)' 'PDI' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' "polymer dP" "polymer dD" "polymer dH" 'solvent dP' 'solvent dD' 'solvent dH' 

conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   44.30 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   22 sec.
    Turnaround time :                            21 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n13>
Subject: Job 347346: <numerical_MLP_with_feats_on_log Rg (nm)_20250520> in cluster <Hazel> Done

Job <numerical_MLP_with_feats_on_log Rg (nm)_20250520> was submitted from host <c203n13> by user <sdehgha2> in cluster <Hazel> at Wed May 21 00:09:16 2025
Job was executed on host(s) <6*c203n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed May 21 00:09:18 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed May 21 00:09:18 2025
Terminated at Wed May 21 00:09:30 2025
Results reported at Wed May 21 00:09:30 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 20:01
#BSUB -x
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_MLP_with_feats_on_log Rg (nm)_20250520"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "MLP"                                   --numerical_feats 'Xn' 'Mw (g/mol)' 'PDI' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' "polymer dP" "polymer dD" "polymer dH" 'solvent dP' 'solvent dD' 'solvent dH' 

conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   13.00 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   14 sec.
    Turnaround time :                            14 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__alpha', 6.498410777236803e-07), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.0006615742508580729), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.0004278334860996038), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__alpha', 6.161938820451174e-06), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.017335964512317413), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 128), ('regressor__regressor__learning_rate_init', 0.0010979775198143374), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 128), ('regressor__regressor__learning_rate_init', 0.0009223744955797581), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__alpha', 1.0), ('regressor__regressor__batch_size', 256), ('regressor__regressor__learning_rate_init', 0.001263029864483422), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__alpha', 0.004180046119613635), ('regressor__regressor__batch_size', 128), ('regressor__regressor__learning_rate_init', 0.0010042221112117724), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__alpha', 1.9199405185711804e-06), ('regressor__regressor__batch_size', 128), ('regressor__regressor__learning_rate_init', 0.0016309931576505778), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__alpha', 1.0), ('regressor__regressor__batch_size', 256), ('regressor__regressor__learning_rate_init', 0.025324073681758878), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__alpha', 1.0182278831634622e-08), ('regressor__regressor__batch_size', 64), ('regressor__regressor__learning_rate_init', 0.0012538694788060344), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 128), ('regressor__regressor__learning_rate_init', 0.001343980165450088), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 64), ('regressor__regressor__learning_rate_init', 0.0005595490434649263), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.0008666167774072739), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__alpha', 4.9340316268455766e-05), ('regressor__regressor__batch_size', 64), ('regressor__regressor__learning_rate_init', 0.00035890733421703545), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__alpha', 8.651802232299344e-05), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.0011126130882419228), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__alpha', 0.00016274356859524733), ('regressor__regressor__batch_size', 256), ('regressor__regressor__learning_rate_init', 0.0032259649736700526), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__alpha', 3.351458522894035e-06), ('regressor__regressor__batch_size', 64), ('regressor__regressor__learning_rate_init', 0.0005744247502698656), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__alpha', 3.1067165080118477e-07), ('regressor__regressor__batch_size', 128), ('regressor__regressor__learning_rate_init', 0.0006805748693139633), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 64), ('regressor__regressor__learning_rate_init', 0.0015848722616231376), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.00043085968116137434), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__alpha', 1.6568733808394798e-07), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.0004556589946363439), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.0023863033967662815), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 64), ('regressor__regressor__learning_rate_init', 0.0007990140311703282), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.001051311612645535), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 256), ('regressor__regressor__learning_rate_init', 0.0010809453191141737), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__alpha', 0.0002968681574006739), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.000965489193490481), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__alpha', 1.0114396451643861e-08), ('regressor__regressor__batch_size', 256), ('regressor__regressor__learning_rate_init', 0.0012870940768877138), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.0043614844444045875), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.011916388220725011), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.0019445732766568073), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.0011047816215412642), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__alpha', 1e-08), ('regressor__regressor__batch_size', 32), ('regressor__regressor__learning_rate_init', 0.001449028395035683), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__alpha', 0.00010297249699804213), ('regressor__regressor__batch_size', 64), ('regressor__regressor__learning_rate_init', 0.0046116193980522925), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__alpha', 2.4349891755080895e-06), ('regressor__regressor__batch_size', 128), ('regressor__regressor__learning_rate_init', 0.0005272535010633554), ('regressor__regressor__max_iter', 500)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR MLP 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__alpha', 0.11620738285507774), ('regressor__regressor__batch_size', 256), ('regressor__regressor__learning_rate_init', 0.1), ('regressor__regressor__max_iter', 500)])


Average scores:	 r: 0.83±0.09	 r2: 0.65±0.2
Filename: (Xn-Mw-PDI-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_MLP_Standard
{6: {'fit_time': array([0.5098114 , 0.49971771, 0.52131891, 0.5108707 , 0.48178673]), 'score_time': array([0.01017427, 0.00909567, 0.00948811, 0.00943637, 0.01070809]), 'test_pearson_r': array([0.93574418, 0.85080767, 0.73254662, 0.83419391, 0.73202213]), 'test_spearman_r': array([0.92911891, 0.85820402, 0.7674935 , 0.84643797, 0.7297493 ]), 'test_rmse': array([-0.23370518, -0.33969928, -0.44674635, -0.34871045, -0.40109302]), 'test_mae': array([-0.14986474, -0.20320736, -0.22443343, -0.19525797, -0.23215444]), 'test_r2': array([0.87486715, 0.71162305, 0.49338665, 0.69036062, 0.47292238]), 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__activation': 'relu', 'regressor__alpha': 1e-08, 'regressor__batch_size': 128, 'regressor__beta_1': 0.9, 'regressor__beta_2': 0.999, 'regressor__early_stopping': False, 'regressor__epsilon': 1e-08, 'regressor__learning_rate': 'constant', 'regressor__learning_rate_init': 0.0010979775198143374, 'regressor__max_fun': 15000, 'regressor__max_iter': 500, 'regressor__momentum': 0.9, 'regressor__n_iter_no_change': 10, 'regressor__nesterovs_momentum': True, 'regressor__power_t': 0.5, 'regressor__random_state': None, 'regressor__shuffle': True, 'regressor__solver': 'adam', 'regressor__tol': 0.0001, 'regressor__validation_fraction': 0.1, 'regressor__verbose': False, 'regressor__warm_start': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 13: {'fit_time': array([0.51917529, 0.48360705, 0.30774307, 0.49363685, 0.50428629]), 'score_time': array([0.00904059, 0.00897622, 0.00878   , 0.00847554, 0.00849843]), 'test_pearson_r': array([0.90940506, 0.90311514, 0.63418567, 0.80562985, 0.86646424]), 'test_spearman_r': array([0.89925063, 0.90554358, 0.70801222, 0.81001689, 0.83065575]), 'test_rmse': array([-0.27208276, -0.29035524, -0.53178482, -0.39079602, -0.2924796 ]), 'test_mae': array([-0.17269215, -0.15540057, -0.27675089, -0.21551095, -0.17063926]), 'test_r2': array([0.81181409, 0.80057046, 0.27436187, 0.6316088 , 0.73281263]), 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__activation': 'relu', 'regressor__alpha': 1.9199405185711804e-06, 'regressor__batch_size': 128, 'regressor__beta_1': 0.9, 'regressor__beta_2': 0.999, 'regressor__early_stopping': False, 'regressor__epsilon': 1e-08, 'regressor__learning_rate': 'constant', 'regressor__learning_rate_init': 0.0016309931576505778, 'regressor__max_fun': 15000, 'regressor__max_iter': 500, 'regressor__momentum': 0.9, 'regressor__n_iter_no_change': 10, 'regressor__nesterovs_momentum': True, 'regressor__power_t': 0.5, 'regressor__random_state': None, 'regressor__shuffle': True, 'regressor__solver': 'adam', 'regressor__tol': 0.0001, 'regressor__validation_fraction': 0.1, 'regressor__verbose': False, 'regressor__warm_start': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 42: {'fit_time': array([0.49999404, 0.49864483, 0.51400638, 0.48706388, 0.46463275]), 'score_time': array([0.00848079, 0.00838637, 0.00901413, 0.00877643, 0.00855494]), 'test_pearson_r': array([0.89359895, 0.79897532, 0.75325785, 0.94446953, 0.80504133]), 'test_spearman_r': array([0.86105241, 0.81067236, 0.81107388, 0.94512156, 0.8048828 ]), 'test_rmse': array([-0.30294058, -0.39918044, -0.36433542, -0.21257324, -0.39551732]), 'test_mae': array([-0.18684267, -0.23566749, -0.20719166, -0.13355333, -0.17815914]), 'test_r2': array([0.79325709, 0.6301877 , 0.46520813, 0.89032884, 0.62472391]), 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__activation': 'relu', 'regressor__alpha': 1e-08, 'regressor__batch_size': 128, 'regressor__beta_1': 0.9, 'regressor__beta_2': 0.999, 'regressor__early_stopping': False, 'regressor__epsilon': 1e-08, 'regressor__learning_rate': 'constant', 'regressor__learning_rate_init': 0.001343980165450088, 'regressor__max_fun': 15000, 'regressor__max_iter': 500, 'regressor__momentum': 0.9, 'regressor__n_iter_no_change': 10, 'regressor__nesterovs_momentum': True, 'regressor__power_t': 0.5, 'regressor__random_state': None, 'regressor__shuffle': True, 'regressor__solver': 'adam', 'regressor__tol': 0.0001, 'regressor__validation_fraction': 0.1, 'regressor__verbose': False, 'regressor__warm_start': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 69: {'fit_time': array([0.6680975 , 0.47067237, 0.75372863, 0.40533757, 0.60215735]), 'score_time': array([0.00841856, 0.00872111, 0.00799966, 0.0090723 , 0.00873733]), 'test_pearson_r': array([0.91664222, 0.89946187, 0.9215614 , 0.60531407, 0.8643954 ]), 'test_spearman_r': array([0.92227393, 0.88431329, 0.9204594 , 0.72482763, 0.8566451 ]), 'test_rmse': array([-0.25368657, -0.26025278, -0.25605628, -0.59163084, -0.35256414]), 'test_mae': array([-0.14332241, -0.1764705 , -0.13324223, -0.27587391, -0.18757356]), 'test_r2': array([0.83317257, 0.80733738, 0.83294731, 0.01194837, 0.73575513]), 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__activation': 'relu', 'regressor__alpha': 1e-08, 'regressor__batch_size': 64, 'regressor__beta_1': 0.9, 'regressor__beta_2': 0.999, 'regressor__early_stopping': False, 'regressor__epsilon': 1e-08, 'regressor__learning_rate': 'constant', 'regressor__learning_rate_init': 0.0015848722616231376, 'regressor__max_fun': 15000, 'regressor__max_iter': 500, 'regressor__momentum': 0.9, 'regressor__n_iter_no_change': 10, 'regressor__nesterovs_momentum': True, 'regressor__power_t': 0.5, 'regressor__random_state': None, 'regressor__shuffle': True, 'regressor__solver': 'adam', 'regressor__tol': 0.0001, 'regressor__validation_fraction': 0.1, 'regressor__verbose': False, 'regressor__warm_start': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 420: {'fit_time': array([0.36379194, 0.3660109 , 0.35953879, 0.37481856, 0.32525253]), 'score_time': array([0.0084393 , 0.00844288, 0.00847054, 0.00909257, 0.0088563 ]), 'test_pearson_r': array([0.92562796, 0.81025599, 0.80536849, 0.88631162, 0.74574004]), 'test_spearman_r': array([0.89519101, 0.82353946, 0.81057195, 0.87293219, 0.77874064]), 'test_rmse': array([-0.22870262, -0.38350491, -0.36511133, -0.30723249, -0.44055756]), 'test_mae': array([-0.14842762, -0.23145552, -0.2162667 , -0.20520938, -0.22098311]), 'test_r2': array([0.85628667, 0.62312448, 0.58709432, 0.78194444, 0.54573964]), 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__activation': 'relu', 'regressor__alpha': 1e-08, 'regressor__batch_size': 256, 'regressor__beta_1': 0.9, 'regressor__beta_2': 0.999, 'regressor__early_stopping': False, 'regressor__epsilon': 1e-08, 'regressor__learning_rate': 'constant', 'regressor__learning_rate_init': 0.0010809453191141737, 'regressor__max_fun': 15000, 'regressor__max_iter': 500, 'regressor__momentum': 0.9, 'regressor__n_iter_no_change': 10, 'regressor__nesterovs_momentum': True, 'regressor__power_t': 0.5, 'regressor__random_state': None, 'regressor__shuffle': True, 'regressor__solver': 'adam', 'regressor__tol': 0.0001, 'regressor__validation_fraction': 0.1, 'regressor__verbose': False, 'regressor__warm_start': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 1234567890: {'fit_time': array([0.66257429, 0.57818556, 0.53247809, 0.52740216, 0.4210434 ]), 'score_time': array([0.00953579, 0.00806046, 0.00839925, 0.00886655, 0.00865912]), 'test_pearson_r': array([0.8692139 , 0.94820802, 0.8914618 , 0.83808345, 0.69885704]), 'test_spearman_r': array([0.86890075, 0.92030148, 0.88170253, 0.85096392, 0.72378286]), 'test_rmse': array([-0.30484786, -0.18289409, -0.30606553, -0.36379711, -0.51843693]), 'test_mae': array([-0.18689711, -0.12094574, -0.21135338, -0.21250573, -0.2478578 ]), 'test_r2': array([0.71823171, 0.89886397, 0.76904615, 0.67467944, 0.43564207]), 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__activation': 'relu', 'regressor__alpha': 1e-08, 'regressor__batch_size': 32, 'regressor__beta_1': 0.9, 'regressor__beta_2': 0.999, 'regressor__early_stopping': False, 'regressor__epsilon': 1e-08, 'regressor__learning_rate': 'constant', 'regressor__learning_rate_init': 0.0019445732766568073, 'regressor__max_fun': 15000, 'regressor__max_iter': 500, 'regressor__momentum': 0.9, 'regressor__n_iter_no_change': 10, 'regressor__nesterovs_momentum': True, 'regressor__power_t': 0.5, 'regressor__random_state': None, 'regressor__shuffle': True, 'regressor__solver': 'adam', 'regressor__tol': 0.0001, 'regressor__validation_fraction': 0.1, 'regressor__verbose': False, 'regressor__warm_start': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 473129: {'fit_time': array([0.77486396, 0.77815986, 0.81532717, 0.87650824, 0.96583748]), 'score_time': array([0.00856018, 0.00845695, 0.00796962, 0.0090487 , 0.00871062]), 'test_pearson_r': array([0.76595974, 0.73557159, 0.69648909, 0.94733495, 0.83287648]), 'test_spearman_r': array([0.83738717, 0.75093939, 0.81515637, 0.94483116, 0.77909502]), 'test_rmse': array([-0.4343087 , -0.43281266, -0.52634284, -0.21091986, -0.34783972]), 'test_mae': array([-0.18901591, -0.22674289, -0.2515698 , -0.13110013, -0.21100362]), 'test_r2': array([0.57401399, 0.41938525, 0.1982105 , 0.89286458, 0.67944735]), 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__activation': 'relu', 'regressor__alpha': 1e-08, 'regressor__batch_size': 32, 'regressor__beta_1': 0.9, 'regressor__beta_2': 0.999, 'regressor__early_stopping': False, 'regressor__epsilon': 1e-08, 'regressor__learning_rate': 'constant', 'regressor__learning_rate_init': 0.0011047816215412642, 'regressor__max_fun': 15000, 'regressor__max_iter': 500, 'regressor__momentum': 0.9, 'regressor__n_iter_no_change': 10, 'regressor__nesterovs_momentum': True, 'regressor__power_t': 0.5, 'regressor__random_state': None, 'regressor__shuffle': True, 'regressor__solver': 'adam', 'regressor__tol': 0.0001, 'regressor__validation_fraction': 0.1, 'regressor__verbose': False, 'regressor__warm_start': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 'pearson_r_avg': 0.8286912161652015, 'pearson_r_stdev': 0.08963021134515103, 'spearman_r_avg': 0.8394240293579794, 'spearman_r_stdev': 0.06547099119832027, 'rmse_avg': 0.35113041542770423, 'rmse_stdev': 0.09776198689635224, 'mae_avg': 0.1961469463276309, 'mae_stdev': 0.0398749771898694, 'r2_avg': 0.6506791052290771, 'r2_stdev': 0.20385354941541026}
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n11>
Subject: Job 392668: <numerical_MLP_with_feats_on_log Rg (nm)_20250520> in cluster <Hazel> Done

Job <numerical_MLP_with_feats_on_log Rg (nm)_20250520> was submitted from host <c207n11> by user <sdehgha2> in cluster <Hazel> at Wed May 21 17:17:31 2025
Job was executed on host(s) <6*c207n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed May 21 17:17:32 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed May 21 17:17:32 2025
Terminated at Wed May 21 17:58:30 2025
Results reported at Wed May 21 17:58:30 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 20:01
#BSUB -x
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_MLP_with_feats_on_log Rg (nm)_20250520"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "MLP"                                   --numerical_feats 'Xn' 'Mw (g/mol)' 'PDI' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' "polymer dP" "polymer dD" "polymer dH" 'solvent dP' 'solvent dD' 'solvent dH' 

conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4173.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.79 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               6.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   2459 sec.
    Turnaround time :                            2459 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250520/numerical_MLP_log Rg (nm)_20250520.err> for stderr output of this job.

