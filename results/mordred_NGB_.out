yes



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13
yes



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2153 val_loss=0.0000 scale=2.0000 norm=1.7445
[iter 200] loss=1.2057 val_loss=0.0000 scale=2.0000 norm=1.7795
[iter 300] loss=1.2054 val_loss=0.0000 scale=2.0000 norm=1.7869
[iter 400] loss=1.2054 val_loss=0.0000 scale=2.0000 norm=1.7879
== Quitting at iteration / GRAD 448
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1575 val_loss=0.0000 scale=2.0000 norm=1.6633
[iter 200] loss=1.1223 val_loss=0.0000 scale=2.0000 norm=1.6597
[iter 300] loss=1.1183 val_loss=0.0000 scale=2.0000 norm=1.6522
[iter 400] loss=1.1182 val_loss=0.0000 scale=2.0000 norm=1.6521
== Quitting at iteration / GRAD 492
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2405 val_loss=0.0000 scale=2.0000 norm=1.7595
[iter 200] loss=1.2327 val_loss=0.0000 scale=2.0000 norm=1.7860
[iter 300] loss=1.2325 val_loss=0.0000 scale=2.0000 norm=1.7917
[iter 400] loss=1.2325 val_loss=0.0000 scale=2.0000 norm=1.7925
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2075 val_loss=0.0000 scale=2.0000 norm=1.7075
[iter 200] loss=1.1973 val_loss=0.0000 scale=2.0000 norm=1.7232
[iter 300] loss=1.1970 val_loss=0.0000 scale=2.0000 norm=1.7280
[iter 400] loss=1.1970 val_loss=0.0000 scale=2.0000 norm=1.7287
== Quitting at iteration / GRAD 447



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1824 val_loss=0.0000 scale=2.0000 norm=1.6703
[iter 200] loss=1.1648 val_loss=0.0000 scale=2.0000 norm=1.6864
[iter 300] loss=1.1643 val_loss=0.0000 scale=2.0000 norm=1.6927
[iter 400] loss=1.1643 val_loss=0.0000 scale=2.0000 norm=1.6936
== Quitting at iteration / GRAD 461
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2468 val_loss=0.0000 scale=2.0000 norm=1.7708
[iter 200] loss=1.2389 val_loss=0.0000 scale=2.0000 norm=1.7927
[iter 300] loss=1.2387 val_loss=0.0000 scale=2.0000 norm=1.7977
[iter 400] loss=1.2387 val_loss=0.0000 scale=2.0000 norm=1.7984
== Quitting at iteration / GRAD 441
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1716 val_loss=0.0000 scale=2.0000 norm=1.7145
[iter 200] loss=1.1557 val_loss=0.0000 scale=2.0000 norm=1.7643
[iter 300] loss=1.1553 val_loss=0.0000 scale=2.0000 norm=1.7762
[iter 400] loss=1.1553 val_loss=0.0000 scale=2.0000 norm=1.7780
== Quitting at iteration / GRAD 460
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2347 val_loss=0.0000 scale=2.0000 norm=1.7604
[iter 200] loss=1.2262 val_loss=0.0000 scale=2.0000 norm=1.7922
[iter 300] loss=1.2260 val_loss=0.0000 scale=2.0000 norm=1.7990
[iter 400] loss=1.2260 val_loss=0.0000 scale=2.0000 norm=1.8000
== Quitting at iteration / GRAD 446
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1577 val_loss=0.0000 scale=2.0000 norm=1.7135
[iter 200] loss=1.1199 val_loss=0.0000 scale=2.0000 norm=1.7479
[iter 300] loss=1.1114 val_loss=0.0000 scale=2.0000 norm=1.7632
[iter 400] loss=1.1108 val_loss=0.0000 scale=2.0000 norm=1.7679


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2446 val_loss=0.0000 scale=2.0000 norm=1.7793
[iter 200] loss=1.2372 val_loss=0.0000 scale=2.0000 norm=1.8081
[iter 300] loss=1.2370 val_loss=0.0000 scale=2.0000 norm=1.8140
[iter 400] loss=1.2370 val_loss=0.0000 scale=2.0000 norm=1.8149
== Quitting at iteration / GRAD 441
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2352 val_loss=0.0000 scale=2.0000 norm=1.7707
[iter 200] loss=1.2272 val_loss=0.0000 scale=2.0000 norm=1.8038
[iter 300] loss=1.2270 val_loss=0.0000 scale=2.0000 norm=1.8107
[iter 400] loss=1.2270 val_loss=0.0000 scale=2.0000 norm=1.8116
== Quitting at iteration / GRAD 443
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2194 val_loss=0.0000 scale=2.0000 norm=1.7572
[iter 200] loss=1.2105 val_loss=0.0000 scale=2.0000 norm=1.7945
[iter 300] loss=1.2103 val_loss=0.0000 scale=2.0000 norm=1.8021
[iter 400] loss=1.2103 val_loss=0.0000 scale=2.0000 norm=1.8032
== Quitting at iteration / GRAD 446


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


Average scores:	 r2: -0.02Â±0.03
structural
Filename: (Mordred)_NGB_Standard_generalizability
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/informatics/MSE723_final/results/target_hole mobility/structural/(Mordred)_NGB_Standard_generalizability_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/informatics/MSE723_final/results/target_hole mobility/structural/(Mordred)_NGB_Standard_generalizability_predictions.csv
Done Saving scores!
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2073 val_loss=0.0000 scale=2.0000 norm=1.7348
[iter 200] loss=1.1969 val_loss=0.0000 scale=2.0000 norm=1.7709
[iter 300] loss=1.1967 val_loss=0.0000 scale=2.0000 norm=1.7788
[iter 400] loss=1.1967 val_loss=0.0000 scale=2.0000 norm=1.7799
== Quitting at iteration / GRAD 449
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2538 val_loss=0.0000 scale=2.0000 norm=1.7873
[iter 200] loss=1.2472 val_loss=0.0000 scale=2.0000 norm=1.8148
[iter 300] loss=1.2470 val_loss=0.0000 scale=2.0000 norm=1.8205
[iter 400] loss=1.2470 val_loss=0.0000 scale=2.0000 norm=1.8213
== Quitting at iteration / GRAD 439
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1735 val_loss=0.0000 scale=2.0000 norm=1.6559
[iter 200] loss=1.1612 val_loss=0.0000 scale=2.0000 norm=1.6918
[iter 300] loss=1.1609 val_loss=0.0000 scale=2.0000 norm=1.7006
[iter 400] loss=1.1609 val_loss=0.0000 scale=2.0000 norm=1.7018
== Quitting at iteration / GRAD 455
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1912 val_loss=0.0000 scale=2.0000 norm=1.7302
[iter 200] loss=1.1785 val_loss=0.0000 scale=2.0000 norm=1.7713
[iter 300] loss=1.1782 val_loss=0.0000 scale=2.0000 norm=1.7805
[iter 400] loss=1.1782 val_loss=0.0000 scale=2.0000 norm=1.7818
== Quitting at iteration / GRAD 454
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1725 val_loss=0.0000 scale=2.0000 norm=1.6558
[iter 200] loss=1.1618 val_loss=0.0000 scale=2.0000 norm=1.6853
[iter 300] loss=1.1616 val_loss=0.0000 scale=2.0000 norm=1.6923
[iter 400] loss=1.1616 val_loss=0.0000 scale=2.0000 norm=1.6933
== Quitting at iteration / GRAD 453
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2059 val_loss=0.0000 scale=2.0000 norm=1.7454
[iter 200] loss=1.1943 val_loss=0.0000 scale=2.0000 norm=1.7879
[iter 300] loss=1.1940 val_loss=0.0000 scale=2.0000 norm=1.7976
[iter 400] loss=1.1940 val_loss=0.0000 scale=2.0000 norm=1.7990
== Quitting at iteration / GRAD 452
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1509 val_loss=0.0000 scale=2.0000 norm=1.6682
[iter 200] loss=1.1347 val_loss=0.0000 scale=2.0000 norm=1.7135
[iter 300] loss=1.1343 val_loss=0.0000 scale=2.0000 norm=1.7239
[iter 400] loss=1.1343 val_loss=0.0000 scale=2.0000 norm=1.7254
== Quitting at iteration / GRAD 462
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1320 val_loss=0.0000 scale=2.0000 norm=1.6612
[iter 200] loss=1.0635 val_loss=0.0000 scale=4.0000 norm=3.4374
[iter 300] loss=0.9537 val_loss=0.0000 scale=8.0000 norm=6.8958
[iter 400] loss=0.7368 val_loss=0.0000 scale=16.0000 norm=13.7917
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1595 val_loss=0.0000 scale=2.0000 norm=1.7416
[iter 200] loss=1.1445 val_loss=0.0000 scale=2.0000 norm=1.8209
[iter 300] loss=1.1441 val_loss=0.0000 scale=2.0000 norm=1.8376
[iter 400] loss=1.1441 val_loss=0.0000 scale=2.0000 norm=1.8399
== Quitting at iteration / GRAD 461
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1131 val_loss=0.0000 scale=2.0000 norm=1.5819
[iter 200] loss=1.0845 val_loss=0.0000 scale=2.0000 norm=1.5853
[iter 300] loss=1.0835 val_loss=0.0000 scale=2.0000 norm=1.5899
[iter 400] loss=1.0835 val_loss=0.0000 scale=2.0000 norm=1.5907
== Quitting at iteration / GRAD 474
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2529 val_loss=0.0000 scale=2.0000 norm=1.7860
[iter 200] loss=1.2462 val_loss=0.0000 scale=2.0000 norm=1.8151
[iter 300] loss=1.2460 val_loss=0.0000 scale=2.0000 norm=1.8209
[iter 400] loss=1.2460 val_loss=0.0000 scale=2.0000 norm=1.8218
== Quitting at iteration / GRAD 440
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1734 val_loss=0.0000 scale=2.0000 norm=1.6841
[iter 200] loss=1.1324 val_loss=0.0000 scale=2.0000 norm=1.7255
[iter 300] loss=1.0776 val_loss=0.0000 scale=4.0000 norm=3.4719
[iter 400] loss=0.9806 val_loss=0.0000 scale=8.0000 norm=6.9449
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2240 val_loss=0.0000 scale=2.0000 norm=1.7614
[iter 200] loss=1.2142 val_loss=0.0000 scale=2.0000 norm=1.7954
[iter 300] loss=1.2139 val_loss=0.0000 scale=2.0000 norm=1.8028
[iter 400] loss=1.2139 val_loss=0.0000 scale=2.0000 norm=1.8039
== Quitting at iteration / GRAD 448
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1368 val_loss=0.0000 scale=2.0000 norm=1.6677
[iter 200] loss=1.1138 val_loss=0.0000 scale=2.0000 norm=1.6902
[iter 300] loss=1.1131 val_loss=0.0000 scale=2.0000 norm=1.6973
[iter 400] loss=1.1131 val_loss=0.0000 scale=2.0000 norm=1.6985
== Quitting at iteration / GRAD 470
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1875 val_loss=0.0000 scale=2.0000 norm=1.7182
[iter 200] loss=1.1737 val_loss=0.0000 scale=2.0000 norm=1.7585
[iter 300] loss=1.1734 val_loss=0.0000 scale=2.0000 norm=1.7678
[iter 400] loss=1.1733 val_loss=0.0000 scale=2.0000 norm=1.7692
== Quitting at iteration / GRAD 456
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2153 val_loss=0.0000 scale=2.0000 norm=1.7445
[iter 200] loss=1.2057 val_loss=0.0000 scale=2.0000 norm=1.7795
[iter 300] loss=1.2054 val_loss=0.0000 scale=2.0000 norm=1.7869
[iter 400] loss=1.2054 val_loss=0.0000 scale=2.0000 norm=1.7879
== Quitting at iteration / GRAD 448
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1485 val_loss=0.0000 scale=2.0000 norm=1.6367
[iter 200] loss=1.1322 val_loss=0.0000 scale=2.0000 norm=1.6749
[iter 300] loss=1.1318 val_loss=0.0000 scale=2.0000 norm=1.6846
[iter 400] loss=1.1318 val_loss=0.0000 scale=2.0000 norm=1.6860
== Quitting at iteration / GRAD 463
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2457 val_loss=0.0000 scale=2.0000 norm=1.7810
[iter 200] loss=1.2390 val_loss=0.0000 scale=2.0000 norm=1.8154
[iter 300] loss=1.2388 val_loss=0.0000 scale=2.0000 norm=1.8223
[iter 400] loss=1.2388 val_loss=0.0000 scale=2.0000 norm=1.8232
== Quitting at iteration / GRAD 440
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2232 val_loss=0.0000 scale=2.0000 norm=1.7813
[iter 200] loss=1.1811 val_loss=0.0000 scale=4.0000 norm=3.6281
[iter 300] loss=1.1199 val_loss=0.0000 scale=8.0000 norm=7.2734
[iter 400] loss=0.9986 val_loss=0.0000 scale=8.0000 norm=7.2737
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2259 val_loss=0.0000 scale=2.0000 norm=1.7573
[iter 200] loss=1.2177 val_loss=0.0000 scale=2.0000 norm=1.7913
[iter 300] loss=1.2175 val_loss=0.0000 scale=2.0000 norm=1.7983
[iter 400] loss=1.2175 val_loss=0.0000 scale=2.0000 norm=1.7993
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1716 val_loss=0.0000 scale=2.0000 norm=1.7145
[iter 200] loss=1.1557 val_loss=0.0000 scale=2.0000 norm=1.7643
[iter 300] loss=1.1553 val_loss=0.0000 scale=2.0000 norm=1.7762
[iter 400] loss=1.1553 val_loss=0.0000 scale=2.0000 norm=1.7780
== Quitting at iteration / GRAD 460
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1921 val_loss=0.0000 scale=2.0000 norm=1.7376
[iter 200] loss=1.1765 val_loss=0.0000 scale=2.0000 norm=1.7770
[iter 300] loss=1.1759 val_loss=0.0000 scale=2.0000 norm=1.7868
[iter 400] loss=1.1759 val_loss=0.0000 scale=2.0000 norm=1.7883
== Quitting at iteration / GRAD 459
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2362 val_loss=0.0000 scale=2.0000 norm=1.7640
[iter 200] loss=1.2287 val_loss=0.0000 scale=2.0000 norm=1.7928
[iter 300] loss=1.2285 val_loss=0.0000 scale=2.0000 norm=1.7989
[iter 400] loss=1.2285 val_loss=0.0000 scale=2.0000 norm=1.7997
== Quitting at iteration / GRAD 442
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2502 val_loss=0.0000 scale=2.0000 norm=1.7808
[iter 200] loss=1.2428 val_loss=0.0000 scale=2.0000 norm=1.8098
[iter 300] loss=1.2427 val_loss=0.0000 scale=2.0000 norm=1.8159
[iter 400] loss=1.2427 val_loss=0.0000 scale=2.0000 norm=1.8168
== Quitting at iteration / GRAD 441
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2481 val_loss=0.0000 scale=2.0000 norm=1.7737
[iter 200] loss=1.2257 val_loss=0.0000 scale=2.0000 norm=1.7966
[iter 300] loss=1.2213 val_loss=0.0000 scale=2.0000 norm=1.8006
[iter 400] loss=1.2210 val_loss=0.0000 scale=2.0000 norm=1.8020
== Quitting at iteration / GRAD 488
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2446 val_loss=0.0000 scale=2.0000 norm=1.7793
[iter 200] loss=1.2372 val_loss=0.0000 scale=2.0000 norm=1.8081
[iter 300] loss=1.2370 val_loss=0.0000 scale=2.0000 norm=1.8140
[iter 400] loss=1.2370 val_loss=0.0000 scale=2.0000 norm=1.8149
== Quitting at iteration / GRAD 441
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.3211 val_loss=0.0000 scale=2.0000 norm=1.8447
[iter 200] loss=1.3179 val_loss=0.0000 scale=2.0000 norm=1.8486
[iter 300] loss=1.3178 val_loss=0.0000 scale=2.0000 norm=1.8499
[iter 400] loss=1.3178 val_loss=0.0000 scale=2.0000 norm=1.8501
== Quitting at iteration / GRAD 421
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1919 val_loss=0.0000 scale=2.0000 norm=1.7077
[iter 200] loss=1.1810 val_loss=0.0000 scale=2.0000 norm=1.7449
[iter 300] loss=1.1808 val_loss=0.0000 scale=2.0000 norm=1.7529
[iter 400] loss=1.1808 val_loss=0.0000 scale=2.0000 norm=1.7541
== Quitting at iteration / GRAD 451
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1817 val_loss=0.0000 scale=2.0000 norm=1.7203
[iter 200] loss=1.1693 val_loss=0.0000 scale=2.0000 norm=1.7687
[iter 300] loss=1.1690 val_loss=0.0000 scale=2.0000 norm=1.7790
[iter 400] loss=1.1690 val_loss=0.0000 scale=2.0000 norm=1.7804
== Quitting at iteration / GRAD 455
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1923 val_loss=0.0000 scale=2.0000 norm=1.7927
[iter 200] loss=1.1356 val_loss=0.0000 scale=4.0000 norm=3.6215
[iter 300] loss=1.0440 val_loss=0.0000 scale=8.0000 norm=7.2481
[iter 400] loss=0.8634 val_loss=0.0000 scale=16.0000 norm=14.4961
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2405 val_loss=0.0000 scale=2.0000 norm=1.7595
[iter 200] loss=1.2327 val_loss=0.0000 scale=2.0000 norm=1.7860
[iter 300] loss=1.2325 val_loss=0.0000 scale=2.0000 norm=1.7917
[iter 400] loss=1.2325 val_loss=0.0000 scale=2.0000 norm=1.7925
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2062 val_loss=0.0000 scale=2.0000 norm=1.7465
[iter 200] loss=1.1957 val_loss=0.0000 scale=2.0000 norm=1.7894
[iter 300] loss=1.1955 val_loss=0.0000 scale=2.0000 norm=1.7984
[iter 400] loss=1.1955 val_loss=0.0000 scale=2.0000 norm=1.7997
== Quitting at iteration / GRAD 450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8833 val_loss=0.0000 scale=2.0000 norm=1.3942
[iter 200] loss=0.7734 val_loss=0.0000 scale=2.0000 norm=1.4410
[iter 300] loss=0.7667 val_loss=0.0000 scale=2.0000 norm=1.4841
[iter 400] loss=0.7665 val_loss=0.0000 scale=2.0000 norm=1.4922
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1506 val_loss=0.0000 scale=2.0000 norm=1.6992
[iter 200] loss=1.1313 val_loss=0.0000 scale=2.0000 norm=1.7522
[iter 300] loss=1.1306 val_loss=0.0000 scale=2.0000 norm=1.7644
[iter 400] loss=1.1306 val_loss=0.0000 scale=2.0000 norm=1.7662
== Quitting at iteration / GRAD 466
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2061 val_loss=0.0000 scale=2.0000 norm=1.7152
[iter 200] loss=1.1961 val_loss=0.0000 scale=2.0000 norm=1.7399
[iter 300] loss=1.1958 val_loss=0.0000 scale=2.0000 norm=1.7459
[iter 400] loss=1.1958 val_loss=0.0000 scale=2.0000 norm=1.7467
== Quitting at iteration / GRAD 450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2316 val_loss=0.0000 scale=2.0000 norm=1.7565
[iter 200] loss=1.2233 val_loss=0.0000 scale=2.0000 norm=1.7833
[iter 300] loss=1.2230 val_loss=0.0000 scale=2.0000 norm=1.7892
[iter 400] loss=1.2230 val_loss=0.0000 scale=2.0000 norm=1.7900
== Quitting at iteration / GRAD 444
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0203 val_loss=0.0000 scale=2.0000 norm=1.5381
[iter 200] loss=0.9462 val_loss=0.0000 scale=2.0000 norm=1.6276
[iter 300] loss=0.8940 val_loss=0.0000 scale=4.0000 norm=3.3174
[iter 400] loss=0.8043 val_loss=0.0000 scale=8.0000 norm=6.6398
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2167 val_loss=0.0000 scale=2.0000 norm=1.7441
[iter 200] loss=1.2065 val_loss=0.0000 scale=2.0000 norm=1.7766
[iter 300] loss=1.2063 val_loss=0.0000 scale=2.0000 norm=1.7841
[iter 400] loss=1.2063 val_loss=0.0000 scale=2.0000 norm=1.7851
== Quitting at iteration / GRAD 449
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2590 val_loss=0.0000 scale=2.0000 norm=1.8082
[iter 200] loss=1.2030 val_loss=0.0000 scale=4.0000 norm=3.6415
[iter 300] loss=1.1085 val_loss=0.0000 scale=8.0000 norm=7.2857
[iter 400] loss=0.9218 val_loss=0.0000 scale=16.0000 norm=14.5714
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1983 val_loss=0.0000 scale=2.0000 norm=1.7110
[iter 200] loss=1.1876 val_loss=0.0000 scale=2.0000 norm=1.7424
[iter 300] loss=1.1873 val_loss=0.0000 scale=2.0000 norm=1.7499
[iter 400] loss=1.1873 val_loss=0.0000 scale=2.0000 norm=1.7510
== Quitting at iteration / GRAD 451
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2468 val_loss=0.0000 scale=2.0000 norm=1.7708
[iter 200] loss=1.2389 val_loss=0.0000 scale=2.0000 norm=1.7927
[iter 300] loss=1.2387 val_loss=0.0000 scale=2.0000 norm=1.7977
[iter 400] loss=1.2387 val_loss=0.0000 scale=2.0000 norm=1.7984
== Quitting at iteration / GRAD 441
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1955 val_loss=0.0000 scale=2.0000 norm=1.7328
[iter 200] loss=1.1830 val_loss=0.0000 scale=2.0000 norm=1.7747
[iter 300] loss=1.1826 val_loss=0.0000 scale=2.0000 norm=1.7840
[iter 400] loss=1.1826 val_loss=0.0000 scale=2.0000 norm=1.7853
== Quitting at iteration / GRAD 453
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2152 val_loss=0.0000 scale=2.0000 norm=1.7473
[iter 200] loss=1.2066 val_loss=0.0000 scale=2.0000 norm=1.7894
[iter 300] loss=1.2064 val_loss=0.0000 scale=2.0000 norm=1.7980
[iter 400] loss=1.2064 val_loss=0.0000 scale=2.0000 norm=1.7992
== Quitting at iteration / GRAD 447
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2038 val_loss=0.0000 scale=2.0000 norm=1.7443
[iter 200] loss=1.1935 val_loss=0.0000 scale=2.0000 norm=1.7840
[iter 300] loss=1.1932 val_loss=0.0000 scale=2.0000 norm=1.7925
[iter 400] loss=1.1932 val_loss=0.0000 scale=2.0000 norm=1.7937
== Quitting at iteration / GRAD 450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1671 val_loss=0.0000 scale=2.0000 norm=1.7005
[iter 200] loss=1.1494 val_loss=0.0000 scale=2.0000 norm=1.7485
[iter 300] loss=1.1487 val_loss=0.0000 scale=2.0000 norm=1.7608
[iter 400] loss=1.1487 val_loss=0.0000 scale=2.0000 norm=1.7626
== Quitting at iteration / GRAD 464
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2442 val_loss=0.0000 scale=2.0000 norm=1.7784
[iter 200] loss=1.2367 val_loss=0.0000 scale=2.0000 norm=1.8095
[iter 300] loss=1.2365 val_loss=0.0000 scale=2.0000 norm=1.8160
[iter 400] loss=1.2365 val_loss=0.0000 scale=2.0000 norm=1.8169
== Quitting at iteration / GRAD 442
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1082 val_loss=0.0000 scale=2.0000 norm=1.6225
[iter 200] loss=1.0874 val_loss=0.0000 scale=2.0000 norm=1.6593
[iter 300] loss=1.0868 val_loss=0.0000 scale=2.0000 norm=1.6692
[iter 400] loss=1.0868 val_loss=0.0000 scale=2.0000 norm=1.6707
== Quitting at iteration / GRAD 469
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2133 val_loss=0.0000 scale=2.0000 norm=1.7417
[iter 200] loss=1.2040 val_loss=0.0000 scale=2.0000 norm=1.7790
[iter 300] loss=1.2038 val_loss=0.0000 scale=2.0000 norm=1.7867
[iter 400] loss=1.2038 val_loss=0.0000 scale=2.0000 norm=1.7878
== Quitting at iteration / GRAD 447
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2307 val_loss=0.0000 scale=2.0000 norm=1.7634
[iter 200] loss=1.2227 val_loss=0.0000 scale=2.0000 norm=1.7972
[iter 300] loss=1.2226 val_loss=0.0000 scale=2.0000 norm=1.8041
[iter 400] loss=1.2226 val_loss=0.0000 scale=2.0000 norm=1.8051
== Quitting at iteration / GRAD 444
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1664 val_loss=0.0000 scale=2.0000 norm=1.7016
[iter 200] loss=1.1496 val_loss=0.0000 scale=2.0000 norm=1.7450
[iter 300] loss=1.1492 val_loss=0.0000 scale=2.0000 norm=1.7555
[iter 400] loss=1.1492 val_loss=0.0000 scale=2.0000 norm=1.7570
== Quitting at iteration / GRAD 461
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2347 val_loss=0.0000 scale=2.0000 norm=1.7604
[iter 200] loss=1.2262 val_loss=0.0000 scale=2.0000 norm=1.7922
[iter 300] loss=1.2260 val_loss=0.0000 scale=2.0000 norm=1.7990
[iter 400] loss=1.2260 val_loss=0.0000 scale=2.0000 norm=1.8000
== Quitting at iteration / GRAD 446
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1968 val_loss=0.0000 scale=2.0000 norm=1.7337
[iter 200] loss=1.1857 val_loss=0.0000 scale=2.0000 norm=1.7757
[iter 300] loss=1.1854 val_loss=0.0000 scale=2.0000 norm=1.7848
[iter 400] loss=1.1854 val_loss=0.0000 scale=2.0000 norm=1.7861
== Quitting at iteration / GRAD 452
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2092 val_loss=0.0000 scale=2.0000 norm=1.7469
[iter 200] loss=1.1983 val_loss=0.0000 scale=2.0000 norm=1.7868
[iter 300] loss=1.1980 val_loss=0.0000 scale=2.0000 norm=1.7954
[iter 400] loss=1.1980 val_loss=0.0000 scale=2.0000 norm=1.7967
== Quitting at iteration / GRAD 450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2702 val_loss=0.0000 scale=2.0000 norm=1.8104
[iter 200] loss=1.2642 val_loss=0.0000 scale=2.0000 norm=1.8353
[iter 300] loss=1.2641 val_loss=0.0000 scale=2.0000 norm=1.8405
[iter 400] loss=1.2641 val_loss=0.0000 scale=2.0000 norm=1.8412
== Quitting at iteration / GRAD 435
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2461 val_loss=0.0000 scale=2.0000 norm=1.7565
[iter 200] loss=1.2401 val_loss=0.0000 scale=2.0000 norm=1.7770
[iter 300] loss=1.2400 val_loss=0.0000 scale=2.0000 norm=1.7814
[iter 400] loss=1.2400 val_loss=0.0000 scale=2.0000 norm=1.7821
== Quitting at iteration / GRAD 439
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2702 val_loss=0.0000 scale=2.0000 norm=1.8465
[iter 200] loss=1.2355 val_loss=0.0000 scale=4.0000 norm=3.7645
[iter 300] loss=1.1748 val_loss=0.0000 scale=4.0000 norm=3.7798
[iter 400] loss=1.0663 val_loss=0.0000 scale=8.0000 norm=7.5602
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2343 val_loss=0.0000 scale=2.0000 norm=1.7704
[iter 200] loss=1.2257 val_loss=0.0000 scale=2.0000 norm=1.8034
[iter 300] loss=1.2255 val_loss=0.0000 scale=2.0000 norm=1.8107
[iter 400] loss=1.2255 val_loss=0.0000 scale=2.0000 norm=1.8117
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2188 val_loss=0.0000 scale=2.0000 norm=1.7488
[iter 200] loss=1.2101 val_loss=0.0000 scale=2.0000 norm=1.7809
[iter 300] loss=1.2100 val_loss=0.0000 scale=2.0000 norm=1.7876
[iter 400] loss=1.2099 val_loss=0.0000 scale=2.0000 norm=1.7885
== Quitting at iteration / GRAD 446
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2431 val_loss=0.0000 scale=2.0000 norm=1.7856
[iter 200] loss=1.2344 val_loss=0.0000 scale=2.0000 norm=1.8175
[iter 300] loss=1.2341 val_loss=0.0000 scale=2.0000 norm=1.8243
[iter 400] loss=1.2341 val_loss=0.0000 scale=2.0000 norm=1.8252
== Quitting at iteration / GRAD 444
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1463 val_loss=0.0000 scale=2.0000 norm=1.6864
[iter 200] loss=1.1251 val_loss=0.0000 scale=2.0000 norm=1.7285
[iter 300] loss=1.1245 val_loss=0.0000 scale=2.0000 norm=1.7403
[iter 400] loss=1.1244 val_loss=0.0000 scale=2.0000 norm=1.7420
== Quitting at iteration / GRAD 468
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2352 val_loss=0.0000 scale=2.0000 norm=1.7707
[iter 200] loss=1.2272 val_loss=0.0000 scale=2.0000 norm=1.8038
[iter 300] loss=1.2270 val_loss=0.0000 scale=2.0000 norm=1.8107
[iter 400] loss=1.2270 val_loss=0.0000 scale=2.0000 norm=1.8116
== Quitting at iteration / GRAD 443
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2382 val_loss=0.0000 scale=2.0000 norm=1.7651
[iter 200] loss=1.2308 val_loss=0.0000 scale=2.0000 norm=1.7939
[iter 300] loss=1.2306 val_loss=0.0000 scale=2.0000 norm=1.7999
[iter 400] loss=1.2306 val_loss=0.0000 scale=2.0000 norm=1.8008
== Quitting at iteration / GRAD 442
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1843 val_loss=0.0000 scale=2.0000 norm=1.7086
[iter 200] loss=1.1716 val_loss=0.0000 scale=2.0000 norm=1.7479
[iter 300] loss=1.1713 val_loss=0.0000 scale=2.0000 norm=1.7569
[iter 400] loss=1.1713 val_loss=0.0000 scale=2.0000 norm=1.7582
== Quitting at iteration / GRAD 455
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2310 val_loss=0.0000 scale=2.0000 norm=1.7642
[iter 200] loss=1.2226 val_loss=0.0000 scale=2.0000 norm=1.7985
[iter 300] loss=1.2224 val_loss=0.0000 scale=2.0000 norm=1.8059
[iter 400] loss=1.2224 val_loss=0.0000 scale=2.0000 norm=1.8070
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2176 val_loss=0.0000 scale=2.0000 norm=1.7501
[iter 200] loss=1.2077 val_loss=0.0000 scale=2.0000 norm=1.7848
[iter 300] loss=1.2075 val_loss=0.0000 scale=2.0000 norm=1.7922
[iter 400] loss=1.2075 val_loss=0.0000 scale=2.0000 norm=1.7933
== Quitting at iteration / GRAD 448
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1577 val_loss=0.0000 scale=2.0000 norm=1.7135
[iter 200] loss=1.1199 val_loss=0.0000 scale=2.0000 norm=1.7479
[iter 300] loss=1.1114 val_loss=0.0000 scale=2.0000 norm=1.7632
[iter 400] loss=1.1108 val_loss=0.0000 scale=2.0000 norm=1.7679
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0667 val_loss=0.0000 scale=2.0000 norm=1.5629
[iter 200] loss=1.0062 val_loss=0.0000 scale=2.0000 norm=1.5909
[iter 300] loss=0.9451 val_loss=0.0000 scale=4.0000 norm=3.2086
[iter 400] loss=0.8354 val_loss=0.0000 scale=8.0000 norm=6.4184
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2251 val_loss=0.0000 scale=2.0000 norm=1.7570
[iter 200] loss=1.2167 val_loss=0.0000 scale=2.0000 norm=1.7911
[iter 300] loss=1.2165 val_loss=0.0000 scale=2.0000 norm=1.7982
[iter 400] loss=1.2165 val_loss=0.0000 scale=2.0000 norm=1.7992
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2348 val_loss=0.0000 scale=2.0000 norm=1.7775
[iter 200] loss=1.2270 val_loss=0.0000 scale=2.0000 norm=1.8120
[iter 300] loss=1.2269 val_loss=0.0000 scale=2.0000 norm=1.8190
[iter 400] loss=1.2269 val_loss=0.0000 scale=2.0000 norm=1.8200
== Quitting at iteration / GRAD 443
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9951 val_loss=0.0000 scale=2.0000 norm=1.5448
[iter 200] loss=0.8523 val_loss=0.0000 scale=2.0000 norm=1.5332
[iter 300] loss=0.7901 val_loss=0.0000 scale=4.0000 norm=3.1455
[iter 400] loss=0.6871 val_loss=0.0000 scale=8.0000 norm=6.3000
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1824 val_loss=0.0000 scale=2.0000 norm=1.6703
[iter 200] loss=1.1648 val_loss=0.0000 scale=2.0000 norm=1.6864
[iter 300] loss=1.1643 val_loss=0.0000 scale=2.0000 norm=1.6927
[iter 400] loss=1.1643 val_loss=0.0000 scale=2.0000 norm=1.6936
== Quitting at iteration / GRAD 461
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2194 val_loss=0.0000 scale=2.0000 norm=1.7525
[iter 200] loss=1.2102 val_loss=0.0000 scale=2.0000 norm=1.7875
[iter 300] loss=1.2100 val_loss=0.0000 scale=2.0000 norm=1.7951
[iter 400] loss=1.2100 val_loss=0.0000 scale=2.0000 norm=1.7962
== Quitting at iteration / GRAD 447
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2405 val_loss=0.0000 scale=2.0000 norm=1.7562
[iter 200] loss=1.2322 val_loss=0.0000 scale=2.0000 norm=1.7695
[iter 300] loss=1.2320 val_loss=0.0000 scale=2.0000 norm=1.7732
[iter 400] loss=1.2320 val_loss=0.0000 scale=2.0000 norm=1.7738
== Quitting at iteration / GRAD 444
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2099 val_loss=0.0000 scale=2.0000 norm=1.7378
[iter 200] loss=1.2003 val_loss=0.0000 scale=2.0000 norm=1.7733
[iter 300] loss=1.2001 val_loss=0.0000 scale=2.0000 norm=1.7808
[iter 400] loss=1.2001 val_loss=0.0000 scale=2.0000 norm=1.7819
== Quitting at iteration / GRAD 448
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2519 val_loss=0.0000 scale=2.0000 norm=1.7885
[iter 200] loss=1.2453 val_loss=0.0000 scale=2.0000 norm=1.8179
[iter 300] loss=1.2451 val_loss=0.0000 scale=2.0000 norm=1.8238
[iter 400] loss=1.2451 val_loss=0.0000 scale=2.0000 norm=1.8247
== Quitting at iteration / GRAD 439
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1575 val_loss=0.0000 scale=2.0000 norm=1.6633
[iter 200] loss=1.1223 val_loss=0.0000 scale=2.0000 norm=1.6597
[iter 300] loss=1.1183 val_loss=0.0000 scale=2.0000 norm=1.6522
[iter 400] loss=1.1182 val_loss=0.0000 scale=2.0000 norm=1.6521
== Quitting at iteration / GRAD 492
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2006 val_loss=0.0000 scale=2.0000 norm=1.7310
[iter 200] loss=1.1903 val_loss=0.0000 scale=2.0000 norm=1.7700
[iter 300] loss=1.1901 val_loss=0.0000 scale=2.0000 norm=1.7783
[iter 400] loss=1.1901 val_loss=0.0000 scale=2.0000 norm=1.7794
== Quitting at iteration / GRAD 450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2262 val_loss=0.0000 scale=2.0000 norm=1.7561
[iter 200] loss=1.2173 val_loss=0.0000 scale=2.0000 norm=1.7877
[iter 300] loss=1.2171 val_loss=0.0000 scale=2.0000 norm=1.7946
[iter 400] loss=1.2171 val_loss=0.0000 scale=2.0000 norm=1.7956
== Quitting at iteration / GRAD 446
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2827 val_loss=0.0000 scale=2.0000 norm=1.8286
[iter 200] loss=1.2766 val_loss=0.0000 scale=2.0000 norm=1.8511
[iter 300] loss=1.2765 val_loss=0.0000 scale=2.0000 norm=1.8559
[iter 400] loss=1.2765 val_loss=0.0000 scale=2.0000 norm=1.8566
== Quitting at iteration / GRAD 434
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2515 val_loss=0.0000 scale=2.0000 norm=1.7930
[iter 200] loss=1.2437 val_loss=0.0000 scale=2.0000 norm=1.8243
[iter 300] loss=1.2435 val_loss=0.0000 scale=2.0000 norm=1.8310
[iter 400] loss=1.2435 val_loss=0.0000 scale=2.0000 norm=1.8320
== Quitting at iteration / GRAD 441
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2565 val_loss=0.0000 scale=2.0000 norm=1.7872
[iter 200] loss=1.2496 val_loss=0.0000 scale=2.0000 norm=1.8108
[iter 300] loss=1.2494 val_loss=0.0000 scale=2.0000 norm=1.8155
[iter 400] loss=1.2494 val_loss=0.0000 scale=2.0000 norm=1.8162
== Quitting at iteration / GRAD 439
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2272 val_loss=0.0000 scale=2.0000 norm=1.7608
[iter 200] loss=1.2182 val_loss=0.0000 scale=2.0000 norm=1.7941
[iter 300] loss=1.2179 val_loss=0.0000 scale=2.0000 norm=1.8014
[iter 400] loss=1.2179 val_loss=0.0000 scale=2.0000 norm=1.8024
== Quitting at iteration / GRAD 446
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1691 val_loss=0.0000 scale=2.0000 norm=1.7264
[iter 200] loss=1.1224 val_loss=0.0000 scale=2.0000 norm=1.7193
[iter 300] loss=1.1179 val_loss=0.0000 scale=2.0000 norm=1.7298
[iter 400] loss=1.1178 val_loss=0.0000 scale=2.0000 norm=1.7330
== Quitting at iteration / GRAD 498
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2235 val_loss=0.0000 scale=2.0000 norm=1.7534
[iter 200] loss=1.2151 val_loss=0.0000 scale=2.0000 norm=1.7894
[iter 300] loss=1.2149 val_loss=0.0000 scale=2.0000 norm=1.7969
[iter 400] loss=1.2149 val_loss=0.0000 scale=2.0000 norm=1.7979
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1748 val_loss=0.0000 scale=2.0000 norm=1.7131
[iter 200] loss=1.1598 val_loss=0.0000 scale=2.0000 norm=1.7580
[iter 300] loss=1.1594 val_loss=0.0000 scale=2.0000 norm=1.7685
[iter 400] loss=1.1594 val_loss=0.0000 scale=2.0000 norm=1.7700
== Quitting at iteration / GRAD 459
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2075 val_loss=0.0000 scale=2.0000 norm=1.7075
[iter 200] loss=1.1973 val_loss=0.0000 scale=2.0000 norm=1.7232
[iter 300] loss=1.1970 val_loss=0.0000 scale=2.0000 norm=1.7280
[iter 400] loss=1.1970 val_loss=0.0000 scale=2.0000 norm=1.7287
== Quitting at iteration / GRAD 447
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1766 val_loss=0.0000 scale=2.0000 norm=1.7065
[iter 200] loss=1.1634 val_loss=0.0000 scale=2.0000 norm=1.7491
[iter 300] loss=1.1631 val_loss=0.0000 scale=2.0000 norm=1.7585
[iter 400] loss=1.1631 val_loss=0.0000 scale=2.0000 norm=1.7598
== Quitting at iteration / GRAD 456
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2174 val_loss=0.0000 scale=2.0000 norm=1.7524
[iter 200] loss=1.2080 val_loss=0.0000 scale=2.0000 norm=1.7888
[iter 300] loss=1.2078 val_loss=0.0000 scale=2.0000 norm=1.7964
[iter 400] loss=1.2078 val_loss=0.0000 scale=2.0000 norm=1.7975
== Quitting at iteration / GRAD 447
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2269 val_loss=0.0000 scale=2.0000 norm=1.7669
[iter 200] loss=1.2171 val_loss=0.0000 scale=2.0000 norm=1.8020
[iter 300] loss=1.2169 val_loss=0.0000 scale=2.0000 norm=1.8096
[iter 400] loss=1.2169 val_loss=0.0000 scale=2.0000 norm=1.8107
== Quitting at iteration / GRAD 447
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1552 val_loss=0.0000 scale=2.0000 norm=1.6899
[iter 200] loss=1.1372 val_loss=0.0000 scale=2.0000 norm=1.7378
[iter 300] loss=1.1367 val_loss=0.0000 scale=2.0000 norm=1.7495
[iter 400] loss=1.1367 val_loss=0.0000 scale=2.0000 norm=1.7512
== Quitting at iteration / GRAD 463
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2194 val_loss=0.0000 scale=2.0000 norm=1.7572
[iter 200] loss=1.2105 val_loss=0.0000 scale=2.0000 norm=1.7945
[iter 300] loss=1.2103 val_loss=0.0000 scale=2.0000 norm=1.8021
[iter 400] loss=1.2103 val_loss=0.0000 scale=2.0000 norm=1.8032
== Quitting at iteration / GRAD 446
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2222 val_loss=0.0000 scale=2.0000 norm=1.7382
[iter 200] loss=1.2132 val_loss=0.0000 scale=2.0000 norm=1.7695
[iter 300] loss=1.2130 val_loss=0.0000 scale=2.0000 norm=1.7764
[iter 400] loss=1.2130 val_loss=0.0000 scale=2.0000 norm=1.7774
== Quitting at iteration / GRAD 447
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2239 val_loss=0.0000 scale=2.0000 norm=1.7593
[iter 200] loss=1.2150 val_loss=0.0000 scale=2.0000 norm=1.7933
[iter 300] loss=1.2148 val_loss=0.0000 scale=2.0000 norm=1.8004
[iter 400] loss=1.2148 val_loss=0.0000 scale=2.0000 norm=1.8014
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2345 val_loss=0.0000 scale=2.0000 norm=1.7674
[iter 200] loss=1.2277 val_loss=0.0000 scale=2.0000 norm=1.8085
[iter 300] loss=1.2275 val_loss=0.0000 scale=2.0000 norm=1.8165
[iter 400] loss=1.2275 val_loss=0.0000 scale=2.0000 norm=1.8177
== Quitting at iteration / GRAD 442
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2203 val_loss=0.0000 scale=2.0000 norm=1.7388
[iter 200] loss=1.2119 val_loss=0.0000 scale=2.0000 norm=1.7686
[iter 300] loss=1.2117 val_loss=0.0000 scale=2.0000 norm=1.7751
[iter 400] loss=1.2117 val_loss=0.0000 scale=2.0000 norm=1.7760
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1195 val_loss=0.0000 scale=2.0000 norm=1.6560
[iter 200] loss=1.0331 val_loss=0.0000 scale=2.0000 norm=1.6658
[iter 300] loss=0.9757 val_loss=0.0000 scale=4.0000 norm=3.3675
[iter 400] loss=0.8763 val_loss=0.0000 scale=8.0000 norm=6.7379
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2284 val_loss=0.0000 scale=2.0000 norm=1.7546
[iter 200] loss=1.2207 val_loss=0.0000 scale=2.0000 norm=1.7861
[iter 300] loss=1.2205 val_loss=0.0000 scale=2.0000 norm=1.7925
[iter 400] loss=1.2205 val_loss=0.0000 scale=2.0000 norm=1.7935
== Quitting at iteration / GRAD 443
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1960 val_loss=0.0000 scale=2.0000 norm=1.7264
[iter 200] loss=1.1856 val_loss=0.0000 scale=2.0000 norm=1.7702
[iter 300] loss=1.1853 val_loss=0.0000 scale=2.0000 norm=1.7794
[iter 400] loss=1.1853 val_loss=0.0000 scale=2.0000 norm=1.7807
== Quitting at iteration / GRAD 451
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2201 val_loss=0.0000 scale=2.0000 norm=1.7557
[iter 200] loss=1.2096 val_loss=0.0000 scale=2.0000 norm=1.7901
[iter 300] loss=1.2093 val_loss=0.0000 scale=2.0000 norm=1.7979
[iter 400] loss=1.2093 val_loss=0.0000 scale=2.0000 norm=1.7990
== Quitting at iteration / GRAD 448
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2094 val_loss=0.0000 scale=2.0000 norm=1.7394
[iter 200] loss=1.1997 val_loss=0.0000 scale=2.0000 norm=1.7783
[iter 300] loss=1.1995 val_loss=0.0000 scale=2.0000 norm=1.7865
[iter 400] loss=1.1995 val_loss=0.0000 scale=2.0000 norm=1.7876
== Quitting at iteration / GRAD 448

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n10>
Subject: Job 719980: <mordred_NGB_> in cluster <Hazel> Done

Job <mordred_NGB_> was submitted from host <c201n09> by user <sdehgha2> in cluster <Hazel> at Tue Dec 10 20:14:26 2024
Job was executed on host(s) <2*c200n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Dec 10 20:15:27 2024
                            <2*c200n12>
                            <2*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/informatics/MSE723_final/code_/training/hpc> was used as the working directory.
Started at Tue Dec 10 20:15:27 2024
Terminated at Tue Dec 10 22:41:32 2024
Results reported at Tue Dec 10 22:41:32 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 6
#BSUB -W 10:01
#BSUB -R span[ptile=2]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB 

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   35943.03 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.99 GB
    Total Requested Memory :                     48.00 GB
    Delta Memory :                               46.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   8792 sec.
    Turnaround time :                            8826 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_.err> for stderr output of this job.

[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2059 val_loss=0.0000 scale=2.0000 norm=1.7454
[iter 200] loss=1.1943 val_loss=0.0000 scale=2.0000 norm=1.7879
[iter 300] loss=1.1940 val_loss=0.0000 scale=2.0000 norm=1.7976
[iter 400] loss=1.1940 val_loss=0.0000 scale=2.0000 norm=1.7990
== Quitting at iteration / GRAD 452
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2442 val_loss=0.0000 scale=2.0000 norm=1.7784
[iter 200] loss=1.2367 val_loss=0.0000 scale=2.0000 norm=1.8095
[iter 300] loss=1.2365 val_loss=0.0000 scale=2.0000 norm=1.8160
[iter 400] loss=1.2365 val_loss=0.0000 scale=2.0000 norm=1.8169
== Quitting at iteration / GRAD 442
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2073 val_loss=0.0000 scale=2.0000 norm=1.7348
[iter 200] loss=1.1969 val_loss=0.0000 scale=2.0000 norm=1.7709
[iter 300] loss=1.1967 val_loss=0.0000 scale=2.0000 norm=1.7788
[iter 400] loss=1.1967 val_loss=0.0000 scale=2.0000 norm=1.7799
== Quitting at iteration / GRAD 449
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2529 val_loss=0.0000 scale=2.0000 norm=1.7860
[iter 200] loss=1.2462 val_loss=0.0000 scale=2.0000 norm=1.8151
[iter 300] loss=1.2460 val_loss=0.0000 scale=2.0000 norm=1.8209
[iter 400] loss=1.2460 val_loss=0.0000 scale=2.0000 norm=1.8218
== Quitting at iteration / GRAD 440


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1195 val_loss=0.0000 scale=2.0000 norm=1.6560
[iter 200] loss=1.0331 val_loss=0.0000 scale=2.0000 norm=1.6658
[iter 300] loss=0.9757 val_loss=0.0000 scale=4.0000 norm=3.3675
[iter 400] loss=0.8763 val_loss=0.0000 scale=8.0000 norm=6.7379
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2565 val_loss=0.0000 scale=2.0000 norm=1.7872
[iter 200] loss=1.2496 val_loss=0.0000 scale=2.0000 norm=1.8108
[iter 300] loss=1.2494 val_loss=0.0000 scale=2.0000 norm=1.8155
[iter 400] loss=1.2494 val_loss=0.0000 scale=2.0000 norm=1.8162
== Quitting at iteration / GRAD 439
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2316 val_loss=0.0000 scale=2.0000 norm=1.7565
[iter 200] loss=1.2233 val_loss=0.0000 scale=2.0000 norm=1.7833
[iter 300] loss=1.2230 val_loss=0.0000 scale=2.0000 norm=1.7892
[iter 400] loss=1.2230 val_loss=0.0000 scale=2.0000 norm=1.7900
== Quitting at iteration / GRAD 444


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2702 val_loss=0.0000 scale=2.0000 norm=1.8465
[iter 200] loss=1.2355 val_loss=0.0000 scale=4.0000 norm=3.7645
[iter 300] loss=1.1748 val_loss=0.0000 scale=4.0000 norm=3.7798
[iter 400] loss=1.0663 val_loss=0.0000 scale=8.0000 norm=7.5602


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1734 val_loss=0.0000 scale=2.0000 norm=1.6841
[iter 200] loss=1.1324 val_loss=0.0000 scale=2.0000 norm=1.7255
[iter 300] loss=1.0776 val_loss=0.0000 scale=4.0000 norm=3.4719
[iter 400] loss=0.9806 val_loss=0.0000 scale=8.0000 norm=6.9449
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0667 val_loss=0.0000 scale=2.0000 norm=1.5629
[iter 200] loss=1.0062 val_loss=0.0000 scale=2.0000 norm=1.5909
[iter 300] loss=0.9451 val_loss=0.0000 scale=4.0000 norm=3.2086
[iter 400] loss=0.8354 val_loss=0.0000 scale=8.0000 norm=6.4184



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2343 val_loss=0.0000 scale=2.0000 norm=1.7704
[iter 200] loss=1.2257 val_loss=0.0000 scale=2.0000 norm=1.8034
[iter 300] loss=1.2255 val_loss=0.0000 scale=2.0000 norm=1.8107
[iter 400] loss=1.2255 val_loss=0.0000 scale=2.0000 norm=1.8117
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1955 val_loss=0.0000 scale=2.0000 norm=1.7328
[iter 200] loss=1.1830 val_loss=0.0000 scale=2.0000 norm=1.7747
[iter 300] loss=1.1826 val_loss=0.0000 scale=2.0000 norm=1.7840
[iter 400] loss=1.1826 val_loss=0.0000 scale=2.0000 norm=1.7853
== Quitting at iteration / GRAD 453
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1968 val_loss=0.0000 scale=2.0000 norm=1.7337
[iter 200] loss=1.1857 val_loss=0.0000 scale=2.0000 norm=1.7757
[iter 300] loss=1.1854 val_loss=0.0000 scale=2.0000 norm=1.7848
[iter 400] loss=1.1854 val_loss=0.0000 scale=2.0000 norm=1.7861
== Quitting at iteration / GRAD 452
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2272 val_loss=0.0000 scale=2.0000 norm=1.7608
[iter 200] loss=1.2182 val_loss=0.0000 scale=2.0000 norm=1.7941
[iter 300] loss=1.2179 val_loss=0.0000 scale=2.0000 norm=1.8014
[iter 400] loss=1.2179 val_loss=0.0000 scale=2.0000 norm=1.8024
== Quitting at iteration / GRAD 446
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2538 val_loss=0.0000 scale=2.0000 norm=1.7873
[iter 200] loss=1.2472 val_loss=0.0000 scale=2.0000 norm=1.8148
[iter 300] loss=1.2470 val_loss=0.0000 scale=2.0000 norm=1.8205
[iter 400] loss=1.2470 val_loss=0.0000 scale=2.0000 norm=1.8213
== Quitting at iteration / GRAD 439


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1485 val_loss=0.0000 scale=2.0000 norm=1.6367
[iter 200] loss=1.1322 val_loss=0.0000 scale=2.0000 norm=1.6749
[iter 300] loss=1.1318 val_loss=0.0000 scale=2.0000 norm=1.6846
[iter 400] loss=1.1318 val_loss=0.0000 scale=2.0000 norm=1.6860
== Quitting at iteration / GRAD 463
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2222 val_loss=0.0000 scale=2.0000 norm=1.7382
[iter 200] loss=1.2132 val_loss=0.0000 scale=2.0000 norm=1.7695
[iter 300] loss=1.2130 val_loss=0.0000 scale=2.0000 norm=1.7764
[iter 400] loss=1.2130 val_loss=0.0000 scale=2.0000 norm=1.7774
== Quitting at iteration / GRAD 447
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0203 val_loss=0.0000 scale=2.0000 norm=1.5381
[iter 200] loss=0.9462 val_loss=0.0000 scale=2.0000 norm=1.6276
[iter 300] loss=0.8940 val_loss=0.0000 scale=4.0000 norm=3.3174
[iter 400] loss=0.8043 val_loss=0.0000 scale=8.0000 norm=6.6398
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2382 val_loss=0.0000 scale=2.0000 norm=1.7651
[iter 200] loss=1.2308 val_loss=0.0000 scale=2.0000 norm=1.7939
[iter 300] loss=1.2306 val_loss=0.0000 scale=2.0000 norm=1.7999
[iter 400] loss=1.2306 val_loss=0.0000 scale=2.0000 norm=1.8008
== Quitting at iteration / GRAD 442
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1509 val_loss=0.0000 scale=2.0000 norm=1.6682
[iter 200] loss=1.1347 val_loss=0.0000 scale=2.0000 norm=1.7135
[iter 300] loss=1.1343 val_loss=0.0000 scale=2.0000 norm=1.7239
[iter 400] loss=1.1343 val_loss=0.0000 scale=2.0000 norm=1.7254
== Quitting at iteration / GRAD 462
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1082 val_loss=0.0000 scale=2.0000 norm=1.6225
[iter 200] loss=1.0874 val_loss=0.0000 scale=2.0000 norm=1.6593
[iter 300] loss=1.0868 val_loss=0.0000 scale=2.0000 norm=1.6692
[iter 400] loss=1.0868 val_loss=0.0000 scale=2.0000 norm=1.6707
== Quitting at iteration / GRAD 469


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1766 val_loss=0.0000 scale=2.0000 norm=1.7065
[iter 200] loss=1.1634 val_loss=0.0000 scale=2.0000 norm=1.7491
[iter 300] loss=1.1631 val_loss=0.0000 scale=2.0000 norm=1.7585
[iter 400] loss=1.1631 val_loss=0.0000 scale=2.0000 norm=1.7598
== Quitting at iteration / GRAD 456
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2006 val_loss=0.0000 scale=2.0000 norm=1.7310
[iter 200] loss=1.1903 val_loss=0.0000 scale=2.0000 norm=1.7700
[iter 300] loss=1.1901 val_loss=0.0000 scale=2.0000 norm=1.7783
[iter 400] loss=1.1901 val_loss=0.0000 scale=2.0000 norm=1.7794
== Quitting at iteration / GRAD 450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1921 val_loss=0.0000 scale=2.0000 norm=1.7376
[iter 200] loss=1.1765 val_loss=0.0000 scale=2.0000 norm=1.7770
[iter 300] loss=1.1759 val_loss=0.0000 scale=2.0000 norm=1.7868
[iter 400] loss=1.1759 val_loss=0.0000 scale=2.0000 norm=1.7883
== Quitting at iteration / GRAD 459
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2194 val_loss=0.0000 scale=2.0000 norm=1.7525
[iter 200] loss=1.2102 val_loss=0.0000 scale=2.0000 norm=1.7875
[iter 300] loss=1.2100 val_loss=0.0000 scale=2.0000 norm=1.7951
[iter 400] loss=1.2100 val_loss=0.0000 scale=2.0000 norm=1.7962
== Quitting at iteration / GRAD 447


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2284 val_loss=0.0000 scale=2.0000 norm=1.7546
[iter 200] loss=1.2207 val_loss=0.0000 scale=2.0000 norm=1.7861
[iter 300] loss=1.2205 val_loss=0.0000 scale=2.0000 norm=1.7925
[iter 400] loss=1.2205 val_loss=0.0000 scale=2.0000 norm=1.7935
== Quitting at iteration / GRAD 443
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.3211 val_loss=0.0000 scale=2.0000 norm=1.8447
[iter 200] loss=1.3179 val_loss=0.0000 scale=2.0000 norm=1.8486
[iter 300] loss=1.3178 val_loss=0.0000 scale=2.0000 norm=1.8499
[iter 400] loss=1.3178 val_loss=0.0000 scale=2.0000 norm=1.8501
== Quitting at iteration / GRAD 421


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2405 val_loss=0.0000 scale=2.0000 norm=1.7562
[iter 200] loss=1.2322 val_loss=0.0000 scale=2.0000 norm=1.7695
[iter 300] loss=1.2320 val_loss=0.0000 scale=2.0000 norm=1.7732
[iter 400] loss=1.2320 val_loss=0.0000 scale=2.0000 norm=1.7738
== Quitting at iteration / GRAD 444
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2251 val_loss=0.0000 scale=2.0000 norm=1.7570
[iter 200] loss=1.2167 val_loss=0.0000 scale=2.0000 norm=1.7911
[iter 300] loss=1.2165 val_loss=0.0000 scale=2.0000 norm=1.7982
[iter 400] loss=1.2165 val_loss=0.0000 scale=2.0000 norm=1.7992
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2174 val_loss=0.0000 scale=2.0000 norm=1.7524
[iter 200] loss=1.2080 val_loss=0.0000 scale=2.0000 norm=1.7888
[iter 300] loss=1.2078 val_loss=0.0000 scale=2.0000 norm=1.7964
[iter 400] loss=1.2078 val_loss=0.0000 scale=2.0000 norm=1.7975
== Quitting at iteration / GRAD 447



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2188 val_loss=0.0000 scale=2.0000 norm=1.7488
[iter 200] loss=1.2101 val_loss=0.0000 scale=2.0000 norm=1.7809
[iter 300] loss=1.2100 val_loss=0.0000 scale=2.0000 norm=1.7876
[iter 400] loss=1.2099 val_loss=0.0000 scale=2.0000 norm=1.7885
== Quitting at iteration / GRAD 446
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1843 val_loss=0.0000 scale=2.0000 norm=1.7086
[iter 200] loss=1.1716 val_loss=0.0000 scale=2.0000 norm=1.7479
[iter 300] loss=1.1713 val_loss=0.0000 scale=2.0000 norm=1.7569
[iter 400] loss=1.1713 val_loss=0.0000 scale=2.0000 norm=1.7582
== Quitting at iteration / GRAD 455
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1919 val_loss=0.0000 scale=2.0000 norm=1.7077
[iter 200] loss=1.1810 val_loss=0.0000 scale=2.0000 norm=1.7449
[iter 300] loss=1.1808 val_loss=0.0000 scale=2.0000 norm=1.7529
[iter 400] loss=1.1808 val_loss=0.0000 scale=2.0000 norm=1.7541
== Quitting at iteration / GRAD 451
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2362 val_loss=0.0000 scale=2.0000 norm=1.7640
[iter 200] loss=1.2287 val_loss=0.0000 scale=2.0000 norm=1.7928
[iter 300] loss=1.2285 val_loss=0.0000 scale=2.0000 norm=1.7989
[iter 400] loss=1.2285 val_loss=0.0000 scale=2.0000 norm=1.7997
== Quitting at iteration / GRAD 442
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2457 val_loss=0.0000 scale=2.0000 norm=1.7810
[iter 200] loss=1.2390 val_loss=0.0000 scale=2.0000 norm=1.8154
[iter 300] loss=1.2388 val_loss=0.0000 scale=2.0000 norm=1.8223
[iter 400] loss=1.2388 val_loss=0.0000 scale=2.0000 norm=1.8232
== Quitting at iteration / GRAD 440
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1691 val_loss=0.0000 scale=2.0000 norm=1.7264
[iter 200] loss=1.1224 val_loss=0.0000 scale=2.0000 norm=1.7193
[iter 300] loss=1.1179 val_loss=0.0000 scale=2.0000 norm=1.7298
[iter 400] loss=1.1178 val_loss=0.0000 scale=2.0000 norm=1.7330
== Quitting at iteration / GRAD 498
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2062 val_loss=0.0000 scale=2.0000 norm=1.7465
[iter 200] loss=1.1957 val_loss=0.0000 scale=2.0000 norm=1.7894
[iter 300] loss=1.1955 val_loss=0.0000 scale=2.0000 norm=1.7984
[iter 400] loss=1.1955 val_loss=0.0000 scale=2.0000 norm=1.7997
== Quitting at iteration / GRAD 450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8833 val_loss=0.0000 scale=2.0000 norm=1.3942
[iter 200] loss=0.7734 val_loss=0.0000 scale=2.0000 norm=1.4410
[iter 300] loss=0.7667 val_loss=0.0000 scale=2.0000 norm=1.4841
[iter 400] loss=0.7665 val_loss=0.0000 scale=2.0000 norm=1.4922
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2239 val_loss=0.0000 scale=2.0000 norm=1.7593
[iter 200] loss=1.2150 val_loss=0.0000 scale=2.0000 norm=1.7933
[iter 300] loss=1.2148 val_loss=0.0000 scale=2.0000 norm=1.8004
[iter 400] loss=1.2148 val_loss=0.0000 scale=2.0000 norm=1.8014
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2240 val_loss=0.0000 scale=2.0000 norm=1.7614
[iter 200] loss=1.2142 val_loss=0.0000 scale=2.0000 norm=1.7954
[iter 300] loss=1.2139 val_loss=0.0000 scale=2.0000 norm=1.8028
[iter 400] loss=1.2139 val_loss=0.0000 scale=2.0000 norm=1.8039
== Quitting at iteration / GRAD 448
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2167 val_loss=0.0000 scale=2.0000 norm=1.7441
[iter 200] loss=1.2065 val_loss=0.0000 scale=2.0000 norm=1.7766
[iter 300] loss=1.2063 val_loss=0.0000 scale=2.0000 norm=1.7841
[iter 400] loss=1.2063 val_loss=0.0000 scale=2.0000 norm=1.7851
== Quitting at iteration / GRAD 449
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2092 val_loss=0.0000 scale=2.0000 norm=1.7469
[iter 200] loss=1.1983 val_loss=0.0000 scale=2.0000 norm=1.7868
[iter 300] loss=1.1980 val_loss=0.0000 scale=2.0000 norm=1.7954
[iter 400] loss=1.1980 val_loss=0.0000 scale=2.0000 norm=1.7967
== Quitting at iteration / GRAD 450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1960 val_loss=0.0000 scale=2.0000 norm=1.7264
[iter 200] loss=1.1856 val_loss=0.0000 scale=2.0000 norm=1.7702
[iter 300] loss=1.1853 val_loss=0.0000 scale=2.0000 norm=1.7794
[iter 400] loss=1.1853 val_loss=0.0000 scale=2.0000 norm=1.7807
== Quitting at iteration / GRAD 451
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2133 val_loss=0.0000 scale=2.0000 norm=1.7417
[iter 200] loss=1.2040 val_loss=0.0000 scale=2.0000 norm=1.7790
[iter 300] loss=1.2038 val_loss=0.0000 scale=2.0000 norm=1.7867
[iter 400] loss=1.2038 val_loss=0.0000 scale=2.0000 norm=1.7878
== Quitting at iteration / GRAD 447
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2262 val_loss=0.0000 scale=2.0000 norm=1.7561
[iter 200] loss=1.2173 val_loss=0.0000 scale=2.0000 norm=1.7877
[iter 300] loss=1.2171 val_loss=0.0000 scale=2.0000 norm=1.7946
[iter 400] loss=1.2171 val_loss=0.0000 scale=2.0000 norm=1.7956
== Quitting at iteration / GRAD 446


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2152 val_loss=0.0000 scale=2.0000 norm=1.7473
[iter 200] loss=1.2066 val_loss=0.0000 scale=2.0000 norm=1.7894
[iter 300] loss=1.2064 val_loss=0.0000 scale=2.0000 norm=1.7980
[iter 400] loss=1.2064 val_loss=0.0000 scale=2.0000 norm=1.7992
== Quitting at iteration / GRAD 447
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1735 val_loss=0.0000 scale=2.0000 norm=1.6559
[iter 200] loss=1.1612 val_loss=0.0000 scale=2.0000 norm=1.6918
[iter 300] loss=1.1609 val_loss=0.0000 scale=2.0000 norm=1.7006
[iter 400] loss=1.1609 val_loss=0.0000 scale=2.0000 norm=1.7018
== Quitting at iteration / GRAD 455


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1320 val_loss=0.0000 scale=2.0000 norm=1.6612
[iter 200] loss=1.0635 val_loss=0.0000 scale=4.0000 norm=3.4374
[iter 300] loss=0.9537 val_loss=0.0000 scale=8.0000 norm=6.8958
[iter 400] loss=0.7368 val_loss=0.0000 scale=16.0000 norm=13.7917


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2269 val_loss=0.0000 scale=2.0000 norm=1.7669
[iter 200] loss=1.2171 val_loss=0.0000 scale=2.0000 norm=1.8020
[iter 300] loss=1.2169 val_loss=0.0000 scale=2.0000 norm=1.8096
[iter 400] loss=1.2169 val_loss=0.0000 scale=2.0000 norm=1.8107
== Quitting at iteration / GRAD 447
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2099 val_loss=0.0000 scale=2.0000 norm=1.7378
[iter 200] loss=1.2003 val_loss=0.0000 scale=2.0000 norm=1.7733
[iter 300] loss=1.2001 val_loss=0.0000 scale=2.0000 norm=1.7808
[iter 400] loss=1.2001 val_loss=0.0000 scale=2.0000 norm=1.7819
== Quitting at iteration / GRAD 448
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2310 val_loss=0.0000 scale=2.0000 norm=1.7642
[iter 200] loss=1.2226 val_loss=0.0000 scale=2.0000 norm=1.7985
[iter 300] loss=1.2224 val_loss=0.0000 scale=2.0000 norm=1.8059
[iter 400] loss=1.2224 val_loss=0.0000 scale=2.0000 norm=1.8070
== Quitting at iteration / GRAD 445



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2348 val_loss=0.0000 scale=2.0000 norm=1.7775
[iter 200] loss=1.2270 val_loss=0.0000 scale=2.0000 norm=1.8120
[iter 300] loss=1.2269 val_loss=0.0000 scale=2.0000 norm=1.8190
[iter 400] loss=1.2269 val_loss=0.0000 scale=2.0000 norm=1.8200
== Quitting at iteration / GRAD 443
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2038 val_loss=0.0000 scale=2.0000 norm=1.7443
[iter 200] loss=1.1935 val_loss=0.0000 scale=2.0000 norm=1.7840
[iter 300] loss=1.1932 val_loss=0.0000 scale=2.0000 norm=1.7925
[iter 400] loss=1.1932 val_loss=0.0000 scale=2.0000 norm=1.7937
== Quitting at iteration / GRAD 450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2590 val_loss=0.0000 scale=2.0000 norm=1.8082
[iter 200] loss=1.2030 val_loss=0.0000 scale=4.0000 norm=3.6415
[iter 300] loss=1.1085 val_loss=0.0000 scale=8.0000 norm=7.2857
[iter 400] loss=0.9218 val_loss=0.0000 scale=16.0000 norm=14.5714
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1912 val_loss=0.0000 scale=2.0000 norm=1.7302
[iter 200] loss=1.1785 val_loss=0.0000 scale=2.0000 norm=1.7713
[iter 300] loss=1.1782 val_loss=0.0000 scale=2.0000 norm=1.7805
[iter 400] loss=1.1782 val_loss=0.0000 scale=2.0000 norm=1.7818
== Quitting at iteration / GRAD 454
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2307 val_loss=0.0000 scale=2.0000 norm=1.7634
[iter 200] loss=1.2227 val_loss=0.0000 scale=2.0000 norm=1.7972
[iter 300] loss=1.2226 val_loss=0.0000 scale=2.0000 norm=1.8041
[iter 400] loss=1.2226 val_loss=0.0000 scale=2.0000 norm=1.8051
== Quitting at iteration / GRAD 444
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2345 val_loss=0.0000 scale=2.0000 norm=1.7674
[iter 200] loss=1.2277 val_loss=0.0000 scale=2.0000 norm=1.8085
[iter 300] loss=1.2275 val_loss=0.0000 scale=2.0000 norm=1.8165
[iter 400] loss=1.2275 val_loss=0.0000 scale=2.0000 norm=1.8177
== Quitting at iteration / GRAD 442
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2235 val_loss=0.0000 scale=2.0000 norm=1.7534
[iter 200] loss=1.2151 val_loss=0.0000 scale=2.0000 norm=1.7894
[iter 300] loss=1.2149 val_loss=0.0000 scale=2.0000 norm=1.7969
[iter 400] loss=1.2149 val_loss=0.0000 scale=2.0000 norm=1.7979
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1368 val_loss=0.0000 scale=2.0000 norm=1.6677
[iter 200] loss=1.1138 val_loss=0.0000 scale=2.0000 norm=1.6902
[iter 300] loss=1.1131 val_loss=0.0000 scale=2.0000 norm=1.6973
[iter 400] loss=1.1131 val_loss=0.0000 scale=2.0000 norm=1.6985
== Quitting at iteration / GRAD 470
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2431 val_loss=0.0000 scale=2.0000 norm=1.7856
[iter 200] loss=1.2344 val_loss=0.0000 scale=2.0000 norm=1.8175
[iter 300] loss=1.2341 val_loss=0.0000 scale=2.0000 norm=1.8243
[iter 400] loss=1.2341 val_loss=0.0000 scale=2.0000 norm=1.8252
== Quitting at iteration / GRAD 444


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1506 val_loss=0.0000 scale=2.0000 norm=1.6992
[iter 200] loss=1.1313 val_loss=0.0000 scale=2.0000 norm=1.7522
[iter 300] loss=1.1306 val_loss=0.0000 scale=2.0000 norm=1.7644
[iter 400] loss=1.1306 val_loss=0.0000 scale=2.0000 norm=1.7662
== Quitting at iteration / GRAD 466
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2502 val_loss=0.0000 scale=2.0000 norm=1.7808
[iter 200] loss=1.2428 val_loss=0.0000 scale=2.0000 norm=1.8098
[iter 300] loss=1.2427 val_loss=0.0000 scale=2.0000 norm=1.8159
[iter 400] loss=1.2427 val_loss=0.0000 scale=2.0000 norm=1.8168
== Quitting at iteration / GRAD 441
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1595 val_loss=0.0000 scale=2.0000 norm=1.7416
[iter 200] loss=1.1445 val_loss=0.0000 scale=2.0000 norm=1.8209
[iter 300] loss=1.1441 val_loss=0.0000 scale=2.0000 norm=1.8376
[iter 400] loss=1.1441 val_loss=0.0000 scale=2.0000 norm=1.8399
== Quitting at iteration / GRAD 461
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2702 val_loss=0.0000 scale=2.0000 norm=1.8104
[iter 200] loss=1.2642 val_loss=0.0000 scale=2.0000 norm=1.8353
[iter 300] loss=1.2641 val_loss=0.0000 scale=2.0000 norm=1.8405
[iter 400] loss=1.2641 val_loss=0.0000 scale=2.0000 norm=1.8412
== Quitting at iteration / GRAD 435


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2232 val_loss=0.0000 scale=2.0000 norm=1.7813
[iter 200] loss=1.1811 val_loss=0.0000 scale=4.0000 norm=3.6281
[iter 300] loss=1.1199 val_loss=0.0000 scale=8.0000 norm=7.2734
[iter 400] loss=0.9986 val_loss=0.0000 scale=8.0000 norm=7.2737
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2827 val_loss=0.0000 scale=2.0000 norm=1.8286
[iter 200] loss=1.2766 val_loss=0.0000 scale=2.0000 norm=1.8511
[iter 300] loss=1.2765 val_loss=0.0000 scale=2.0000 norm=1.8559
[iter 400] loss=1.2765 val_loss=0.0000 scale=2.0000 norm=1.8566
== Quitting at iteration / GRAD 434


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1817 val_loss=0.0000 scale=2.0000 norm=1.7203
[iter 200] loss=1.1693 val_loss=0.0000 scale=2.0000 norm=1.7687
[iter 300] loss=1.1690 val_loss=0.0000 scale=2.0000 norm=1.7790
[iter 400] loss=1.1690 val_loss=0.0000 scale=2.0000 norm=1.7804
== Quitting at iteration / GRAD 455


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2201 val_loss=0.0000 scale=2.0000 norm=1.7557
[iter 200] loss=1.2096 val_loss=0.0000 scale=2.0000 norm=1.7901
[iter 300] loss=1.2093 val_loss=0.0000 scale=2.0000 norm=1.7979
[iter 400] loss=1.2093 val_loss=0.0000 scale=2.0000 norm=1.7990
== Quitting at iteration / GRAD 448


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


Average scores:	 r2: -0.02Â±0.03
structural
Filename: (ECFP)_NGB_Standard_generalizability
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/informatics/MSE723_final/results/target_hole mobility/structural/(ECFP)_NGB_Standard_generalizability_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/informatics/MSE723_final/results/target_hole mobility/structural/(ECFP)_NGB_Standard_generalizability_predictions.csv
Done Saving scores!
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9951 val_loss=0.0000 scale=2.0000 norm=1.5448
[iter 200] loss=0.8523 val_loss=0.0000 scale=2.0000 norm=1.5332
[iter 300] loss=0.7901 val_loss=0.0000 scale=4.0000 norm=3.1455
[iter 400] loss=0.6871 val_loss=0.0000 scale=8.0000 norm=6.3000
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1725 val_loss=0.0000 scale=2.0000 norm=1.6558
[iter 200] loss=1.1618 val_loss=0.0000 scale=2.0000 norm=1.6853
[iter 300] loss=1.1616 val_loss=0.0000 scale=2.0000 norm=1.6923
[iter 400] loss=1.1616 val_loss=0.0000 scale=2.0000 norm=1.6933
== Quitting at iteration / GRAD 453
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1983 val_loss=0.0000 scale=2.0000 norm=1.7110
[iter 200] loss=1.1876 val_loss=0.0000 scale=2.0000 norm=1.7424
[iter 300] loss=1.1873 val_loss=0.0000 scale=2.0000 norm=1.7499
[iter 400] loss=1.1873 val_loss=0.0000 scale=2.0000 norm=1.7510
== Quitting at iteration / GRAD 451
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2461 val_loss=0.0000 scale=2.0000 norm=1.7565
[iter 200] loss=1.2401 val_loss=0.0000 scale=2.0000 norm=1.7770
[iter 300] loss=1.2400 val_loss=0.0000 scale=2.0000 norm=1.7814
[iter 400] loss=1.2400 val_loss=0.0000 scale=2.0000 norm=1.7821
== Quitting at iteration / GRAD 439
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1552 val_loss=0.0000 scale=2.0000 norm=1.6899
[iter 200] loss=1.1372 val_loss=0.0000 scale=2.0000 norm=1.7378
[iter 300] loss=1.1367 val_loss=0.0000 scale=2.0000 norm=1.7495
[iter 400] loss=1.1367 val_loss=0.0000 scale=2.0000 norm=1.7512
== Quitting at iteration / GRAD 463
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2515 val_loss=0.0000 scale=2.0000 norm=1.7930
[iter 200] loss=1.2437 val_loss=0.0000 scale=2.0000 norm=1.8243
[iter 300] loss=1.2435 val_loss=0.0000 scale=2.0000 norm=1.8310
[iter 400] loss=1.2435 val_loss=0.0000 scale=2.0000 norm=1.8320
== Quitting at iteration / GRAD 441
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1923 val_loss=0.0000 scale=2.0000 norm=1.7927
[iter 200] loss=1.1356 val_loss=0.0000 scale=4.0000 norm=3.6215
[iter 300] loss=1.0440 val_loss=0.0000 scale=8.0000 norm=7.2481
[iter 400] loss=0.8634 val_loss=0.0000 scale=16.0000 norm=14.4961
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2061 val_loss=0.0000 scale=2.0000 norm=1.7152
[iter 200] loss=1.1961 val_loss=0.0000 scale=2.0000 norm=1.7399
[iter 300] loss=1.1958 val_loss=0.0000 scale=2.0000 norm=1.7459
[iter 400] loss=1.1958 val_loss=0.0000 scale=2.0000 norm=1.7467
== Quitting at iteration / GRAD 450
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.1131 val_loss=0.0000 scale=2.0000 norm=1.5819
[iter 200] loss=1.0845 val_loss=0.0000 scale=2.0000 norm=1.5853
[iter 300] loss=1.0835 val_loss=0.0000 scale=2.0000 norm=1.5899
[iter 400] loss=1.0835 val_loss=0.0000 scale=2.0000 norm=1.5907
== Quitting at iteration / GRAD 474
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1671 val_loss=0.0000 scale=2.0000 norm=1.7005
[iter 200] loss=1.1494 val_loss=0.0000 scale=2.0000 norm=1.7485
[iter 300] loss=1.1487 val_loss=0.0000 scale=2.0000 norm=1.7608
[iter 400] loss=1.1487 val_loss=0.0000 scale=2.0000 norm=1.7626
== Quitting at iteration / GRAD 464
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2519 val_loss=0.0000 scale=2.0000 norm=1.7885
[iter 200] loss=1.2453 val_loss=0.0000 scale=2.0000 norm=1.8179
[iter 300] loss=1.2451 val_loss=0.0000 scale=2.0000 norm=1.8238
[iter 400] loss=1.2451 val_loss=0.0000 scale=2.0000 norm=1.8247
== Quitting at iteration / GRAD 439
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1748 val_loss=0.0000 scale=2.0000 norm=1.7131
[iter 200] loss=1.1598 val_loss=0.0000 scale=2.0000 norm=1.7580
[iter 300] loss=1.1594 val_loss=0.0000 scale=2.0000 norm=1.7685
[iter 400] loss=1.1594 val_loss=0.0000 scale=2.0000 norm=1.7700
== Quitting at iteration / GRAD 459
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.2481 val_loss=0.0000 scale=2.0000 norm=1.7737
[iter 200] loss=1.2257 val_loss=0.0000 scale=2.0000 norm=1.7966
[iter 300] loss=1.2213 val_loss=0.0000 scale=2.0000 norm=1.8006
[iter 400] loss=1.2210 val_loss=0.0000 scale=2.0000 norm=1.8020
== Quitting at iteration / GRAD 488
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1463 val_loss=0.0000 scale=2.0000 norm=1.6864
[iter 200] loss=1.1251 val_loss=0.0000 scale=2.0000 norm=1.7285
[iter 300] loss=1.1245 val_loss=0.0000 scale=2.0000 norm=1.7403
[iter 400] loss=1.1244 val_loss=0.0000 scale=2.0000 norm=1.7420
== Quitting at iteration / GRAD 468
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2176 val_loss=0.0000 scale=2.0000 norm=1.7501
[iter 200] loss=1.2077 val_loss=0.0000 scale=2.0000 norm=1.7848
[iter 300] loss=1.2075 val_loss=0.0000 scale=2.0000 norm=1.7922
[iter 400] loss=1.2075 val_loss=0.0000 scale=2.0000 norm=1.7933
== Quitting at iteration / GRAD 448
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1664 val_loss=0.0000 scale=2.0000 norm=1.7016
[iter 200] loss=1.1496 val_loss=0.0000 scale=2.0000 norm=1.7450
[iter 300] loss=1.1492 val_loss=0.0000 scale=2.0000 norm=1.7555
[iter 400] loss=1.1492 val_loss=0.0000 scale=2.0000 norm=1.7570
== Quitting at iteration / GRAD 461
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1875 val_loss=0.0000 scale=2.0000 norm=1.7182
[iter 200] loss=1.1737 val_loss=0.0000 scale=2.0000 norm=1.7585
[iter 300] loss=1.1734 val_loss=0.0000 scale=2.0000 norm=1.7678
[iter 400] loss=1.1733 val_loss=0.0000 scale=2.0000 norm=1.7692
== Quitting at iteration / GRAD 456
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2203 val_loss=0.0000 scale=2.0000 norm=1.7388
[iter 200] loss=1.2119 val_loss=0.0000 scale=2.0000 norm=1.7686
[iter 300] loss=1.2117 val_loss=0.0000 scale=2.0000 norm=1.7751
[iter 400] loss=1.2117 val_loss=0.0000 scale=2.0000 norm=1.7760
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2259 val_loss=0.0000 scale=2.0000 norm=1.7573
[iter 200] loss=1.2177 val_loss=0.0000 scale=2.0000 norm=1.7913
[iter 300] loss=1.2175 val_loss=0.0000 scale=2.0000 norm=1.7983
[iter 400] loss=1.2175 val_loss=0.0000 scale=2.0000 norm=1.7993
== Quitting at iteration / GRAD 445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.2094 val_loss=0.0000 scale=2.0000 norm=1.7394
[iter 200] loss=1.1997 val_loss=0.0000 scale=2.0000 norm=1.7783
[iter 300] loss=1.1995 val_loss=0.0000 scale=2.0000 norm=1.7865
[iter 400] loss=1.1995 val_loss=0.0000 scale=2.0000 norm=1.7876
== Quitting at iteration / GRAD 448

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n06>
Subject: Job 719981: <ecfp_NGB_> in cluster <Hazel> Done

Job <ecfp_NGB_> was submitted from host <c201n09> by user <sdehgha2> in cluster <Hazel> at Tue Dec 10 20:14:27 2024
Job was executed on host(s) <2*c201n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Dec 10 20:16:28 2024
                            <2*c201n10>
                            <2*c201n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/informatics/MSE723_final/code_/training/hpc> was used as the working directory.
Started at Tue Dec 10 20:16:28 2024
Terminated at Wed Dec 11 04:42:38 2024
Results reported at Wed Dec 11 04:42:38 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 6
#BSUB -W 10:01
#BSUB -R span[ptile=2]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_NGB_"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp              --regressor_type NGB 

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   132656.00 sec.
    Max Memory :                                 6 GB
    Average Memory :                             4.35 GB
    Total Requested Memory :                     48.00 GB
    Delta Memory :                               42.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   30371 sec.
    Turnaround time :                            30491 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_.err> for stderr output of this job.

