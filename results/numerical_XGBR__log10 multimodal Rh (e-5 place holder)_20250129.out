


OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c025n01>
Subject: Job 429769: <numerical_XGBR_polymer_size_feats_on_log10 multimodal Rh (e-5 place holder)_all_num_20250129> in cluster <Hazel> Done

Job <numerical_XGBR_polymer_size_feats_on_log10 multimodal Rh (e-5 place holder)_all_num_20250129> was submitted from host <c010n01> by user <sdehgha2> in cluster <Hazel> at Fri Jan 31 11:02:18 2025
Job was executed on host(s) <6*c025n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Jan 31 11:02:19 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Jan 31 11:02:19 2025
Terminated at Fri Jan 31 11:03:55 2025
Results reported at Fri Jan 31 11:03:55 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 25:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "numerical_XGBR_polymer_size_feats_on_log10 multimodal Rh (e-5 place holder)_all_num_20250129"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR__log10 multimodal Rh (e-5 place holder)_20250129.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR__log10 multimodal Rh (e-5 place holder)_20250129.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log10 multimodal Rh (e-5 place holder)"                                     --regressor_type "XGBR"                                     --numerical_feats 'PDI' 'Mn (g/mol)' 'Mw (g/mol)' 'Concentration (mg/ml)' "Temperature SANS/SLS/DLS/SEC (K)" "polymer dP" "polymer dD" "polymer dH" "solvent dP" "solvent dD" "solvent dH"                                     --columns_to_impute "PDI" "Temperature SANS/SLS/DLS/SEC (K)" "Concentration (mg/ml)"                                     --special_impute 'Mw (g/mol)'                                     --imputer mean


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   29.28 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   113 sec.
    Turnaround time :                            97 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR__log10 multimodal Rh (e-5 place holder)_20250129.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0068338879806179595), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 241), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.012971213496043816), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 110), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0012978564360448853), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 1363), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.001316258543687923), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 1280), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02815155613736647), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.001), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 1430), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0066479727600754724), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 239), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.015519294239134614), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 96), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0313357855666764), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.003003493207716339), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 482), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.026553087849873133), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.029511879888797114), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.004965907143377908), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 295), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0022921559105011386), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 780), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005521069834416753), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 217), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.015448938394384357), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 98), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002197081850451975), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 620), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.008902545231620053), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 202), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.009906623823855243), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 170), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01644422542451718), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 78), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.012506667453931851), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 113), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02000189126553943), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 81), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.012151158758339454), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 114), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.03148706122598677), ('regressor__regressor__estimator__max_depth', 10000), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.018845538842695755), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 88), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.007835477213767683), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 210), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.003818509282303996), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 404), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.001973725111752731), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 799), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02732714669558909), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 61), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01897780778540449), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 81), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.019145049008994646), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 79), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.021386941170384536), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 73), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0045496878479097435), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 347), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002331100131198999), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 534), ('regressor__regressor__estimator__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01759514854332157), ('regressor__regressor__estimator__max_depth', 10), ('regressor__regressor__estimator__n_estimators', 86), ('regressor__regressor__estimator__n_jobs', -2)])


Average scores:	 r2: [0.405 0.389 0.394]Â±[0.118 0.089 0.131]
[array([0.40528948, 0.38941873, 0.39357258]), array([2.24238393, 2.7351596 , 2.79234335]), array([1.61407467, 2.04479303, 1.72323186])]
scaler
Filename: (PDI-Mw-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_XGBR_mean_transformerOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_log10 multimodal Rh (e-5 place holder)/scaler/(PDI-Mw-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_XGBR_mean_transformerOFF_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_log10 multimodal Rh (e-5 place holder)/scaler/(PDI-Mw-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_XGBR_mean_transformerOFF_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_log10 multimodal Rh (e-5 place holder)/scaler/(PDI-Mw-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_XGBR_mean_transformerOFF_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c020n03>
Subject: Job 429728: <numerical_XGBR_polymer_size_feats_on_log10 multimodal Rh (e-5 place holder)_all_num_20250129> in cluster <Hazel> Done

Job <numerical_XGBR_polymer_size_feats_on_log10 multimodal Rh (e-5 place holder)_all_num_20250129> was submitted from host <c025n01> by user <sdehgha2> in cluster <Hazel> at Fri Jan 31 10:53:19 2025
Job was executed on host(s) <6*c020n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Jan 31 10:53:20 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Jan 31 10:53:20 2025
Terminated at Fri Jan 31 11:48:33 2025
Results reported at Fri Jan 31 11:48:33 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 25:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "numerical_XGBR_polymer_size_feats_on_log10 multimodal Rh (e-5 place holder)_all_num_20250129"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR__log10 multimodal Rh (e-5 place holder)_20250129.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR__log10 multimodal Rh (e-5 place holder)_20250129.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log10 multimodal Rh (e-5 place holder)"                                     --regressor_type "XGBR"                                     --numerical_feats 'PDI' 'Mn (g/mol)' 'Mw (g/mol)' 'Concentration (mg/ml)' "Temperature SANS/SLS/DLS/SEC (K)" "polymer dP" "polymer dD" "polymer dH" "solvent dP" "solvent dD" "solvent dH"                                     --columns_to_impute "PDI" "Temperature SANS/SLS/DLS/SEC (K)" "Concentration (mg/ml)"                                     --special_impute 'Mw (g/mol)'                                     --imputer mean


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   10060.00 sec.
    Max Memory :                                 10 GB
    Average Memory :                             9.03 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               6.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   3327 sec.
    Turnaround time :                            3314 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_XGBR__log10 multimodal Rh (e-5 place holder)_20250129.err> for stderr output of this job.

