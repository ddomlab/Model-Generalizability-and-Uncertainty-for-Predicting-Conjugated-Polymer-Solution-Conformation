Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n14>
Subject: Job 292225: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Exited

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:15 2025
Job was executed on host(s) <8*c200n14>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:16 2025
Terminated at Mon Jan 20 12:35:26 2025
Results reported at Mon Jan 20 12:35:26 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "RRU Trimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   4463.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             2.95 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               11.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   1170 sec.
    Turnaround time :                            1151 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c037n03>
Subject: Job 292219: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Exited

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:15 2025
Job was executed on host(s) <8*c037n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:16 2025
Terminated at Mon Jan 20 12:44:09 2025
Results reported at Mon Jan 20 12:44:09 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "Trimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   6610.31 sec.
    Max Memory :                                 7 GB
    Average Memory :                             4.41 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               9.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   1700 sec.
    Turnaround time :                            1674 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c027n01>
Subject: Job 292217: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Exited

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:15 2025
Job was executed on host(s) <8*c027n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:16 2025
Terminated at Mon Jan 20 12:47:38 2025
Results reported at Mon Jan 20 12:47:38 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "Dimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   7629.00 sec.
    Max Memory :                                 7 GB
    Average Memory :                             4.29 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               9.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   1908 sec.
    Turnaround time :                            1883 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n04>
Subject: Job 292223: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Exited

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:15 2025
Job was executed on host(s) <8*c207n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:16 2025
Terminated at Mon Jan 20 12:53:10 2025
Results reported at Mon Jan 20 12:53:10 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "RRU Dimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   9496.40 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.56 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               11.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   2235 sec.
    Turnaround time :                            2215 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c036n03>
Subject: Job 292215: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Exited

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:15 2025
Job was executed on host(s) <8*c036n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:15 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:15 2025
Terminated at Mon Jan 20 12:54:38 2025
Results reported at Mon Jan 20 12:54:38 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "Monomer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   9709.00 sec.
    Max Memory :                                 7 GB
    Average Memory :                             4.38 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               9.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   2302 sec.
    Turnaround time :                            2303 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c028n03>
Subject: Job 292221: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Exited

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:15 2025
Job was executed on host(s) <8*c028n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:16 2025
Terminated at Mon Jan 20 12:55:55 2025
Results reported at Mon Jan 20 12:55:55 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "RRU Monomer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   10482.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             3.90 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               8.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   2403 sec.
    Turnaround time :                            2380 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.009826834653305678), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 70), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0008499753896122645), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1015), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 5.0587024885658266e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0006253992712583399), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 934), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.08568138636446951), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1905), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 3.866373305292499e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004166888675011666), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1805), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.009423250961775384), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.036925826021813775), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 327), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0012085382236302997), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 548), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.281691683468901e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.010620094256978962), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00028530731053899426), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.000364363410129749), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.008265466538926541), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 61), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.00015034073911460313), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.008967428328811752), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 66), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0070194536185544405), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 104), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0035396389501452905), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 177), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.010376089180582769), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.013432433039309636), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 3.0458100287898922e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0028890016554601926), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 143), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.013100766527389698), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0006186361359096944), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0012166790918605442), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 547), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.796420341013348e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005246196001967956), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 102), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0004765516050414903), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.015414422099002446), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.003784309937861299), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 191), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 2.1498457841523116e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0010781041905605908), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 819), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.013946762535838672), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0006052914256971956), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1221), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.016955748813257024), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00038179465630513196), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0006476619102635798), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n03>
Subject: Job 292229: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Exited

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:15 2025
Job was executed on host(s) <8*c200n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:16 2025
Terminated at Mon Jan 20 21:22:19 2025
Results reported at Mon Jan 20 21:22:19 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "Dimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   148518.08 sec.
    Max Memory :                                 5 GB
    Average Memory :                             4.95 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               11.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   32778 sec.
    Turnaround time :                            32764 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.013691388901387115), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.015511119695630094), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 51), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 2.77990317345008e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.004634662486974106), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 123), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.00019805749929187384), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.1), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.00048732886369416516), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0007413925455330007), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 958), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 6.386341806018089e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.009275692219941867), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.05507703829396271), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 586), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 3.541254600438871e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.013481692346241648), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 4.3917087077450715e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01153564685477384), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00029416121827641347), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.014341325549391505), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.009490617683422941), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 3.423099794989676e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0006183805584230764), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 886), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.950161208522797e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004757455705418458), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1182), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.00047621638959098427), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00437475016174287), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 147), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0005542688537234405), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 910), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005244053112878154), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 130), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00958652493393468), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 7.749203163840802e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0009617530048520604), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 671), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.2664860087681802e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00035043747736930136), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0003796362767758621), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1615), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.0862407365666193e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.013538910054843254), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 56), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005409233231266193), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 128), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 2.0589595765257106e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.017637461992661186), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 51), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.1477284875192273e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0016152896635831695), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 505), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002308658997805156), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 344), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.014978019631699025), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0005008841536376041), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1984), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 8.650576925177668e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00593540071853379), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 162), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.009372744129500527), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 5.235684197748739e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00020994085816644536), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.007674740033058654), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 88), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00020181084225968585), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.00022898077080918262), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0009097467049007282), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 534), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.2518065637094096e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.011158160528421258), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 73), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])


Average scores:	 r2: [ 0.04   0.044 -0.041]Â±[0.133 0.069 0.022]
[array([ 0.03984513,  0.04444349, -0.04057391]), array([8.51478951e+00, 1.54558554e+02, 9.38188954e+04]), array([6.48346139e+00, 1.05203119e+02, 1.41217194e+04])]
Trimer
Filename: (MACCS)_NGB_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Trimer/(MACCS)_NGB_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Trimer/(MACCS)_NGB_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Trimer/(MACCS)_NGB_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n02>
Subject: Job 292231: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Done

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
Job was executed on host(s) <8*c200n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:16 2025
Terminated at Mon Jan 20 22:54:00 2025
Results reported at Mon Jan 20 22:54:00 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "Trimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   173797.11 sec.
    Max Memory :                                 5 GB
    Average Memory :                             4.97 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               11.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   38268 sec.
    Turnaround time :                            38264 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0006485835828921119), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1446), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 9.590941089616503e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.006254884166924361), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 146), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.000340291439939486), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1702), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.010028087604727225), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 134), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004221695472936509), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1855), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.008572241787964844), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0007278433389007974), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1999), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 2.7423440306222484e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.003802815341313288), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 179), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0007672448996642622), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.011204844813515972), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.0743403756357603e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0002976728691947934), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004063213002326674), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1998), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 3.9144926592266954e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0002752810582877258), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1972), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.8567549017298e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.003353521491675057), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 172), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005522495785832126), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 121), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0008612383820354793), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0003541672774032728), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0007100503352266149), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 628), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.0855526480423722e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00037088691720972865), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0003846787942082012), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.004417745097498184), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 98), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0015459249013341947), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 459), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0003736459296568639), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0034738787054413973), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 202), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0007978309388774966), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002908749692332876), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 293), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0027687068809719656), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 277), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 2.987406840721962e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0007629460073026826), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1490), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.017985163837578297), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.018599478589141183), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.00061285355058393), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.008343353542624955), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 115), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0006070511346672848), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1291), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.000543108435768091), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0016660738156312849), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 508), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0012375222516958161), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 425), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.003246770403205877), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 139), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.012173995878879134), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.7109942005249744e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.008063797523945015), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 51), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.00022603832623575723), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.010276875880036875), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.2994266984671718e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00998870584684348), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 89), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])


Average scores:	 r2: [ 0.046  0.042 -0.041]Â±[0.131 0.071 0.022]
[array([ 0.04640476,  0.04191123, -0.04057391]), array([8.48391178e+00, 1.54759680e+02, 9.38188954e+04]), array([6.45889866e+00, 1.05355392e+02, 1.41217155e+04])]
RRU Trimer
Filename: (MACCS)_NGB_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Trimer/(MACCS)_NGB_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Trimer/(MACCS)_NGB_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Trimer/(MACCS)_NGB_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n06>
Subject: Job 292237: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Done

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
Job was executed on host(s) <8*c203n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:17 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:17 2025
Terminated at Mon Jan 20 23:06:11 2025
Results reported at Mon Jan 20 23:06:11 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "RRU Trimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   178900.36 sec.
    Max Memory :                                 5 GB
    Average Memory :                             4.97 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               11.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   39013 sec.
    Turnaround time :                            38995 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0008499753896122645), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1015), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 5.0587024885658266e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.008420410438135477), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 86), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.0523492586534476e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.012136490658140847), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.1), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00040414908231421444), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.008055890978536017), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.03754771314269894), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.008417806934679663), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 105), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.010892029875663529), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0008695262106359266), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0009806166285605228), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 521), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 7.928954096909757e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0007229043073006687), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.004934847660451702), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 144), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0002764398209557418), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.007641298987724169), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 89), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0036888907977059553), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 181), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0016035565012305524), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 346), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0002661732595997613), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0017143916063948601), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 414), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.009579884247894624), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.006190171057017657), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 115), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0019759004449897456), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 387), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.016602345839363385), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 70), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004094987178983217), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1975), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 2.6548591968901355e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0027740153668595947), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 228), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.007403755416920729), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 150), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00285543934638404), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 301), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.013624599324581132), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 67), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.015564988182012324), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0008735226045445961), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0005422337038446817), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1985), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.5416496302101466e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.1), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 5.395478770895561e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004373165240753095), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1132), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.007824865913073903), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 61), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.013794016983071733), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0009930016561723969), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0008311736090198264), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 489), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 2.793705520050427e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.001006781162447506), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 684), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.0288327976294859e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.004185802382248886), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 199), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])


Average scores:	 r2: [ 0.038  0.052 -0.041]Â±[0.135 0.071 0.022]
[array([ 0.03765015,  0.05245962, -0.0405739 ]), array([8.52220879e+00, 1.53930432e+02, 9.38188954e+04]), array([6.49041809e+00, 1.04319920e+02, 1.41217167e+04])]
Monomer
Filename: (MACCS)_NGB_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Monomer/(MACCS)_NGB_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Monomer/(MACCS)_NGB_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Monomer/(MACCS)_NGB_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n05>
Subject: Job 292227: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Done

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:15 2025
Job was executed on host(s) <8*c200n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:16 2025
Terminated at Mon Jan 20 23:07:05 2025
Results reported at Mon Jan 20 23:07:05 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "Monomer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   177342.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             4.96 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               11.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   39061 sec.
    Turnaround time :                            39050 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004917772900832932), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.006571434359484591), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 164), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.012063065070560961), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.09695345905438146), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 460), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 3.4189578987962306e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004271987667865666), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00733367366621701), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0004766641293250073), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0028101430684568675), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 571), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0034309365007661316), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 210), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0002843669746413145), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004249182168305973), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1426), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.018443108624425972), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0035176427711141327), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 166), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.2735436091889348e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.004677410949223556), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 120), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.003245912530283656), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 192), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00035333117865295123), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.010762683304798076), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0001249236352871444), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.004084223317214517), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 162), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 2.5160751158165927e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0020417051845447957), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 252), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 4.776025313290357e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.014148535467550996), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0004952639254234795), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.000703714275667182), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1210), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.011235957552742471), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 61), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0004985076042978921), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0005131035330150745), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.014539940776127299), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.019200264670540226), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.4015260574519028e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01617403625695129), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.018435350700066605), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.017911027679142252), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0005250243644693158), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1992), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 2.4040719870902607e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00048157757216342516), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005606827464135254), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 92), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0024398870054685726), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 196), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.5748494145699243e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0019880679031631177), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 360), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004929080781246617), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 907), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0009299225497508278), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0003308467182448057), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0008317054418213474), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.017431703166005962), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])


Average scores:	 r2: [ 0.041  0.045 -0.041]Â±[0.135 0.07  0.022]
[array([ 0.04130278,  0.04513986, -0.0405739 ]), array([8.50455629e+00, 1.54482671e+02, 9.38188954e+04]), array([6.48327379e+00, 1.05023003e+02, 1.41217132e+04])]
RRU Dimer
Filename: (MACCS)_NGB_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Dimer/(MACCS)_NGB_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Dimer/(MACCS)_NGB_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Dimer/(MACCS)_NGB_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n04>
Subject: Job 292235: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Done

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
Job was executed on host(s) <8*c200n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:17 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:17 2025
Terminated at Mon Jan 20 23:19:49 2025
Results reported at Mon Jan 20 23:19:49 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "RRU Dimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   179245.34 sec.
    Max Memory :                                 5 GB
    Average Memory :                             4.96 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               11.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   39834 sec.
    Turnaround time :                            39813 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.009255417216686596), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 119), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005112509498629501), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 240), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0006902362275998203), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 825), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.1), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 323), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0008499753896122645), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1015), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 5.0587024885658266e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005115830416409307), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 65), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.00048820191889505267), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.06459156914147546), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 289), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 4.1200688201979395e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0011318619641829037), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 706), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.2908734110748798e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0016423141667538152), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 381), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01196784298668179), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004506579574202475), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1976), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0005592092771056974), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0005929772872444972), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 919), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01297966166668086), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 6.294911523507205e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002743333188591885), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 437), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.012946325663690018), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.011047350558241471), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.015940973732643103), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.007396589119836818), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 66), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005358414119046701), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 139), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00040387566452613024), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 3.888962217327559e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.019691889905259493), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.1229758645296387e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.021622531467978583), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0006378880588141801), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1176), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.019462755731431563), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 58), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0008756735129014289), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01641952852953265), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.007059917225244243), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 143), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0005208064051433357), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004869904238932107), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.000856014193477452), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1082), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0004401410303044528), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1151), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.01015802953622447), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 8.585771690063211e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002556620690898692), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 337), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0006998265758651679), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0018396535566732803), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 212), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002979873270576351), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 197), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.0001900401366044244), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0008852816182827914), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 923), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])


Average scores:	 r2: [ 0.037  0.051 -0.041]Â±[0.136 0.074 0.022]
[array([ 0.03697632,  0.05112954, -0.0405739 ]), array([8.52286132e+00, 1.54012188e+02, 9.38188954e+04]), array([6.47840058e+00, 1.04327943e+02, 1.41217206e+04])]
RRU Monomer
Filename: (MACCS)_NGB_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Monomer/(MACCS)_NGB_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Monomer/(MACCS)_NGB_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Monomer/(MACCS)_NGB_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 292233: <mordred_NGB_Standard_multimodal Rh_20250120> in cluster <Hazel> Done

Job <mordred_NGB_Standard_multimodal Rh_20250120> was submitted from host <c038n02> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:16 2025
Job was executed on host(s) <8*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:16:17 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:16:17 2025
Terminated at Mon Jan 20 23:20:18 2025
Results reported at Mon Jan 20 23:20:18 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Standard_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "RRU Monomer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   183163.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             4.96 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               11.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   39844 sec.
    Turnaround time :                            39842 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Standard_multimodal Rh_20250120.err> for stderr output of this job.

