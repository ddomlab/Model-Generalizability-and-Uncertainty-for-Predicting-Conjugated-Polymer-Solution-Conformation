Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.370.15	 r2: 0.10.11
RRU Dimer
Filename: (ECFP3.binary.512)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.binary.512)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.binary.512)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.binary.512)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n04>
Subject: Job 598214: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:34 2024
Job was executed on host(s) <4*c005n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:23:09 2024
                            <4*c012n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:23:09 2024
Terminated at Mon Dec  2 14:06:45 2024
Results reported at Mon Dec  2 14:06:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 3                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   27487.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             1.58 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   49416 sec.
    Turnaround time :                            51611 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
Monomer
Filename: (ECFP3.binary.512)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.binary.512)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.binary.512)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.binary.512)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n05>
Subject: Job 598205: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:25 2024
Job was executed on host(s) <4*c202n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:12:12 2024
                            <4*c202n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:12:12 2024
Terminated at Mon Dec  2 14:31:21 2024
Results reported at Mon Dec  2 14:31:21 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 3                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   38651.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.52 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               29.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   51566 sec.
    Turnaround time :                            53096 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
RRU Monomer
Filename: (ECFP4.binary.1024)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP4.binary.1024)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP4.binary.1024)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP4.binary.1024)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c013n01>
Subject: Job 598228: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:48 2024
Job was executed on host(s) <4*c013n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:44:23 2024
                            <4*c006n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:44:23 2024
Terminated at Mon Dec  2 15:17:18 2024
Results reported at Mon Dec  2 15:17:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 4                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   36454.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.70 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   52375 sec.
    Turnaround time :                            55830 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
Monomer
Filename: (ECFP4.binary.1024)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.binary.1024)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.binary.1024)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.binary.1024)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c006n03>
Subject: Job 598220: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:40 2024
Job was executed on host(s) <4*c006n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:29:54 2024
                            <4*c013n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:29:54 2024
Terminated at Mon Dec  2 16:48:22 2024
Results reported at Mon Dec  2 16:48:22 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 4                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   43051.03 sec.
    Max Memory :                                 4 GB
    Average Memory :                             1.81 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   58708 sec.
    Turnaround time :                            61302 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.15	 r2: 0.110.11
RRU Dimer
Filename: (ECFP4.binary.1024)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.binary.1024)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.binary.1024)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.binary.1024)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c016n01>
Subject: Job 598231: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:50 2024
Job was executed on host(s) <4*c016n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:55:05 2024
                            <4*c021n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:55:05 2024
Terminated at Mon Dec  2 17:17:46 2024
Results reported at Mon Dec  2 17:17:46 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 4                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   42791.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             1.70 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   58962 sec.
    Turnaround time :                            63056 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
Dimer
Filename: (ECFP4.binary.1024)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.binary.1024)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.binary.1024)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.binary.1024)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c003n01>
Subject: Job 598223: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:43 2024
Job was executed on host(s) <4*c003n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:30:36 2024
                            <4*c006n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:30:36 2024
Terminated at Mon Dec  2 19:01:16 2024
Results reported at Mon Dec  2 19:01:16 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 4                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   36149.00 sec.
    Max Memory :                                 6 GB
    Average Memory :                             1.81 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   66640 sec.
    Turnaround time :                            69273 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
Trimer
Filename: (ECFP5.binary.2048)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP5.binary.2048)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP5.binary.2048)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP5.binary.2048)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c027n02>
Subject: Job 598242: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:58 2024
Job was executed on host(s) <4*c027n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:17:29 2024
                            <4*c036n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 01:17:29 2024
Terminated at Mon Dec  2 22:21:26 2024
Results reported at Mon Dec  2 22:21:26 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 5                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   61551.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             2.08 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               24.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   75849 sec.
    Turnaround time :                            81268 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.15	 r2: 0.110.11
Monomer
Filename: (ECFP5.binary.2048)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP5.binary.2048)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP5.binary.2048)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP5.binary.2048)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c037n04>
Subject: Job 598237: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:54 2024
Job was executed on host(s) <4*c037n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:08:53 2024
                            <4*c039n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 01:08:53 2024
Terminated at Tue Dec  3 00:05:13 2024
Results reported at Tue Dec  3 00:05:13 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 5                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   55362.00 sec.
    Max Memory :                                 6 GB
    Average Memory :                             2.11 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   82595 sec.
    Turnaround time :                            87499 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.15	 r2: 0.10.11
RRU Trimer
Filename: (ECFP3.binary.512)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP3.binary.512)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP3.binary.512)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP3.binary.512)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c025n04>
Subject: Job 598216: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:37 2024
Job was executed on host(s) <4*c025n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:24:50 2024
                            <4*c016n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:24:50 2024
Terminated at Tue Dec  3 01:31:00 2024
Results reported at Tue Dec  3 01:31:00 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 3                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   34616.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.43 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               29.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   90371 sec.
    Turnaround time :                            92663 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.10.11
Trimer
Filename: (ECFP3.binary.512)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.binary.512)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.binary.512)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.binary.512)_GPR_Robust Scaler_shape.json
Done Saving scores!


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890

------------------------------------------------------------
Sender: LSF System <lsfadmin@c004n04>
Subject: Job 598209: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:31 2024
Job was executed on host(s) <4*c004n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:19:19 2024
                            <4*c011n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:19:19 2024
Terminated at Tue Dec  3 03:33:48 2024
Results reported at Tue Dec  3 03:33:48 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 3                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   32299.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             1.31 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   98071 sec.
    Turnaround time :                            100037 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
RRU Trimer
Filename: (ECFP4.binary.1024)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP4.binary.1024)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP4.binary.1024)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP4.binary.1024)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n01>
Subject: Job 598235: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:52 2024
Job was executed on host(s) <4*c203n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:58:47 2024
                            <4*c203n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:58:47 2024
Terminated at Tue Dec  3 05:04:16 2024
Results reported at Tue Dec  3 05:04:16 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 4                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   42295.29 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.35 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               29.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   101139 sec.
    Turnaround time :                            105444 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.15	 r2: 0.110.11
Monomer
Filename: (ECFP6.binary.4096)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP6.binary.4096)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP6.binary.4096)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP6.binary.4096)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c038n04>
Subject: Job 598254: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:47:14 2024
Job was executed on host(s) <4*c038n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:26:58 2024
                            <4*c034n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 01:26:58 2024
Terminated at Tue Dec  3 06:12:11 2024
Results reported at Tue Dec  3 06:12:11 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 6                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   85943.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             2.02 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               24.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   103519 sec.
    Turnaround time :                            109497 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.15	 r2: 0.10.11
RRU Monomer
Filename: (ECFP3.binary.512)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.binary.512)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.binary.512)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.binary.512)_GPR_Robust Scaler_shape.json
Done Saving scores!


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n10>
Subject: Job 598212: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:33 2024
Job was executed on host(s) <4*c202n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:21:18 2024
                            <4*c202n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:21:18 2024
Terminated at Tue Dec  3 07:34:27 2024
Results reported at Tue Dec  3 07:34:27 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 3                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   38598.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.21 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               29.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   112388 sec.
    Turnaround time :                            114474 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
RRU Trimer
Filename: (ECFP5.binary.2048)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP5.binary.2048)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP5.binary.2048)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP5.binary.2048)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n13>
Subject: Job 598252: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:47:12 2024
Job was executed on host(s) <4*c203n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:25:00 2024
                            <4*c203n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 01:25:00 2024
Terminated at Tue Dec  3 08:49:28 2024
Results reported at Tue Dec  3 08:49:28 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 5                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   53952.07 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.54 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               29.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   113068 sec.
    Turnaround time :                            118936 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.10.11
Dimer
Filename: (ECFP3.binary.512)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP3.binary.512)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP3.binary.512)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP3.binary.512)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c004n02>
Subject: Job 598207: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:28 2024
Job was executed on host(s) <4*c004n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:15:16 2024
                            <4*c007n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:15:16 2024
Terminated at Tue Dec  3 09:30:22 2024
Results reported at Tue Dec  3 09:30:22 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 3                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   33317.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.28 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               29.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   119708 sec.
    Turnaround time :                            121434 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
RRU Dimer
Filename: (ECFP6.binary.4096)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP6.binary.4096)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP6.binary.4096)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP6.binary.4096)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n01>
Subject: Job 598263: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:47:26 2024
Job was executed on host(s) <4*c005n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:04:01 2024
                            <4*c013n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:04:01 2024
Terminated at Tue Dec  3 19:40:19 2024
Results reported at Tue Dec  3 19:40:19 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 6                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   69287.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.60 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   149800 sec.
    Turnaround time :                            157973 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.15	 r2: 0.110.11
RRU Dimer
Filename: (ECFP5.binary.2048)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP5.binary.2048)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP5.binary.2048)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP5.binary.2048)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n03>
Subject: Job 598250: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:47:08 2024
Job was executed on host(s) <4*c202n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:22:47 2024
                            <4*c202n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 01:22:47 2024
Terminated at Tue Dec  3 20:36:02 2024
Results reported at Tue Dec  3 20:36:02 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 5                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   62705.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.27 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               29.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   155593 sec.
    Turnaround time :                            161334 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
Trimer
Filename: (ECFP4.binary.1024)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP4.binary.1024)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP4.binary.1024)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP4.binary.1024)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c033n03>
Subject: Job 598225: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:44 2024
Job was executed on host(s) <4*c033n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 00:41:56 2024
                            <4*c036n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 00:41:56 2024
Terminated at Tue Dec  3 22:54:09 2024
Results reported at Tue Dec  3 22:54:09 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 4                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   53975.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.24 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               29.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   166348 sec.
    Turnaround time :                            169645 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
Trimer
Filename: (ECFP6.binary.4096)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP6.binary.4096)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP6.binary.4096)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP6.binary.4096)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c032n02>
Subject: Job 598258: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:47:20 2024
Job was executed on host(s) <4*c032n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:49:35 2024
                            <4*c037n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 01:49:35 2024
Terminated at Tue Dec  3 23:48:45 2024
Results reported at Tue Dec  3 23:48:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 6                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   90758.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.60 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               29.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   165550 sec.
    Turnaround time :                            172885 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
RRU Monomer
Filename: (ECFP5.binary.2048)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP5.binary.2048)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP5.binary.2048)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP5.binary.2048)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n07>
Subject: Job 598245: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:47:02 2024
Job was executed on host(s) <4*c203n07>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:21:27 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 01:21:27 2024
Terminated at Wed Dec  4 02:40:27 2024
Results reported at Wed Dec  4 02:40:27 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 5                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   58703.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.24 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               29.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   177541 sec.
    Turnaround time :                            183205 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
Dimer
Filename: (ECFP5.binary.2048)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP5.binary.2048)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP5.binary.2048)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP5.binary.2048)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n12>
Subject: Job 598240: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:46:56 2024
Job was executed on host(s) <4*c203n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:13:28 2024
                            <4*c203n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 01:13:28 2024
Terminated at Wed Dec  4 04:24:05 2024
Results reported at Wed Dec  4 04:24:05 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 5                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   60274.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             1.25 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   184262 sec.
    Turnaround time :                            189429 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
Dimer
Filename: (ECFP6.binary.4096)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP6.binary.4096)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP6.binary.4096)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP6.binary.4096)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n08>
Subject: Job 598256: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:47:17 2024
Job was executed on host(s) <4*c205n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:42:05 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 01:42:05 2024
Terminated at Wed Dec  4 11:33:34 2024
Results reported at Wed Dec  4 11:33:34 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 6                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   98422.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.42 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   208314 sec.
    Turnaround time :                            215177 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.15	 r2: 0.110.11
RRU Monomer
Filename: (ECFP6.binary.4096)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP6.binary.4096)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP6.binary.4096)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP6.binary.4096)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n04>
Subject: Job 598261: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:47:23 2024
Job was executed on host(s) <4*c205n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:55:50 2024
                            <4*c205n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 01:55:50 2024
Terminated at Wed Dec  4 18:11:30 2024
Results reported at Wed Dec  4 18:11:30 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 6                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   93220.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             1.33 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   231340 sec.
    Turnaround time :                            239047 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.380.16	 r2: 0.110.11
RRU Trimer
Filename: (ECFP6.binary.4096)_GPR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP6.binary.4096)_GPR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP6.binary.4096)_GPR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP6.binary.4096)_GPR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c011n04>
Subject: Job 598266: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c009n02> by user <sdehgha2> in cluster <Hazel> at Sun Dec  1 23:47:28 2024
Job was executed on host(s) <4*c011n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:12:02 2024
                            <4*c010n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:12:02 2024
Terminated at Wed Dec  4 18:15:18 2024
Results reported at Wed Dec  4 18:15:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "tanimoto"                                        --radius 6                                        --vector binary                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   71429.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.34 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   230610 sec.
    Turnaround time :                            239270 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n04>
Subject: Job 619857: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:15 2024
Job was executed on host(s) <4*c207n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Dec  4 22:41:26 2024
                            <4*c207n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed Dec  4 22:41:26 2024
Terminated at Wed Dec  4 22:55:05 2024
Results reported at Wed Dec  4 22:55:05 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   66.35 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.73 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               30.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   819 sec.
    Turnaround time :                            18110 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n04>
Subject: Job 619859: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:15 2024
Job was executed on host(s) <4*c207n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Dec  4 23:06:35 2024
                            <4*c207n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed Dec  4 23:06:35 2024
Terminated at Wed Dec  4 23:58:06 2024
Results reported at Wed Dec  4 23:58:06 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   825.05 sec.
    Max Memory :                                 4 GB
    Average Memory :                             2.23 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   3091 sec.
    Turnaround time :                            21891 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c037n01>
Subject: Job 619866: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:17 2024
Job was executed on host(s) <4*c037n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 02:52:42 2024
                            <4*c035n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 02:52:42 2024
Terminated at Thu Dec  5 02:59:42 2024
Results reported at Thu Dec  5 02:59:42 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   176.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.56 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   426 sec.
    Turnaround time :                            32785 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c037n01>
Subject: Job 619869: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:18 2024
Job was executed on host(s) <4*c037n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:06:22 2024
                            <4*c035n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:06:22 2024
Terminated at Thu Dec  5 03:08:01 2024
Results reported at Thu Dec  5 03:08:01 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   135.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.50 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   109 sec.
    Turnaround time :                            33283 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c035n03>
Subject: Job 619877: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:21 2024
Job was executed on host(s) <4*c035n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:15:23 2024
                            <4*c037n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:15:23 2024
Terminated at Thu Dec  5 03:17:51 2024
Results reported at Thu Dec  5 03:17:51 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   153.40 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.86 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   155 sec.
    Turnaround time :                            33870 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 619878: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:22 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:15:23 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:15:23 2024
Terminated at Thu Dec  5 03:18:00 2024
Results reported at Thu Dec  5 03:18:00 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   172.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.12 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   158 sec.
    Turnaround time :                            33878 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 619885: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:24 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:21:47 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:21:47 2024
Terminated at Thu Dec  5 03:30:25 2024
Results reported at Thu Dec  5 03:30:25 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   664.10 sec.
    Max Memory :                                 6 GB
    Average Memory :                             3.05 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   520 sec.
    Turnaround time :                            34621 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c037n01>
Subject: Job 619887: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:25 2024
Job was executed on host(s) <4*c037n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:30:27 2024
                            <4*c035n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:30:27 2024
Terminated at Thu Dec  5 03:38:37 2024
Results reported at Thu Dec  5 03:38:37 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   969.08 sec.
    Max Memory :                                 8 GB
    Average Memory :                             4.84 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               24.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   492 sec.
    Turnaround time :                            35112 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n10>
Subject: Job 619895: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:28 2024
Job was executed on host(s) <4*c205n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:41:11 2024
                            <4*c205n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:41:11 2024
Terminated at Thu Dec  5 03:44:30 2024
Results reported at Thu Dec  5 03:44:30 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   211.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.67 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   199 sec.
    Turnaround time :                            35462 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n08>
Subject: Job 619898: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:30 2024
Job was executed on host(s) <4*c202n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:44:32 2024
                            <4*c202n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:44:32 2024
Terminated at Thu Dec  5 03:46:18 2024
Results reported at Thu Dec  5 03:46:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   158.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             1.33 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   116 sec.
    Turnaround time :                            35568 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n10>
Subject: Job 619902: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:31 2024
Job was executed on host(s) <4*c205n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:46:18 2024
                            <4*c205n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:46:18 2024
Terminated at Thu Dec  5 03:48:28 2024
Results reported at Thu Dec  5 03:48:28 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   243.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.71 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   142 sec.
    Turnaround time :                            35697 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c035n04>
Subject: Job 619903: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:32 2024
Job was executed on host(s) <4*c035n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:48:28 2024
                            <4*c035n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:48:28 2024
Terminated at Thu Dec  5 03:51:11 2024
Results reported at Thu Dec  5 03:51:11 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   127.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             2.25 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   189 sec.
    Turnaround time :                            35859 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c035n04>
Subject: Job 619906: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:34 2024
Job was executed on host(s) <4*c035n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:52:09 2024
                            <4*c035n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:52:09 2024
Terminated at Thu Dec  5 03:57:34 2024
Results reported at Thu Dec  5 03:57:34 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   163.17 sec.
    Max Memory :                                 5 GB
    Average Memory :                             2.69 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   353 sec.
    Turnaround time :                            36240 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619910: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:36 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:03:16 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:03:16 2024
Terminated at Thu Dec  5 04:05:25 2024
Results reported at Thu Dec  5 04:05:25 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   190.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.57 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   129 sec.
    Turnaround time :                            36709 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619911: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:36 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:05:26 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:05:26 2024
Terminated at Thu Dec  5 04:06:32 2024
Results reported at Thu Dec  5 04:06:32 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   118.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             0.60 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               29.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   67 sec.
    Turnaround time :                            36776 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n12>
Subject: Job 619907: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:34 2024
Job was executed on host(s) <4*c202n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:52:09 2024
                            <4*c202n08>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:52:09 2024
Terminated at Thu Dec  5 04:09:28 2024
Results reported at Thu Dec  5 04:09:28 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2274.10 sec.
    Max Memory :                                 6 GB
    Average Memory :                             4.51 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   1039 sec.
    Turnaround time :                            36954 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619914: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:39 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:12:38 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:12:38 2024
Terminated at Thu Dec  5 04:14:18 2024
Results reported at Thu Dec  5 04:14:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   233.45 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.67 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   102 sec.
    Turnaround time :                            37239 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619915: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:40 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:14:20 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:14:20 2024
Terminated at Thu Dec  5 04:15:36 2024
Results reported at Thu Dec  5 04:15:36 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   108.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.60 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   78 sec.
    Turnaround time :                            37316 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619918: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:43 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:26:29 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:26:29 2024
Terminated at Thu Dec  5 04:30:01 2024
Results reported at Thu Dec  5 04:30:01 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   144.41 sec.
    Max Memory :                                 4 GB
    Average Memory :                             2.00 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   212 sec.
    Turnaround time :                            38178 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619919: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:43 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:30:03 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:30:03 2024
Terminated at Thu Dec  5 04:31:29 2024
Results reported at Thu Dec  5 04:31:29 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   117.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             2.60 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   88 sec.
    Turnaround time :                            38266 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619922: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:46 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:35:36 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:35:36 2024
Terminated at Thu Dec  5 04:37:43 2024
Results reported at Thu Dec  5 04:37:43 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   201.19 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.43 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   127 sec.
    Turnaround time :                            38637 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619923: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:46 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:37:44 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:37:44 2024
Terminated at Thu Dec  5 04:39:23 2024
Results reported at Thu Dec  5 04:39:23 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   115.38 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.67 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   100 sec.
    Turnaround time :                            38737 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619926: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:48 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:42:39 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:42:39 2024
Terminated at Thu Dec  5 04:43:54 2024
Results reported at Thu Dec  5 04:43:54 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   167.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             2.80 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   76 sec.
    Turnaround time :                            39006 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619927: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:48 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:43:54 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:43:54 2024
Terminated at Thu Dec  5 04:45:06 2024
Results reported at Thu Dec  5 04:45:06 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   110.30 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.60 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   72 sec.
    Turnaround time :                            39078 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619930: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:50 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:48:48 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:48:48 2024
Terminated at Thu Dec  5 04:56:25 2024
Results reported at Thu Dec  5 04:56:25 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1645.66 sec.
    Max Memory :                                 6 GB
    Average Memory :                             4.33 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   458 sec.
    Turnaround time :                            39755 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619931: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:51 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 04:56:26 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 04:56:26 2024
Terminated at Thu Dec  5 04:58:15 2024
Results reported at Thu Dec  5 04:58:15 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   133.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             1.17 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   110 sec.
    Turnaround time :                            39864 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619934: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:52 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:03:45 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:03:45 2024
Terminated at Thu Dec  5 05:06:01 2024
Results reported at Thu Dec  5 05:06:01 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   297.39 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.43 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   137 sec.
    Turnaround time :                            40329 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619935: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:53 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:06:01 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:06:01 2024
Terminated at Thu Dec  5 05:12:44 2024
Results reported at Thu Dec  5 05:12:44 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1038.17 sec.
    Max Memory :                                 6 GB
    Average Memory :                             4.87 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   403 sec.
    Turnaround time :                            40731 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619939: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:55 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:20:44 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:20:44 2024
Terminated at Thu Dec  5 05:23:15 2024
Results reported at Thu Dec  5 05:23:15 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   128.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.00 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   151 sec.
    Turnaround time :                            41360 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n05>
Subject: Job 619938: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:54 2024
Job was executed on host(s) <4*c200n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:20:44 2024
                            <4*c200n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:20:44 2024
Terminated at Thu Dec  5 05:23:39 2024
Results reported at Thu Dec  5 05:23:39 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   208.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             2.75 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   204 sec.
    Turnaround time :                            41385 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619943: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:56 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:26:18 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:26:18 2024
Terminated at Thu Dec  5 05:28:33 2024
Results reported at Thu Dec  5 05:28:33 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   136.49 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.00 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   144 sec.
    Turnaround time :                            41677 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n05>
Subject: Job 619942: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:56 2024
Job was executed on host(s) <4*c200n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:26:18 2024
                            <4*c200n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:26:18 2024
Terminated at Thu Dec  5 05:28:57 2024
Results reported at Thu Dec  5 05:28:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   200.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.88 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   159 sec.
    Turnaround time :                            41701 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619946: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:57 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:28:57 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:28:57 2024
Terminated at Thu Dec  5 05:35:20 2024
Results reported at Thu Dec  5 05:35:20 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   384.18 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.80 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   404 sec.
    Turnaround time :                            42083 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n04>
Subject: Job 619947: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:57 2024
Job was executed on host(s) <4*c039n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:42:17 2024
                            <4*c039n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:42:17 2024
Terminated at Thu Dec  5 05:45:14 2024
Results reported at Thu Dec  5 05:45:14 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   179.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.00 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   177 sec.
    Turnaround time :                            42677 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n05>
Subject: Job 619951: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:00 2024
Job was executed on host(s) <4*c200n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:46:14 2024
                            <4*c200n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:46:14 2024
Terminated at Thu Dec  5 05:48:01 2024
Results reported at Thu Dec  5 05:48:01 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   132.53 sec.
    Max Memory :                                 4 GB
    Average Memory :                             1.33 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   109 sec.
    Turnaround time :                            42841 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n04>
Subject: Job 619950: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:59 2024
Job was executed on host(s) <4*c039n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:46:14 2024
                            <4*c039n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:46:14 2024
Terminated at Thu Dec  5 05:48:12 2024
Results reported at Thu Dec  5 05:48:12 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   224.22 sec.
    Max Memory :                                 4 GB
    Average Memory :                             2.50 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   148 sec.
    Turnaround time :                            42853 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 619954: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:02 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:48:53 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:48:53 2024
Terminated at Thu Dec  5 05:52:28 2024
Results reported at Thu Dec  5 05:52:28 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   404.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             2.00 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   226 sec.
    Turnaround time :                            43106 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n05>
Subject: Job 619955: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:02 2024
Job was executed on host(s) <4*c200n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 05:48:53 2024
                            <4*c200n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 05:48:53 2024
Terminated at Thu Dec  5 05:57:43 2024
Results reported at Thu Dec  5 05:57:43 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1600.18 sec.
    Max Memory :                                 7 GB
    Average Memory :                             4.35 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               25.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   549 sec.
    Turnaround time :                            43421 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n12>
Subject: Job 619959: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:05 2024
Job was executed on host(s) <4*c200n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 06:05:51 2024
                            <4*c200n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 06:05:51 2024
Terminated at Thu Dec  5 06:09:59 2024
Results reported at Thu Dec  5 06:09:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   173.44 sec.
    Max Memory :                                 6 GB
    Average Memory :                             1.09 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   269 sec.
    Turnaround time :                            44154 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n02>
Subject: Job 619958: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:04 2024
Job was executed on host(s) <4*c207n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 06:05:51 2024
                            <4*c207n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 06:05:51 2024
Terminated at Thu Dec  5 06:10:45 2024
Results reported at Thu Dec  5 06:10:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   554.58 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.08 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   315 sec.
    Turnaround time :                            44201 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n04>
Subject: Job 619962: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:07 2024
Job was executed on host(s) <4*c039n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 06:12:43 2024
                            <4*c039n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 06:12:43 2024
Terminated at Thu Dec  5 06:17:21 2024
Results reported at Thu Dec  5 06:17:21 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   284.00 sec.
    Max Memory :                                 6 GB
    Average Memory :                             3.58 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   279 sec.
    Turnaround time :                            44594 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n04>
Subject: Job 619963: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:08 2024
Job was executed on host(s) <4*c205n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 06:12:43 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 06:12:43 2024
Terminated at Thu Dec  5 06:17:52 2024
Results reported at Thu Dec  5 06:17:52 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   276.34 sec.
    Max Memory :                                 6 GB
    Average Memory :                             3.23 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   336 sec.
    Turnaround time :                            44624 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 619968: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:10 2024
Job was executed on host(s) <4*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 06:12:43 2024
                            <4*c203n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 06:12:43 2024
Terminated at Thu Dec  5 06:18:10 2024
Results reported at Thu Dec  5 06:18:10 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   197.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.00 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   337 sec.
    Turnaround time :                            44640 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 619967: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:09 2024
Job was executed on host(s) <4*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 06:12:43 2024
                            <4*c200n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 06:12:43 2024
Terminated at Thu Dec  5 06:29:42 2024
Results reported at Thu Dec  5 06:29:42 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2901.61 sec.
    Max Memory :                                 7 GB
    Average Memory :                             5.42 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               25.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   1049 sec.
    Turnaround time :                            45333 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n04>
Subject: Job 619971: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:12 2024
Job was executed on host(s) <4*c039n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 06:31:46 2024
                            <4*c036n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 06:31:46 2024
Terminated at Thu Dec  5 06:34:36 2024
Results reported at Thu Dec  5 06:34:36 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   497.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             2.12 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   194 sec.
    Turnaround time :                            45624 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c037n02>
Subject: Job 619972: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:12 2024
Job was executed on host(s) <4*c037n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 06:34:38 2024
                            <4*c039n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 06:34:38 2024
Terminated at Thu Dec  5 06:41:01 2024
Results reported at Thu Dec  5 06:41:01 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1300.10 sec.
    Max Memory :                                 10 GB
    Average Memory :                             7.13 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               22.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   386 sec.
    Turnaround time :                            46009 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c031n03>
Subject: Job 619975: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:13 2024
Job was executed on host(s) <4*c031n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 06:40:15 2024
                            <4*c039n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 06:40:15 2024
Terminated at Thu Dec  5 06:42:38 2024
Results reported at Thu Dec  5 06:42:38 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   318.01 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.14 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               27.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   145 sec.
    Turnaround time :                            46105 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n04>
Subject: Job 619976: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:14 2024
Job was executed on host(s) <4*c039n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 06:41:01 2024
                            <4*c037n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 06:41:01 2024
Terminated at Thu Dec  5 06:42:59 2024
Results reported at Thu Dec  5 06:42:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   197.05 sec.
    Max Memory :                                 4 GB
    Average Memory :                             2.33 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               28.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   118 sec.
    Turnaround time :                            46125 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Trimer
Filename: (ECFP3.count.512)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c021n04>
Subject: Job 627225: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:13 2024
Job was executed on host(s) <4*c021n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:53:36 2024
                            <4*c015n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 10:53:36 2024
Terminated at Thu Dec  5 12:25:05 2024
Results reported at Thu Dec  5 12:25:05 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   13548.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             5.83 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   5509 sec.
    Turnaround time :                            5992 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Dimer
Filename: (ECFP3.count.512)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c006n03>
Subject: Job 627221: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:13 2024
Job was executed on host(s) <4*c006n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:53:36 2024
                            <4*c005n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 10:53:36 2024
Terminated at Thu Dec  5 12:29:33 2024
Results reported at Thu Dec  5 12:29:33 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   14240.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             5.81 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   5764 sec.
    Turnaround time :                            6260 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Monomer
Filename: (ECFP3.count.512)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.count.512)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.count.512)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.count.512)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 627213: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:12 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:24 2024
                            <4*c205n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 10:45:24 2024
Terminated at Thu Dec  5 12:45:09 2024
Results reported at Thu Dec  5 12:45:09 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   25395.27 sec.
    Max Memory :                                 7 GB
    Average Memory :                             6.18 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               25.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   7199 sec.
    Turnaround time :                            7197 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Dimer
Filename: (ECFP3.count.512)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c006n02>
Subject: Job 627220: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:13 2024
Job was executed on host(s) <4*c006n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:53:36 2024
                            <4*c006n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 10:53:36 2024
Terminated at Thu Dec  5 13:52:27 2024
Results reported at Thu Dec  5 13:52:27 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   31559.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             6.85 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   10734 sec.
    Turnaround time :                            11234 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Monomer
Filename: (ECFP3.count.512)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n02>
Subject: Job 627215: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:12 2024
Job was executed on host(s) <4*c207n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:51:35 2024
                            <4*c207n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 10:51:35 2024
Terminated at Thu Dec  5 14:30:42 2024
Results reported at Thu Dec  5 14:30:42 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   39608.00 sec.
    Max Memory :                                 6 GB
    Average Memory :                             4.54 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   13153 sec.
    Turnaround time :                            13530 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Monomer
Filename: (ECFP3.count.512)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.count.512)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.count.512)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.count.512)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n13>
Subject: Job 627228: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:13 2024
Job was executed on host(s) <4*c200n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 12:34:03 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 12:34:03 2024
Terminated at Thu Dec  5 14:42:15 2024
Results reported at Thu Dec  5 14:42:15 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   22947.04 sec.
    Max Memory :                                 6 GB
    Average Memory :                             4.43 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   7698 sec.
    Turnaround time :                            14222 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Trimer
Filename: (ECFP3.count.512)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c016n04>
Subject: Job 627224: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:13 2024
Job was executed on host(s) <4*c016n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:53:36 2024
                            <4*c015n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 10:53:36 2024
Terminated at Thu Dec  5 15:08:55 2024
Results reported at Thu Dec  5 15:08:55 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   47535.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             7.49 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   15320 sec.
    Turnaround time :                            15822 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Monomer
Filename: (ECFP3.count.512)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c031n03>
Subject: Job 627229: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:13 2024
Job was executed on host(s) <4*c031n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 12:44:24 2024
                            <4*c028n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 12:44:24 2024
Terminated at Thu Dec  5 15:17:16 2024
Results reported at Thu Dec  5 15:17:16 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   27023.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             7.13 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   9180 sec.
    Turnaround time :                            16323 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Dimer
Filename: (ECFP3.count.512)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c038n02>
Subject: Job 627233: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:14 2024
Job was executed on host(s) <4*c038n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 15:32:42 2024
                            <4*c030n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 15:32:42 2024
Terminated at Thu Dec  5 16:53:31 2024
Results reported at Thu Dec  5 16:53:31 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   17722.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             7.78 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   4879 sec.
    Turnaround time :                            22097 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Trimer
Filename: (ECFP3.count.512)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP3.count.512)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n04>
Subject: Job 627237: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:14 2024
Job was executed on host(s) <4*c005n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 17:32:27 2024
                            <4*c006n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 17:32:27 2024
Terminated at Thu Dec  5 18:19:17 2024
Results reported at Thu Dec  5 18:19:17 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   10808.50 sec.
    Max Memory :                                 9 GB
    Average Memory :                             7.82 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   2838 sec.
    Turnaround time :                            27243 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Dimer
Filename: (ECFP3.count.512)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c028n03>
Subject: Job 627232: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:14 2024
Job was executed on host(s) <4*c028n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 15:31:42 2024
                            <4*c038n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 15:31:42 2024
Terminated at Thu Dec  5 18:51:38 2024
Results reported at Thu Dec  5 18:51:38 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   44746.51 sec.
    Max Memory :                                 9 GB
    Average Memory :                             8.35 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   11998 sec.
    Turnaround time :                            29184 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Trimer
Filename: (ECFP4.count.1024)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n13>
Subject: Job 627249: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:15 2024
Job was executed on host(s) <4*c207n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 18:17:09 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 18:17:09 2024
Terminated at Thu Dec  5 20:44:13 2024
Results reported at Thu Dec  5 20:44:13 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   21412.00 sec.
    Max Memory :                                 6 GB
    Average Memory :                             4.55 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   8852 sec.
    Turnaround time :                            35938 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Dimer
Filename: (ECFP4.count.1024)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n01>
Subject: Job 627245: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:15 2024
Job was executed on host(s) <4*c039n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 18:15:08 2024
                            <4*c033n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 18:15:08 2024
Terminated at Thu Dec  5 21:17:14 2024
Results reported at Thu Dec  5 21:17:14 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   26664.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             6.84 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   10926 sec.
    Turnaround time :                            37919 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Dimer
Filename: (ECFP4.count.1024)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c034n01>
Subject: Job 627257: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:16 2024
Job was executed on host(s) <4*c034n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 18:25:19 2024
                            <4*c028n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 18:25:19 2024
Terminated at Thu Dec  5 21:19:25 2024
Results reported at Thu Dec  5 21:19:25 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   27531.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             6.90 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   10472 sec.
    Turnaround time :                            38049 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Monomer
Filename: (ECFP4.count.1024)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Monomer
Filename: (ECFP4.count.1024)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n10>
Subject: Job 627241: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:14 2024
Job was executed on host(s) <4*c207n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 18:15:08 2024
                            <4*c207n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 18:15:08 2024
Terminated at Thu Dec  5 21:56:46 2024
Results reported at Thu Dec  5 21:56:46 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   32211.00 sec.
    Max Memory :                                 6 GB
    Average Memory :                             4.92 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   13322 sec.
    Turnaround time :                            40292 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n05>
Subject: Job 627253: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:15 2024
Job was executed on host(s) <4*c203n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 18:18:28 2024
                            <4*c203n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 18:18:28 2024
Terminated at Thu Dec  5 21:56:47 2024
Results reported at Thu Dec  5 21:56:47 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   29942.49 sec.
    Max Memory :                                 6 GB
    Average Memory :                             4.88 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   13107 sec.
    Turnaround time :                            40292 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Monomer
Filename: (ECFP4.count.1024)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n12>
Subject: Job 627252: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:15 2024
Job was executed on host(s) <4*c203n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 18:18:28 2024
                            <4*c203n07>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 18:18:28 2024
Terminated at Thu Dec  5 22:33:16 2024
Results reported at Thu Dec  5 22:33:16 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   50298.09 sec.
    Max Memory :                                 6 GB
    Average Memory :                             5.43 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   15308 sec.
    Turnaround time :                            42481 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Monomer
Filename: (ECFP4.count.1024)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 627240: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:14 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 18:15:08 2024
                            <4*c205n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 18:15:08 2024
Terminated at Thu Dec  5 22:47:17 2024
Results reported at Thu Dec  5 22:47:17 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   48475.00 sec.
    Max Memory :                                 7 GB
    Average Memory :                             5.89 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               25.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   16359 sec.
    Turnaround time :                            43323 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Trimer
Filename: (ECFP3.count.512)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP3.count.512)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c030n04>
Subject: Job 627236: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:14 2024
Job was executed on host(s) <4*c030n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 16:53:31 2024
                            <4*c038n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 16:53:31 2024
Terminated at Thu Dec  5 23:08:17 2024
Results reported at Thu Dec  5 23:08:17 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 3                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   85603.23 sec.
    Max Memory :                                 9 GB
    Average Memory :                             8.37 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   22486 sec.
    Turnaround time :                            44583 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Trimer
Filename: (ECFP4.count.1024)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP4.count.1024)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n03>
Subject: Job 627261: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:16 2024
Job was executed on host(s) <4*c207n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 22:09:52 2024
                            <4*c207n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 22:09:52 2024
Terminated at Thu Dec  5 23:35:11 2024
Results reported at Thu Dec  5 23:35:11 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   21049.01 sec.
    Max Memory :                                 6 GB
    Average Memory :                             5.66 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   5140 sec.
    Turnaround time :                            46195 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Dimer
Filename: (ECFP4.count.1024)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c028n02>
Subject: Job 627244: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:15 2024
Job was executed on host(s) <4*c028n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 18:15:08 2024
                            <4*c034n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 18:15:08 2024
Terminated at Fri Dec  6 00:51:05 2024
Results reported at Fri Dec  6 00:51:05 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   79850.28 sec.
    Max Memory :                                 10 GB
    Average Memory :                             8.03 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               22.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   23756 sec.
    Turnaround time :                            50750 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Monomer
Filename: (ECFP5.count.2048)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n08>
Subject: Job 627265: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:16 2024
Job was executed on host(s) <4*c202n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 22:47:19 2024
                            <4*c202n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 22:47:19 2024
Terminated at Fri Dec  6 01:51:22 2024
Results reported at Fri Dec  6 01:51:22 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   44994.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             7.71 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   11072 sec.
    Turnaround time :                            54366 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Dimer
Filename: (ECFP4.count.1024)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n08>
Subject: Job 627256: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:16 2024
Job was executed on host(s) <4*c203n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 18:23:11 2024
                            <4*c203n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 18:23:11 2024
Terminated at Fri Dec  6 03:18:52 2024
Results reported at Fri Dec  6 03:18:52 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   115231.00 sec.
    Max Memory :                                 6 GB
    Average Memory :                             5.57 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   32151 sec.
    Turnaround time :                            59616 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Dimer
Filename: (ECFP5.count.2048)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c038n03>
Subject: Job 627269: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:17 2024
Job was executed on host(s) <4*c038n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 00:21:49 2024
                            <4*c036n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 00:21:49 2024
Terminated at Fri Dec  6 03:46:28 2024
Results reported at Fri Dec  6 03:46:28 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   46833.00 sec.
    Max Memory :                                 11 GB
    Average Memory :                             10.04 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               21.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   12279 sec.
    Turnaround time :                            61271 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Trimer
Filename: (ECFP5.count.2048)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n10>
Subject: Job 627273: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:17 2024
Job was executed on host(s) <4*c202n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 01:51:23 2024
                            <4*c202n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 01:51:23 2024
Terminated at Fri Dec  6 04:33:40 2024
Results reported at Fri Dec  6 04:33:40 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   40209.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             7.74 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   9740 sec.
    Turnaround time :                            64103 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Monomer
Filename: (ECFP5.count.2048)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n07>
Subject: Job 627264: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:16 2024
Job was executed on host(s) <4*c202n07>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 22:47:19 2024
                            <4*c202n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 22:47:19 2024
Terminated at Fri Dec  6 05:20:40 2024
Results reported at Fri Dec  6 05:20:40 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   94941.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             7.98 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   23626 sec.
    Turnaround time :                            66924 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Trimer
Filename: (ECFP4.count.1024)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n03>
Subject: Job 627248: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:15 2024
Job was executed on host(s) <4*c202n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 18:17:09 2024
                            <4*c202n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 18:17:09 2024
Terminated at Fri Dec  6 06:07:31 2024
Results reported at Fri Dec  6 06:07:31 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   133727.25 sec.
    Max Memory :                                 7 GB
    Average Memory :                             6.55 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               25.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   42650 sec.
    Turnaround time :                            69736 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Trimer
Filename: (ECFP4.count.1024)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP4.count.1024)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n02>
Subject: Job 627260: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:16 2024
Job was executed on host(s) <4*c207n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 22:09:52 2024
                            <4*c207n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 22:09:52 2024
Terminated at Fri Dec  6 06:25:02 2024
Results reported at Fri Dec  6 06:25:02 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 4                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   94134.41 sec.
    Max Memory :                                 6 GB
    Average Memory :                             5.90 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               26.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   29715 sec.
    Turnaround time :                            70786 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Monomer
Filename: (ECFP5.count.2048)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n03>
Subject: Job 627276: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:18 2024
Job was executed on host(s) <4*c207n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 01:51:23 2024
                            <4*c207n08>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 01:51:23 2024
Terminated at Fri Dec  6 07:56:04 2024
Results reported at Fri Dec  6 07:56:04 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   91784.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             6.87 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               24.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   21884 sec.
    Turnaround time :                            76246 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Monomer
Filename: (ECFP5.count.2048)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n02>
Subject: Job 627277: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:18 2024
Job was executed on host(s) <4*c207n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 04:55:12 2024
                            <4*c207n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 04:55:12 2024
Terminated at Fri Dec  6 08:04:34 2024
Results reported at Fri Dec  6 08:04:34 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   46000.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             6.66 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               24.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   11375 sec.
    Turnaround time :                            76756 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Trimer
Filename: (ECFP5.count.2048)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c017n02>
Subject: Job 627285: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:19 2024
Job was executed on host(s) <4*c017n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 06:18:17 2024
                            <4*c025n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 06:18:17 2024
Terminated at Fri Dec  6 08:59:45 2024
Results reported at Fri Dec  6 08:59:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   33905.00 sec.
    Max Memory :                                 11 GB
    Average Memory :                             9.50 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               21.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   9715 sec.
    Turnaround time :                            80066 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Dimer
Filename: (ECFP5.count.2048)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP5.count.2048)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n12>
Subject: Job 627281: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:18 2024
Job was executed on host(s) <4*c205n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 05:41:54 2024
                            <4*c205n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 05:41:54 2024
Terminated at Fri Dec  6 09:25:42 2024
Results reported at Fri Dec  6 09:25:42 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   51282.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             7.77 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               23.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   13454 sec.
    Turnaround time :                            81624 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Dimer
Filename: (ECFP6.count.4096)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c024n01>
Subject: Job 627293: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:20 2024
Job was executed on host(s) <4*c024n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 06:18:17 2024
                            <4*c019n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 06:18:17 2024
Terminated at Fri Dec  6 10:09:45 2024
Results reported at Fri Dec  6 10:09:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   58282.00 sec.
    Max Memory :                                 14 GB
    Average Memory :                             11.74 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               18.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   13915 sec.
    Turnaround time :                            84265 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Dimer
Filename: (ECFP5.count.2048)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c038n02>
Subject: Job 627268: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:17 2024
Job was executed on host(s) <4*c038n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 23:08:19 2024
                            <4*c030n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 23:08:19 2024
Terminated at Fri Dec  6 10:13:42 2024
Results reported at Fri Dec  6 10:13:42 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   144805.00 sec.
    Max Memory :                                 11 GB
    Average Memory :                             10.33 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               21.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   39895 sec.
    Turnaround time :                            84505 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Monomer
Filename: (ECFP6.count.4096)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c025n01>
Subject: Job 627289: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:19 2024
Job was executed on host(s) <4*c025n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 06:18:17 2024
                            <4*c020n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 06:18:17 2024
Terminated at Fri Dec  6 10:24:10 2024
Results reported at Fri Dec  6 10:24:10 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   62849.00 sec.
    Max Memory :                                 14 GB
    Average Memory :                             11.76 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               18.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   14780 sec.
    Turnaround time :                            85131 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Trimer
Filename: (ECFP5.count.2048)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n12>
Subject: Job 627272: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:17 2024
Job was executed on host(s) <4*c203n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 01:33:55 2024
                            <4*c203n07>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 01:33:55 2024
Terminated at Fri Dec  6 11:37:06 2024
Results reported at Fri Dec  6 11:37:06 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   130674.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             6.97 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               24.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   36196 sec.
    Turnaround time :                            89509 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Trimer
Filename: (ECFP5.count.2048)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c016n01>
Subject: Job 627284: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:19 2024
Job was executed on host(s) <4*c016n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 06:17:16 2024
                            <4*c018n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 06:17:16 2024
Terminated at Fri Dec  6 12:04:54 2024
Results reported at Fri Dec  6 12:04:54 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   90654.00 sec.
    Max Memory :                                 11 GB
    Average Memory :                             10.20 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               21.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   20872 sec.
    Turnaround time :                            91175 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Monomer
Filename: (ECFP6.count.4096)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c024n03>
Subject: Job 627288: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:19 2024
Job was executed on host(s) <4*c024n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 06:18:17 2024
                            <4*c022n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 06:18:17 2024
Terminated at Fri Dec  6 14:22:27 2024
Results reported at Fri Dec  6 14:22:27 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   121311.00 sec.
    Max Memory :                                 14 GB
    Average Memory :                             12.45 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               18.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   29077 sec.
    Turnaround time :                            99428 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Trimer
Filename: (ECFP6.count.4096)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n09>
Subject: Job 627297: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:20 2024
Job was executed on host(s) <4*c202n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 06:28:29 2024
                            <4*c202n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 06:28:29 2024
Terminated at Fri Dec  6 14:27:45 2024
Results reported at Fri Dec  6 14:27:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   118566.00 sec.
    Max Memory :                                 12 GB
    Average Memory :                             9.64 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               20.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   28781 sec.
    Turnaround time :                            99745 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Trimer
Filename: (ECFP6.count.4096)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n13>
Subject: Job 627296: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:20 2024
Job was executed on host(s) <4*c207n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 06:25:02 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 06:25:02 2024
Terminated at Fri Dec  6 23:10:57 2024
Results reported at Fri Dec  6 23:10:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   233131.00 sec.
    Max Memory :                                 11 GB
    Average Memory :                             8.62 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               21.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   60355 sec.
    Turnaround time :                            131137 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Dimer
Filename: (ECFP6.count.4096)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c038n01>
Subject: Job 627305: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:21 2024
Job was executed on host(s) <4*c038n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 18:51:06 2024
                            <4*c038n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 18:51:06 2024
Terminated at Fri Dec  6 23:34:38 2024
Results reported at Fri Dec  6 23:34:38 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   70924.00 sec.
    Max Memory :                                 14 GB
    Average Memory :                             11.56 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               18.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   17023 sec.
    Turnaround time :                            132557 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Monomer
Filename: (ECFP6.count.4096)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c021n03>
Subject: Job 627301: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:20 2024
Job was executed on host(s) <4*c021n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 18:51:06 2024
                            <4*c014n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 18:51:06 2024
Terminated at Fri Dec  6 23:49:15 2024
Results reported at Fri Dec  6 23:49:15 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   72321.00 sec.
    Max Memory :                                 14 GB
    Average Memory :                             11.68 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               18.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   17900 sec.
    Turnaround time :                            133435 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Trimer
Filename: (ECFP6.count.4096)_GPR.rbf_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP6.count.4096)_GPR.rbf_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n12>
Subject: Job 627309: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:22 2024
Job was executed on host(s) <4*c205n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 18:51:06 2024
                            <4*c205n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 18:51:06 2024
Terminated at Sat Dec  7 00:51:02 2024
Results reported at Sat Dec  7 00:51:02 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "rbf"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   79571.00 sec.
    Max Memory :                                 11 GB
    Average Memory :                             9.11 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               21.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   21614 sec.
    Turnaround time :                            137140 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Dimer
Filename: (ECFP5.count.2048)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP5.count.2048)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n05>
Subject: Job 627280: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:18 2024
Job was executed on host(s) <4*c207n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 05:38:42 2024
                            <4*c207n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 05:38:42 2024
Terminated at Sat Dec  7 01:44:32 2024
Results reported at Sat Dec  7 01:44:32 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 5                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   295886.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             6.90 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               24.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   72369 sec.
    Turnaround time :                            140354 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Monomer
Filename: (ECFP6.count.4096)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c037n02>
Subject: Job 627300: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:20 2024
Job was executed on host(s) <4*c037n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 18:15:14 2024
                            <4*c034n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 18:15:14 2024
Terminated at Sat Dec  7 03:26:10 2024
Results reported at Sat Dec  7 03:26:10 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Monomer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   141993.00 sec.
    Max Memory :                                 15 GB
    Average Memory :                             12.61 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               17.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   33057 sec.
    Turnaround time :                            146450 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
Dimer
Filename: (ECFP6.count.4096)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c021n04>
Subject: Job 627292: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:19 2024
Job was executed on host(s) <4*c021n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 06:18:17 2024
                            <4*c018n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 06:18:17 2024
Terminated at Sat Dec  7 07:23:04 2024
Results reported at Sat Dec  7 07:23:04 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   307855.00 sec.
    Max Memory :                                 11 GB
    Average Memory :                             8.54 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               21.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   90314 sec.
    Turnaround time :                            160665 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Trimer
Filename: (ECFP6.count.4096)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n08>
Subject: Job 627308: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:21 2024
Job was executed on host(s) <4*c202n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 18:51:06 2024
                            <4*c202n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 18:51:06 2024
Terminated at Sat Dec  7 10:20:27 2024
Results reported at Sat Dec  7 10:20:27 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Trimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   235200.00 sec.
    Max Memory :                                 12 GB
    Average Memory :                             9.81 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               20.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   55761 sec.
    Turnaround time :                            171306 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.390.16	 r2: 0.130.14
RRU Dimer
Filename: (ECFP6.count.4096)_GPR.matern_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(ECFP6.count.4096)_GPR.matern_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c020n04>
Subject: Job 627304: <ecfp_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Done

Job <ecfp_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c031n03> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:45:21 2024
Job was executed on host(s) <4*c020n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri Dec  6 18:51:06 2024
                            <4*c021n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri Dec  6 18:51:06 2024
Terminated at Sat Dec  7 19:42:07 2024
Results reported at Sat Dec  7 19:42:07 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "ecfp_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py ecfp --regressor_type GPR                                        --kernel "matern"                                        --radius 6                                        --vector count                                        --target "Rg1 (nm)"                                        --oligo_type "RRU Dimer"                                        --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   369533.00 sec.
    Max Memory :                                 11 GB
    Average Memory :                             9.08 GB
    Total Requested Memory :                     32.00 GB
    Delta Memory :                               21.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   89464 sec.
    Turnaround time :                            205006 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

