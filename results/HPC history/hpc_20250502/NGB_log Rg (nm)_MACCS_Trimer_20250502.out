


OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 96), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01682391565700581), ('regressor__regressor__n_estimators', 448), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020004168804276495), ('regressor__regressor__n_estimators', 1891), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.009649263746069905), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.017476564810755303), ('regressor__regressor__n_estimators', 1959), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.007112366178450431), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0064069043948089335), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09635269256555729), ('regressor__regressor__n_estimators', 1286), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001062027724562034), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04176125638686122), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00840653263123084), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07693173041362837), ('regressor__regressor__n_estimators', 290), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.004184475212836049), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07693173041362837), ('regressor__regressor__n_estimators', 290), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.004184475212836049), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09952537737704825), ('regressor__regressor__n_estimators', 1087), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00011959421638584355), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 277), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.053297898050774735), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07146911519843621), ('regressor__regressor__n_estimators', 1412), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0054499217845547275), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0027553675806529674), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020561730470258936), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09939793013929618), ('regressor__regressor__n_estimators', 1351), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0029011736234682817), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 1021), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0026586290750428665), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.009854782672762111), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.060140540252401946), ('regressor__regressor__n_estimators', 1870), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001147602839085674), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09745251308734482), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.009185905394486424), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02470120216932634), ('regressor__regressor__n_estimators', 598), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 403), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0681886386818438), ('regressor__regressor__n_estimators', 155), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06570254082814599), ('regressor__regressor__n_estimators', 144), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0527631593943281), ('regressor__regressor__n_estimators', 145), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00855904457269526), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0002700962765296116), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.056445258854969545), ('regressor__regressor__n_estimators', 913), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.036984235225905486), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0033215141150121385), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])


Average scores:	 r: 0.84±0.07	 r2: 0.69±0.12
Filename: (MACCS-concentration-temperature-solvent dP-solvent dD-solvent dH)_NGB_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n10>
Subject: Job 646905: <NGB_log Rg (nm)_MACCS_Trimer_20250502> in cluster <Hazel> Done

Job <NGB_log Rg (nm)_MACCS_Trimer_20250502> was submitted from host <c205n12> by user <sdehgha2> in cluster <Hazel> at Fri May  2 09:47:45 2025
Job was executed on host(s) <6*c203n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Fri May  2 09:47:47 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Fri May  2 09:47:47 2025
Terminated at Fri May  2 15:28:03 2025
Results reported at Fri May  2 15:28:03 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 6
#BSUB -W 40:05
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "NGB_log Rg (nm)_MACCS_Trimer_20250502"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250502/NGB_log Rg (nm)_MACCS_Trimer_20250502.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250502/NGB_log Rg (nm)_MACCS__Trimer_20250502.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py --target_features "log Rg (nm)"                                       --representation "MACCS"                                       --regressor_type "NGB"                                       --oligomer_representation "Trimer"                                       --numerical_feats 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' 'solvent dP' 'solvent dD' 'solvent dH' 




------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   88541.34 sec.
    Max Memory :                                 2 GB
    Average Memory :                             2.00 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               14.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   20443 sec.
    Turnaround time :                            20418 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250502/NGB_log Rg (nm)_MACCS__Trimer_20250502.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026832499653323565), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08646479880356304), ('regressor__regressor__n_estimators', 918), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0005053971397801153), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0353193535510676), ('regressor__regressor__n_estimators', 442), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07286955726797978), ('regressor__regressor__n_estimators', 1394), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03243007648536063), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07693173041362837), ('regressor__regressor__n_estimators', 290), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.004184475212836049), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.034769668589043685), ('regressor__regressor__n_estimators', 910), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0005733412006678069), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02132135928305193), ('regressor__regressor__n_estimators', 777), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001351353171697879), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0588097799103768), ('regressor__regressor__n_estimators', 309), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.018449924493270414), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09963327573797505), ('regressor__regressor__n_estimators', 512), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.004619669483482056), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06999280668539078), ('regressor__regressor__n_estimators', 346), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001421993924174433), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.059656560958713106), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04557118193518429), ('regressor__regressor__n_estimators', 1233), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 1400), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0564701517117167), ('regressor__regressor__n_estimators', 1991), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.002899434009735149), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 504), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0588817588159012), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.028116422355794795), ('regressor__regressor__n_estimators', 1179), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001022099901948608), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09305234206473378), ('regressor__regressor__n_estimators', 1962), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00011587004077345913), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03510354933231233), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 951), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.019772052326399267), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07138291593528936), ('regressor__regressor__n_estimators', 388), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03366672813099007), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03967273933946009), ('regressor__regressor__n_estimators', 627), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.054776802736944526), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__n_estimators', 942), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.010145311843854138), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0287369637754504), ('regressor__regressor__n_estimators', 782), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.01), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09894093017872352), ('regressor__regressor__n_estimators', 348), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00014338373288354673), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.030858029872242218), ('regressor__regressor__n_estimators', 1128), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.029465635951720526), ('regressor__regressor__n_estimators', 1280), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0001), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09764039407470008), ('regressor__regressor__n_estimators', 1992), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00010913168330612965), ('regressor__regressor__verbose', False)])


Average scores:	 r: 0.94±0.05	 r2: 0.88±0.1
Filename: (MACCS-Xn-Mw-PDI-concentration-temperature-solvent dP-solvent dD-solvent dH)_NGB_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c026n01>
Subject: Job 806626: <NGB_log Rg (nm)_MACCS_Trimer_20250502> in cluster <Hazel> Done

Job <NGB_log Rg (nm)_MACCS_Trimer_20250502> was submitted from host <c004n03> by user <sdehgha2> in cluster <Hazel> at Wed May  7 09:53:04 2025
Job was executed on host(s) <6*c026n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed May  7 10:20:30 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed May  7 10:20:30 2025
Terminated at Wed May  7 15:42:51 2025
Results reported at Wed May  7 15:42:51 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 6
#BSUB -W 40:05
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "NGB_log Rg (nm)_MACCS_Trimer_20250502"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250502/NGB_log Rg (nm)_MACCS_Trimer_20250502.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250502/NGB_log Rg (nm)_MACCS__Trimer_20250502.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py --target_features "log Rg (nm)"                                       --representation "MACCS"                                       --regressor_type "NGB"                                       --oligomer_representation "Trimer"                                       --numerical_feats  'Xn' 'Mw (g/mol)' 'PDI' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' 'solvent dP' 'solvent dD' 'solvent dH' 




------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   84167.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.99 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               13.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   19358 sec.
    Turnaround time :                            20987 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250502/NGB_log Rg (nm)_MACCS__Trimer_20250502.err> for stderr output of this job.

