
------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n01>
Subject: Job 511216: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:15 2024
Job was executed on host(s) <4*c205n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:26:42 2024
                            <4*c205n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:26:42 2024
Terminated at Tue Nov 19 19:27:49 2024
Results reported at Tue Nov 19 19:27:49 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model mordred --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "Monomer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   5.40 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   68 sec.
    Turnaround time :                            154 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n03>
Subject: Job 511219: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:15 2024
Job was executed on host(s) <4*c202n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:27:04 2024
                            <4*c202n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:27:04 2024
Terminated at Tue Nov 19 19:28:24 2024
Results reported at Tue Nov 19 19:28:24 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model mordred --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "RRU Monomer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   6.96 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   80 sec.
    Turnaround time :                            189 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n01>
Subject: Job 511217: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:15 2024
Job was executed on host(s) <4*c202n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:27:01 2024
                            <4*c202n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:27:01 2024
Terminated at Tue Nov 19 19:28:29 2024
Results reported at Tue Nov 19 19:28:29 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model mordred --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "Dimer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   4.13 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   90 sec.
    Turnaround time :                            194 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n14>
Subject: Job 511218: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:15 2024
Job was executed on host(s) <4*c207n14>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:27:01 2024
                            <4*c207n07>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:27:01 2024
Terminated at Tue Nov 19 19:28:45 2024
Results reported at Tue Nov 19 19:28:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model mordred --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "Trimer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   5.68 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   106 sec.
    Turnaround time :                            210 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n08>
Subject: Job 511224: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:16 2024
Job was executed on host(s) <4*c200n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:27:40 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:27:40 2024
Terminated at Tue Nov 19 19:29:04 2024
Results reported at Tue Nov 19 19:29:04 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model maccs --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "Trimer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   4.05 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   84 sec.
    Turnaround time :                            228 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n12>
Subject: Job 511222: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:16 2024
Job was executed on host(s) <4*c207n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:27:24 2024
                            <4*c207n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:27:24 2024
Terminated at Tue Nov 19 19:29:15 2024
Results reported at Tue Nov 19 19:29:15 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model maccs --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "Monomer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   5.59 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   117 sec.
    Turnaround time :                            239 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n11>
Subject: Job 511220: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:15 2024
Job was executed on host(s) <4*c205n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:27:08 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:27:08 2024
Terminated at Tue Nov 19 19:29:22 2024
Results reported at Tue Nov 19 19:29:22 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model mordred --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "RRU Dimer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   9.23 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   143 sec.
    Turnaround time :                            247 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n10>
Subject: Job 511223: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:16 2024
Job was executed on host(s) <4*c200n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:27:31 2024
                            <4*c200n05>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:27:31 2024
Terminated at Tue Nov 19 19:29:24 2024
Results reported at Tue Nov 19 19:29:24 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model maccs --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "Dimer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   4.14 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   115 sec.
    Turnaround time :                            248 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n04>
Subject: Job 511221: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:16 2024
Job was executed on host(s) <4*c202n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:27:23 2024
                            <4*c202n11>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:27:23 2024
Terminated at Tue Nov 19 19:29:25 2024
Results reported at Tue Nov 19 19:29:25 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model mordred --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "RRU Trimer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   7.44 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   122 sec.
    Turnaround time :                            249 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n01>
Subject: Job 511225: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:16 2024
Job was executed on host(s) <4*c205n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:27:51 2024
                            <4*c205n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:27:51 2024
Terminated at Tue Nov 19 19:29:34 2024
Results reported at Tue Nov 19 19:29:34 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model maccs --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "RRU Monomer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   5.23 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   105 sec.
    Turnaround time :                            258 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n11>
Subject: Job 511226: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:16 2024
Job was executed on host(s) <4*c202n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:28:24 2024
                            <4*c202n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:28:24 2024
Terminated at Tue Nov 19 19:29:57 2024
Results reported at Tue Nov 19 19:29:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model maccs --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "RRU Dimer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   7.42 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   94 sec.
    Turnaround time :                            281 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n01>
Subject: Job 511227: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Exited

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c035n04> by user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:25:16 2024
Job was executed on host(s) <4*c202n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov 19 19:28:29 2024
                            <4*c202n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Nov 19 19:28:29 2024
Terminated at Tue Nov 19 19:30:14 2024
Results reported at Tue Nov 19 19:30:14 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py --model maccs --regressor_type XGBR --target "Rh (IW avg log)" --oligo_type "RRU Trimer" --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   4.21 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   105 sec.
    Turnaround time :                            298 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 90), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.057195635442128155), ('regressor__regressor__max_depth', 48), ('regressor__regressor__n_estimators', 139), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 20), ('regressor__regressor__n_estimators', 90), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010128292123380018), ('regressor__regressor__max_depth', 120), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.003932009561973325), ('regressor__regressor__max_depth', 1689), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009337376268518058), ('regressor__regressor__max_depth', 2167), ('regressor__regressor__n_estimators', 344), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008024420390581061), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 183), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 80), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009103865799915442), ('regressor__regressor__max_depth', 3150), ('regressor__regressor__n_estimators', 848), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09981487044124525), ('regressor__regressor__max_depth', 313), ('regressor__regressor__n_estimators', 374), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009355077351287489), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 932), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 14), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007742203083020234), ('regressor__regressor__max_depth', 21), ('regressor__regressor__n_estimators', 123), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 6224), ('regressor__regressor__n_estimators', 108), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026603996052334742), ('regressor__regressor__max_depth', 21), ('regressor__regressor__n_estimators', 326), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0076511853490285235), ('regressor__regressor__max_depth', 5322), ('regressor__regressor__n_estimators', 345), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.033346434195597685), ('regressor__regressor__max_depth', 1402), ('regressor__regressor__n_estimators', 87), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004449725018085163), ('regressor__regressor__max_depth', 3869), ('regressor__regressor__n_estimators', 445), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09799816489511388), ('regressor__regressor__max_depth', 1381), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.015023773308087641), ('regressor__regressor__max_depth', 316), ('regressor__regressor__n_estimators', 429), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004711699857959211), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1791), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004711699857959211), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1791), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014131439790967832), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 672), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001051722623041562), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1993), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013989885884054077), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 596), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020987276405089007), ('regressor__regressor__max_depth', 685), ('regressor__regressor__n_estimators', 465), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03129462738933014), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020987276405089007), ('regressor__regressor__max_depth', 685), ('regressor__regressor__n_estimators', 465), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 82), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05147843384617103), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 168), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 80), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05147843384617103), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 168), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.012091557115976577), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 699), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09536663039284317), ('regressor__regressor__max_depth', 124), ('regressor__regressor__n_estimators', 73), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.02±0.05	 r2: -0.06±0.06
Dimer
Filename: (MACCS)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Dimer/(MACCS)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Dimer/(MACCS)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Dimer/(MACCS)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n05>
Subject: Job 547862: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c207n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 11:06:30 2024
                            <4*c207n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 11:06:30 2024
Terminated at Sun Nov 24 11:50:53 2024
Results reported at Sun Nov 24 11:50:53 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "Dimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   6486.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.66 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   2689 sec.
    Turnaround time :                            6390 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 90), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.057363573323149876), ('regressor__regressor__max_depth', 48), ('regressor__regressor__n_estimators', 140), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 20), ('regressor__regressor__n_estimators', 90), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010128292123380018), ('regressor__regressor__max_depth', 120), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.003933382753926422), ('regressor__regressor__max_depth', 1689), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009337376268518058), ('regressor__regressor__max_depth', 2167), ('regressor__regressor__n_estimators', 344), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008024420390581061), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 183), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 80), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009103865799915442), ('regressor__regressor__max_depth', 3150), ('regressor__regressor__n_estimators', 848), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020493949433424117), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 394), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 78), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 3489), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0017089022196515897), ('regressor__regressor__max_depth', 12), ('regressor__regressor__n_estimators', 604), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 3815), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026623899718778894), ('regressor__regressor__max_depth', 21), ('regressor__regressor__n_estimators', 326), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.033346434195597685), ('regressor__regressor__max_depth', 1402), ('regressor__regressor__n_estimators', 87), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.033346434195597685), ('regressor__regressor__max_depth', 1402), ('regressor__regressor__n_estimators', 87), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005910777736527285), ('regressor__regressor__max_depth', 710), ('regressor__regressor__n_estimators', 330), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09799816489511388), ('regressor__regressor__max_depth', 1381), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.015023773308087641), ('regressor__regressor__max_depth', 316), ('regressor__regressor__n_estimators', 429), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09999731394071319), ('regressor__regressor__max_depth', 906), ('regressor__regressor__n_estimators', 115), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004711699857959211), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1791), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014102175618005336), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 674), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001051722623041562), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1993), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004435962283289924), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020987276405089007), ('regressor__regressor__max_depth', 685), ('regressor__regressor__n_estimators', 465), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01490463041121351), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 108), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 7244), ('regressor__regressor__n_estimators', 317), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 81), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.029432740675943186), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 310), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05147843384617103), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 168), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05147843384617103), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 168), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.022256050662486735), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 347), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.011305580286953344), ('regressor__regressor__max_depth', 191), ('regressor__regressor__n_estimators', 615), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.02±0.06	 r2: -0.06±0.06
Trimer
Filename: (MACCS)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer/(MACCS)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer/(MACCS)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer/(MACCS)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n09>
Subject: Job 547863: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c200n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 11:06:45 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 11:06:45 2024
Terminated at Sun Nov 24 11:50:59 2024
Results reported at Sun Nov 24 11:50:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "Trimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   6517.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.69 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   2654 sec.
    Turnaround time :                            6396 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 89), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03187479839379561), ('regressor__regressor__max_depth', 444), ('regressor__regressor__n_estimators', 249), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0564142677618746), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 145), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.021723169447991613), ('regressor__regressor__max_depth', 1230), ('regressor__regressor__n_estimators', 65), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05012776449398259), ('regressor__regressor__max_depth', 5279), ('regressor__regressor__n_estimators', 177), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006337573557896935), ('regressor__regressor__max_depth', 2888), ('regressor__regressor__n_estimators', 605), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008024420390581061), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 183), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.061327314756961786), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 108), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.017068556343363933), ('regressor__regressor__max_depth', 8735), ('regressor__regressor__n_estimators', 461), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020430933097912635), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 403), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00977102690427452), ('regressor__regressor__max_depth', 7979), ('regressor__regressor__n_estimators', 952), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004357775431145272), ('regressor__regressor__max_depth', 96), ('regressor__regressor__n_estimators', 1733), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02601049559489493), ('regressor__regressor__max_depth', 16), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.029910360099713346), ('regressor__regressor__max_depth', 46), ('regressor__regressor__n_estimators', 345), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026608429712205783), ('regressor__regressor__max_depth', 21), ('regressor__regressor__n_estimators', 326), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06106569999650349), ('regressor__regressor__max_depth', 8795), ('regressor__regressor__n_estimators', 70), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0027149637444921606), ('regressor__regressor__max_depth', 123), ('regressor__regressor__n_estimators', 1553), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03786904453052785), ('regressor__regressor__max_depth', 4300), ('regressor__regressor__n_estimators', 59), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09799816489511388), ('regressor__regressor__max_depth', 1381), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0730966843039904), ('regressor__regressor__max_depth', 9985), ('regressor__regressor__n_estimators', 89), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.030619740094156578), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 318), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004711699857959211), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1791), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014160989475803095), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 672), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001051722623041562), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1993), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004417487779776176), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 325), ('regressor__regressor__n_estimators', 87), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0038381254100539755), ('regressor__regressor__max_depth', 37), ('regressor__regressor__n_estimators', 503), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.012225004913346642), ('regressor__regressor__max_depth', 7153), ('regressor__regressor__n_estimators', 640), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 82), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05147843384617103), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 168), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005065015326861442), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05147843384617103), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 168), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05147843384617103), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 168), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09389644109597703), ('regressor__regressor__max_depth', 149), ('regressor__regressor__n_estimators', 67), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.04±0.06	 r2: -0.06±0.06
Monomer
Filename: (MACCS)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Monomer/(MACCS)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Monomer/(MACCS)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Monomer/(MACCS)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n05>
Subject: Job 547861: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c207n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 11:05:30 2024
                            <4*c207n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 11:05:30 2024
Terminated at Sun Nov 24 11:52:02 2024
Results reported at Sun Nov 24 11:52:02 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "Monomer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   6704.51 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.64 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   2822 sec.
    Turnaround time :                            6459 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07746435615971585), ('regressor__regressor__max_depth', 176), ('regressor__regressor__n_estimators', 1826), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09075966624522726), ('regressor__regressor__max_depth', 2326), ('regressor__regressor__n_estimators', 847), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002733858946775268), ('regressor__regressor__max_depth', 151), ('regressor__regressor__n_estimators', 865), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0017574022441569728), ('regressor__regressor__max_depth', 56), ('regressor__regressor__n_estimators', 693), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1023), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 47), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001342574451086675), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 890), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.024207646722039275), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 89), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0019409357762290662), ('regressor__regressor__max_depth', 2699), ('regressor__regressor__n_estimators', 1035), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.029910360099713346), ('regressor__regressor__max_depth', 46), ('regressor__regressor__n_estimators', 345), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08146216961026964), ('regressor__regressor__max_depth', 1264), ('regressor__regressor__n_estimators', 1246), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010168587136004646), ('regressor__regressor__max_depth', 2841), ('regressor__regressor__n_estimators', 770), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0027491387032859867), ('regressor__regressor__max_depth', 3311), ('regressor__regressor__n_estimators', 255), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.010366414278307123), ('regressor__regressor__max_depth', 3620), ('regressor__regressor__n_estimators', 274), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026184041993893195), ('regressor__regressor__max_depth', 96), ('regressor__regressor__n_estimators', 1103), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02271508247804575), ('regressor__regressor__max_depth', 2934), ('regressor__regressor__n_estimators', 188), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006443675674024319), ('regressor__regressor__max_depth', 7856), ('regressor__regressor__n_estimators', 323), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01129260294917783), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1179), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 16), ('regressor__regressor__n_estimators', 340), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04275403229986768), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 66), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0521873163587536), ('regressor__regressor__max_depth', 549), ('regressor__regressor__n_estimators', 220), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.028190051989359532), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 370), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0012139195238351733), ('regressor__regressor__max_depth', 95), ('regressor__regressor__n_estimators', 1096), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09941051382224342), ('regressor__regressor__max_depth', 31), ('regressor__regressor__n_estimators', 1867), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.058959683166841095), ('regressor__regressor__max_depth', 99), ('regressor__regressor__n_estimators', 62), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0376515915313303), ('regressor__regressor__max_depth', 2212), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09941051382224342), ('regressor__regressor__max_depth', 31), ('regressor__regressor__n_estimators', 1867), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09976059724915255), ('regressor__regressor__max_depth', 25), ('regressor__regressor__n_estimators', 443), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08091703337929072), ('regressor__regressor__max_depth', 502), ('regressor__regressor__n_estimators', 56), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04023311955494941), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 289), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.012078478582624032), ('regressor__regressor__max_depth', 8736), ('regressor__regressor__n_estimators', 206), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.024095947498285582), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 67), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07553285102182826), ('regressor__regressor__max_depth', 597), ('regressor__regressor__n_estimators', 459), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.04±0.05	 r2: -0.06±0.06
Monomer
Filename: (Mordred)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Monomer/(Mordred)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Monomer/(Mordred)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Monomer/(Mordred)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n04>
Subject: Job 547855: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c207n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:06:24 2024
                            <4*c207n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 10:06:24 2024
Terminated at Sun Nov 24 12:05:51 2024
Results reported at Sun Nov 24 12:05:51 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "Monomer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   28563.55 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.58 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   7195 sec.
    Turnaround time :                            7288 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 90), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05712851282705234), ('regressor__regressor__max_depth', 48), ('regressor__regressor__n_estimators', 139), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05875210314134852), ('regressor__regressor__max_depth', 16), ('regressor__regressor__n_estimators', 145), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 5432), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0787749672705762), ('regressor__regressor__max_depth', 25), ('regressor__regressor__n_estimators', 586), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006337573557896935), ('regressor__regressor__max_depth', 2888), ('regressor__regressor__n_estimators', 605), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008024420390581061), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 183), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 1677), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02915100515733359), ('regressor__regressor__max_depth', 13), ('regressor__regressor__n_estimators', 344), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020487828917694732), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 394), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009384018770590872), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 931), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04729080547055919), ('regressor__regressor__max_depth', 4466), ('regressor__regressor__n_estimators', 153), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013289540083875267), ('regressor__regressor__max_depth', 21), ('regressor__regressor__n_estimators', 75), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 6224), ('regressor__regressor__n_estimators', 108), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 79), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010298736142931799), ('regressor__regressor__max_depth', 22), ('regressor__regressor__n_estimators', 1993), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.033346434195597685), ('regressor__regressor__max_depth', 1402), ('regressor__regressor__n_estimators', 87), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0044690046309168755), ('regressor__regressor__max_depth', 3869), ('regressor__regressor__n_estimators', 445), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09799816489511388), ('regressor__regressor__max_depth', 1381), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07404379251416722), ('regressor__regressor__max_depth', 8191), ('regressor__regressor__n_estimators', 83), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.030636014025345232), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 319), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004711699857959211), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1791), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.031011260912329645), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 315), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001051722623041562), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1993), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01400261668822777), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 598), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020987276405089007), ('regressor__regressor__max_depth', 685), ('regressor__regressor__n_estimators', 465), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008784176345855597), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 194), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.028592463830482116), ('regressor__regressor__max_depth', 1112), ('regressor__regressor__n_estimators', 375), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 84), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05147843384617103), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 168), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 81), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 103), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0066566772806614756), ('regressor__regressor__max_depth', 565), ('regressor__regressor__n_estimators', 961), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09536663039284317), ('regressor__regressor__max_depth', 124), ('regressor__regressor__n_estimators', 73), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.02±0.05	 r2: -0.06±0.06
RRU Dimer
Filename: (MACCS)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Dimer/(MACCS)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Dimer/(MACCS)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Dimer/(MACCS)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c034n01>
Subject: Job 547865: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c034n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 11:34:51 2024
                            <4*c032n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 11:34:51 2024
Terminated at Sun Nov 24 12:11:06 2024
Results reported at Sun Nov 24 12:11:06 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "RRU Dimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   5910.25 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.14 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   2175 sec.
    Turnaround time :                            7603 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 89), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 92), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 89), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.021723169447991613), ('regressor__regressor__max_depth', 1230), ('regressor__regressor__n_estimators', 65), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04950909879971377), ('regressor__regressor__max_depth', 6555), ('regressor__regressor__n_estimators', 183), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009337376268518058), ('regressor__regressor__max_depth', 2167), ('regressor__regressor__n_estimators', 344), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01906985552726707), ('regressor__regressor__max_depth', 105), ('regressor__regressor__n_estimators', 75), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06095339978043557), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 107), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0839595948176321), ('regressor__regressor__max_depth', 269), ('regressor__regressor__n_estimators', 289), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020399612987293834), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 402), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009767523195081165), ('regressor__regressor__max_depth', 7976), ('regressor__regressor__n_estimators', 952), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004357775431145272), ('regressor__regressor__max_depth', 96), ('regressor__regressor__n_estimators', 1733), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1186), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004079193905639195), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.051244777182015054), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 188), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.033346434195597685), ('regressor__regressor__max_depth', 1402), ('regressor__regressor__n_estimators', 87), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005044423814628519), ('regressor__regressor__max_depth', 2783), ('regressor__regressor__n_estimators', 731), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03729505634482424), ('regressor__regressor__max_depth', 4297), ('regressor__regressor__n_estimators', 59), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09799816489511388), ('regressor__regressor__max_depth', 1381), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00868735926650343), ('regressor__regressor__max_depth', 16), ('regressor__regressor__n_estimators', 963), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.030446695942015337), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 313), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004711699857959211), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1791), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006718824992039133), ('regressor__regressor__max_depth', 13), ('regressor__regressor__n_estimators', 1261), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001051722623041562), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1993), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004429941851413666), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020987276405089007), ('regressor__regressor__max_depth', 685), ('regressor__regressor__n_estimators', 465), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007053150059173009), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 252), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020987276405089007), ('regressor__regressor__max_depth', 685), ('regressor__regressor__n_estimators', 465), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 82), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05147843384617103), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 168), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009860782061388277), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1111), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01620536729834905), ('regressor__regressor__max_depth', 2049), ('regressor__regressor__n_estimators', 578), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.022216355566154904), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 350), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.025656501184639358), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 320), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.04±0.06	 r2: -0.06±0.06
RRU Monomer
Filename: (MACCS)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Monomer/(MACCS)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Monomer/(MACCS)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Monomer/(MACCS)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n09>
Subject: Job 547864: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c203n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 11:26:59 2024
                            <4*c203n08>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 11:26:59 2024
Terminated at Sun Nov 24 12:12:14 2024
Results reported at Sun Nov 24 12:12:14 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "RRU Monomer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   6747.08 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.72 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   2741 sec.
    Turnaround time :                            7671 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 90), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08545290835516325), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 94), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 20), ('regressor__regressor__n_estimators', 89), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010128292123380018), ('regressor__regressor__max_depth', 120), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0039159723492709075), ('regressor__regressor__max_depth', 1689), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009337376268518058), ('regressor__regressor__max_depth', 2167), ('regressor__regressor__n_estimators', 344), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01969509641159622), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 75), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 117), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.017764067314651456), ('regressor__regressor__max_depth', 7508), ('regressor__regressor__n_estimators', 470), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 55), ('regressor__regressor__n_estimators', 105), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0093546051623687), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 930), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 14), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013330965723941709), ('regressor__regressor__max_depth', 21), ('regressor__regressor__n_estimators', 75), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0066100982954191536), ('regressor__regressor__max_depth', 1525), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05124334466390639), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 189), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.048868387564926334), ('regressor__regressor__max_depth', 6530), ('regressor__regressor__n_estimators', 57), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.033346434195597685), ('regressor__regressor__max_depth', 1402), ('regressor__regressor__n_estimators', 87), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04055325078323704), ('regressor__regressor__max_depth', 281), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09799816489511388), ('regressor__regressor__max_depth', 1381), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0731043127103908), ('regressor__regressor__max_depth', 8129), ('regressor__regressor__n_estimators', 85), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.030425967751108916), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 313), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004711699857959211), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1791), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 73), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001051722623041562), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 1993), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004444252126416736), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020987276405089007), ('regressor__regressor__max_depth', 685), ('regressor__regressor__n_estimators', 465), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014074947451579906), ('regressor__regressor__max_depth', 37), ('regressor__regressor__n_estimators', 107), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020987276405089007), ('regressor__regressor__max_depth', 685), ('regressor__regressor__n_estimators', 465), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 81), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.029768235540648127), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 311), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05147843384617103), ('regressor__regressor__max_depth', 28), ('regressor__regressor__n_estimators', 168), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02977235709379502), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 313), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0038650026260172135), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09536663039284317), ('regressor__regressor__max_depth', 124), ('regressor__regressor__n_estimators', 73), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.02±0.05	 r2: -0.06±0.06
RRU Trimer
Filename: (MACCS)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Trimer/(MACCS)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Trimer/(MACCS)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Trimer/(MACCS)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n01>
Subject: Job 547866: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c205n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 11:35:28 2024
                            <4*c205n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 11:35:28 2024
Terminated at Sun Nov 24 12:15:50 2024
Results reported at Sun Nov 24 12:15:50 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py maccs              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "RRU Trimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   6776.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.90 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   2422 sec.
    Turnaround time :                            7887 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 186), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07746435615971585), ('regressor__regressor__max_depth', 176), ('regressor__regressor__n_estimators', 1826), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0038878150366567755), ('regressor__regressor__max_depth', 359), ('regressor__regressor__n_estimators', 601), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0063535726565237425), ('regressor__regressor__max_depth', 5113), ('regressor__regressor__n_estimators', 183), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09746287591772174), ('regressor__regressor__max_depth', 16), ('regressor__regressor__n_estimators', 1943), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 71), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0011608638207695361), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 920), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.024283228704554852), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 89), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0019409357762290662), ('regressor__regressor__max_depth', 2699), ('regressor__regressor__n_estimators', 1035), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05861759873885743), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 818), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09711397996126861), ('regressor__regressor__max_depth', 29), ('regressor__regressor__n_estimators', 1987), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006723263911552581), ('regressor__regressor__max_depth', 588), ('regressor__regressor__n_estimators', 1821), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002692090640888375), ('regressor__regressor__max_depth', 9157), ('regressor__regressor__n_estimators', 265), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006962366546619066), ('regressor__regressor__max_depth', 417), ('regressor__regressor__n_estimators', 285), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0066214706772980865), ('regressor__regressor__max_depth', 3037), ('regressor__regressor__n_estimators', 430), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09959821624674509), ('regressor__regressor__max_depth', 4800), ('regressor__regressor__n_estimators', 161), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.033346434195597685), ('regressor__regressor__max_depth', 1402), ('regressor__regressor__n_estimators', 87), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04749966461918858), ('regressor__regressor__max_depth', 3720), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01371267456691987), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1126), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09880631751784562), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 787), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04302176363340416), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 66), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 7479), ('regressor__regressor__n_estimators', 119), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004444287835922006), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0014054776755722895), ('regressor__regressor__max_depth', 12), ('regressor__regressor__n_estimators', 612), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07866263422277885), ('regressor__regressor__max_depth', 18), ('regressor__regressor__n_estimators', 1165), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0582286730307757), ('regressor__regressor__max_depth', 7537), ('regressor__regressor__n_estimators', 702), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.027167216037053594), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 69), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06992978143208003), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 303), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09941051382224342), ('regressor__regressor__max_depth', 31), ('regressor__regressor__n_estimators', 1867), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08091703337929072), ('regressor__regressor__max_depth', 502), ('regressor__regressor__n_estimators', 56), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05739331220743488), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.019156120321040106), ('regressor__regressor__max_depth', 2576), ('regressor__regressor__n_estimators', 123), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1578), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07553285102182826), ('regressor__regressor__max_depth', 597), ('regressor__regressor__n_estimators', 459), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.04±0.06	 r2: -0.06±0.06
Trimer
Filename: (Mordred)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer/(Mordred)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer/(Mordred)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer/(Mordred)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n13>
Subject: Job 547857: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c207n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:26:31 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 10:26:31 2024
Terminated at Sun Nov 24 12:20:07 2024
Results reported at Sun Nov 24 12:20:07 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "Trimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   27525.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.73 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   6823 sec.
    Turnaround time :                            8144 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07746435615971585), ('regressor__regressor__max_depth', 176), ('regressor__regressor__n_estimators', 1826), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07746435615971585), ('regressor__regressor__max_depth', 176), ('regressor__regressor__n_estimators', 1826), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001978166083294936), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1348), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009938892399576309), ('regressor__regressor__max_depth', 251), ('regressor__regressor__n_estimators', 115), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.043925245707544275), ('regressor__regressor__max_depth', 29), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 20), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.018712595346591445), ('regressor__regressor__max_depth', 9002), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02355802670210019), ('regressor__regressor__max_depth', 43), ('regressor__regressor__n_estimators', 88), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0019409357762290662), ('regressor__regressor__max_depth', 2699), ('regressor__regressor__n_estimators', 1035), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09725494337657269), ('regressor__regressor__max_depth', 4932), ('regressor__regressor__n_estimators', 300), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08146216961026964), ('regressor__regressor__max_depth', 1264), ('regressor__regressor__n_estimators', 1246), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006313078171647144), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 125), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0037669438835184785), ('regressor__regressor__max_depth', 4990), ('regressor__regressor__n_estimators', 184), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006962366546619066), ('regressor__regressor__max_depth', 417), ('regressor__regressor__n_estimators', 285), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04078256970305606), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 82), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.012726541893050352), ('regressor__regressor__max_depth', 8931), ('regressor__regressor__n_estimators', 1863), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.033346434195597685), ('regressor__regressor__max_depth', 1402), ('regressor__regressor__n_estimators', 87), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.036830658898036175), ('regressor__regressor__max_depth', 486), ('regressor__regressor__n_estimators', 60), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007514913989574574), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009858843652909285), ('regressor__regressor__max_depth', 8045), ('regressor__regressor__n_estimators', 1985), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.021001459919489908), ('regressor__regressor__max_depth', 2016), ('regressor__regressor__n_estimators', 135), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0521873163587536), ('regressor__regressor__max_depth', 549), ('regressor__regressor__n_estimators', 220), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06578184289706959), ('regressor__regressor__max_depth', 107), ('regressor__regressor__n_estimators', 1484), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008386560960732736), ('regressor__regressor__max_depth', 3856), ('regressor__regressor__n_estimators', 95), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09854243147101058), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 420), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07866263422277885), ('regressor__regressor__max_depth', 18), ('regressor__regressor__n_estimators', 1165), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.017018589912199004), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 113), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 14), ('regressor__regressor__n_estimators', 151), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 84), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08091703337929072), ('regressor__regressor__max_depth', 502), ('regressor__regressor__n_estimators', 56), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 140), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014348505612168097), ('regressor__regressor__max_depth', 21), ('regressor__regressor__n_estimators', 171), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03222035618307274), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 65), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04045013381580617), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 325), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.04±0.05	 r2: -0.06±0.06
Dimer
Filename: (Mordred)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Dimer/(Mordred)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Dimer/(Mordred)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Dimer/(Mordred)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n02>
Subject: Job 547856: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c202n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:17:46 2024
                            <4*c202n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 10:17:46 2024
Terminated at Sun Nov 24 12:23:18 2024
Results reported at Sun Nov 24 12:23:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "Dimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   29850.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.96 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   7542 sec.
    Turnaround time :                            8335 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09075966624522726), ('regressor__regressor__max_depth', 2326), ('regressor__regressor__n_estimators', 847), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 2653), ('regressor__regressor__n_estimators', 66), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.021723169447991613), ('regressor__regressor__max_depth', 1230), ('regressor__regressor__n_estimators', 65), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09746287591772174), ('regressor__regressor__max_depth', 16), ('regressor__regressor__n_estimators', 1943), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 115), ('regressor__regressor__n_estimators', 109), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010012079096657766), ('regressor__regressor__max_depth', 238), ('regressor__regressor__n_estimators', 1379), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.016366828522656727), ('regressor__regressor__max_depth', 69), ('regressor__regressor__n_estimators', 134), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0019409357762290662), ('regressor__regressor__max_depth', 2699), ('regressor__regressor__n_estimators', 1035), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 55), ('regressor__regressor__n_estimators', 104), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09725494337657269), ('regressor__regressor__max_depth', 4932), ('regressor__regressor__n_estimators', 300), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07606770251128109), ('regressor__regressor__max_depth', 13), ('regressor__regressor__n_estimators', 71), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0019461069173372133), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 352), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004425094325386756), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 301), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.023008043818870488), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 139), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07886786577142631), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 596), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0027149637444921606), ('regressor__regressor__max_depth', 123), ('regressor__regressor__n_estimators', 1553), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007014053316172056), ('regressor__regressor__max_depth', 5106), ('regressor__regressor__n_estimators', 236), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09959821624674509), ('regressor__regressor__max_depth', 4800), ('regressor__regressor__n_estimators', 161), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03467890557466617), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 370), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04183228634772074), ('regressor__regressor__max_depth', 8980), ('regressor__regressor__n_estimators', 65), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.060133996753636246), ('regressor__regressor__max_depth', 9365), ('regressor__regressor__n_estimators', 293), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.028480783989440855), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 371), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0050246208926062064), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 169), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00482097958496894), ('regressor__regressor__max_depth', 25), ('regressor__regressor__n_estimators', 420), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 8974), ('regressor__regressor__n_estimators', 155), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0096884342456261), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 202), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05852674777567472), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 510), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 89), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08091703337929072), ('regressor__regressor__max_depth', 502), ('regressor__regressor__n_estimators', 56), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08033245588311584), ('regressor__regressor__max_depth', 2730), ('regressor__regressor__n_estimators', 1243), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014897956606772886), ('regressor__regressor__max_depth', 16), ('regressor__regressor__n_estimators', 159), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005847982124295329), ('regressor__regressor__max_depth', 8860), ('regressor__regressor__n_estimators', 302), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07052156161866532), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1364), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.04±0.05	 r2: -0.06±0.06
RRU Monomer
Filename: (Mordred)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Monomer/(Mordred)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Monomer/(Mordred)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Monomer/(Mordred)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n13>
Subject: Job 547858: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c207n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:27:30 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 10:27:30 2024
Terminated at Sun Nov 24 13:53:09 2024
Results reported at Sun Nov 24 13:53:09 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "RRU Monomer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   53279.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.97 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   12365 sec.
    Turnaround time :                            13726 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.052473050504964654), ('regressor__regressor__max_depth', 4790), ('regressor__regressor__n_estimators', 1332), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09075966624522726), ('regressor__regressor__max_depth', 2326), ('regressor__regressor__n_estimators', 847), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0027401832738705866), ('regressor__regressor__max_depth', 151), ('regressor__regressor__n_estimators', 865), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009938892399576309), ('regressor__regressor__max_depth', 251), ('regressor__regressor__n_estimators', 115), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09746287591772174), ('regressor__regressor__max_depth', 16), ('regressor__regressor__n_estimators', 1943), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03367843782203477), ('regressor__regressor__max_depth', 3395), ('regressor__regressor__n_estimators', 57), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013637936378813113), ('regressor__regressor__max_depth', 107), ('regressor__regressor__n_estimators', 80), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.024224239434065355), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 89), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0019409357762290662), ('regressor__regressor__max_depth', 2699), ('regressor__regressor__n_estimators', 1035), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0066100982954191536), ('regressor__regressor__max_depth', 1525), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09981487044124525), ('regressor__regressor__max_depth', 313), ('regressor__regressor__n_estimators', 374), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 7189), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006455118289764819), ('regressor__regressor__max_depth', 30), ('regressor__regressor__n_estimators', 106), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.023248822461803025), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 84), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04885076374445214), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 74), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07694779408123155), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1490), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.033346434195597685), ('regressor__regressor__max_depth', 1402), ('regressor__regressor__n_estimators', 87), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04337229533182606), ('regressor__regressor__max_depth', 713), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.036999726441820285), ('regressor__regressor__max_depth', 231), ('regressor__regressor__n_estimators', 611), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009858843652909285), ('regressor__regressor__max_depth', 8045), ('regressor__regressor__n_estimators', 1985), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020852507228580845), ('regressor__regressor__max_depth', 2016), ('regressor__regressor__n_estimators', 133), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.018638069368871017), ('regressor__regressor__max_depth', 6224), ('regressor__regressor__n_estimators', 630), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06578184289706959), ('regressor__regressor__max_depth', 107), ('regressor__regressor__n_estimators', 1484), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0014130144683380398), ('regressor__regressor__max_depth', 13), ('regressor__regressor__n_estimators', 579), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09941051382224342), ('regressor__regressor__max_depth', 31), ('regressor__regressor__n_estimators', 1867), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 8429), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009744768358695092), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 201), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05989531930968641), ('regressor__regressor__max_depth', 1488), ('regressor__regressor__n_estimators', 732), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07866263422277885), ('regressor__regressor__max_depth', 18), ('regressor__regressor__n_estimators', 1165), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08091703337929072), ('regressor__regressor__max_depth', 502), ('regressor__regressor__n_estimators', 56), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006426925462776248), ('regressor__regressor__max_depth', 307), ('regressor__regressor__n_estimators', 1539), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03478086149528304), ('regressor__regressor__max_depth', 20), ('regressor__regressor__n_estimators', 64), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01978384172909283), ('regressor__regressor__max_depth', 9210), ('regressor__regressor__n_estimators', 78), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09813068196761551), ('regressor__regressor__max_depth', 7870), ('regressor__regressor__n_estimators', 834), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.04±0.05	 r2: -0.06±0.06
RRU Dimer
Filename: (Mordred)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Dimer/(Mordred)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Dimer/(Mordred)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Dimer/(Mordred)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n13>
Subject: Job 547859: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c200n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:29:23 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 10:29:23 2024
Terminated at Sun Nov 24 14:14:14 2024
Results reported at Sun Nov 24 14:14:14 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "RRU Dimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   57638.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.96 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   13510 sec.
    Turnaround time :                            14991 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09932537994143073), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 1927), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014630650415646108), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 193), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09075966624522726), ('regressor__regressor__max_depth', 2326), ('regressor__regressor__n_estimators', 847), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002737732487175683), ('regressor__regressor__max_depth', 151), ('regressor__regressor__n_estimators', 865), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0063535726565237425), ('regressor__regressor__max_depth', 5113), ('regressor__regressor__n_estimators', 183), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02865157292879455), ('regressor__regressor__max_depth', 18), ('regressor__regressor__n_estimators', 1007), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05106417967330418), ('regressor__regressor__max_depth', 732), ('regressor__regressor__n_estimators', 92), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.011191556136938565), ('regressor__regressor__max_depth', 426), ('regressor__regressor__n_estimators', 99), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02415645721396358), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 89), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0019409357762290662), ('regressor__regressor__max_depth', 2699), ('regressor__regressor__n_estimators', 1035), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0066100982954191536), ('regressor__regressor__max_depth', 1525), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09725494337657269), ('regressor__regressor__max_depth', 4932), ('regressor__regressor__n_estimators', 300), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0690098527126882), ('regressor__regressor__max_depth', 7642), ('regressor__regressor__n_estimators', 423), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0021030374572180294), ('regressor__regressor__max_depth', 14), ('regressor__regressor__n_estimators', 337), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006962366546619066), ('regressor__regressor__max_depth', 417), ('regressor__regressor__n_estimators', 285), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03535105521104511), ('regressor__regressor__max_depth', 499), ('regressor__regressor__n_estimators', 92), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09988710565534124), ('regressor__regressor__max_depth', 169), ('regressor__regressor__n_estimators', 1992), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04984357949272885), ('regressor__regressor__max_depth', 741), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.045790112298984054), ('regressor__regressor__max_depth', 147), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020918477208043254), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07045473403216711), ('regressor__regressor__max_depth', 2125), ('regressor__regressor__n_estimators', 1123), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04267575348828841), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 66), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 112), ('regressor__regressor__n_estimators', 120), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0521873163587536), ('regressor__regressor__max_depth', 549), ('regressor__regressor__n_estimators', 220), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01688234083648538), ('regressor__regressor__max_depth', 7488), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 151), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002690998392117692), ('regressor__regressor__max_depth', 6356), ('regressor__regressor__n_estimators', 479), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013281424952128815), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 139), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09941051382224342), ('regressor__regressor__max_depth', 31), ('regressor__regressor__n_estimators', 1867), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09941051382224342), ('regressor__regressor__max_depth', 31), ('regressor__regressor__n_estimators', 1867), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08091703337929072), ('regressor__regressor__max_depth', 502), ('regressor__regressor__n_estimators', 56), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04357533349199398), ('regressor__regressor__max_depth', 36), ('regressor__regressor__n_estimators', 652), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014875247093137279), ('regressor__regressor__max_depth', 16), ('regressor__regressor__n_estimators', 160), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0013063872468941009), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1344), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07553285102182826), ('regressor__regressor__max_depth', 597), ('regressor__regressor__n_estimators', 459), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.04±0.06	 r2: -0.06±0.06
RRU Trimer
Filename: (Mordred)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Trimer/(Mordred)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Trimer/(Mordred)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Trimer/(Mordred)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n08>
Subject: Job 547860: <mordred_XGBR_Standard_Rh (IW avg log)> in cluster <Hazel> Done

Job <mordred_XGBR_Standard_Rh (IW avg log)> was submitted from host <c026n04> by user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:04:23 2024
Job was executed on host(s) <4*c200n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Nov 24 10:53:03 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Nov 24 10:53:03 2024
Terminated at Sun Nov 24 14:33:43 2024
Results reported at Sun Nov 24 14:33:43 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_XGBR_Standard_Rh (IW avg log)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type XGBR              --target "Rh (IW avg log)"              --oligo_type "RRU Trimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   57031.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.97 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   13240 sec.
    Turnaround time :                            16160 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_XGBR_Standard_Rh (IW avg log).err> for stderr output of this job.

