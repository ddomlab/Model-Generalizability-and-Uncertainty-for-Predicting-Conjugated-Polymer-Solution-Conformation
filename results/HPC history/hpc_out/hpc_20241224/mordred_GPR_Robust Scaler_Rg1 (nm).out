Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n09>
Subject: Job 598374: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:13 2024
Job was executed on host(s) <4*c200n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:32:08 2024
                            <4*c200n08>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:32:08 2024
Terminated at Mon Dec  2 02:51:36 2024
Results reported at Mon Dec  2 02:51:36 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   44.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.75 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   1144 sec.
    Turnaround time :                            5303 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n01>
Subject: Job 598379: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:25 2024
Job was executed on host(s) <4*c039n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:57:47 2024
                            <4*c031n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:57:47 2024
Terminated at Mon Dec  2 03:16:49 2024
Results reported at Mon Dec  2 03:16:49 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Trimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   63.20 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.75 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   1142 sec.
    Turnaround time :                            6804 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c006n02>
Subject: Job 598371: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:05 2024
Job was executed on host(s) <4*c006n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:28:10 2024
                            <4*c009n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:28:10 2024
Terminated at Mon Dec  2 03:20:15 2024
Results reported at Mon Dec  2 03:20:15 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   64.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.59 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   3129 sec.
    Turnaround time :                            7030 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c005n03>
Subject: Job 598369: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:03 2024
Job was executed on host(s) <4*c005n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:24:49 2024
                            <4*c010n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:24:49 2024
Terminated at Mon Dec  2 03:27:45 2024
Results reported at Mon Dec  2 03:27:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   66.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.39 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   3798 sec.
    Turnaround time :                            7482 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n08>
Subject: Job 598378: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:22 2024
Job was executed on host(s) <4*c200n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:51:37 2024
                            <4*c200n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:51:37 2024
Terminated at Mon Dec  2 03:34:59 2024
Results reported at Mon Dec  2 03:34:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Trimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   42.58 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.55 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   2603 sec.
    Turnaround time :                            7897 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n01>
Subject: Job 598382: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:34 2024
Job was executed on host(s) <4*c039n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:16:51 2024
                            <4*c031n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:16:51 2024
Terminated at Mon Dec  2 03:40:41 2024
Results reported at Mon Dec  2 03:40:41 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Monomer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   66.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.68 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   1431 sec.
    Turnaround time :                            8227 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c004n03>
Subject: Job 598383: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:36 2024
Job was executed on host(s) <4*c004n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:19:09 2024
                            <4*c012n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:19:09 2024
Terminated at Mon Dec  2 04:11:45 2024
Results reported at Mon Dec  2 04:11:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Monomer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   64.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.67 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   3182 sec.
    Turnaround time :                            10089 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n08>
Subject: Job 598386: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:44 2024
Job was executed on host(s) <4*c200n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:35:01 2024
                            <4*c200n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:35:01 2024
Terminated at Mon Dec  2 04:17:45 2024
Results reported at Mon Dec  2 04:17:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   45.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.78 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   2566 sec.
    Turnaround time :                            10441 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c009n01>
Subject: Job 598390: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:53 2024
Job was executed on host(s) <4*c009n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:48:34 2024
                            <4*c012n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:48:34 2024
Terminated at Mon Dec  2 04:20:05 2024
Results reported at Mon Dec  2 04:20:05 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Trimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   68.02 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.72 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   1913 sec.
    Turnaround time :                            10572 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c022n04>
Subject: Job 598375: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:16 2024
Job was executed on host(s) <4*c022n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:42:48 2024
                            <4*c022n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:42:48 2024
Terminated at Mon Dec  2 04:30:20 2024
Results reported at Mon Dec  2 04:30:20 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   73.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.50 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   6454 sec.
    Turnaround time :                            11224 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c003n02>
Subject: Job 598391: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:58 2024
Job was executed on host(s) <4*c003n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:57:26 2024
                            <4*c003n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:57:26 2024
Terminated at Mon Dec  2 04:31:08 2024
Results reported at Mon Dec  2 04:31:08 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Trimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   64.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.67 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   2036 sec.
    Turnaround time :                            11230 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c038n01>
Subject: Job 598387: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:47 2024
Job was executed on host(s) <4*c038n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:40:42 2024
                            <4*c039n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:40:42 2024
Terminated at Mon Dec  2 05:07:13 2024
Results reported at Mon Dec  2 05:07:13 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   82.00 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.51 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   5194 sec.
    Turnaround time :                            13406 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 619855: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:14 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Dec  4 22:27:52 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed Dec  4 22:27:52 2024
Terminated at Wed Dec  4 22:40:22 2024
Results reported at Wed Dec  4 22:40:22 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   63.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.67 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   769 sec.
    Turnaround time :                            17229 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n04>
Subject: Job 619858: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:15 2024
Job was executed on host(s) <4*c207n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Dec  4 22:55:05 2024
                            <4*c207n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed Dec  4 22:55:05 2024
Terminated at Wed Dec  4 23:06:34 2024
Results reported at Wed Dec  4 23:06:34 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   35.43 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.72 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   689 sec.
    Turnaround time :                            18799 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c027n02>
Subject: Job 619864: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:16 2024
Job was executed on host(s) <4*c027n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 01:41:47 2024
                            <4*c032n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 01:41:47 2024
Terminated at Thu Dec  5 01:46:48 2024
Results reported at Thu Dec  5 01:46:48 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   58.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.83 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   316 sec.
    Turnaround time :                            28412 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c031n03>
Subject: Job 619865: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:17 2024
Job was executed on host(s) <4*c031n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 01:41:47 2024
                            <4*c034n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 01:41:47 2024
Terminated at Thu Dec  5 01:48:39 2024
Results reported at Thu Dec  5 01:48:39 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   64.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.88 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   423 sec.
    Turnaround time :                            28522 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 619873: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:20 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:08:11 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:08:11 2024
Terminated at Thu Dec  5 03:09:01 2024
Results reported at Thu Dec  5 03:09:01 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Trimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   50.06 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.25 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   52 sec.
    Turnaround time :                            33341 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c037n01>
Subject: Job 619874: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:20 2024
Job was executed on host(s) <4*c037n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:11:06 2024
                            <4*c035n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:11:06 2024
Terminated at Thu Dec  5 03:12:16 2024
Results reported at Thu Dec  5 03:12:16 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Trimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   45.42 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.60 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   71 sec.
    Turnaround time :                            33536 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n08>
Subject: Job 619881: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:23 2024
Job was executed on host(s) <4*c202n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:18:08 2024
                            <4*c202n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:18:08 2024
Terminated at Thu Dec  5 03:19:57 2024
Results reported at Thu Dec  5 03:19:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Monomer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   54.21 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.50 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   115 sec.
    Turnaround time :                            33994 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c037n01>
Subject: Job 619880: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:23 2024
Job was executed on host(s) <4*c037n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:18:08 2024
                            <4*c035n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:18:08 2024
Terminated at Thu Dec  5 03:20:07 2024
Results reported at Thu Dec  5 03:20:07 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Monomer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   50.37 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.50 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               63.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   136 sec.
    Turnaround time :                            34004 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n10>
Subject: Job 619888: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:26 2024
Job was executed on host(s) <4*c205n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:30:27 2024
                            <4*c205n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:30:27 2024
Terminated at Thu Dec  5 03:32:06 2024
Results reported at Thu Dec  5 03:32:06 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   56.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.50 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   101 sec.
    Turnaround time :                            34720 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c035n04>
Subject: Job 619889: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:26 2024
Job was executed on host(s) <4*c035n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:38:37 2024
                            <4*c035n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:38:37 2024
Terminated at Thu Dec  5 03:39:33 2024
Results reported at Thu Dec  5 03:39:33 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   43.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.50 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   66 sec.
    Turnaround time :                            35167 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c035n04>
Subject: Job 619896: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:29 2024
Job was executed on host(s) <4*c035n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:44:32 2024
                            <4*c035n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:44:32 2024
Terminated at Thu Dec  5 03:45:22 2024
Results reported at Thu Dec  5 03:45:22 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Trimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   42.34 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.50 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   61 sec.
    Turnaround time :                            35513 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 619899: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:30 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:44:32 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:44:32 2024
Terminated at Thu Dec  5 03:45:36 2024
Results reported at Thu Dec  5 03:45:36 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Trimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   52.05 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   66 sec.
    Turnaround time :                            35526 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n09>
Subject: Job 627083: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:24:02 2024
Job was executed on host(s) <4*c200n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:24:05 2024
                            <4*c200n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 10:24:05 2024
Terminated at Thu Dec  5 10:25:46 2024
Results reported at Thu Dec  5 10:25:46 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   31.37 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.17 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   107 sec.
    Turnaround time :                            104 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n13>
Subject: Job 627081: <mordred_GPR_Robust Scaler_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Robust Scaler_Rg1 (nm)> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:24:02 2024
Job was executed on host(s) <4*c200n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:24:05 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 10:24:05 2024
Terminated at Thu Dec  5 10:26:25 2024
Results reported at Thu Dec  5 10:26:25 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Robust Scaler_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   35.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   148 sec.
    Turnaround time :                            143 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Robust Scaler_Rg1 (nm).err> for stderr output of this job.

