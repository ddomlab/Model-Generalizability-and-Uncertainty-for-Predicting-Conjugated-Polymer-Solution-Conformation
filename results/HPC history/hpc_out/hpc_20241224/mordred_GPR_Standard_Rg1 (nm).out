Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.37±0.18	 r2: 0.04±0.09
RRU Monomer
Filename: (Mordred)_GPR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(Mordred)_GPR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(Mordred)_GPR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(Mordred)_GPR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c010n03>
Subject: Job 598381: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Done

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:31 2024
Job was executed on host(s) <4*c010n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:14:33 2024
                            <4*c012n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:14:33 2024
Terminated at Tue Dec  3 07:53:14 2024
Results reported at Tue Dec  3 07:53:14 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Monomer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   74827.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.95 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               59.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   103141 sec.
    Turnaround time :                            109783 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.37±0.18	 r2: 0.04±0.09
Trimer
Filename: (Mordred)_GPR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(Mordred)_GPR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(Mordred)_GPR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(Mordred)_GPR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c021n02>
Subject: Job 598377: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Done

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:19 2024
Job was executed on host(s) <4*c021n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:48:49 2024
                            <4*c024n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:48:49 2024
Terminated at Tue Dec  3 19:37:38 2024
Results reported at Tue Dec  3 19:37:38 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Trimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   85106.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.68 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               59.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   146930 sec.
    Turnaround time :                            152059 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.37±0.18	 r2: 0.04±0.09
RRU Trimer
Filename: (Mordred)_GPR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(Mordred)_GPR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(Mordred)_GPR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(Mordred)_GPR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n01>
Subject: Job 598389: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Done

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:50 2024
Job was executed on host(s) <4*c039n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:47:19 2024
                            <4*c029n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:47:19 2024
Terminated at Tue Dec  3 19:43:42 2024
Results reported at Tue Dec  3 19:43:42 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Trimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   90251.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.55 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   143796 sec.
    Turnaround time :                            152392 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.37±0.18	 r2: 0.04±0.09
RRU Dimer
Filename: (Mordred)_GPR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(Mordred)_GPR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(Mordred)_GPR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(Mordred)_GPR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c010n02>
Subject: Job 598385: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Done

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:41 2024
Job was executed on host(s) <4*c010n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:27:46 2024
                            <4*c005n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:27:46 2024
Terminated at Tue Dec  3 20:38:37 2024
Results reported at Tue Dec  3 20:38:37 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Dimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   84504.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.49 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   148252 sec.
    Turnaround time :                            155696 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.37±0.18	 r2: 0.04±0.09
Monomer
Filename: (Mordred)_GPR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(Mordred)_GPR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(Mordred)_GPR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(Mordred)_GPR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c028n03>
Subject: Job 598368: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Done

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:22:58 2024
Job was executed on host(s) <4*c028n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:21:23 2024
                            <4*c036n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:21:23 2024
Terminated at Tue Dec  3 22:18:14 2024
Results reported at Tue Dec  3 22:18:14 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   87554.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.53 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               59.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   158211 sec.
    Turnaround time :                            161716 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n12>
Subject: Job 619853: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:14 2024
Job was executed on host(s) <4*c207n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:08 2024
                            <4*c207n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed Dec  4 17:54:08 2024
Terminated at Wed Dec  4 18:31:08 2024
Results reported at Wed Dec  4 18:31:08 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   70.39 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.71 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   2246 sec.
    Turnaround time :                            2274 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42

------------------------------------------------------------
Sender: LSF System <lsfadmin@c039n02>
Subject: Job 619852: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:13 2024
Job was executed on host(s) <4*c039n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:54:08 2024
                            <4*c037n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed Dec  4 17:54:08 2024
Terminated at Wed Dec  4 19:36:11 2024
Results reported at Wed Dec  4 19:36:11 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   161.26 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.31 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   6149 sec.
    Turnaround time :                            6178 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.37±0.18	 r2: 0.04±0.09
Dimer
Filename: (Mordred)_GPR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(Mordred)_GPR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(Mordred)_GPR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(Mordred)_GPR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c026n03>
Subject: Job 598373: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Done

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:10 2024
Job was executed on host(s) <4*c026n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:32:03 2024
                            <4*c023n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:32:03 2024
Terminated at Thu Dec  5 00:45:03 2024
Results reported at Thu Dec  5 00:45:03 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Dimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   86837.00 sec.
    Max Memory :                                 6 GB
    Average Memory :                             1.34 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               58.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   252780 sec.
    Turnaround time :                            256913 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420

------------------------------------------------------------
Sender: LSF System <lsfadmin@c012n04>
Subject: Job 619861: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:16 2024
Job was executed on host(s) <4*c012n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 01:10:28 2024
                            <4*c010n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 01:10:28 2024
Terminated at Thu Dec  5 01:18:36 2024
Results reported at Thu Dec  5 01:18:36 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Dimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   174.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             2.44 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               59.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   515 sec.
    Turnaround time :                            26720 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c012n04>
Subject: Job 619862: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:16 2024
Job was executed on host(s) <4*c012n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 01:41:47 2024
                            <4*c010n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 01:41:47 2024
Terminated at Thu Dec  5 02:08:18 2024
Results reported at Thu Dec  5 02:08:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Dimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   531.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             4.39 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               56.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   1610 sec.
    Turnaround time :                            29702 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.37±0.18	 r2: 0.04±0.09
RRU Dimer
Filename: (Mordred)_GPR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(Mordred)_GPR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(Mordred)_GPR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(Mordred)_GPR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c006n02>
Subject: Job 598384: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Done

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:38 2024
Job was executed on host(s) <4*c006n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:20:15 2024
                            <4*c009n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:20:15 2024
Terminated at Thu Dec  5 02:52:41 2024
Results reported at Thu Dec  5 02:52:41 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Dimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   154956.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             1.75 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               55.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   257546 sec.
    Turnaround time :                            264543 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])




Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 619868: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:18 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 02:52:42 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 02:52:42 2024
Terminated at Thu Dec  5 03:06:22 2024
Results reported at Thu Dec  5 03:06:22 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Trimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1514.11 sec.
    Max Memory :                                 6 GB
    Average Memory :                             3.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               58.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   843 sec.
    Turnaround time :                            33184 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.

Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n10>
Subject: Job 619871: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:19 2024
Job was executed on host(s) <4*c205n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:06:22 2024
                            <4*c205n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:06:22 2024
Terminated at Thu Dec  5 03:08:09 2024
Results reported at Thu Dec  5 03:08:09 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Trimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   163.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             2.67 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   107 sec.
    Turnaround time :                            33290 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n05>
Subject: Job 619876: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:21 2024
Job was executed on host(s) <4*c200n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:11:06 2024
                            <4*c200n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:11:06 2024
Terminated at Thu Dec  5 03:13:40 2024
Results reported at Thu Dec  5 03:13:40 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Monomer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   128.31 sec.
    Max Memory :                                 4 GB
    Average Memory :                             1.12 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   156 sec.
    Turnaround time :                            33619 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.

RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n05>
Subject: Job 619879: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:22 2024
Job was executed on host(s) <4*c200n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:15:23 2024
                            <4*c200n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:15:23 2024
Terminated at Thu Dec  5 03:18:08 2024
Results reported at Thu Dec  5 03:18:08 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Monomer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   112.27 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.12 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               59.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   175 sec.
    Turnaround time :                            33886 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n08>
Subject: Job 619884: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:24 2024
Job was executed on host(s) <4*c202n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:21:47 2024
                            <4*c202n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:21:47 2024
Terminated at Thu Dec  5 03:27:24 2024
Results reported at Thu Dec  5 03:27:24 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Dimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   204.05 sec.
    Max Memory :                                 5 GB
    Average Memory :                             1.71 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               59.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   357 sec.
    Turnaround time :                            34440 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.001), ('regressor__regressor__n_epoch', 100)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n12>
Subject: Job 619886: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:25 2024
Job was executed on host(s) <4*c202n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:30:27 2024
                            <4*c202n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:30:27 2024
Terminated at Thu Dec  5 03:36:18 2024
Results reported at Thu Dec  5 03:36:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Dimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   556.00 sec.
    Max Memory :                                 6 GB
    Average Memory :                             3.07 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               58.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   353 sec.
    Turnaround time :                            34973 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n13>
Subject: Job 619891: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:27 2024
Job was executed on host(s) <4*c205n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:38:37 2024
                            <4*c205n10>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:38:37 2024
Terminated at Thu Dec  5 03:41:11 2024
Results reported at Thu Dec  5 03:41:11 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Trimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   201.35 sec.
    Max Memory :                                 4 GB
    Average Memory :                             1.57 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   155 sec.
    Turnaround time :                            35264 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.

RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n12>
Subject: Job 619893: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c024n03> by user <sdehgha2> in cluster <Hazel> at Wed Dec  4 17:53:28 2024
Job was executed on host(s) <4*c202n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 03:41:11 2024
                            <4*c202n08>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 03:41:11 2024
Terminated at Thu Dec  5 03:44:22 2024
Results reported at Thu Dec  5 03:44:22 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "RRU Trimer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   179.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             2.78 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               59.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   195 sec.
    Turnaround time :                            35454 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.37±0.18	 r2: 0.04±0.09
RRU Trimer
Filename: (Mordred)_GPR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(Mordred)_GPR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(Mordred)_GPR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(Mordred)_GPR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c029n02>
Subject: Job 598388: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Done

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:49 2024
Job was executed on host(s) <4*c029n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:44:22 2024
                            <4*c031n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:44:22 2024
Terminated at Thu Dec  5 05:25:02 2024
Results reported at Thu Dec  5 05:25:02 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Trimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   200688.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             1.91 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               55.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   265242 sec.
    Turnaround time :                            273673 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.37±0.18	 r2: 0.04±0.09
RRU Monomer
Filename: (Mordred)_GPR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(Mordred)_GPR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(Mordred)_GPR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(Mordred)_GPR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c034n03>
Subject: Job 598380: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Done

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:27 2024
Job was executed on host(s) <4*c034n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 03:04:30 2024
                            <4*c030n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 03:04:30 2024
Terminated at Thu Dec  5 06:09:30 2024
Results reported at Thu Dec  5 06:09:30 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "RRU Monomer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   185607.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             1.92 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               55.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   270300 sec.
    Turnaround time :                            276363 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])


Average scores:	 r: 0.37±0.18	 r2: 0.04±0.09
Dimer
Filename: (Mordred)_GPR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(Mordred)_GPR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(Mordred)_GPR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(Mordred)_GPR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c015n02>
Subject: Job 598372: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Done

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:09 2024
Job was executed on host(s) <4*c015n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:28:44 2024
                            <4*c015n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:28:44 2024
Terminated at Thu Dec  5 09:02:10 2024
Results reported at Thu Dec  5 09:02:10 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Dimer"              --transform_type "Standard"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   270241.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             2.15 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               55.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   282807 sec.
    Turnaround time :                            286741 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6
Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n03>
Subject: Job 627079: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:24:02 2024
Job was executed on host(s) <4*c207n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:24:02 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 10:24:02 2024
Terminated at Thu Dec  5 10:44:59 2024
Results reported at Thu Dec  5 10:44:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Standard"

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   3936.16 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.93 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               59.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   1262 sec.
    Turnaround time :                            1257 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n08>
Subject: Job 627084: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:24:02 2024
Job was executed on host(s) <4*c203n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:24:05 2024
                            <4*c203n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 10:24:05 2024
Terminated at Thu Dec  5 10:45:10 2024
Results reported at Thu Dec  5 10:45:10 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Dimer"              --transform_type "Standard"

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   4249.00 sec.
    Max Memory :                                 5 GB
    Average Memory :                             4.11 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               59.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   1273 sec.
    Turnaround time :                            1268 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890

------------------------------------------------------------
Sender: LSF System <lsfadmin@c026n03>
Subject: Job 598367: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:22:55 2024
Job was executed on host(s) <4*c026n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:20:24 2024
                            <4*c023n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:20:24 2024
Terminated at Thu Dec  5 11:05:05 2024
Results reported at Thu Dec  5 11:05:05 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Standard"

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   254960.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             2.01 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               56.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   290710 sec.
    Turnaround time :                            294130 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 1000)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129

------------------------------------------------------------
Sender: LSF System <lsfadmin@c012n03>
Subject: Job 598376: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c203n12> by user <sdehgha2> in cluster <Hazel> at Mon Dec  2 01:23:18 2024
Job was executed on host(s) <4*c012n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Dec  2 02:43:55 2024
                            <4*c006n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Dec  2 02:43:55 2024
Terminated at Thu Dec  5 11:05:17 2024
Results reported at Thu Dec  5 11:05:17 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "matern"              --target "Rg1 (nm)"              --oligo_type "Trimer"              --transform_type "Standard"

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   235815.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             2.09 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               56.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   289291 sec.
    Turnaround time :                            294119 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR GPR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__lr', 0.01), ('regressor__regressor__n_epoch', 200)])


Average scores:	 r: 0.4±0.2	 r2: 0.1±0.14

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n12>
Subject: Job 627080: <mordred_GPR_Standard_Rg1 (nm)> in cluster <Hazel> Exited

Job <mordred_GPR_Standard_Rg1 (nm)> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:24:02 2024
Job was executed on host(s) <4*c207n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Dec  5 10:24:05 2024
                            <4*c207n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Dec  5 10:24:05 2024
Terminated at Thu Dec  5 11:50:17 2024
Results reported at Thu Dec  5 11:50:17 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "mordred_GPR_Standard_Rg1 (nm)"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type GPR              --kernel "rbf"              --target "Rg1 (nm)"              --oligo_type "Monomer"              --transform_type "Standard"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   17644.00 sec.
    Max Memory :                                 6 GB
    Average Memory :                             5.09 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               58.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   5175 sec.
    Turnaround time :                            5175 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_GPR_Standard_Rg1 (nm).err> for stderr output of this job.

