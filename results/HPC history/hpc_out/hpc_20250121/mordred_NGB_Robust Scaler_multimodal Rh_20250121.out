RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c012n03>
Subject: Job 296406: <mordred_NGB_Robust Scaler_multimodal Rh_20250120> in cluster <Hazel> Exited

Job <mordred_NGB_Robust Scaler_multimodal Rh_20250120> was submitted from host <c200n12> by user <sdehgha2> in cluster <Hazel> at Tue Jan 21 14:15:48 2025
Job was executed on host(s) <6*c012n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Jan 21 14:15:49 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Jan 21 14:15:49 2025
Terminated at Tue Jan 21 14:36:33 2025
Results reported at Tue Jan 21 14:36:33 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 6
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Robust Scaler_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   4957.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             5.12 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               8.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   1266 sec.
    Turnaround time :                            1245 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c008n03>
Subject: Job 296407: <mordred_NGB_Robust Scaler_multimodal Rh_20250120> in cluster <Hazel> Exited

Job <mordred_NGB_Robust Scaler_multimodal Rh_20250120> was submitted from host <c200n12> by user <sdehgha2> in cluster <Hazel> at Tue Jan 21 14:15:49 2025
Job was executed on host(s) <6*c008n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Jan 21 14:15:49 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Jan 21 14:15:49 2025
Terminated at Tue Jan 21 14:51:47 2025
Results reported at Tue Jan 21 14:51:47 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 6
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Robust Scaler_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "RRU Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   9116.00 sec.
    Max Memory :                                 7 GB
    Average Memory :                             4.14 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               9.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   2170 sec.
    Turnaround time :                            2158 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err> for stderr output of this job.

RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c013n01>
Subject: Job 304681: <mordred_NGB_Robust Scaler_multimodal Rh_20250120> in cluster <Hazel> Exited

Job <mordred_NGB_Robust Scaler_multimodal Rh_20250120> was submitted from host <c012n04> by user <sdehgha2> in cluster <Hazel> at Tue Jan 21 17:05:24 2025
Job was executed on host(s) <6*c013n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Jan 21 17:05:24 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Jan 21 17:05:24 2025
Terminated at Tue Jan 21 17:26:32 2025
Results reported at Tue Jan 21 17:26:32 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 6
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Robust Scaler_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   5492.22 sec.
    Max Memory :                                 8 GB
    Average Memory :                             5.35 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               8.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   1281 sec.
    Turnaround time :                            1268 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c014n03>
Subject: Job 304682: <mordred_NGB_Robust Scaler_multimodal Rh_20250120> in cluster <Hazel> Exited

Job <mordred_NGB_Robust Scaler_multimodal Rh_20250120> was submitted from host <c012n04> by user <sdehgha2> in cluster <Hazel> at Tue Jan 21 17:05:24 2025
Job was executed on host(s) <6*c014n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Jan 21 17:05:26 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Jan 21 17:05:26 2025
Terminated at Tue Jan 21 17:39:24 2025
Results reported at Tue Jan 21 17:39:24 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 6
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Robust Scaler_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "RRU Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   9270.00 sec.
    Max Memory :                                 7 GB
    Average Memory :                             4.65 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               9.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   2057 sec.
    Turnaround time :                            2040 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err> for stderr output of this job.

Dimer
RRU Dimer
Average scores:	 r2: [ 0.004  0.052 -0.041]±[0.174 0.099 0.022]
[array([ 0.00398919,  0.05197613, -0.04056716]), array([8.64474143e+00, 1.53834767e+02, 9.38188364e+04]), array([6.39359090e+00, 1.01081644e+02, 1.41209775e+04])]
Dimer
Filename: (Mordred)_NGB_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Dimer/(Mordred)_NGB_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Dimer/(Mordred)_NGB_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Dimer/(Mordred)_NGB_Robust Scaler_shape.json
Done Saving scores!
[iter 0] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=0.9370
[iter 100] loss=1.0109 val_loss=0.0000 scale=2.0000 norm=1.6028
[iter 200] loss=0.9037 val_loss=0.0000 scale=2.0000 norm=1.6227
[iter 300] loss=0.8110 val_loss=0.0000 scale=4.0000 norm=3.2663
[iter 400] loss=0.6700 val_loss=0.0000 scale=2.0000 norm=1.6337
[iter 0] loss=1.3571 val_loss=0.0000 scale=2.0000 norm=1.9347
[iter 100] loss=1.0754 val_loss=0.0000 scale=2.0000 norm=1.7054
[iter 200] loss=0.9539 val_loss=0.0000 scale=2.0000 norm=1.7036
[iter 300] loss=0.8710 val_loss=0.0000 scale=2.0000 norm=1.7112
[iter 400] loss=0.7850 val_loss=0.0000 scale=4.0000 norm=3.4321
[iter 0] loss=2.8250 val_loss=0.0000 scale=2.0000 norm=6.8125
[iter 100] loss=2.4903 val_loss=0.0000 scale=2.0000 norm=6.0143
[iter 200] loss=2.2218 val_loss=0.0000 scale=4.0000 norm=11.9647
[iter 300] loss=1.7038 val_loss=0.0000 scale=4.0000 norm=11.9685
[iter 400] loss=1.1845 val_loss=0.0000 scale=4.0000 norm=11.9681
[iter 0] loss=1.3472 val_loss=0.0000 scale=1.0000 norm=0.9569
[iter 100] loss=1.0969 val_loss=0.0000 scale=2.0000 norm=1.6752
[iter 200] loss=1.0409 val_loss=0.0000 scale=2.0000 norm=1.7022
[iter 300] loss=1.0103 val_loss=0.0000 scale=2.0000 norm=1.7132
[iter 400] loss=0.9766 val_loss=0.0000 scale=4.0000 norm=3.4305
[iter 0] loss=1.3050 val_loss=0.0000 scale=2.0000 norm=1.8775
[iter 100] loss=1.0018 val_loss=0.0000 scale=2.0000 norm=1.6214
[iter 200] loss=0.8649 val_loss=0.0000 scale=2.0000 norm=1.6162
[iter 300] loss=0.7591 val_loss=0.0000 scale=2.0000 norm=1.6286
[iter 400] loss=0.6444 val_loss=0.0000 scale=4.0000 norm=3.2624
[iter 0] loss=2.8434 val_loss=0.0000 scale=2.0000 norm=7.0188
[iter 100] loss=2.5362 val_loss=0.0000 scale=2.0000 norm=6.2412
[iter 200] loss=2.2772 val_loss=0.0000 scale=2.0000 norm=6.1961
[iter 300] loss=1.7897 val_loss=0.0000 scale=4.0000 norm=12.3919
[iter 400] loss=1.3016 val_loss=0.0000 scale=4.0000 norm=12.3908
[iter 0] loss=1.3202 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0556 val_loss=0.0000 scale=2.0000 norm=1.6373
[iter 200] loss=0.9917 val_loss=0.0000 scale=2.0000 norm=1.6667
[iter 300] loss=0.9507 val_loss=0.0000 scale=2.0000 norm=1.6758
[iter 400] loss=0.9131 val_loss=0.0000 scale=2.0000 norm=1.6799
[iter 0] loss=1.3569 val_loss=0.0000 scale=1.0000 norm=0.9670
[iter 100] loss=1.0278 val_loss=0.0000 scale=2.0000 norm=1.6611
[iter 200] loss=0.8824 val_loss=0.0000 scale=2.0000 norm=1.6499
[iter 300] loss=0.7887 val_loss=0.0000 scale=4.0000 norm=3.3305
[iter 400] loss=0.6945 val_loss=0.0000 scale=2.0000 norm=1.6743
[iter 0] loss=2.8478 val_loss=0.0000 scale=2.0000 norm=7.0856
[iter 100] loss=2.5191 val_loss=0.0000 scale=2.0000 norm=6.1703
[iter 200] loss=2.2216 val_loss=0.0000 scale=4.0000 norm=12.2406
[iter 300] loss=1.7206 val_loss=0.0000 scale=4.0000 norm=12.2424
[iter 400] loss=1.2193 val_loss=0.0000 scale=4.0000 norm=12.2431
[iter 0] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=0.9362
[iter 100] loss=1.0620 val_loss=0.0000 scale=2.0000 norm=1.6600
[iter 200] loss=0.9924 val_loss=0.0000 scale=2.0000 norm=1.6811
[iter 300] loss=0.9480 val_loss=0.0000 scale=2.0000 norm=1.6921
[iter 400] loss=0.9103 val_loss=0.0000 scale=4.0000 norm=3.3942
[iter 0] loss=1.3435 val_loss=0.0000 scale=2.0000 norm=1.9213
[iter 100] loss=0.9969 val_loss=0.0000 scale=2.0000 norm=1.6251
[iter 200] loss=0.8426 val_loss=0.0000 scale=2.0000 norm=1.6215
[iter 300] loss=0.7148 val_loss=0.0000 scale=2.0000 norm=1.6252
[iter 400] loss=0.5677 val_loss=0.0000 scale=4.0000 norm=3.2396
[iter 0] loss=2.8188 val_loss=0.0000 scale=2.0000 norm=6.7092
[iter 100] loss=2.5230 val_loss=0.0000 scale=2.0000 norm=6.0265
[iter 200] loss=2.2014 val_loss=0.0000 scale=4.0000 norm=12.0141
[iter 300] loss=1.7301 val_loss=0.0000 scale=4.0000 norm=12.0170
[iter 400] loss=1.0924 val_loss=0.0000 scale=4.0000 norm=12.0175
[iter 0] loss=1.2729 val_loss=0.0000 scale=1.0000 norm=0.9145
[iter 100] loss=1.0173 val_loss=0.0000 scale=2.0000 norm=1.6181
[iter 200] loss=0.9630 val_loss=0.0000 scale=2.0000 norm=1.6483
[iter 300] loss=0.9367 val_loss=0.0000 scale=2.0000 norm=1.6632
[iter 400] loss=0.9118 val_loss=0.0000 scale=2.0000 norm=1.6655
[iter 0] loss=1.3675 val_loss=0.0000 scale=2.0000 norm=1.9472
[iter 100] loss=1.0375 val_loss=0.0000 scale=2.0000 norm=1.6795
[iter 200] loss=0.8819 val_loss=0.0000 scale=2.0000 norm=1.6687
[iter 300] loss=0.7668 val_loss=0.0000 scale=2.0000 norm=1.6746
[iter 400] loss=0.6415 val_loss=0.0000 scale=2.0000 norm=1.6787
[iter 0] loss=2.8016 val_loss=0.0000 scale=2.0000 norm=6.5394
[iter 100] loss=2.4862 val_loss=0.0000 scale=2.0000 norm=5.8765
[iter 200] loss=2.1494 val_loss=0.0000 scale=4.0000 norm=11.7124
[iter 300] loss=1.6493 val_loss=0.0000 scale=4.0000 norm=11.7142
[iter 400] loss=1.1541 val_loss=0.0000 scale=4.0000 norm=11.7149
[iter 0] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=0.9372
[iter 100] loss=1.0489 val_loss=0.0000 scale=2.0000 norm=1.6387
[iter 200] loss=0.9567 val_loss=0.0000 scale=2.0000 norm=1.6564
[iter 300] loss=0.8692 val_loss=0.0000 scale=4.0000 norm=3.3265
[iter 400] loss=0.7836 val_loss=0.0000 scale=2.0000 norm=1.6625
[iter 0] loss=1.3703 val_loss=0.0000 scale=1.0000 norm=0.9735
[iter 100] loss=1.0532 val_loss=0.0000 scale=2.0000 norm=1.6570
[iter 200] loss=0.9194 val_loss=0.0000 scale=2.0000 norm=1.6461
[iter 300] loss=0.8214 val_loss=0.0000 scale=2.0000 norm=1.6485
[iter 400] loss=0.7282 val_loss=0.0000 scale=2.0000 norm=1.6543
[iter 0] loss=2.8038 val_loss=0.0000 scale=2.0000 norm=6.5481
[iter 100] loss=2.5070 val_loss=0.0000 scale=2.0000 norm=5.8467
[iter 200] loss=2.2141 val_loss=0.0000 scale=4.0000 norm=11.6090
[iter 300] loss=1.7431 val_loss=0.0000 scale=4.0000 norm=11.6090
[iter 400] loss=1.2780 val_loss=0.0000 scale=4.0000 norm=11.6096
[iter 0] loss=1.2623 val_loss=0.0000 scale=1.0000 norm=0.9083
[iter 100] loss=1.0263 val_loss=0.0000 scale=2.0000 norm=1.6178
[iter 200] loss=0.9768 val_loss=0.0000 scale=2.0000 norm=1.6460
[iter 300] loss=0.9478 val_loss=0.0000 scale=2.0000 norm=1.6542
[iter 400] loss=0.9188 val_loss=0.0000 scale=2.0000 norm=1.6553
[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=0.9900
[iter 100] loss=1.0734 val_loss=0.0000 scale=2.0000 norm=1.7074
[iter 200] loss=0.9194 val_loss=0.0000 scale=2.0000 norm=1.6891
[iter 300] loss=0.8118 val_loss=0.0000 scale=4.0000 norm=3.3848
[iter 400] loss=0.6710 val_loss=0.0000 scale=4.0000 norm=3.3911
[iter 0] loss=2.8504 val_loss=0.0000 scale=2.0000 norm=7.1258
[iter 100] loss=2.5705 val_loss=0.0000 scale=2.0000 norm=6.3684
[iter 200] loss=2.3330 val_loss=0.0000 scale=2.0000 norm=6.3135
[iter 300] loss=1.9361 val_loss=0.0000 scale=4.0000 norm=12.6304
[iter 400] loss=1.4843 val_loss=0.0000 scale=4.0000 norm=12.6321
[iter 0] loss=1.2604 val_loss=0.0000 scale=1.0000 norm=0.9080
[iter 100] loss=1.0411 val_loss=0.0000 scale=2.0000 norm=1.6226
[iter 200] loss=0.9859 val_loss=0.0000 scale=2.0000 norm=1.6476
[iter 300] loss=0.9467 val_loss=0.0000 scale=2.0000 norm=1.6547
[iter 400] loss=0.8960 val_loss=0.0000 scale=4.0000 norm=3.3116
[iter 0] loss=1.4348 val_loss=0.0000 scale=1.0000 norm=1.0085
[iter 100] loss=1.0593 val_loss=0.0000 scale=2.0000 norm=1.6821
[iter 200] loss=0.9211 val_loss=0.0000 scale=2.0000 norm=1.6808
[iter 300] loss=0.8254 val_loss=0.0000 scale=2.0000 norm=1.6884
[iter 400] loss=0.7192 val_loss=0.0000 scale=2.0000 norm=1.6903
[iter 0] loss=2.7725 val_loss=0.0000 scale=2.0000 norm=6.1605
[iter 100] loss=2.4565 val_loss=0.0000 scale=2.0000 norm=5.5497
[iter 200] loss=2.1635 val_loss=0.0000 scale=4.0000 norm=11.0602
[iter 300] loss=1.6681 val_loss=0.0000 scale=4.0000 norm=11.0651
[iter 400] loss=1.1785 val_loss=0.0000 scale=4.0000 norm=11.0649
[iter 0] loss=1.2441 val_loss=0.0000 scale=1.0000 norm=0.9004
[iter 100] loss=1.0093 val_loss=0.0000 scale=2.0000 norm=1.6054
[iter 200] loss=0.9531 val_loss=0.0000 scale=2.0000 norm=1.6402
[iter 300] loss=0.9195 val_loss=0.0000 scale=2.0000 norm=1.6504
[iter 400] loss=0.8842 val_loss=0.0000 scale=2.0000 norm=1.6519
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0550
[iter 100] loss=1.1329 val_loss=0.0000 scale=2.0000 norm=1.7849
[iter 200] loss=0.9799 val_loss=0.0000 scale=2.0000 norm=1.7793
[iter 300] loss=0.8571 val_loss=0.0000 scale=2.0000 norm=1.7819
[iter 400] loss=0.7322 val_loss=0.0000 scale=2.0000 norm=1.7818
[iter 0] loss=2.8149 val_loss=0.0000 scale=2.0000 norm=6.6187
[iter 100] loss=2.4846 val_loss=0.0000 scale=2.0000 norm=5.9024
[iter 200] loss=2.1836 val_loss=0.0000 scale=2.0000 norm=5.8652
[iter 300] loss=1.6614 val_loss=0.0000 scale=4.0000 norm=11.7355
[iter 400] loss=1.1428 val_loss=0.0000 scale=4.0000 norm=11.7370
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9178
[iter 100] loss=1.0396 val_loss=0.0000 scale=2.0000 norm=1.6143
[iter 200] loss=0.9904 val_loss=0.0000 scale=2.0000 norm=1.6464
[iter 300] loss=0.9610 val_loss=0.0000 scale=4.0000 norm=3.3158
[iter 400] loss=0.9295 val_loss=0.0000 scale=2.0000 norm=1.6597
[iter 0] loss=1.3920 val_loss=0.0000 scale=1.0000 norm=0.9856
[iter 100] loss=1.0409 val_loss=0.0000 scale=2.0000 norm=1.6860
[iter 200] loss=0.8918 val_loss=0.0000 scale=2.0000 norm=1.6817
[iter 300] loss=0.7876 val_loss=0.0000 scale=4.0000 norm=3.3962
[iter 400] loss=0.6758 val_loss=0.0000 scale=2.0000 norm=1.7026
[iter 0] loss=2.8013 val_loss=0.0000 scale=2.0000 norm=6.5371
[iter 100] loss=2.4637 val_loss=0.0000 scale=2.0000 norm=5.7678
[iter 200] loss=2.1839 val_loss=0.0000 scale=4.0000 norm=11.4463
[iter 300] loss=1.6739 val_loss=0.0000 scale=4.0000 norm=11.4500
[iter 400] loss=1.1487 val_loss=0.0000 scale=4.0000 norm=11.4494
[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=0.9091
[iter 100] loss=1.0224 val_loss=0.0000 scale=2.0000 norm=1.6074
[iter 200] loss=0.9596 val_loss=0.0000 scale=2.0000 norm=1.6402
[iter 300] loss=0.9181 val_loss=0.0000 scale=2.0000 norm=1.6505
[iter 400] loss=0.8795 val_loss=0.0000 scale=2.0000 norm=1.6531
[iter 0] loss=1.3826 val_loss=0.0000 scale=1.0000 norm=0.9804
[iter 100] loss=1.0818 val_loss=0.0000 scale=2.0000 norm=1.7116
[iter 200] loss=0.9502 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 300] loss=0.8568 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 400] loss=0.7723 val_loss=0.0000 scale=2.0000 norm=1.6998
[iter 0] loss=2.8328 val_loss=0.0000 scale=2.0000 norm=6.8945
[iter 100] loss=2.5201 val_loss=0.0000 scale=2.0000 norm=6.1040
[iter 200] loss=2.2502 val_loss=0.0000 scale=2.0000 norm=6.0604
[iter 300] loss=1.7565 val_loss=0.0000 scale=4.0000 norm=12.1255
[iter 400] loss=1.2494 val_loss=0.0000 scale=4.0000 norm=12.1262
[iter 0] loss=1.2880 val_loss=0.0000 scale=1.0000 norm=0.9224
[iter 100] loss=1.0308 val_loss=0.0000 scale=2.0000 norm=1.6242
[iter 200] loss=0.9644 val_loss=0.0000 scale=2.0000 norm=1.6502
[iter 300] loss=0.9237 val_loss=0.0000 scale=4.0000 norm=3.3221
[iter 400] loss=0.8834 val_loss=0.0000 scale=2.0000 norm=1.6658
[iter 0] loss=1.3802 val_loss=0.0000 scale=2.0000 norm=1.9592
[iter 100] loss=1.0403 val_loss=0.0000 scale=2.0000 norm=1.6620
[iter 200] loss=0.9015 val_loss=0.0000 scale=2.0000 norm=1.6522
[iter 300] loss=0.8031 val_loss=0.0000 scale=2.0000 norm=1.6539
[iter 400] loss=0.6923 val_loss=0.0000 scale=4.0000 norm=3.3130
[iter 0] loss=2.8298 val_loss=0.0000 scale=2.0000 norm=6.8441
[iter 100] loss=2.5391 val_loss=0.0000 scale=2.0000 norm=6.1816
[iter 200] loss=2.2363 val_loss=0.0000 scale=4.0000 norm=12.3107
[iter 300] loss=1.7532 val_loss=0.0000 scale=4.0000 norm=12.3112
[iter 400] loss=1.2808 val_loss=0.0000 scale=4.0000 norm=12.3117
[iter 0] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=0.9362
[iter 100] loss=1.0620 val_loss=0.0000 scale=2.0000 norm=1.6600
[iter 200] loss=0.9924 val_loss=0.0000 scale=2.0000 norm=1.6811
[iter 300] loss=0.9480 val_loss=0.0000 scale=2.0000 norm=1.6921
[iter 400] loss=0.9103 val_loss=0.0000 scale=4.0000 norm=3.3942
[iter 0] loss=1.3435 val_loss=0.0000 scale=2.0000 norm=1.9213
[iter 100] loss=0.9969 val_loss=0.0000 scale=2.0000 norm=1.6251
[iter 200] loss=0.8426 val_loss=0.0000 scale=2.0000 norm=1.6215
[iter 300] loss=0.7148 val_loss=0.0000 scale=2.0000 norm=1.6252
[iter 400] loss=0.5677 val_loss=0.0000 scale=4.0000 norm=3.2396
[iter 0] loss=2.8188 val_loss=0.0000 scale=2.0000 norm=6.7092
[iter 100] loss=2.5230 val_loss=0.0000 scale=2.0000 norm=6.0265
[iter 200] loss=2.2014 val_loss=0.0000 scale=4.0000 norm=12.0141
[iter 300] loss=1.7303 val_loss=0.0000 scale=4.0000 norm=12.0170
[iter 400] loss=1.0956 val_loss=0.0000 scale=8.0000 norm=24.0347
[iter 0] loss=1.2729 val_loss=0.0000 scale=1.0000 norm=0.9145
[iter 100] loss=1.0173 val_loss=0.0000 scale=2.0000 norm=1.6181
[iter 200] loss=0.9630 val_loss=0.0000 scale=2.0000 norm=1.6483
[iter 300] loss=0.9367 val_loss=0.0000 scale=2.0000 norm=1.6632
[iter 400] loss=0.9118 val_loss=0.0000 scale=2.0000 norm=1.6655
[iter 0] loss=1.3675 val_loss=0.0000 scale=2.0000 norm=1.9472
[iter 100] loss=1.0375 val_loss=0.0000 scale=2.0000 norm=1.6795
[iter 200] loss=0.8819 val_loss=0.0000 scale=2.0000 norm=1.6687
[iter 300] loss=0.7668 val_loss=0.0000 scale=2.0000 norm=1.6746
[iter 400] loss=0.6415 val_loss=0.0000 scale=2.0000 norm=1.6787
[iter 0] loss=2.8016 val_loss=0.0000 scale=2.0000 norm=6.5394
[iter 100] loss=2.4862 val_loss=0.0000 scale=2.0000 norm=5.8765
[iter 200] loss=2.1494 val_loss=0.0000 scale=4.0000 norm=11.7124
[iter 300] loss=1.6493 val_loss=0.0000 scale=4.0000 norm=11.7142
[iter 400] loss=1.1541 val_loss=0.0000 scale=4.0000 norm=11.7149
[iter 0] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=0.9372
[iter 100] loss=1.0489 val_loss=0.0000 scale=2.0000 norm=1.6387
[iter 200] loss=0.9567 val_loss=0.0000 scale=2.0000 norm=1.6564
[iter 300] loss=0.8692 val_loss=0.0000 scale=4.0000 norm=3.3265
[iter 400] loss=0.7836 val_loss=0.0000 scale=2.0000 norm=1.6625
[iter 0] loss=1.3703 val_loss=0.0000 scale=1.0000 norm=0.9735
[iter 100] loss=1.0532 val_loss=0.0000 scale=2.0000 norm=1.6570
[iter 200] loss=0.9194 val_loss=0.0000 scale=2.0000 norm=1.6461
[iter 300] loss=0.8214 val_loss=0.0000 scale=2.0000 norm=1.6485
[iter 400] loss=0.7282 val_loss=0.0000 scale=2.0000 norm=1.6543
[iter 0] loss=2.8038 val_loss=0.0000 scale=2.0000 norm=6.5481
[iter 100] loss=2.5070 val_loss=0.0000 scale=2.0000 norm=5.8467
[iter 200] loss=2.2141 val_loss=0.0000 scale=4.0000 norm=11.6090
[iter 300] loss=1.7431 val_loss=0.0000 scale=4.0000 norm=11.6090
[iter 400] loss=1.2780 val_loss=0.0000 scale=4.0000 norm=11.6096
[iter 0] loss=1.2534 val_loss=0.0000 scale=1.0000 norm=0.9046
[iter 100] loss=1.0132 val_loss=0.0000 scale=2.0000 norm=1.5963
[iter 200] loss=0.9577 val_loss=0.0000 scale=2.0000 norm=1.6262
[iter 300] loss=0.9238 val_loss=0.0000 scale=2.0000 norm=1.6345
[iter 400] loss=0.8926 val_loss=0.0000 scale=2.0000 norm=1.6362
[iter 0] loss=1.4004 val_loss=0.0000 scale=2.0000 norm=1.9799
[iter 100] loss=1.0805 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 200] loss=0.9358 val_loss=0.0000 scale=2.0000 norm=1.6809
[iter 300] loss=0.8339 val_loss=0.0000 scale=2.0000 norm=1.6850
[iter 400] loss=0.7159 val_loss=0.0000 scale=2.0000 norm=1.6894
[iter 0] loss=2.7906 val_loss=0.0000 scale=2.0000 norm=6.3336
[iter 100] loss=2.4734 val_loss=0.0000 scale=2.0000 norm=5.6845
[iter 200] loss=2.1863 val_loss=0.0000 scale=4.0000 norm=11.3001
[iter 300] loss=1.7004 val_loss=0.0000 scale=4.0000 norm=11.3081
[iter 400] loss=1.1842 val_loss=0.0000 scale=4.0000 norm=11.3077
[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=0.9091
[iter 100] loss=1.0224 val_loss=0.0000 scale=2.0000 norm=1.6074
[iter 200] loss=0.9596 val_loss=0.0000 scale=2.0000 norm=1.6402
[iter 300] loss=0.9181 val_loss=0.0000 scale=2.0000 norm=1.6505
[iter 400] loss=0.8795 val_loss=0.0000 scale=2.0000 norm=1.6531
[iter 0] loss=1.3826 val_loss=0.0000 scale=1.0000 norm=0.9804
[iter 100] loss=1.0818 val_loss=0.0000 scale=2.0000 norm=1.7116
[iter 200] loss=0.9502 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 300] loss=0.8568 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 400] loss=0.7723 val_loss=0.0000 scale=2.0000 norm=1.6998
[iter 0] loss=2.8328 val_loss=0.0000 scale=2.0000 norm=6.8945
[iter 100] loss=2.5201 val_loss=0.0000 scale=2.0000 norm=6.1040
[iter 200] loss=2.2502 val_loss=0.0000 scale=2.0000 norm=6.0604
[iter 300] loss=1.7565 val_loss=0.0000 scale=4.0000 norm=12.1255
[iter 400] loss=1.2494 val_loss=0.0000 scale=4.0000 norm=12.1262
[iter 0] loss=1.2519 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0116 val_loss=0.0000 scale=2.0000 norm=1.5855
[iter 200] loss=0.9555 val_loss=0.0000 scale=2.0000 norm=1.6143
[iter 300] loss=0.9197 val_loss=0.0000 scale=2.0000 norm=1.6243
[iter 400] loss=0.8842 val_loss=0.0000 scale=2.0000 norm=1.6258
[iter 0] loss=1.3513 val_loss=0.0000 scale=2.0000 norm=1.9289
[iter 100] loss=1.0553 val_loss=0.0000 scale=2.0000 norm=1.6897
[iter 200] loss=0.9193 val_loss=0.0000 scale=2.0000 norm=1.6881
[iter 300] loss=0.8247 val_loss=0.0000 scale=2.0000 norm=1.6929
[iter 400] loss=0.7072 val_loss=0.0000 scale=2.0000 norm=1.7001
[iter 0] loss=2.7907 val_loss=0.0000 scale=2.0000 norm=6.4321
[iter 100] loss=2.4682 val_loss=0.0000 scale=2.0000 norm=5.7491
[iter 200] loss=2.1702 val_loss=0.0000 scale=4.0000 norm=11.4543
[iter 300] loss=1.6496 val_loss=0.0000 scale=4.0000 norm=11.4654
[iter 400] loss=1.1148 val_loss=0.0000 scale=4.0000 norm=11.4667
[iter 0] loss=1.2623 val_loss=0.0000 scale=1.0000 norm=0.9083
[iter 100] loss=1.0263 val_loss=0.0000 scale=2.0000 norm=1.6178
[iter 200] loss=0.9768 val_loss=0.0000 scale=2.0000 norm=1.6460
[iter 300] loss=0.9478 val_loss=0.0000 scale=2.0000 norm=1.6542
[iter 400] loss=0.9188 val_loss=0.0000 scale=2.0000 norm=1.6553
[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=0.9900
[iter 100] loss=1.0734 val_loss=0.0000 scale=2.0000 norm=1.7074
[iter 200] loss=0.9194 val_loss=0.0000 scale=2.0000 norm=1.6891
[iter 300] loss=0.8118 val_loss=0.0000 scale=4.0000 norm=3.3848
[iter 400] loss=0.6710 val_loss=0.0000 scale=4.0000 norm=3.3911
[iter 0] loss=2.8504 val_loss=0.0000 scale=2.0000 norm=7.1258
[iter 100] loss=2.5705 val_loss=0.0000 scale=2.0000 norm=6.3684
[iter 200] loss=2.3330 val_loss=0.0000 scale=2.0000 norm=6.3135
[iter 300] loss=1.9314 val_loss=0.0000 scale=4.0000 norm=12.6304
[iter 400] loss=1.4796 val_loss=0.0000 scale=4.0000 norm=12.6320
[iter 0] loss=1.2548 val_loss=0.0000 scale=1.0000 norm=0.9050
[iter 100] loss=0.9649 val_loss=0.0000 scale=2.0000 norm=1.5464
[iter 200] loss=0.8691 val_loss=0.0000 scale=2.0000 norm=1.5707
[iter 300] loss=0.7834 val_loss=0.0000 scale=4.0000 norm=3.1592
[iter 400] loss=0.6969 val_loss=0.0000 scale=2.0000 norm=1.5802
[iter 0] loss=1.4331 val_loss=0.0000 scale=2.0000 norm=2.0154
[iter 100] loss=1.1054 val_loss=0.0000 scale=2.0000 norm=1.7389
[iter 200] loss=0.9588 val_loss=0.0000 scale=2.0000 norm=1.7307
[iter 300] loss=0.8565 val_loss=0.0000 scale=2.0000 norm=1.7336
[iter 400] loss=0.7581 val_loss=0.0000 scale=4.0000 norm=3.4794
[iter 0] loss=2.8338 val_loss=0.0000 scale=2.0000 norm=6.8988
[iter 100] loss=2.5250 val_loss=0.0000 scale=2.0000 norm=6.1036
[iter 200] loss=2.2521 val_loss=0.0000 scale=4.0000 norm=12.0971
[iter 300] loss=1.7614 val_loss=0.0000 scale=4.0000 norm=12.0993
[iter 400] loss=1.2550 val_loss=0.0000 scale=4.0000 norm=12.0994
[iter 0] loss=1.3202 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0556 val_loss=0.0000 scale=2.0000 norm=1.6373
[iter 200] loss=0.9917 val_loss=0.0000 scale=2.0000 norm=1.6667
[iter 300] loss=0.9507 val_loss=0.0000 scale=2.0000 norm=1.6758
[iter 400] loss=0.9131 val_loss=0.0000 scale=2.0000 norm=1.6799
[iter 0] loss=1.3569 val_loss=0.0000 scale=1.0000 norm=0.9670
[iter 100] loss=1.0278 val_loss=0.0000 scale=2.0000 norm=1.6611
[iter 200] loss=0.8824 val_loss=0.0000 scale=2.0000 norm=1.6499
[iter 300] loss=0.7887 val_loss=0.0000 scale=4.0000 norm=3.3305
[iter 400] loss=0.6945 val_loss=0.0000 scale=2.0000 norm=1.6743
[iter 0] loss=2.8478 val_loss=0.0000 scale=2.0000 norm=7.0856
[iter 100] loss=2.5191 val_loss=0.0000 scale=2.0000 norm=6.1703
[iter 200] loss=2.2216 val_loss=0.0000 scale=4.0000 norm=12.2406
[iter 300] loss=1.7206 val_loss=0.0000 scale=4.0000 norm=12.2424
[iter 400] loss=1.2193 val_loss=0.0000 scale=4.0000 norm=12.2431
[iter 0] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=0.9370
[iter 100] loss=1.0109 val_loss=0.0000 scale=2.0000 norm=1.6028
[iter 200] loss=0.9037 val_loss=0.0000 scale=2.0000 norm=1.6227
[iter 300] loss=0.8110 val_loss=0.0000 scale=4.0000 norm=3.2663
[iter 400] loss=0.6700 val_loss=0.0000 scale=2.0000 norm=1.6337
[iter 0] loss=1.3571 val_loss=0.0000 scale=2.0000 norm=1.9347
[iter 100] loss=1.0754 val_loss=0.0000 scale=2.0000 norm=1.7054
[iter 200] loss=0.9539 val_loss=0.0000 scale=2.0000 norm=1.7036
[iter 300] loss=0.8710 val_loss=0.0000 scale=2.0000 norm=1.7112
[iter 400] loss=0.7850 val_loss=0.0000 scale=4.0000 norm=3.4321
[iter 0] loss=2.8250 val_loss=0.0000 scale=2.0000 norm=6.8125
[iter 100] loss=2.4903 val_loss=0.0000 scale=2.0000 norm=6.0143
[iter 200] loss=2.2218 val_loss=0.0000 scale=4.0000 norm=11.9647
[iter 300] loss=1.7038 val_loss=0.0000 scale=4.0000 norm=11.9685
[iter 400] loss=1.1845 val_loss=0.0000 scale=4.0000 norm=11.9681
[iter 0] loss=1.2604 val_loss=0.0000 scale=1.0000 norm=0.9080
[iter 100] loss=1.0411 val_loss=0.0000 scale=2.0000 norm=1.6226
[iter 200] loss=0.9859 val_loss=0.0000 scale=2.0000 norm=1.6476
[iter 300] loss=0.9467 val_loss=0.0000 scale=2.0000 norm=1.6547
[iter 400] loss=0.8960 val_loss=0.0000 scale=4.0000 norm=3.3116
[iter 0] loss=1.4348 val_loss=0.0000 scale=1.0000 norm=1.0085
[iter 100] loss=1.0593 val_loss=0.0000 scale=2.0000 norm=1.6821
[iter 200] loss=0.9211 val_loss=0.0000 scale=2.0000 norm=1.6808
[iter 300] loss=0.8254 val_loss=0.0000 scale=2.0000 norm=1.6884
[iter 400] loss=0.7192 val_loss=0.0000 scale=2.0000 norm=1.6903
[iter 0] loss=2.7725 val_loss=0.0000 scale=2.0000 norm=6.1605
[iter 100] loss=2.4565 val_loss=0.0000 scale=2.0000 norm=5.5497
[iter 200] loss=2.1635 val_loss=0.0000 scale=4.0000 norm=11.0602
[iter 300] loss=1.6681 val_loss=0.0000 scale=4.0000 norm=11.0651
[iter 400] loss=1.1785 val_loss=0.0000 scale=4.0000 norm=11.0649
[iter 0] loss=1.2519 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0116 val_loss=0.0000 scale=2.0000 norm=1.5855
[iter 200] loss=0.9555 val_loss=0.0000 scale=2.0000 norm=1.6143
[iter 300] loss=0.9197 val_loss=0.0000 scale=2.0000 norm=1.6243
[iter 400] loss=0.8842 val_loss=0.0000 scale=2.0000 norm=1.6258
[iter 0] loss=1.3513 val_loss=0.0000 scale=2.0000 norm=1.9289
[iter 100] loss=1.0553 val_loss=0.0000 scale=2.0000 norm=1.6897
[iter 200] loss=0.9193 val_loss=0.0000 scale=2.0000 norm=1.6881
[iter 300] loss=0.8247 val_loss=0.0000 scale=2.0000 norm=1.6929
[iter 400] loss=0.7072 val_loss=0.0000 scale=2.0000 norm=1.7001
[iter 0] loss=2.7907 val_loss=0.0000 scale=2.0000 norm=6.4321
[iter 100] loss=2.4682 val_loss=0.0000 scale=2.0000 norm=5.7491
[iter 200] loss=2.1702 val_loss=0.0000 scale=4.0000 norm=11.4543
[iter 300] loss=1.6496 val_loss=0.0000 scale=4.0000 norm=11.4654
[iter 400] loss=1.1148 val_loss=0.0000 scale=4.0000 norm=11.4667
[iter 0] loss=1.2534 val_loss=0.0000 scale=1.0000 norm=0.9046
[iter 100] loss=1.0132 val_loss=0.0000 scale=2.0000 norm=1.5963
[iter 200] loss=0.9577 val_loss=0.0000 scale=2.0000 norm=1.6262
[iter 300] loss=0.9238 val_loss=0.0000 scale=2.0000 norm=1.6345
[iter 400] loss=0.8926 val_loss=0.0000 scale=2.0000 norm=1.6362
[iter 0] loss=1.4004 val_loss=0.0000 scale=2.0000 norm=1.9799
[iter 100] loss=1.0805 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 200] loss=0.9358 val_loss=0.0000 scale=2.0000 norm=1.6809
[iter 300] loss=0.8339 val_loss=0.0000 scale=2.0000 norm=1.6850
[iter 400] loss=0.7159 val_loss=0.0000 scale=2.0000 norm=1.6894
[iter 0] loss=2.7906 val_loss=0.0000 scale=2.0000 norm=6.3336
[iter 100] loss=2.4734 val_loss=0.0000 scale=2.0000 norm=5.6845
[iter 200] loss=2.1863 val_loss=0.0000 scale=4.0000 norm=11.3001
[iter 300] loss=1.7004 val_loss=0.0000 scale=4.0000 norm=11.3081
[iter 400] loss=1.1842 val_loss=0.0000 scale=4.0000 norm=11.3077
[iter 0] loss=1.3472 val_loss=0.0000 scale=1.0000 norm=0.9569
[iter 100] loss=1.0969 val_loss=0.0000 scale=2.0000 norm=1.6752
[iter 200] loss=1.0409 val_loss=0.0000 scale=2.0000 norm=1.7022
[iter 300] loss=1.0103 val_loss=0.0000 scale=2.0000 norm=1.7132
[iter 400] loss=0.9766 val_loss=0.0000 scale=4.0000 norm=3.4305
[iter 0] loss=1.3050 val_loss=0.0000 scale=2.0000 norm=1.8775
[iter 100] loss=1.0018 val_loss=0.0000 scale=2.0000 norm=1.6214
[iter 200] loss=0.8649 val_loss=0.0000 scale=2.0000 norm=1.6162
[iter 300] loss=0.7591 val_loss=0.0000 scale=2.0000 norm=1.6286
[iter 400] loss=0.6444 val_loss=0.0000 scale=4.0000 norm=3.2624
[iter 0] loss=2.8434 val_loss=0.0000 scale=2.0000 norm=7.0188
[iter 100] loss=2.5362 val_loss=0.0000 scale=2.0000 norm=6.2414
[iter 200] loss=2.2746 val_loss=0.0000 scale=2.0000 norm=6.1961
[iter 300] loss=1.7920 val_loss=0.0000 scale=4.0000 norm=12.3918
[iter 400] loss=1.3039 val_loss=0.0000 scale=4.0000 norm=12.3906
[iter 0] loss=1.2441 val_loss=0.0000 scale=1.0000 norm=0.9004
[iter 100] loss=1.0093 val_loss=0.0000 scale=2.0000 norm=1.6054
[iter 200] loss=0.9531 val_loss=0.0000 scale=2.0000 norm=1.6402
[iter 300] loss=0.9195 val_loss=0.0000 scale=2.0000 norm=1.6504
[iter 400] loss=0.8842 val_loss=0.0000 scale=2.0000 norm=1.6519
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0550
[iter 100] loss=1.1329 val_loss=0.0000 scale=2.0000 norm=1.7849
[iter 200] loss=0.9799 val_loss=0.0000 scale=2.0000 norm=1.7793
[iter 300] loss=0.8571 val_loss=0.0000 scale=2.0000 norm=1.7819
[iter 400] loss=0.7322 val_loss=0.0000 scale=2.0000 norm=1.7818
[iter 0] loss=2.8149 val_loss=0.0000 scale=2.0000 norm=6.6187
[iter 100] loss=2.4846 val_loss=0.0000 scale=2.0000 norm=5.9024
[iter 200] loss=2.1836 val_loss=0.0000 scale=2.0000 norm=5.8652
[iter 300] loss=1.6614 val_loss=0.0000 scale=4.0000 norm=11.7355
[iter 400] loss=1.1428 val_loss=0.0000 scale=4.0000 norm=11.7370
[iter 0] loss=1.2520 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0181 val_loss=0.0000 scale=2.0000 norm=1.5975
[iter 200] loss=0.9687 val_loss=0.0000 scale=2.0000 norm=1.6183
[iter 300] loss=0.9408 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 400] loss=0.9160 val_loss=0.0000 scale=2.0000 norm=1.6300
[iter 0] loss=1.3875 val_loss=0.0000 scale=2.0000 norm=1.9660
[iter 100] loss=1.1015 val_loss=0.0000 scale=2.0000 norm=1.7399
[iter 200] loss=0.9645 val_loss=0.0000 scale=2.0000 norm=1.7311
[iter 300] loss=0.8642 val_loss=0.0000 scale=4.0000 norm=3.4825
[iter 400] loss=0.7063 val_loss=0.0000 scale=4.0000 norm=3.4921
[iter 0] loss=2.8579 val_loss=0.0000 scale=2.0000 norm=7.2441
[iter 100] loss=2.5639 val_loss=0.0000 scale=2.0000 norm=6.4547
[iter 200] loss=2.3004 val_loss=0.0000 scale=4.0000 norm=12.8292
[iter 300] loss=1.8116 val_loss=0.0000 scale=4.0000 norm=12.8366
[iter 400] loss=1.3223 val_loss=0.0000 scale=4.0000 norm=12.8373
[iter 0] loss=1.2247 val_loss=0.0000 scale=1.0000 norm=0.8869
[iter 100] loss=0.9850 val_loss=0.0000 scale=2.0000 norm=1.5884
[iter 200] loss=0.9192 val_loss=0.0000 scale=2.0000 norm=1.6129
[iter 300] loss=0.8816 val_loss=0.0000 scale=2.0000 norm=1.6191
[iter 400] loss=0.8580 val_loss=0.0000 scale=2.0000 norm=1.6250
[iter 0] loss=1.3487 val_loss=0.0000 scale=1.0000 norm=0.9639
[iter 100] loss=1.0250 val_loss=0.0000 scale=2.0000 norm=1.6668
[iter 200] loss=0.8868 val_loss=0.0000 scale=2.0000 norm=1.6526
[iter 300] loss=0.7885 val_loss=0.0000 scale=2.0000 norm=1.6540
[iter 400] loss=0.6747 val_loss=0.0000 scale=4.0000 norm=3.3082
[iter 0] loss=2.7909 val_loss=0.0000 scale=2.0000 norm=6.4074
[iter 100] loss=2.4895 val_loss=0.0000 scale=2.0000 norm=5.8073
[iter 200] loss=2.2151 val_loss=0.0000 scale=4.0000 norm=11.5870
[iter 300] loss=1.7497 val_loss=0.0000 scale=4.0000 norm=11.5952
[iter 400] loss=1.2748 val_loss=0.0000 scale=4.0000 norm=11.5957
[iter 0] loss=1.3199 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0369 val_loss=0.0000 scale=2.0000 norm=1.6437
[iter 200] loss=0.9494 val_loss=0.0000 scale=2.0000 norm=1.6677
[iter 300] loss=0.8740 val_loss=0.0000 scale=2.0000 norm=1.6755
[iter 400] loss=0.7823 val_loss=0.0000 scale=2.0000 norm=1.6761
[iter 0] loss=1.3652 val_loss=0.0000 scale=2.0000 norm=1.9423
[iter 100] loss=1.0792 val_loss=0.0000 scale=2.0000 norm=1.7078
[iter 200] loss=0.9431 val_loss=0.0000 scale=2.0000 norm=1.6916
[iter 300] loss=0.8457 val_loss=0.0000 scale=2.0000 norm=1.6926
[iter 400] loss=0.7326 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 0] loss=2.8172 val_loss=0.0000 scale=2.0000 norm=6.6655
[iter 100] loss=2.5102 val_loss=0.0000 scale=2.0000 norm=5.9937
[iter 200] loss=2.2533 val_loss=0.0000 scale=2.0000 norm=5.9622
[iter 300] loss=1.8007 val_loss=0.0000 scale=4.0000 norm=11.9334
[iter 400] loss=1.3178 val_loss=0.0000 scale=4.0000 norm=11.9352
[iter 0] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=0.9089
[iter 100] loss=1.0088 val_loss=0.0000 scale=2.0000 norm=1.5960
[iter 200] loss=0.9144 val_loss=0.0000 scale=2.0000 norm=1.6158
[iter 300] loss=0.8368 val_loss=0.0000 scale=4.0000 norm=3.2463
[iter 400] loss=0.7502 val_loss=0.0000 scale=2.0000 norm=1.6251
[iter 0] loss=1.3664 val_loss=0.0000 scale=2.0000 norm=1.9434
[iter 100] loss=1.0314 val_loss=0.0000 scale=2.0000 norm=1.6698
[iter 200] loss=0.8786 val_loss=0.0000 scale=2.0000 norm=1.6630
[iter 300] loss=0.7650 val_loss=0.0000 scale=4.0000 norm=3.3494
[iter 400] loss=0.6117 val_loss=0.0000 scale=4.0000 norm=3.3548
[iter 0] loss=2.8399 val_loss=0.0000 scale=2.0000 norm=6.9733
[iter 100] loss=2.5337 val_loss=0.0000 scale=2.0000 norm=6.2192
[iter 200] loss=2.2629 val_loss=0.0000 scale=4.0000 norm=12.3774
[iter 300] loss=1.7623 val_loss=0.0000 scale=4.0000 norm=12.3857
[iter 400] loss=1.2613 val_loss=0.0000 scale=4.0000 norm=12.3870
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9178
[iter 100] loss=1.0396 val_loss=0.0000 scale=2.0000 norm=1.6143
[iter 200] loss=0.9904 val_loss=0.0000 scale=2.0000 norm=1.6464
[iter 300] loss=0.9610 val_loss=0.0000 scale=4.0000 norm=3.3158
[iter 400] loss=0.9295 val_loss=0.0000 scale=2.0000 norm=1.6597
[iter 0] loss=1.3920 val_loss=0.0000 scale=1.0000 norm=0.9856
[iter 100] loss=1.0409 val_loss=0.0000 scale=2.0000 norm=1.6860
[iter 200] loss=0.8918 val_loss=0.0000 scale=2.0000 norm=1.6817
[iter 300] loss=0.7876 val_loss=0.0000 scale=4.0000 norm=3.3962
[iter 400] loss=0.6758 val_loss=0.0000 scale=2.0000 norm=1.7026
[iter 0] loss=2.8013 val_loss=0.0000 scale=2.0000 norm=6.5371
[iter 100] loss=2.4637 val_loss=0.0000 scale=2.0000 norm=5.7678
[iter 200] loss=2.1839 val_loss=0.0000 scale=4.0000 norm=11.4463
[iter 300] loss=1.6739 val_loss=0.0000 scale=4.0000 norm=11.4500
[iter 400] loss=1.1487 val_loss=0.0000 scale=4.0000 norm=11.4494
[iter 0] loss=1.2548 val_loss=0.0000 scale=1.0000 norm=0.9050
[iter 100] loss=0.9649 val_loss=0.0000 scale=2.0000 norm=1.5464
[iter 200] loss=0.8691 val_loss=0.0000 scale=2.0000 norm=1.5707
[iter 300] loss=0.7834 val_loss=0.0000 scale=4.0000 norm=3.1592
[iter 400] loss=0.6969 val_loss=0.0000 scale=2.0000 norm=1.5802
[iter 0] loss=1.4331 val_loss=0.0000 scale=2.0000 norm=2.0154
[iter 100] loss=1.1054 val_loss=0.0000 scale=2.0000 norm=1.7389
[iter 200] loss=0.9588 val_loss=0.0000 scale=2.0000 norm=1.7307
[iter 300] loss=0.8565 val_loss=0.0000 scale=2.0000 norm=1.7336
[iter 400] loss=0.7581 val_loss=0.0000 scale=4.0000 norm=3.4794
[iter 0] loss=2.8338 val_loss=0.0000 scale=2.0000 norm=6.8988
[iter 100] loss=2.5250 val_loss=0.0000 scale=2.0000 norm=6.1036
[iter 200] loss=2.2521 val_loss=0.0000 scale=4.0000 norm=12.0971
[iter 300] loss=1.7614 val_loss=0.0000 scale=4.0000 norm=12.0993
[iter 400] loss=1.2550 val_loss=0.0000 scale=4.0000 norm=12.0994
[iter 0] loss=1.2880 val_loss=0.0000 scale=1.0000 norm=0.9224
[iter 100] loss=1.0308 val_loss=0.0000 scale=2.0000 norm=1.6242
[iter 200] loss=0.9644 val_loss=0.0000 scale=2.0000 norm=1.6502
[iter 300] loss=0.9237 val_loss=0.0000 scale=4.0000 norm=3.3221
[iter 400] loss=0.8834 val_loss=0.0000 scale=2.0000 norm=1.6658
[iter 0] loss=1.3802 val_loss=0.0000 scale=2.0000 norm=1.9592
[iter 100] loss=1.0403 val_loss=0.0000 scale=2.0000 norm=1.6620
[iter 200] loss=0.9015 val_loss=0.0000 scale=2.0000 norm=1.6522
[iter 300] loss=0.8031 val_loss=0.0000 scale=2.0000 norm=1.6539
[iter 400] loss=0.6923 val_loss=0.0000 scale=4.0000 norm=3.3130
[iter 0] loss=2.8298 val_loss=0.0000 scale=2.0000 norm=6.8441
[iter 100] loss=2.5391 val_loss=0.0000 scale=2.0000 norm=6.1816
[iter 200] loss=2.2363 val_loss=0.0000 scale=4.0000 norm=12.3107
[iter 300] loss=1.7532 val_loss=0.0000 scale=4.0000 norm=12.3112
[iter 400] loss=1.2808 val_loss=0.0000 scale=4.0000 norm=12.3117
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9184
[iter 100] loss=1.0806 val_loss=0.0000 scale=2.0000 norm=1.6685
[iter 200] loss=1.0293 val_loss=0.0000 scale=2.0000 norm=1.6974
[iter 300] loss=1.0011 val_loss=0.0000 scale=2.0000 norm=1.7071
[iter 400] loss=0.9714 val_loss=0.0000 scale=2.0000 norm=1.7091
[iter 0] loss=1.3537 val_loss=0.0000 scale=2.0000 norm=1.9311
[iter 100] loss=1.0272 val_loss=0.0000 scale=2.0000 norm=1.6634
[iter 200] loss=0.8954 val_loss=0.0000 scale=2.0000 norm=1.6530
[iter 300] loss=0.8060 val_loss=0.0000 scale=2.0000 norm=1.6588
[iter 400] loss=0.7127 val_loss=0.0000 scale=2.0000 norm=1.6615
[iter 0] loss=2.8256 val_loss=0.0000 scale=2.0000 norm=6.8131
[iter 100] loss=2.5116 val_loss=0.0000 scale=2.0000 norm=6.0585
[iter 200] loss=2.2509 val_loss=0.0000 scale=2.0000 norm=6.0216
[iter 300] loss=1.7568 val_loss=0.0000 scale=4.0000 norm=12.0466
[iter 400] loss=1.2622 val_loss=0.0000 scale=4.0000 norm=12.0454
[iter 0] loss=1.3108 val_loss=0.0000 scale=1.0000 norm=0.9365
[iter 100] loss=0.9993 val_loss=0.0000 scale=2.0000 norm=1.5717
[iter 200] loss=0.9357 val_loss=0.0000 scale=2.0000 norm=1.6060
[iter 300] loss=0.8997 val_loss=0.0000 scale=2.0000 norm=1.6190
[iter 400] loss=0.8661 val_loss=0.0000 scale=4.0000 norm=3.2434
[iter 0] loss=1.4291 val_loss=0.0000 scale=2.0000 norm=2.0111
[iter 100] loss=1.0707 val_loss=0.0000 scale=2.0000 norm=1.7065
[iter 200] loss=0.9103 val_loss=0.0000 scale=2.0000 norm=1.6925
[iter 300] loss=0.8106 val_loss=0.0000 scale=2.0000 norm=1.7063
[iter 400] loss=0.7160 val_loss=0.0000 scale=2.0000 norm=1.7169
[iter 0] loss=2.8053 val_loss=0.0000 scale=2.0000 norm=6.5256
[iter 100] loss=2.4738 val_loss=0.0000 scale=2.0000 norm=5.7676
[iter 200] loss=2.1087 val_loss=0.0000 scale=4.0000 norm=11.4681
[iter 300] loss=1.5777 val_loss=0.0000 scale=4.0000 norm=11.4641
[iter 400] loss=1.0313 val_loss=0.0000 scale=8.0000 norm=22.9272
[iter 0] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=0.9402
[iter 100] loss=1.0715 val_loss=0.0000 scale=2.0000 norm=1.6594
[iter 200] loss=1.0093 val_loss=0.0000 scale=2.0000 norm=1.6909
[iter 300] loss=0.9698 val_loss=0.0000 scale=2.0000 norm=1.6995
[iter 400] loss=0.9325 val_loss=0.0000 scale=2.0000 norm=1.7009
[iter 0] loss=1.3368 val_loss=0.0000 scale=1.0000 norm=0.9551
[iter 100] loss=1.0019 val_loss=0.0000 scale=2.0000 norm=1.6083
[iter 200] loss=0.8499 val_loss=0.0000 scale=2.0000 norm=1.6034
[iter 300] loss=0.7418 val_loss=0.0000 scale=2.0000 norm=1.6150
[iter 400] loss=0.6343 val_loss=0.0000 scale=4.0000 norm=3.2371
[iter 0] loss=2.8212 val_loss=0.0000 scale=2.0000 norm=6.7590
[iter 100] loss=2.4871 val_loss=0.0000 scale=2.0000 norm=5.9671
[iter 200] loss=2.1782 val_loss=0.0000 scale=4.0000 norm=11.8553
[iter 300] loss=1.6545 val_loss=0.0000 scale=4.0000 norm=11.8617
[iter 400] loss=1.1300 val_loss=0.0000 scale=4.0000 norm=11.8629
[iter 0] loss=1.2359 val_loss=0.0000 scale=1.0000 norm=0.8946
[iter 100] loss=1.0101 val_loss=0.0000 scale=2.0000 norm=1.6050
[iter 200] loss=0.9550 val_loss=0.0000 scale=2.0000 norm=1.6328
[iter 300] loss=0.9188 val_loss=0.0000 scale=2.0000 norm=1.6440
[iter 400] loss=0.8926 val_loss=0.0000 scale=2.0000 norm=1.6492
[iter 0] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=0.9463
[iter 100] loss=1.0072 val_loss=0.0000 scale=2.0000 norm=1.6366
[iter 200] loss=0.8666 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 300] loss=0.7639 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 400] loss=0.6675 val_loss=0.0000 scale=2.0000 norm=1.6326
[iter 0] loss=2.7903 val_loss=0.0000 scale=2.0000 norm=6.3676
[iter 100] loss=2.4692 val_loss=0.0000 scale=2.0000 norm=5.6533
[iter 200] loss=2.1765 val_loss=0.0000 scale=4.0000 norm=11.2381
[iter 300] loss=1.6567 val_loss=0.0000 scale=4.0000 norm=11.2412
[iter 400] loss=1.1386 val_loss=0.0000 scale=4.0000 norm=11.2409
[iter 0] loss=1.2546 val_loss=0.0000 scale=1.0000 norm=0.9029
[iter 100] loss=0.9917 val_loss=0.0000 scale=2.0000 norm=1.5744
[iter 200] loss=0.9387 val_loss=0.0000 scale=2.0000 norm=1.6113
[iter 300] loss=0.9077 val_loss=0.0000 scale=2.0000 norm=1.6220
[iter 400] loss=0.8748 val_loss=0.0000 scale=4.0000 norm=3.2478
[iter 0] loss=1.4090 val_loss=0.0000 scale=2.0000 norm=1.9893
[iter 100] loss=1.0927 val_loss=0.0000 scale=2.0000 norm=1.7285
[iter 200] loss=0.9529 val_loss=0.0000 scale=2.0000 norm=1.7187
[iter 300] loss=0.8509 val_loss=0.0000 scale=4.0000 norm=3.4434
[iter 400] loss=0.7516 val_loss=0.0000 scale=2.0000 norm=1.7281
[iter 0] loss=2.8001 val_loss=0.0000 scale=2.0000 norm=6.4598
[iter 100] loss=2.4933 val_loss=0.0000 scale=2.0000 norm=5.8733
[iter 200] loss=2.2217 val_loss=0.0000 scale=4.0000 norm=11.7211
[iter 300] loss=1.7288 val_loss=0.0000 scale=4.0000 norm=11.7347
[iter 400] loss=1.2276 val_loss=0.0000 scale=4.0000 norm=11.7359
[iter 0] loss=1.2787 val_loss=0.0000 scale=1.0000 norm=0.9183
[iter 100] loss=1.0387 val_loss=0.0000 scale=2.0000 norm=1.6111
[iter 200] loss=0.9869 val_loss=0.0000 scale=2.0000 norm=1.6396
[iter 300] loss=0.9533 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 400] loss=0.9236 val_loss=0.0000 scale=2.0000 norm=1.6470
[iter 0] loss=1.4284 val_loss=0.0000 scale=2.0000 norm=2.0102
[iter 100] loss=1.1100 val_loss=0.0000 scale=2.0000 norm=1.7447
[iter 200] loss=0.9652 val_loss=0.0000 scale=2.0000 norm=1.7429
[iter 300] loss=0.8623 val_loss=0.0000 scale=2.0000 norm=1.7582
[iter 400] loss=0.7494 val_loss=0.0000 scale=2.0000 norm=1.7667
[iter 0] loss=2.8414 val_loss=0.0000 scale=2.0000 norm=7.0053
[iter 100] loss=2.5271 val_loss=0.0000 scale=2.0000 norm=6.1456
[iter 200] loss=2.2528 val_loss=0.0000 scale=4.0000 norm=12.1663
[iter 300] loss=1.8412 val_loss=0.0000 scale=4.0000 norm=12.1655
[iter 400] loss=1.3358 val_loss=0.0000 scale=4.0000 norm=12.1642
[iter 0] loss=1.3013 val_loss=0.0000 scale=1.0000 norm=0.9306
[iter 100] loss=1.0408 val_loss=0.0000 scale=2.0000 norm=1.6283
[iter 200] loss=0.9789 val_loss=0.0000 scale=2.0000 norm=1.6535
[iter 300] loss=0.9393 val_loss=0.0000 scale=2.0000 norm=1.6651
[iter 400] loss=0.8992 val_loss=0.0000 scale=2.0000 norm=1.6666
[iter 0] loss=1.3872 val_loss=0.0000 scale=2.0000 norm=1.9665
[iter 100] loss=1.0493 val_loss=0.0000 scale=2.0000 norm=1.6875
[iter 200] loss=0.8973 val_loss=0.0000 scale=2.0000 norm=1.6719
[iter 300] loss=0.7913 val_loss=0.0000 scale=4.0000 norm=3.3431
[iter 400] loss=0.6705 val_loss=0.0000 scale=2.0000 norm=1.6766
[iter 0] loss=2.8061 val_loss=0.0000 scale=2.0000 norm=6.5567
[iter 100] loss=2.4894 val_loss=0.0000 scale=2.0000 norm=5.8883
[iter 200] loss=2.2065 val_loss=0.0000 scale=2.0000 norm=5.8623
[iter 300] loss=1.7029 val_loss=0.0000 scale=4.0000 norm=11.7279
[iter 400] loss=1.2032 val_loss=0.0000 scale=4.0000 norm=11.7270
[iter 0] loss=1.3236 val_loss=0.0000 scale=1.0000 norm=0.9435
[iter 100] loss=1.0934 val_loss=0.0000 scale=2.0000 norm=1.6802
[iter 200] loss=1.0381 val_loss=0.0000 scale=2.0000 norm=1.7047
[iter 300] loss=1.0037 val_loss=0.0000 scale=2.0000 norm=1.7143
[iter 400] loss=0.9640 val_loss=0.0000 scale=4.0000 norm=3.4323
[iter 0] loss=1.4085 val_loss=0.0000 scale=1.0000 norm=0.9945
[iter 100] loss=1.1023 val_loss=0.0000 scale=2.0000 norm=1.7298
[iter 200] loss=0.9757 val_loss=0.0000 scale=2.0000 norm=1.7190
[iter 300] loss=0.8944 val_loss=0.0000 scale=2.0000 norm=1.7239
[iter 400] loss=0.8147 val_loss=0.0000 scale=2.0000 norm=1.7288
[iter 0] loss=2.8206 val_loss=0.0000 scale=2.0000 norm=6.7138
[iter 100] loss=2.5061 val_loss=0.0000 scale=2.0000 norm=5.9507
[iter 200] loss=2.2274 val_loss=0.0000 scale=4.0000 norm=11.8193
[iter 300] loss=1.7688 val_loss=0.0000 scale=4.0000 norm=11.8186
[iter 400] loss=1.3117 val_loss=0.0000 scale=4.0000 norm=11.8178
[iter 0] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=0.9430
[iter 100] loss=1.0494 val_loss=0.0000 scale=2.0000 norm=1.6522
[iter 200] loss=0.9342 val_loss=0.0000 scale=2.0000 norm=1.6652
[iter 300] loss=0.8300 val_loss=0.0000 scale=2.0000 norm=1.6740
[iter 400] loss=0.7235 val_loss=0.0000 scale=4.0000 norm=3.3510
[iter 0] loss=1.3545 val_loss=0.0000 scale=1.0000 norm=0.9669
[iter 100] loss=1.0568 val_loss=0.0000 scale=2.0000 norm=1.6725
[iter 200] loss=0.9231 val_loss=0.0000 scale=2.0000 norm=1.6627
[iter 300] loss=0.8326 val_loss=0.0000 scale=2.0000 norm=1.6661
[iter 400] loss=0.7465 val_loss=0.0000 scale=2.0000 norm=1.6673
[iter 0] loss=2.8016 val_loss=0.0000 scale=2.0000 norm=6.4951
[iter 100] loss=2.5001 val_loss=0.0000 scale=2.0000 norm=5.8010
[iter 200] loss=2.2249 val_loss=0.0000 scale=2.0000 norm=5.7682
[iter 300] loss=1.7497 val_loss=0.0000 scale=4.0000 norm=11.5391
[iter 400] loss=1.2684 val_loss=0.0000 scale=4.0000 norm=11.5399
[iter 0] loss=1.2674 val_loss=0.0000 scale=1.0000 norm=0.9112
[iter 100] loss=1.0662 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 200] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6878
[iter 300] loss=0.9978 val_loss=0.0000 scale=2.0000 norm=1.6955
[iter 400] loss=0.9679 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 0] loss=1.3258 val_loss=0.0000 scale=2.0000 norm=1.9016
[iter 100] loss=1.0257 val_loss=0.0000 scale=2.0000 norm=1.6484
[iter 200] loss=0.8896 val_loss=0.0000 scale=2.0000 norm=1.6399
[iter 300] loss=0.7865 val_loss=0.0000 scale=4.0000 norm=3.2952
[iter 400] loss=0.6791 val_loss=0.0000 scale=2.0000 norm=1.6498
[iter 0] loss=2.8212 val_loss=0.0000 scale=2.0000 norm=6.7227
[iter 100] loss=2.5007 val_loss=0.0000 scale=2.0000 norm=6.0177
[iter 200] loss=2.2456 val_loss=0.0000 scale=2.0000 norm=5.9848
[iter 300] loss=1.7576 val_loss=0.0000 scale=4.0000 norm=11.9794
[iter 400] loss=1.2564 val_loss=0.0000 scale=4.0000 norm=11.9804
[iter 0] loss=1.2773 val_loss=0.0000 scale=1.0000 norm=0.9179
[iter 100] loss=1.0564 val_loss=0.0000 scale=2.0000 norm=1.6369
[iter 200] loss=1.0084 val_loss=0.0000 scale=2.0000 norm=1.6634
[iter 300] loss=0.9790 val_loss=0.0000 scale=2.0000 norm=1.6727
[iter 400] loss=0.9533 val_loss=0.0000 scale=2.0000 norm=1.6760
[iter 0] loss=1.3749 val_loss=0.0000 scale=2.0000 norm=1.9544
[iter 100] loss=1.0587 val_loss=0.0000 scale=2.0000 norm=1.6875
[iter 200] loss=0.9353 val_loss=0.0000 scale=2.0000 norm=1.6775
[iter 300] loss=0.8336 val_loss=0.0000 scale=4.0000 norm=3.3657
[iter 400] loss=0.7112 val_loss=0.0000 scale=4.0000 norm=3.3615
[iter 0] loss=2.8275 val_loss=0.0000 scale=2.0000 norm=6.8253
[iter 100] loss=2.5367 val_loss=0.0000 scale=2.0000 norm=6.0890
[iter 200] loss=2.2303 val_loss=0.0000 scale=4.0000 norm=12.1121
[iter 300] loss=1.7650 val_loss=0.0000 scale=4.0000 norm=12.1079
[iter 400] loss=1.3055 val_loss=0.0000 scale=4.0000 norm=12.1073
[iter 0] loss=1.2996 val_loss=0.0000 scale=1.0000 norm=0.9298
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.5421
[iter 200] loss=0.9100 val_loss=0.0000 scale=2.0000 norm=1.5816
[iter 300] loss=0.8770 val_loss=0.0000 scale=4.0000 norm=3.1907
[iter 400] loss=0.8446 val_loss=0.0000 scale=2.0000 norm=1.5972
[iter 0] loss=1.4823 val_loss=0.0000 scale=2.0000 norm=2.0697
[iter 100] loss=1.1600 val_loss=0.0000 scale=2.0000 norm=1.7919
[iter 200] loss=1.0213 val_loss=0.0000 scale=2.0000 norm=1.7795
[iter 300] loss=0.9222 val_loss=0.0000 scale=4.0000 norm=3.5578
[iter 400] loss=0.8067 val_loss=0.0000 scale=2.0000 norm=1.7869
[iter 0] loss=2.8264 val_loss=0.0000 scale=2.0000 norm=6.8194
[iter 100] loss=2.5198 val_loss=0.0000 scale=2.0000 norm=6.1169
[iter 200] loss=2.2741 val_loss=0.0000 scale=2.0000 norm=6.0756
[iter 300] loss=1.8518 val_loss=0.0000 scale=4.0000 norm=12.1583
[iter 400] loss=1.3758 val_loss=0.0000 scale=4.0000 norm=12.1603
[iter 0] loss=1.2821 val_loss=0.0000 scale=1.0000 norm=0.9197
[iter 100] loss=1.0107 val_loss=0.0000 scale=2.0000 norm=1.5829
[iter 200] loss=0.9539 val_loss=0.0000 scale=2.0000 norm=1.6137
[iter 300] loss=0.9258 val_loss=0.0000 scale=2.0000 norm=1.6258
[iter 400] loss=0.8987 val_loss=0.0000 scale=2.0000 norm=1.6268
[iter 0] loss=1.3859 val_loss=0.0000 scale=1.0000 norm=0.9823
[iter 100] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.6291
[iter 200] loss=0.8320 val_loss=0.0000 scale=2.0000 norm=1.6159
[iter 300] loss=0.7114 val_loss=0.0000 scale=2.0000 norm=1.6243
[iter 400] loss=0.5952 val_loss=0.0000 scale=2.0000 norm=1.6292
[iter 0] loss=2.7999 val_loss=0.0000 scale=2.0000 norm=6.4988
[iter 100] loss=2.4535 val_loss=0.0000 scale=2.0000 norm=5.7317
[iter 200] loss=2.1861 val_loss=0.0000 scale=2.0000 norm=5.6923
[iter 300] loss=1.7185 val_loss=0.0000 scale=4.0000 norm=11.3921
[iter 400] loss=1.1997 val_loss=0.0000 scale=4.0000 norm=11.3920
[iter 0] loss=1.3217 val_loss=0.0000 scale=1.0000 norm=0.9423
[iter 100] loss=1.0595 val_loss=0.0000 scale=2.0000 norm=1.6514
[iter 200] loss=0.9534 val_loss=0.0000 scale=2.0000 norm=1.6689
[iter 300] loss=0.8495 val_loss=0.0000 scale=2.0000 norm=1.6728
[iter 400] loss=0.7300 val_loss=0.0000 scale=2.0000 norm=1.6744
[iter 0] loss=1.3780 val_loss=0.0000 scale=2.0000 norm=1.9562
[iter 100] loss=1.0666 val_loss=0.0000 scale=2.0000 norm=1.6932
[iter 200] loss=0.9258 val_loss=0.0000 scale=2.0000 norm=1.6810
[iter 300] loss=0.8001 val_loss=0.0000 scale=4.0000 norm=3.3670
[iter 400] loss=0.6292 val_loss=0.0000 scale=4.0000 norm=3.3717
[iter 0] loss=2.8287 val_loss=0.0000 scale=2.0000 norm=6.8673
[iter 100] loss=2.5270 val_loss=0.0000 scale=2.0000 norm=6.1181
[iter 200] loss=2.2732 val_loss=0.0000 scale=4.0000 norm=12.1529
[iter 300] loss=1.7931 val_loss=0.0000 scale=4.0000 norm=12.1548
[iter 400] loss=1.3168 val_loss=0.0000 scale=4.0000 norm=12.1548
[iter 0] loss=1.2162 val_loss=0.0000 scale=1.0000 norm=0.8824
[iter 100] loss=0.9820 val_loss=0.0000 scale=2.0000 norm=1.5616
[iter 200] loss=0.9276 val_loss=0.0000 scale=2.0000 norm=1.5885
[iter 300] loss=0.8978 val_loss=0.0000 scale=2.0000 norm=1.6000
[iter 400] loss=0.8701 val_loss=0.0000 scale=2.0000 norm=1.6015
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0549
[iter 100] loss=1.1467 val_loss=0.0000 scale=2.0000 norm=1.7823
[iter 200] loss=1.0054 val_loss=0.0000 scale=2.0000 norm=1.7739
[iter 300] loss=0.8971 val_loss=0.0000 scale=2.0000 norm=1.7834
[iter 400] loss=0.7938 val_loss=0.0000 scale=2.0000 norm=1.7885
[iter 0] loss=2.8181 val_loss=0.0000 scale=2.0000 norm=6.7371
[iter 100] loss=2.4956 val_loss=0.0000 scale=2.0000 norm=5.9697
[iter 200] loss=2.2132 val_loss=0.0000 scale=2.0000 norm=5.9263
[iter 300] loss=1.7163 val_loss=0.0000 scale=4.0000 norm=11.8564
[iter 400] loss=1.1914 val_loss=0.0000 scale=4.0000 norm=11.8552
[iter 0] loss=1.2764 val_loss=0.0000 scale=1.0000 norm=0.9169
[iter 100] loss=1.0446 val_loss=0.0000 scale=2.0000 norm=1.6359
[iter 200] loss=0.9854 val_loss=0.0000 scale=2.0000 norm=1.6626
[iter 300] loss=0.9468 val_loss=0.0000 scale=2.0000 norm=1.6673
[iter 400] loss=0.9102 val_loss=0.0000 scale=2.0000 norm=1.6689
[iter 0] loss=1.3441 val_loss=0.0000 scale=2.0000 norm=1.9210
[iter 100] loss=1.0079 val_loss=0.0000 scale=2.0000 norm=1.6354
[iter 200] loss=0.8574 val_loss=0.0000 scale=2.0000 norm=1.6250
[iter 300] loss=0.7369 val_loss=0.0000 scale=2.0000 norm=1.6260
[iter 400] loss=0.6113 val_loss=0.0000 scale=4.0000 norm=3.2522
[iter 0] loss=2.8108 val_loss=0.0000 scale=2.0000 norm=6.6583
[iter 100] loss=2.4963 val_loss=0.0000 scale=2.0000 norm=5.9678
[iter 200] loss=2.2100 val_loss=0.0000 scale=4.0000 norm=11.8810
[iter 300] loss=1.6784 val_loss=0.0000 scale=4.0000 norm=11.8862
[iter 400] loss=1.1530 val_loss=0.0000 scale=4.0000 norm=11.8858
[iter 0] loss=1.2520 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0181 val_loss=0.0000 scale=2.0000 norm=1.5975
[iter 200] loss=0.9687 val_loss=0.0000 scale=2.0000 norm=1.6183
[iter 300] loss=0.9408 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 400] loss=0.9160 val_loss=0.0000 scale=2.0000 norm=1.6300
[iter 0] loss=1.3875 val_loss=0.0000 scale=2.0000 norm=1.9660
[iter 100] loss=1.1015 val_loss=0.0000 scale=2.0000 norm=1.7399
[iter 200] loss=0.9645 val_loss=0.0000 scale=2.0000 norm=1.7311
[iter 300] loss=0.8642 val_loss=0.0000 scale=4.0000 norm=3.4825
[iter 400] loss=0.7063 val_loss=0.0000 scale=4.0000 norm=3.4921
[iter 0] loss=2.8579 val_loss=0.0000 scale=2.0000 norm=7.2441
[iter 100] loss=2.5639 val_loss=0.0000 scale=2.0000 norm=6.4547
[iter 200] loss=2.3004 val_loss=0.0000 scale=4.0000 norm=12.8292
[iter 300] loss=1.8116 val_loss=0.0000 scale=4.0000 norm=12.8366
[iter 400] loss=1.3223 val_loss=0.0000 scale=4.0000 norm=12.8373
[iter 0] loss=1.2247 val_loss=0.0000 scale=1.0000 norm=0.8869
[iter 100] loss=0.9850 val_loss=0.0000 scale=2.0000 norm=1.5884
[iter 200] loss=0.9192 val_loss=0.0000 scale=2.0000 norm=1.6129
[iter 300] loss=0.8816 val_loss=0.0000 scale=2.0000 norm=1.6191
[iter 400] loss=0.8580 val_loss=0.0000 scale=2.0000 norm=1.6250
[iter 0] loss=1.3487 val_loss=0.0000 scale=1.0000 norm=0.9639
[iter 100] loss=1.0250 val_loss=0.0000 scale=2.0000 norm=1.6668
[iter 200] loss=0.8868 val_loss=0.0000 scale=2.0000 norm=1.6526
[iter 300] loss=0.7885 val_loss=0.0000 scale=2.0000 norm=1.6540
[iter 400] loss=0.6747 val_loss=0.0000 scale=4.0000 norm=3.3082
[iter 0] loss=2.7909 val_loss=0.0000 scale=2.0000 norm=6.4074
[iter 100] loss=2.4895 val_loss=0.0000 scale=2.0000 norm=5.8073
[iter 200] loss=2.2128 val_loss=0.0000 scale=4.0000 norm=11.5869
[iter 300] loss=1.7474 val_loss=0.0000 scale=4.0000 norm=11.5952
[iter 400] loss=1.2772 val_loss=0.0000 scale=4.0000 norm=11.5957
[iter 0] loss=1.3199 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0369 val_loss=0.0000 scale=2.0000 norm=1.6437
[iter 200] loss=0.9494 val_loss=0.0000 scale=2.0000 norm=1.6677
[iter 300] loss=0.8740 val_loss=0.0000 scale=2.0000 norm=1.6755
[iter 400] loss=0.7823 val_loss=0.0000 scale=2.0000 norm=1.6761
[iter 0] loss=1.3652 val_loss=0.0000 scale=2.0000 norm=1.9423
[iter 100] loss=1.0792 val_loss=0.0000 scale=2.0000 norm=1.7078
[iter 200] loss=0.9431 val_loss=0.0000 scale=2.0000 norm=1.6916
[iter 300] loss=0.8457 val_loss=0.0000 scale=2.0000 norm=1.6926
[iter 400] loss=0.7326 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 0] loss=2.8172 val_loss=0.0000 scale=2.0000 norm=6.6655
[iter 100] loss=2.5102 val_loss=0.0000 scale=2.0000 norm=5.9937
[iter 200] loss=2.2533 val_loss=0.0000 scale=2.0000 norm=5.9622
[iter 300] loss=1.8007 val_loss=0.0000 scale=4.0000 norm=11.9334
[iter 400] loss=1.3178 val_loss=0.0000 scale=4.0000 norm=11.9352
[iter 0] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=0.9089
[iter 100] loss=1.0088 val_loss=0.0000 scale=2.0000 norm=1.5960
[iter 200] loss=0.9144 val_loss=0.0000 scale=2.0000 norm=1.6158
[iter 300] loss=0.8368 val_loss=0.0000 scale=4.0000 norm=3.2463
[iter 400] loss=0.7502 val_loss=0.0000 scale=2.0000 norm=1.6251
[iter 0] loss=1.3664 val_loss=0.0000 scale=2.0000 norm=1.9434
[iter 100] loss=1.0314 val_loss=0.0000 scale=2.0000 norm=1.6698
[iter 200] loss=0.8786 val_loss=0.0000 scale=2.0000 norm=1.6630
[iter 300] loss=0.7650 val_loss=0.0000 scale=4.0000 norm=3.3494
[iter 400] loss=0.6117 val_loss=0.0000 scale=4.0000 norm=3.3548
[iter 0] loss=2.8399 val_loss=0.0000 scale=2.0000 norm=6.9733
[iter 100] loss=2.5337 val_loss=0.0000 scale=2.0000 norm=6.2192
[iter 200] loss=2.2629 val_loss=0.0000 scale=4.0000 norm=12.3774
[iter 300] loss=1.7623 val_loss=0.0000 scale=4.0000 norm=12.3857
[iter 400] loss=1.2613 val_loss=0.0000 scale=4.0000 norm=12.3870
[iter 0] loss=1.2821 val_loss=0.0000 scale=1.0000 norm=0.9197
[iter 100] loss=1.0107 val_loss=0.0000 scale=2.0000 norm=1.5829
[iter 200] loss=0.9539 val_loss=0.0000 scale=2.0000 norm=1.6137
[iter 300] loss=0.9258 val_loss=0.0000 scale=2.0000 norm=1.6258
[iter 400] loss=0.8987 val_loss=0.0000 scale=2.0000 norm=1.6268
[iter 0] loss=1.3859 val_loss=0.0000 scale=1.0000 norm=0.9823
[iter 100] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.6291
[iter 200] loss=0.8320 val_loss=0.0000 scale=2.0000 norm=1.6159
[iter 300] loss=0.7114 val_loss=0.0000 scale=2.0000 norm=1.6243
[iter 400] loss=0.5952 val_loss=0.0000 scale=2.0000 norm=1.6292
[iter 0] loss=2.7999 val_loss=0.0000 scale=2.0000 norm=6.4988
[iter 100] loss=2.4535 val_loss=0.0000 scale=2.0000 norm=5.7317
[iter 200] loss=2.1861 val_loss=0.0000 scale=2.0000 norm=5.6923
[iter 300] loss=1.7185 val_loss=0.0000 scale=4.0000 norm=11.3921
[iter 400] loss=1.1997 val_loss=0.0000 scale=4.0000 norm=11.3920
[iter 0] loss=1.2787 val_loss=0.0000 scale=1.0000 norm=0.9183
[iter 100] loss=1.0387 val_loss=0.0000 scale=2.0000 norm=1.6111
[iter 200] loss=0.9869 val_loss=0.0000 scale=2.0000 norm=1.6396
[iter 300] loss=0.9533 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 400] loss=0.9236 val_loss=0.0000 scale=2.0000 norm=1.6470
[iter 0] loss=1.4284 val_loss=0.0000 scale=2.0000 norm=2.0102
[iter 100] loss=1.1100 val_loss=0.0000 scale=2.0000 norm=1.7447
[iter 200] loss=0.9652 val_loss=0.0000 scale=2.0000 norm=1.7429
[iter 300] loss=0.8623 val_loss=0.0000 scale=2.0000 norm=1.7582
[iter 400] loss=0.7494 val_loss=0.0000 scale=2.0000 norm=1.7667
[iter 0] loss=2.8414 val_loss=0.0000 scale=2.0000 norm=7.0053
[iter 100] loss=2.5271 val_loss=0.0000 scale=2.0000 norm=6.1456
[iter 200] loss=2.2528 val_loss=0.0000 scale=4.0000 norm=12.1663
[iter 300] loss=1.8412 val_loss=0.0000 scale=4.0000 norm=12.1655
[iter 400] loss=1.3358 val_loss=0.0000 scale=4.0000 norm=12.1642
[iter 0] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=0.9402
[iter 100] loss=1.0715 val_loss=0.0000 scale=2.0000 norm=1.6594
[iter 200] loss=1.0093 val_loss=0.0000 scale=2.0000 norm=1.6909
[iter 300] loss=0.9698 val_loss=0.0000 scale=2.0000 norm=1.6995
[iter 400] loss=0.9325 val_loss=0.0000 scale=2.0000 norm=1.7009
[iter 0] loss=1.3368 val_loss=0.0000 scale=1.0000 norm=0.9551
[iter 100] loss=1.0019 val_loss=0.0000 scale=2.0000 norm=1.6083
[iter 200] loss=0.8499 val_loss=0.0000 scale=2.0000 norm=1.6034
[iter 300] loss=0.7418 val_loss=0.0000 scale=2.0000 norm=1.6150
[iter 400] loss=0.6343 val_loss=0.0000 scale=4.0000 norm=3.2371
[iter 0] loss=2.8212 val_loss=0.0000 scale=2.0000 norm=6.7590
[iter 100] loss=2.4871 val_loss=0.0000 scale=2.0000 norm=5.9671
[iter 200] loss=2.1782 val_loss=0.0000 scale=4.0000 norm=11.8553
[iter 300] loss=1.6545 val_loss=0.0000 scale=4.0000 norm=11.8617
[iter 400] loss=1.1300 val_loss=0.0000 scale=4.0000 norm=11.8629
[iter 0] loss=1.2359 val_loss=0.0000 scale=1.0000 norm=0.8946
[iter 100] loss=1.0101 val_loss=0.0000 scale=2.0000 norm=1.6050
[iter 200] loss=0.9550 val_loss=0.0000 scale=2.0000 norm=1.6328
[iter 300] loss=0.9188 val_loss=0.0000 scale=2.0000 norm=1.6440
[iter 400] loss=0.8926 val_loss=0.0000 scale=2.0000 norm=1.6492
[iter 0] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=0.9463
[iter 100] loss=1.0072 val_loss=0.0000 scale=2.0000 norm=1.6366
[iter 200] loss=0.8666 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 300] loss=0.7639 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 400] loss=0.6675 val_loss=0.0000 scale=2.0000 norm=1.6326
[iter 0] loss=2.7903 val_loss=0.0000 scale=2.0000 norm=6.3676
[iter 100] loss=2.4692 val_loss=0.0000 scale=2.0000 norm=5.6532
[iter 200] loss=2.1816 val_loss=0.0000 scale=4.0000 norm=11.2381
[iter 300] loss=1.6613 val_loss=0.0000 scale=4.0000 norm=11.2412
[iter 400] loss=1.1421 val_loss=0.0000 scale=4.0000 norm=11.2413
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9184
[iter 100] loss=1.0806 val_loss=0.0000 scale=2.0000 norm=1.6685
[iter 200] loss=1.0293 val_loss=0.0000 scale=2.0000 norm=1.6974
[iter 300] loss=1.0011 val_loss=0.0000 scale=2.0000 norm=1.7071
[iter 400] loss=0.9714 val_loss=0.0000 scale=2.0000 norm=1.7091
[iter 0] loss=1.3537 val_loss=0.0000 scale=2.0000 norm=1.9311
[iter 100] loss=1.0272 val_loss=0.0000 scale=2.0000 norm=1.6634
[iter 200] loss=0.8954 val_loss=0.0000 scale=2.0000 norm=1.6530
[iter 300] loss=0.8060 val_loss=0.0000 scale=2.0000 norm=1.6588
[iter 400] loss=0.7127 val_loss=0.0000 scale=2.0000 norm=1.6615
[iter 0] loss=2.8256 val_loss=0.0000 scale=2.0000 norm=6.8131
[iter 100] loss=2.5116 val_loss=0.0000 scale=2.0000 norm=6.0585
[iter 200] loss=2.2509 val_loss=0.0000 scale=2.0000 norm=6.0216
[iter 300] loss=1.7568 val_loss=0.0000 scale=4.0000 norm=12.0466
[iter 400] loss=1.2622 val_loss=0.0000 scale=4.0000 norm=12.0454
[iter 0] loss=1.3108 val_loss=0.0000 scale=1.0000 norm=0.9365
[iter 100] loss=0.9993 val_loss=0.0000 scale=2.0000 norm=1.5717
[iter 200] loss=0.9357 val_loss=0.0000 scale=2.0000 norm=1.6060
[iter 300] loss=0.8997 val_loss=0.0000 scale=2.0000 norm=1.6190
[iter 400] loss=0.8661 val_loss=0.0000 scale=4.0000 norm=3.2434
[iter 0] loss=1.4291 val_loss=0.0000 scale=2.0000 norm=2.0111
[iter 100] loss=1.0707 val_loss=0.0000 scale=2.0000 norm=1.7065
[iter 200] loss=0.9103 val_loss=0.0000 scale=2.0000 norm=1.6925
[iter 300] loss=0.8106 val_loss=0.0000 scale=2.0000 norm=1.7063
[iter 400] loss=0.7160 val_loss=0.0000 scale=2.0000 norm=1.7169
[iter 0] loss=2.8053 val_loss=0.0000 scale=2.0000 norm=6.5256
[iter 100] loss=2.4773 val_loss=0.0000 scale=2.0000 norm=5.7784
[iter 200] loss=2.1455 val_loss=0.0000 scale=4.0000 norm=11.4657
[iter 300] loss=1.6147 val_loss=0.0000 scale=4.0000 norm=11.4643
[iter 400] loss=1.0896 val_loss=0.0000 scale=4.0000 norm=11.4638
[iter 0] loss=1.3013 val_loss=0.0000 scale=1.0000 norm=0.9306
[iter 100] loss=1.0408 val_loss=0.0000 scale=2.0000 norm=1.6283
[iter 200] loss=0.9789 val_loss=0.0000 scale=2.0000 norm=1.6535
[iter 300] loss=0.9393 val_loss=0.0000 scale=2.0000 norm=1.6651
[iter 400] loss=0.8992 val_loss=0.0000 scale=2.0000 norm=1.6666
[iter 0] loss=1.3872 val_loss=0.0000 scale=2.0000 norm=1.9665
[iter 100] loss=1.0493 val_loss=0.0000 scale=2.0000 norm=1.6875
[iter 200] loss=0.8973 val_loss=0.0000 scale=2.0000 norm=1.6719
[iter 300] loss=0.7913 val_loss=0.0000 scale=4.0000 norm=3.3431
[iter 400] loss=0.6705 val_loss=0.0000 scale=2.0000 norm=1.6766
[iter 0] loss=2.8061 val_loss=0.0000 scale=2.0000 norm=6.5567
[iter 100] loss=2.4894 val_loss=0.0000 scale=2.0000 norm=5.8883
[iter 200] loss=2.2065 val_loss=0.0000 scale=2.0000 norm=5.8623
[iter 300] loss=1.7029 val_loss=0.0000 scale=4.0000 norm=11.7279
[iter 400] loss=1.2032 val_loss=0.0000 scale=4.0000 norm=11.7270
[iter 0] loss=1.3236 val_loss=0.0000 scale=1.0000 norm=0.9435
[iter 100] loss=1.0934 val_loss=0.0000 scale=2.0000 norm=1.6802
[iter 200] loss=1.0381 val_loss=0.0000 scale=2.0000 norm=1.7047
[iter 300] loss=1.0037 val_loss=0.0000 scale=2.0000 norm=1.7143
[iter 400] loss=0.9640 val_loss=0.0000 scale=4.0000 norm=3.4323
[iter 0] loss=1.4085 val_loss=0.0000 scale=1.0000 norm=0.9945
[iter 100] loss=1.1023 val_loss=0.0000 scale=2.0000 norm=1.7298
[iter 200] loss=0.9757 val_loss=0.0000 scale=2.0000 norm=1.7190
[iter 300] loss=0.8944 val_loss=0.0000 scale=2.0000 norm=1.7239
[iter 400] loss=0.8147 val_loss=0.0000 scale=2.0000 norm=1.7288
[iter 0] loss=2.8206 val_loss=0.0000 scale=2.0000 norm=6.7138
[iter 100] loss=2.5064 val_loss=0.0000 scale=2.0000 norm=5.9522
[iter 200] loss=2.2392 val_loss=0.0000 scale=4.0000 norm=11.8203
[iter 300] loss=1.7802 val_loss=0.0000 scale=4.0000 norm=11.8187
[iter 400] loss=1.3232 val_loss=0.0000 scale=4.0000 norm=11.8178
[iter 0] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=0.9430
[iter 100] loss=1.0494 val_loss=0.0000 scale=2.0000 norm=1.6522
[iter 200] loss=0.9342 val_loss=0.0000 scale=2.0000 norm=1.6652
[iter 300] loss=0.8300 val_loss=0.0000 scale=2.0000 norm=1.6740
[iter 400] loss=0.7235 val_loss=0.0000 scale=4.0000 norm=3.3510
[iter 0] loss=1.3545 val_loss=0.0000 scale=1.0000 norm=0.9669
[iter 100] loss=1.0568 val_loss=0.0000 scale=2.0000 norm=1.6725
[iter 200] loss=0.9231 val_loss=0.0000 scale=2.0000 norm=1.6627
[iter 300] loss=0.8326 val_loss=0.0000 scale=2.0000 norm=1.6661
[iter 400] loss=0.7465 val_loss=0.0000 scale=2.0000 norm=1.6673
[iter 0] loss=2.8016 val_loss=0.0000 scale=2.0000 norm=6.4951
[iter 100] loss=2.5001 val_loss=0.0000 scale=2.0000 norm=5.8010
[iter 200] loss=2.2249 val_loss=0.0000 scale=2.0000 norm=5.7682
[iter 300] loss=1.7497 val_loss=0.0000 scale=4.0000 norm=11.5391
[iter 400] loss=1.2684 val_loss=0.0000 scale=4.0000 norm=11.5399
[iter 0] loss=1.2674 val_loss=0.0000 scale=1.0000 norm=0.9112
[iter 100] loss=1.0662 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 200] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6878
[iter 300] loss=0.9978 val_loss=0.0000 scale=2.0000 norm=1.6955
[iter 400] loss=0.9679 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 0] loss=1.3258 val_loss=0.0000 scale=2.0000 norm=1.9016
[iter 100] loss=1.0257 val_loss=0.0000 scale=2.0000 norm=1.6484
[iter 200] loss=0.8896 val_loss=0.0000 scale=2.0000 norm=1.6399
[iter 300] loss=0.7865 val_loss=0.0000 scale=4.0000 norm=3.2952
[iter 400] loss=0.6791 val_loss=0.0000 scale=2.0000 norm=1.6498
[iter 0] loss=2.8212 val_loss=0.0000 scale=2.0000 norm=6.7227
[iter 100] loss=2.5007 val_loss=0.0000 scale=2.0000 norm=6.0177
[iter 200] loss=2.2456 val_loss=0.0000 scale=2.0000 norm=5.9848
[iter 300] loss=1.7576 val_loss=0.0000 scale=4.0000 norm=11.9794
[iter 400] loss=1.2564 val_loss=0.0000 scale=4.0000 norm=11.9804
[iter 0] loss=1.2162 val_loss=0.0000 scale=1.0000 norm=0.8824
[iter 100] loss=0.9820 val_loss=0.0000 scale=2.0000 norm=1.5616
[iter 200] loss=0.9276 val_loss=0.0000 scale=2.0000 norm=1.5885
[iter 300] loss=0.8978 val_loss=0.0000 scale=2.0000 norm=1.6000
[iter 400] loss=0.8701 val_loss=0.0000 scale=2.0000 norm=1.6015
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0549
[iter 100] loss=1.1467 val_loss=0.0000 scale=2.0000 norm=1.7823
[iter 200] loss=1.0054 val_loss=0.0000 scale=2.0000 norm=1.7739
[iter 300] loss=0.8971 val_loss=0.0000 scale=2.0000 norm=1.7834
[iter 400] loss=0.7938 val_loss=0.0000 scale=2.0000 norm=1.7885
[iter 0] loss=2.8181 val_loss=0.0000 scale=2.0000 norm=6.7371
[iter 100] loss=2.4956 val_loss=0.0000 scale=2.0000 norm=5.9697
[iter 200] loss=2.2132 val_loss=0.0000 scale=2.0000 norm=5.9263
[iter 300] loss=1.7163 val_loss=0.0000 scale=4.0000 norm=11.8564
[iter 400] loss=1.1914 val_loss=0.0000 scale=4.0000 norm=11.8552
[iter 0] loss=1.2764 val_loss=0.0000 scale=1.0000 norm=0.9169
[iter 100] loss=1.0446 val_loss=0.0000 scale=2.0000 norm=1.6359
[iter 200] loss=0.9851 val_loss=0.0000 scale=2.0000 norm=1.6627
[iter 300] loss=0.9464 val_loss=0.0000 scale=2.0000 norm=1.6675
[iter 400] loss=0.9080 val_loss=0.0000 scale=2.0000 norm=1.6691
[iter 0] loss=1.3441 val_loss=0.0000 scale=2.0000 norm=1.9210
[iter 100] loss=1.0079 val_loss=0.0000 scale=2.0000 norm=1.6354
[iter 200] loss=0.8574 val_loss=0.0000 scale=2.0000 norm=1.6250
[iter 300] loss=0.7369 val_loss=0.0000 scale=2.0000 norm=1.6260
[iter 400] loss=0.6113 val_loss=0.0000 scale=4.0000 norm=3.2522
[iter 0] loss=2.8108 val_loss=0.0000 scale=2.0000 norm=6.6583
[iter 100] loss=2.4962 val_loss=0.0000 scale=2.0000 norm=5.9676
[iter 200] loss=2.2100 val_loss=0.0000 scale=4.0000 norm=11.8809
[iter 300] loss=1.6784 val_loss=0.0000 scale=4.0000 norm=11.8863
[iter 400] loss=1.1530 val_loss=0.0000 scale=4.0000 norm=11.8858
[iter 0] loss=1.2546 val_loss=0.0000 scale=1.0000 norm=0.9029
[iter 100] loss=0.9917 val_loss=0.0000 scale=2.0000 norm=1.5744
[iter 200] loss=0.9387 val_loss=0.0000 scale=2.0000 norm=1.6113
[iter 300] loss=0.9077 val_loss=0.0000 scale=2.0000 norm=1.6220
[iter 400] loss=0.8748 val_loss=0.0000 scale=4.0000 norm=3.2478
[iter 0] loss=1.4090 val_loss=0.0000 scale=2.0000 norm=1.9893
[iter 100] loss=1.0927 val_loss=0.0000 scale=2.0000 norm=1.7285
[iter 200] loss=0.9529 val_loss=0.0000 scale=2.0000 norm=1.7187
[iter 300] loss=0.8509 val_loss=0.0000 scale=4.0000 norm=3.4434
[iter 400] loss=0.7516 val_loss=0.0000 scale=2.0000 norm=1.7281
[iter 0] loss=2.8001 val_loss=0.0000 scale=2.0000 norm=6.4598
[iter 100] loss=2.4933 val_loss=0.0000 scale=2.0000 norm=5.8733
[iter 200] loss=2.2217 val_loss=0.0000 scale=4.0000 norm=11.7211
[iter 300] loss=1.7288 val_loss=0.0000 scale=4.0000 norm=11.7347
[iter 400] loss=1.2276 val_loss=0.0000 scale=4.0000 norm=11.7359
[iter 0] loss=1.3217 val_loss=0.0000 scale=1.0000 norm=0.9423
[iter 100] loss=1.0595 val_loss=0.0000 scale=2.0000 norm=1.6514
[iter 200] loss=0.9534 val_loss=0.0000 scale=2.0000 norm=1.6689
[iter 300] loss=0.8495 val_loss=0.0000 scale=2.0000 norm=1.6728
[iter 400] loss=0.7300 val_loss=0.0000 scale=2.0000 norm=1.6744
[iter 0] loss=1.3780 val_loss=0.0000 scale=2.0000 norm=1.9562
[iter 100] loss=1.0666 val_loss=0.0000 scale=2.0000 norm=1.6932
[iter 200] loss=0.9258 val_loss=0.0000 scale=2.0000 norm=1.6810
[iter 300] loss=0.8001 val_loss=0.0000 scale=4.0000 norm=3.3670
[iter 400] loss=0.6292 val_loss=0.0000 scale=4.0000 norm=3.3717
[iter 0] loss=2.8287 val_loss=0.0000 scale=2.0000 norm=6.8673
[iter 100] loss=2.5270 val_loss=0.0000 scale=2.0000 norm=6.1181
[iter 200] loss=2.2732 val_loss=0.0000 scale=4.0000 norm=12.1529
[iter 300] loss=1.7931 val_loss=0.0000 scale=4.0000 norm=12.1548
[iter 400] loss=1.3168 val_loss=0.0000 scale=4.0000 norm=12.1548
[iter 0] loss=1.2773 val_loss=0.0000 scale=1.0000 norm=0.9179
[iter 100] loss=1.0564 val_loss=0.0000 scale=2.0000 norm=1.6369
[iter 200] loss=1.0084 val_loss=0.0000 scale=2.0000 norm=1.6634
[iter 300] loss=0.9790 val_loss=0.0000 scale=2.0000 norm=1.6727
[iter 400] loss=0.9533 val_loss=0.0000 scale=2.0000 norm=1.6760
[iter 0] loss=1.3749 val_loss=0.0000 scale=2.0000 norm=1.9544
[iter 100] loss=1.0587 val_loss=0.0000 scale=2.0000 norm=1.6875
[iter 200] loss=0.9353 val_loss=0.0000 scale=2.0000 norm=1.6775
[iter 300] loss=0.8336 val_loss=0.0000 scale=4.0000 norm=3.3657
[iter 400] loss=0.7112 val_loss=0.0000 scale=4.0000 norm=3.3615
[iter 0] loss=2.8275 val_loss=0.0000 scale=2.0000 norm=6.8253
[iter 100] loss=2.5367 val_loss=0.0000 scale=2.0000 norm=6.0890
[iter 200] loss=2.2303 val_loss=0.0000 scale=4.0000 norm=12.1121
[iter 300] loss=1.7650 val_loss=0.0000 scale=4.0000 norm=12.1079
[iter 400] loss=1.3055 val_loss=0.0000 scale=4.0000 norm=12.1073
[iter 0] loss=1.2996 val_loss=0.0000 scale=1.0000 norm=0.9298
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.5421
[iter 200] loss=0.9100 val_loss=0.0000 scale=2.0000 norm=1.5816
[iter 300] loss=0.8770 val_loss=0.0000 scale=4.0000 norm=3.1907
[iter 400] loss=0.8446 val_loss=0.0000 scale=2.0000 norm=1.5972
[iter 0] loss=1.4823 val_loss=0.0000 scale=2.0000 norm=2.0697
[iter 100] loss=1.1600 val_loss=0.0000 scale=2.0000 norm=1.7919
[iter 200] loss=1.0213 val_loss=0.0000 scale=2.0000 norm=1.7795
[iter 300] loss=0.9222 val_loss=0.0000 scale=4.0000 norm=3.5578
[iter 400] loss=0.8067 val_loss=0.0000 scale=2.0000 norm=1.7869
[iter 0] loss=2.8264 val_loss=0.0000 scale=2.0000 norm=6.8194
[iter 100] loss=2.5198 val_loss=0.0000 scale=2.0000 norm=6.1174
[iter 200] loss=2.2715 val_loss=0.0000 scale=2.0000 norm=6.0759
[iter 300] loss=1.8348 val_loss=0.0000 scale=4.0000 norm=12.1590
[iter 400] loss=1.3590 val_loss=0.0000 scale=4.0000 norm=12.1609

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n04>
Subject: Job 304960: <mordred_NGB_Robust Scaler_multimodal Rh_20250120> in cluster <Hazel> Done

Job <mordred_NGB_Robust Scaler_multimodal Rh_20250120> was submitted from host <c207n01> by user <sdehgha2> in cluster <Hazel> at Tue Jan 21 18:17:15 2025
Job was executed on host(s) <6*c207n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Jan 21 18:17:15 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Jan 21 18:17:15 2025
Terminated at Tue Jan 21 18:28:30 2025
Results reported at Tue Jan 21 18:28:30 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 6
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Robust Scaler_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3231.09 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.84 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               11.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   676 sec.
    Turnaround time :                            675 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err> for stderr output of this job.

Average scores:	 r2: [-0.005  0.052 -0.041]±[0.176 0.098 0.022]
[array([-0.00483043,  0.05217791, -0.04056716]), array([8.68070608e+00, 1.53792763e+02, 9.38188364e+04]), array([6.42049734e+00, 1.01028485e+02, 1.41209772e+04])]
RRU Dimer
Filename: (Mordred)_NGB_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Dimer/(Mordred)_NGB_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Dimer/(Mordred)_NGB_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/RRU Dimer/(Mordred)_NGB_Robust Scaler_shape.json
Done Saving scores!
[iter 0] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=0.9370
[iter 100] loss=1.0172 val_loss=0.0000 scale=2.0000 norm=1.6018
[iter 200] loss=0.9099 val_loss=0.0000 scale=2.0000 norm=1.6171
[iter 300] loss=0.8243 val_loss=0.0000 scale=2.0000 norm=1.6290
[iter 400] loss=0.7172 val_loss=0.0000 scale=4.0000 norm=3.2628
[iter 0] loss=1.3571 val_loss=0.0000 scale=2.0000 norm=1.9347
[iter 100] loss=1.0670 val_loss=0.0000 scale=2.0000 norm=1.7047
[iter 200] loss=0.9402 val_loss=0.0000 scale=2.0000 norm=1.7046
[iter 300] loss=0.8317 val_loss=0.0000 scale=4.0000 norm=3.4253
[iter 400] loss=0.6765 val_loss=0.0000 scale=4.0000 norm=3.4313
[iter 0] loss=2.8250 val_loss=0.0000 scale=2.0000 norm=6.8125
[iter 100] loss=2.4892 val_loss=0.0000 scale=2.0000 norm=6.0109
[iter 200] loss=2.1737 val_loss=0.0000 scale=4.0000 norm=11.9535
[iter 300] loss=1.6481 val_loss=0.0000 scale=4.0000 norm=11.9528
[iter 400] loss=1.1288 val_loss=0.0000 scale=4.0000 norm=11.9531
[iter 0] loss=1.2604 val_loss=0.0000 scale=1.0000 norm=0.9080
[iter 100] loss=1.0396 val_loss=0.0000 scale=2.0000 norm=1.6185
[iter 200] loss=0.9806 val_loss=0.0000 scale=2.0000 norm=1.6431
[iter 300] loss=0.9388 val_loss=0.0000 scale=2.0000 norm=1.6524
[iter 400] loss=0.8909 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 0] loss=1.4348 val_loss=0.0000 scale=1.0000 norm=1.0085
[iter 100] loss=1.0537 val_loss=0.0000 scale=2.0000 norm=1.6815
[iter 200] loss=0.9128 val_loss=0.0000 scale=2.0000 norm=1.6831
[iter 300] loss=0.7961 val_loss=0.0000 scale=4.0000 norm=3.3855
[iter 400] loss=0.6352 val_loss=0.0000 scale=4.0000 norm=3.3835
[iter 0] loss=2.7725 val_loss=0.0000 scale=2.0000 norm=6.1605
[iter 100] loss=2.4581 val_loss=0.0000 scale=2.0000 norm=5.5531
[iter 200] loss=2.1858 val_loss=0.0000 scale=2.0000 norm=5.5242
[iter 300] loss=1.7385 val_loss=0.0000 scale=4.0000 norm=11.0616
[iter 400] loss=1.2555 val_loss=0.0000 scale=4.0000 norm=11.0638
[iter 0] loss=1.3202 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0501 val_loss=0.0000 scale=2.0000 norm=1.6394
[iter 200] loss=0.9849 val_loss=0.0000 scale=2.0000 norm=1.6647
[iter 300] loss=0.9459 val_loss=0.0000 scale=2.0000 norm=1.6747
[iter 400] loss=0.9040 val_loss=0.0000 scale=2.0000 norm=1.6768
[iter 0] loss=1.3569 val_loss=0.0000 scale=1.0000 norm=0.9670
[iter 100] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6622
[iter 200] loss=0.8712 val_loss=0.0000 scale=2.0000 norm=1.6543
[iter 300] loss=0.7389 val_loss=0.0000 scale=4.0000 norm=3.3446
[iter 400] loss=0.5721 val_loss=0.0000 scale=4.0000 norm=3.3489
[iter 0] loss=2.8478 val_loss=0.0000 scale=2.0000 norm=7.0856
[iter 100] loss=2.5189 val_loss=0.0000 scale=2.0000 norm=6.1708
[iter 200] loss=2.2163 val_loss=0.0000 scale=4.0000 norm=12.2245
[iter 300] loss=1.7156 val_loss=0.0000 scale=4.0000 norm=12.2279
[iter 400] loss=1.2143 val_loss=0.0000 scale=4.0000 norm=12.2287
[iter 0] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=0.9362
[iter 100] loss=1.0602 val_loss=0.0000 scale=2.0000 norm=1.6615
[iter 200] loss=0.9868 val_loss=0.0000 scale=2.0000 norm=1.6830
[iter 300] loss=0.9435 val_loss=0.0000 scale=4.0000 norm=3.3823
[iter 400] loss=0.8817 val_loss=0.0000 scale=4.0000 norm=3.3884
[iter 0] loss=1.3435 val_loss=0.0000 scale=2.0000 norm=1.9213
[iter 100] loss=0.9925 val_loss=0.0000 scale=2.0000 norm=1.6244
[iter 200] loss=0.8366 val_loss=0.0000 scale=2.0000 norm=1.6227
[iter 300] loss=0.6692 val_loss=0.0000 scale=4.0000 norm=3.2530
[iter 400] loss=0.4873 val_loss=0.0000 scale=4.0000 norm=3.2479
[iter 0] loss=2.8188 val_loss=0.0000 scale=2.0000 norm=6.7092
[iter 100] loss=2.5208 val_loss=0.0000 scale=2.0000 norm=6.0236
[iter 200] loss=2.2051 val_loss=0.0000 scale=4.0000 norm=12.0153
[iter 300] loss=1.7280 val_loss=0.0000 scale=4.0000 norm=12.0179
[iter 400] loss=0.9352 val_loss=0.0000 scale=8.0000 norm=24.0362
[iter 0] loss=1.2729 val_loss=0.0000 scale=1.0000 norm=0.9145
[iter 100] loss=1.0199 val_loss=0.0000 scale=2.0000 norm=1.6169
[iter 200] loss=0.9636 val_loss=0.0000 scale=2.0000 norm=1.6486
[iter 300] loss=0.9358 val_loss=0.0000 scale=2.0000 norm=1.6620
[iter 400] loss=0.9098 val_loss=0.0000 scale=2.0000 norm=1.6644
[iter 0] loss=1.3675 val_loss=0.0000 scale=2.0000 norm=1.9472
[iter 100] loss=1.0297 val_loss=0.0000 scale=2.0000 norm=1.6806
[iter 200] loss=0.8680 val_loss=0.0000 scale=2.0000 norm=1.6695
[iter 300] loss=0.6928 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 400] loss=0.5109 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 0] loss=2.8016 val_loss=0.0000 scale=2.0000 norm=6.5394
[iter 100] loss=2.4890 val_loss=0.0000 scale=2.0000 norm=5.8782
[iter 200] loss=2.1748 val_loss=0.0000 scale=4.0000 norm=11.7116
[iter 300] loss=1.6844 val_loss=0.0000 scale=4.0000 norm=11.7174
[iter 400] loss=1.1900 val_loss=0.0000 scale=4.0000 norm=11.7180
[iter 0] loss=1.2880 val_loss=0.0000 scale=1.0000 norm=0.9224
[iter 100] loss=1.0260 val_loss=0.0000 scale=2.0000 norm=1.6254
[iter 200] loss=0.9585 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 300] loss=0.9145 val_loss=0.0000 scale=4.0000 norm=3.3295
[iter 400] loss=0.8600 val_loss=0.0000 scale=4.0000 norm=3.3352
[iter 0] loss=1.3802 val_loss=0.0000 scale=2.0000 norm=1.9592
[iter 100] loss=1.0319 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 200] loss=0.8864 val_loss=0.0000 scale=2.0000 norm=1.6513
[iter 300] loss=0.7410 val_loss=0.0000 scale=4.0000 norm=3.3185
[iter 400] loss=0.5811 val_loss=0.0000 scale=4.0000 norm=3.3123
[iter 0] loss=2.8298 val_loss=0.0000 scale=2.0000 norm=6.8441
[iter 100] loss=2.5412 val_loss=0.0000 scale=2.0000 norm=6.1869
[iter 200] loss=2.2775 val_loss=0.0000 scale=4.0000 norm=12.3081
[iter 300] loss=1.8112 val_loss=0.0000 scale=4.0000 norm=12.3165
[iter 400] loss=1.3397 val_loss=0.0000 scale=8.0000 norm=24.6352
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9178
[iter 100] loss=1.0395 val_loss=0.0000 scale=2.0000 norm=1.6154
[iter 200] loss=0.9883 val_loss=0.0000 scale=2.0000 norm=1.6466
[iter 300] loss=0.9581 val_loss=0.0000 scale=2.0000 norm=1.6570
[iter 400] loss=0.9146 val_loss=0.0000 scale=4.0000 norm=3.3192
[iter 0] loss=1.3920 val_loss=0.0000 scale=1.0000 norm=0.9856
[iter 100] loss=1.0360 val_loss=0.0000 scale=2.0000 norm=1.6852
[iter 200] loss=0.8835 val_loss=0.0000 scale=2.0000 norm=1.6824
[iter 300] loss=0.7421 val_loss=0.0000 scale=4.0000 norm=3.4070
[iter 400] loss=0.5811 val_loss=0.0000 scale=4.0000 norm=3.4111
[iter 0] loss=2.8013 val_loss=0.0000 scale=2.0000 norm=6.5371
[iter 100] loss=2.4677 val_loss=0.0000 scale=2.0000 norm=5.7556
[iter 200] loss=2.1754 val_loss=0.0000 scale=2.0000 norm=5.7196
[iter 300] loss=1.6725 val_loss=0.0000 scale=4.0000 norm=11.4508
[iter 400] loss=1.1533 val_loss=0.0000 scale=4.0000 norm=11.4519
[iter 0] loss=1.2548 val_loss=0.0000 scale=1.0000 norm=0.9050
[iter 100] loss=0.9591 val_loss=0.0000 scale=2.0000 norm=1.5440
[iter 200] loss=0.8639 val_loss=0.0000 scale=2.0000 norm=1.5702
[iter 300] loss=0.7755 val_loss=0.0000 scale=2.0000 norm=1.5797
[iter 400] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.5809
[iter 0] loss=1.4331 val_loss=0.0000 scale=2.0000 norm=2.0154
[iter 100] loss=1.0984 val_loss=0.0000 scale=2.0000 norm=1.7385
[iter 200] loss=0.9489 val_loss=0.0000 scale=2.0000 norm=1.7313
[iter 300] loss=0.7975 val_loss=0.0000 scale=4.0000 norm=3.4795
[iter 400] loss=0.6342 val_loss=0.0000 scale=4.0000 norm=3.4810
[iter 0] loss=2.8338 val_loss=0.0000 scale=2.0000 norm=6.8988
[iter 100] loss=2.5204 val_loss=0.0000 scale=2.0000 norm=6.0980
[iter 200] loss=2.2669 val_loss=0.0000 scale=2.0000 norm=6.0476
[iter 300] loss=1.7995 val_loss=0.0000 scale=4.0000 norm=12.1005
[iter 400] loss=1.3044 val_loss=0.0000 scale=4.0000 norm=12.1014
[iter 0] loss=1.2880 val_loss=0.0000 scale=1.0000 norm=0.9224
[iter 100] loss=1.0260 val_loss=0.0000 scale=2.0000 norm=1.6254
[iter 200] loss=0.9585 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 300] loss=0.9145 val_loss=0.0000 scale=4.0000 norm=3.3295
[iter 400] loss=0.8600 val_loss=0.0000 scale=4.0000 norm=3.3352
[iter 0] loss=1.3802 val_loss=0.0000 scale=2.0000 norm=1.9592
[iter 100] loss=1.0319 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 200] loss=0.8864 val_loss=0.0000 scale=2.0000 norm=1.6513
[iter 300] loss=0.7410 val_loss=0.0000 scale=4.0000 norm=3.3185
[iter 400] loss=0.5811 val_loss=0.0000 scale=4.0000 norm=3.3123
[iter 0] loss=2.8298 val_loss=0.0000 scale=2.0000 norm=6.8441
[iter 100] loss=2.5412 val_loss=0.0000 scale=2.0000 norm=6.1869
[iter 200] loss=2.2775 val_loss=0.0000 scale=4.0000 norm=12.3081
[iter 300] loss=1.8112 val_loss=0.0000 scale=4.0000 norm=12.3165
[iter 400] loss=1.3397 val_loss=0.0000 scale=8.0000 norm=24.6352
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9178
[iter 100] loss=1.0395 val_loss=0.0000 scale=2.0000 norm=1.6154
[iter 200] loss=0.9883 val_loss=0.0000 scale=2.0000 norm=1.6466
[iter 300] loss=0.9581 val_loss=0.0000 scale=2.0000 norm=1.6570
[iter 400] loss=0.9146 val_loss=0.0000 scale=4.0000 norm=3.3192
[iter 0] loss=1.3920 val_loss=0.0000 scale=1.0000 norm=0.9856
[iter 100] loss=1.0360 val_loss=0.0000 scale=2.0000 norm=1.6852
[iter 200] loss=0.8835 val_loss=0.0000 scale=2.0000 norm=1.6824
[iter 300] loss=0.7421 val_loss=0.0000 scale=4.0000 norm=3.4070
[iter 400] loss=0.5811 val_loss=0.0000 scale=4.0000 norm=3.4111
[iter 0] loss=2.8013 val_loss=0.0000 scale=2.0000 norm=6.5371
[iter 100] loss=2.4677 val_loss=0.0000 scale=2.0000 norm=5.7556
[iter 200] loss=2.1754 val_loss=0.0000 scale=2.0000 norm=5.7196
[iter 300] loss=1.6725 val_loss=0.0000 scale=4.0000 norm=11.4508
[iter 400] loss=1.1533 val_loss=0.0000 scale=4.0000 norm=11.4519
[iter 0] loss=1.2548 val_loss=0.0000 scale=1.0000 norm=0.9050
[iter 100] loss=0.9591 val_loss=0.0000 scale=2.0000 norm=1.5440
[iter 200] loss=0.8639 val_loss=0.0000 scale=2.0000 norm=1.5702
[iter 300] loss=0.7755 val_loss=0.0000 scale=2.0000 norm=1.5797
[iter 400] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.5809
[iter 0] loss=1.4331 val_loss=0.0000 scale=2.0000 norm=2.0154
[iter 100] loss=1.0984 val_loss=0.0000 scale=2.0000 norm=1.7385
[iter 200] loss=0.9489 val_loss=0.0000 scale=2.0000 norm=1.7313
[iter 300] loss=0.7975 val_loss=0.0000 scale=4.0000 norm=3.4795
[iter 400] loss=0.6342 val_loss=0.0000 scale=4.0000 norm=3.4810
[iter 0] loss=2.8338 val_loss=0.0000 scale=2.0000 norm=6.8988
[iter 100] loss=2.5204 val_loss=0.0000 scale=2.0000 norm=6.0980
[iter 200] loss=2.2669 val_loss=0.0000 scale=2.0000 norm=6.0476
[iter 300] loss=1.7995 val_loss=0.0000 scale=4.0000 norm=12.1005
[iter 400] loss=1.3044 val_loss=0.0000 scale=4.0000 norm=12.1014
[iter 0] loss=1.2441 val_loss=0.0000 scale=1.0000 norm=0.9004
[iter 100] loss=1.0044 val_loss=0.0000 scale=2.0000 norm=1.6064
[iter 200] loss=0.9462 val_loss=0.0000 scale=2.0000 norm=1.6389
[iter 300] loss=0.9088 val_loss=0.0000 scale=2.0000 norm=1.6496
[iter 400] loss=0.8692 val_loss=0.0000 scale=2.0000 norm=1.6514
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0550
[iter 100] loss=1.1249 val_loss=0.0000 scale=2.0000 norm=1.7851
[iter 200] loss=0.9669 val_loss=0.0000 scale=2.0000 norm=1.7803
[iter 300] loss=0.7978 val_loss=0.0000 scale=4.0000 norm=3.5772
[iter 400] loss=0.6050 val_loss=0.0000 scale=4.0000 norm=3.5714
[iter 0] loss=2.8149 val_loss=0.0000 scale=2.0000 norm=6.6187
[iter 100] loss=2.4812 val_loss=0.0000 scale=2.0000 norm=5.8966
[iter 200] loss=2.1397 val_loss=0.0000 scale=4.0000 norm=11.7337
[iter 300] loss=1.6210 val_loss=0.0000 scale=4.0000 norm=11.7374
[iter 400] loss=1.1026 val_loss=0.0000 scale=4.0000 norm=11.7382
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9184
[iter 100] loss=1.0775 val_loss=0.0000 scale=2.0000 norm=1.6682
[iter 200] loss=1.0245 val_loss=0.0000 scale=2.0000 norm=1.6980
[iter 300] loss=0.9929 val_loss=0.0000 scale=2.0000 norm=1.7064
[iter 400] loss=0.9637 val_loss=0.0000 scale=2.0000 norm=1.7082
[iter 0] loss=1.3537 val_loss=0.0000 scale=1.0000 norm=0.9656
[iter 100] loss=1.0222 val_loss=0.0000 scale=2.0000 norm=1.6641
[iter 200] loss=0.8860 val_loss=0.0000 scale=2.0000 norm=1.6582
[iter 300] loss=0.7488 val_loss=0.0000 scale=4.0000 norm=3.3280
[iter 400] loss=0.5983 val_loss=0.0000 scale=4.0000 norm=3.3276
[iter 0] loss=2.8256 val_loss=0.0000 scale=2.0000 norm=6.8131
[iter 100] loss=2.5133 val_loss=0.0000 scale=2.0000 norm=6.0581
[iter 200] loss=2.1919 val_loss=0.0000 scale=4.0000 norm=12.0335
[iter 300] loss=1.7029 val_loss=0.0000 scale=4.0000 norm=12.0348
[iter 400] loss=1.2195 val_loss=0.0000 scale=4.0000 norm=12.0357
[iter 0] loss=1.3108 val_loss=0.0000 scale=1.0000 norm=0.9365
[iter 100] loss=0.9940 val_loss=0.0000 scale=2.0000 norm=1.5712
[iter 200] loss=0.9297 val_loss=0.0000 scale=2.0000 norm=1.6058
[iter 300] loss=0.8909 val_loss=0.0000 scale=2.0000 norm=1.6198
[iter 400] loss=0.8514 val_loss=0.0000 scale=2.0000 norm=1.6224
[iter 0] loss=1.4291 val_loss=0.0000 scale=2.0000 norm=2.0111
[iter 100] loss=1.0606 val_loss=0.0000 scale=2.0000 norm=1.7062
[iter 200] loss=0.8980 val_loss=0.0000 scale=2.0000 norm=1.6970
[iter 300] loss=0.7616 val_loss=0.0000 scale=4.0000 norm=3.4340
[iter 400] loss=0.5893 val_loss=0.0000 scale=4.0000 norm=3.4400
[iter 0] loss=2.8053 val_loss=0.0000 scale=2.0000 norm=6.5256
[iter 100] loss=2.4796 val_loss=0.0000 scale=2.0000 norm=5.7735
[iter 200] loss=2.1261 val_loss=0.0000 scale=4.0000 norm=11.4642
[iter 300] loss=1.6136 val_loss=0.0000 scale=4.0000 norm=11.4690
[iter 400] loss=1.0955 val_loss=0.0000 scale=4.0000 norm=11.4693
[iter 0] loss=1.3013 val_loss=0.0000 scale=1.0000 norm=0.9306
[iter 100] loss=1.0405 val_loss=0.0000 scale=2.0000 norm=1.6264
[iter 200] loss=0.9732 val_loss=0.0000 scale=2.0000 norm=1.6534
[iter 300] loss=0.9322 val_loss=0.0000 scale=2.0000 norm=1.6651
[iter 400] loss=0.8947 val_loss=0.0000 scale=2.0000 norm=1.6674
[iter 0] loss=1.3872 val_loss=0.0000 scale=2.0000 norm=1.9665
[iter 100] loss=1.0432 val_loss=0.0000 scale=2.0000 norm=1.6865
[iter 200] loss=0.8827 val_loss=0.0000 scale=2.0000 norm=1.6729
[iter 300] loss=0.7208 val_loss=0.0000 scale=4.0000 norm=3.3554
[iter 400] loss=0.5364 val_loss=0.0000 scale=4.0000 norm=3.3580
[iter 0] loss=2.8061 val_loss=0.0000 scale=2.0000 norm=6.5567
[iter 100] loss=2.4903 val_loss=0.0000 scale=2.0000 norm=5.8895
[iter 200] loss=2.2032 val_loss=0.0000 scale=4.0000 norm=11.7206
[iter 300] loss=1.7050 val_loss=0.0000 scale=4.0000 norm=11.7249
[iter 400] loss=1.2111 val_loss=0.0000 scale=4.0000 norm=11.7259
[iter 0] loss=1.3236 val_loss=0.0000 scale=1.0000 norm=0.9435
[iter 100] loss=1.0919 val_loss=0.0000 scale=2.0000 norm=1.6829
[iter 200] loss=1.0370 val_loss=0.0000 scale=2.0000 norm=1.7044
[iter 300] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.7137
[iter 400] loss=0.9685 val_loss=0.0000 scale=2.0000 norm=1.7151
[iter 0] loss=1.4085 val_loss=0.0000 scale=2.0000 norm=1.9890
[iter 100] loss=1.0951 val_loss=0.0000 scale=2.0000 norm=1.7289
[iter 200] loss=0.9685 val_loss=0.0000 scale=2.0000 norm=1.7223
[iter 300] loss=0.8496 val_loss=0.0000 scale=4.0000 norm=3.4546
[iter 400] loss=0.7102 val_loss=0.0000 scale=4.0000 norm=3.4534
[iter 0] loss=2.8206 val_loss=0.0000 scale=2.0000 norm=6.7138
[iter 100] loss=2.5077 val_loss=0.0000 scale=2.0000 norm=5.9564
[iter 200] loss=2.2688 val_loss=0.0000 scale=2.0000 norm=5.9053
[iter 300] loss=1.8513 val_loss=0.0000 scale=4.0000 norm=11.8153
[iter 400] loss=1.4111 val_loss=0.0000 scale=4.0000 norm=11.8162
[iter 0] loss=1.2534 val_loss=0.0000 scale=1.0000 norm=0.9046
[iter 100] loss=1.0102 val_loss=0.0000 scale=2.0000 norm=1.5969
[iter 200] loss=0.9527 val_loss=0.0000 scale=2.0000 norm=1.6279
[iter 300] loss=0.9144 val_loss=0.0000 scale=2.0000 norm=1.6371
[iter 400] loss=0.8693 val_loss=0.0000 scale=4.0000 norm=3.2774
[iter 0] loss=1.4004 val_loss=0.0000 scale=2.0000 norm=1.9799
[iter 100] loss=1.0701 val_loss=0.0000 scale=2.0000 norm=1.6950
[iter 200] loss=0.9238 val_loss=0.0000 scale=2.0000 norm=1.6805
[iter 300] loss=0.7933 val_loss=0.0000 scale=4.0000 norm=3.3734
[iter 400] loss=0.6192 val_loss=0.0000 scale=4.0000 norm=3.3777
[iter 0] loss=2.7906 val_loss=0.0000 scale=2.0000 norm=6.3336
[iter 100] loss=2.4734 val_loss=0.0000 scale=2.0000 norm=5.6805
[iter 200] loss=2.1741 val_loss=0.0000 scale=4.0000 norm=11.3005
[iter 300] loss=1.6654 val_loss=0.0000 scale=4.0000 norm=11.3076
[iter 400] loss=1.1531 val_loss=0.0000 scale=4.0000 norm=11.3084
[iter 0] loss=1.3472 val_loss=0.0000 scale=1.0000 norm=0.9569
[iter 100] loss=1.0987 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 200] loss=1.0385 val_loss=0.0000 scale=2.0000 norm=1.7009
[iter 300] loss=1.0059 val_loss=0.0000 scale=2.0000 norm=1.7106
[iter 400] loss=0.9770 val_loss=0.0000 scale=2.0000 norm=1.7130
[iter 0] loss=1.3050 val_loss=0.0000 scale=2.0000 norm=1.8775
[iter 100] loss=0.9943 val_loss=0.0000 scale=2.0000 norm=1.6221
[iter 200] loss=0.8523 val_loss=0.0000 scale=2.0000 norm=1.6199
[iter 300] loss=0.7074 val_loss=0.0000 scale=4.0000 norm=3.2616
[iter 400] loss=0.5351 val_loss=0.0000 scale=4.0000 norm=3.2642
[iter 0] loss=2.8434 val_loss=0.0000 scale=2.0000 norm=7.0188
[iter 100] loss=2.5383 val_loss=0.0000 scale=2.0000 norm=6.2358
[iter 200] loss=2.2526 val_loss=0.0000 scale=4.0000 norm=12.3839
[iter 300] loss=1.7756 val_loss=0.0000 scale=4.0000 norm=12.3899
[iter 400] loss=1.2997 val_loss=0.0000 scale=4.0000 norm=12.3908
[iter 0] loss=1.2519 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0091 val_loss=0.0000 scale=2.0000 norm=1.5848
[iter 200] loss=0.9486 val_loss=0.0000 scale=2.0000 norm=1.6151
[iter 300] loss=0.9111 val_loss=0.0000 scale=2.0000 norm=1.6235
[iter 400] loss=0.8745 val_loss=0.0000 scale=2.0000 norm=1.6256
[iter 0] loss=1.3513 val_loss=0.0000 scale=2.0000 norm=1.9289
[iter 100] loss=1.0483 val_loss=0.0000 scale=2.0000 norm=1.6896
[iter 200] loss=0.9055 val_loss=0.0000 scale=2.0000 norm=1.6866
[iter 300] loss=0.7701 val_loss=0.0000 scale=4.0000 norm=3.3983
[iter 400] loss=0.6097 val_loss=0.0000 scale=4.0000 norm=3.4031
[iter 0] loss=2.7907 val_loss=0.0000 scale=2.0000 norm=6.4321
[iter 100] loss=2.4655 val_loss=0.0000 scale=2.0000 norm=5.7532
[iter 200] loss=2.1125 val_loss=0.0000 scale=4.0000 norm=11.4576
[iter 300] loss=1.5893 val_loss=0.0000 scale=4.0000 norm=11.4652
[iter 400] loss=1.0657 val_loss=0.0000 scale=4.0000 norm=11.4657
[iter 0] loss=1.2623 val_loss=0.0000 scale=1.0000 norm=0.9083
[iter 100] loss=1.0256 val_loss=0.0000 scale=2.0000 norm=1.6201
[iter 200] loss=0.9742 val_loss=0.0000 scale=2.0000 norm=1.6449
[iter 300] loss=0.9415 val_loss=0.0000 scale=2.0000 norm=1.6526
[iter 400] loss=0.9055 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=0.9900
[iter 100] loss=1.0638 val_loss=0.0000 scale=2.0000 norm=1.7066
[iter 200] loss=0.9054 val_loss=0.0000 scale=2.0000 norm=1.6891
[iter 300] loss=0.7778 val_loss=0.0000 scale=4.0000 norm=3.3903
[iter 400] loss=0.6009 val_loss=0.0000 scale=4.0000 norm=3.3914
[iter 0] loss=2.8504 val_loss=0.0000 scale=2.0000 norm=7.1258
[iter 100] loss=2.5682 val_loss=0.0000 scale=2.0000 norm=6.3642
[iter 200] loss=2.3259 val_loss=0.0000 scale=2.0000 norm=6.3138
[iter 300] loss=1.8774 val_loss=0.0000 scale=4.0000 norm=12.6271
[iter 400] loss=1.4313 val_loss=0.0000 scale=4.0000 norm=12.6279
[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=0.9091
[iter 100] loss=1.0194 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 200] loss=0.9546 val_loss=0.0000 scale=2.0000 norm=1.6400
[iter 300] loss=0.9102 val_loss=0.0000 scale=4.0000 norm=3.3067
[iter 400] loss=0.8570 val_loss=0.0000 scale=4.0000 norm=3.3101
[iter 0] loss=1.3826 val_loss=0.0000 scale=2.0000 norm=1.9608
[iter 100] loss=1.0744 val_loss=0.0000 scale=2.0000 norm=1.7127
[iter 200] loss=0.9397 val_loss=0.0000 scale=2.0000 norm=1.6995
[iter 300] loss=0.8012 val_loss=0.0000 scale=4.0000 norm=3.4047
[iter 400] loss=0.6403 val_loss=0.0000 scale=4.0000 norm=3.4069
[iter 0] loss=2.8328 val_loss=0.0000 scale=2.0000 norm=6.8945
[iter 100] loss=2.5236 val_loss=0.0000 scale=2.0000 norm=6.1119
[iter 200] loss=2.2642 val_loss=0.0000 scale=2.0000 norm=6.0571
[iter 300] loss=1.8017 val_loss=0.0000 scale=4.0000 norm=12.1195
[iter 400] loss=1.3009 val_loss=0.0000 scale=4.0000 norm=12.1211
[iter 0] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=0.9372
[iter 100] loss=1.0508 val_loss=0.0000 scale=2.0000 norm=1.6382
[iter 200] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.6555
[iter 300] loss=0.8687 val_loss=0.0000 scale=4.0000 norm=3.3296
[iter 400] loss=0.7272 val_loss=0.0000 scale=4.0000 norm=3.3318
[iter 0] loss=1.3703 val_loss=0.0000 scale=1.0000 norm=0.9735
[iter 100] loss=1.0408 val_loss=0.0000 scale=2.0000 norm=1.6562
[iter 200] loss=0.8991 val_loss=0.0000 scale=2.0000 norm=1.6445
[iter 300] loss=0.7785 val_loss=0.0000 scale=4.0000 norm=3.3052
[iter 400] loss=0.6163 val_loss=0.0000 scale=4.0000 norm=3.3117
[iter 0] loss=2.8038 val_loss=0.0000 scale=2.0000 norm=6.5481
[iter 100] loss=2.5104 val_loss=0.0000 scale=2.0000 norm=5.8491
[iter 200] loss=2.2715 val_loss=0.0000 scale=2.0000 norm=5.8048
[iter 300] loss=1.8586 val_loss=0.0000 scale=4.0000 norm=11.6115
[iter 400] loss=1.4028 val_loss=0.0000 scale=4.0000 norm=11.6112
[iter 0] loss=1.2821 val_loss=0.0000 scale=1.0000 norm=0.9197
[iter 100] loss=1.0044 val_loss=0.0000 scale=2.0000 norm=1.5821
[iter 200] loss=0.9490 val_loss=0.0000 scale=2.0000 norm=1.6144
[iter 300] loss=0.9167 val_loss=0.0000 scale=2.0000 norm=1.6255
[iter 400] loss=0.8852 val_loss=0.0000 scale=2.0000 norm=1.6272
[iter 0] loss=1.3859 val_loss=0.0000 scale=2.0000 norm=1.9646
[iter 100] loss=0.9927 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 200] loss=0.8177 val_loss=0.0000 scale=2.0000 norm=1.6173
[iter 300] loss=0.6774 val_loss=0.0000 scale=4.0000 norm=3.2543
[iter 400] loss=0.4856 val_loss=0.0000 scale=4.0000 norm=3.2604
[iter 0] loss=2.7999 val_loss=0.0000 scale=2.0000 norm=6.4988
[iter 100] loss=2.4528 val_loss=0.0000 scale=2.0000 norm=5.7314
[iter 200] loss=2.1939 val_loss=0.0000 scale=2.0000 norm=5.6881
[iter 300] loss=1.7333 val_loss=0.0000 scale=4.0000 norm=11.3874
[iter 400] loss=1.2203 val_loss=0.0000 scale=4.0000 norm=11.3890
[iter 0] loss=1.2674 val_loss=0.0000 scale=1.0000 norm=0.9112
[iter 100] loss=1.0665 val_loss=0.0000 scale=2.0000 norm=1.6586
[iter 200] loss=1.0225 val_loss=0.0000 scale=2.0000 norm=1.6857
[iter 300] loss=0.9953 val_loss=0.0000 scale=2.0000 norm=1.6936
[iter 400] loss=0.9671 val_loss=0.0000 scale=2.0000 norm=1.6945
[iter 0] loss=1.3258 val_loss=0.0000 scale=1.0000 norm=0.9508
[iter 100] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6479
[iter 200] loss=0.8887 val_loss=0.0000 scale=2.0000 norm=1.6422
[iter 300] loss=0.7418 val_loss=0.0000 scale=4.0000 norm=3.3003
[iter 400] loss=0.5633 val_loss=0.0000 scale=4.0000 norm=3.3009
[iter 0] loss=2.8212 val_loss=0.0000 scale=2.0000 norm=6.7227
[iter 100] loss=2.5002 val_loss=0.0000 scale=2.0000 norm=6.0149
[iter 200] loss=2.1673 val_loss=0.0000 scale=4.0000 norm=11.9665
[iter 300] loss=1.6783 val_loss=0.0000 scale=4.0000 norm=11.9729
[iter 400] loss=1.1840 val_loss=0.0000 scale=4.0000 norm=11.9735
[iter 0] loss=1.2773 val_loss=0.0000 scale=1.0000 norm=0.9179
[iter 100] loss=1.0535 val_loss=0.0000 scale=2.0000 norm=1.6381
[iter 200] loss=1.0050 val_loss=0.0000 scale=2.0000 norm=1.6641
[iter 300] loss=0.9746 val_loss=0.0000 scale=2.0000 norm=1.6728
[iter 400] loss=0.9470 val_loss=0.0000 scale=2.0000 norm=1.6755
[iter 0] loss=1.3749 val_loss=0.0000 scale=2.0000 norm=1.9544
[iter 100] loss=1.0609 val_loss=0.0000 scale=2.0000 norm=1.6883
[iter 200] loss=0.9339 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 300] loss=0.8363 val_loss=0.0000 scale=4.0000 norm=3.3603
[iter 400] loss=0.7120 val_loss=0.0000 scale=4.0000 norm=3.3612
[iter 0] loss=2.8275 val_loss=0.0000 scale=2.0000 norm=6.8253
[iter 100] loss=2.5420 val_loss=0.0000 scale=2.0000 norm=6.0977
[iter 200] loss=2.2557 val_loss=0.0000 scale=4.0000 norm=12.1072
[iter 300] loss=1.8044 val_loss=0.0000 scale=4.0000 norm=12.1080
[iter 400] loss=1.3569 val_loss=0.0000 scale=4.0000 norm=12.1088
[iter 0] loss=1.2996 val_loss=0.0000 scale=1.0000 norm=0.9298
[iter 100] loss=0.9669 val_loss=0.0000 scale=2.0000 norm=1.5412
[iter 200] loss=0.9059 val_loss=0.0000 scale=2.0000 norm=1.5803
[iter 300] loss=0.8720 val_loss=0.0000 scale=2.0000 norm=1.5935
[iter 400] loss=0.8335 val_loss=0.0000 scale=2.0000 norm=1.5959
[iter 0] loss=1.4823 val_loss=0.0000 scale=1.0000 norm=1.0348
[iter 100] loss=1.1518 val_loss=0.0000 scale=2.0000 norm=1.7906
[iter 200] loss=1.0062 val_loss=0.0000 scale=2.0000 norm=1.7819
[iter 300] loss=0.8605 val_loss=0.0000 scale=4.0000 norm=3.5736
[iter 400] loss=0.6919 val_loss=0.0000 scale=4.0000 norm=3.5782
[iter 0] loss=2.8264 val_loss=0.0000 scale=2.0000 norm=6.8194
[iter 100] loss=2.5169 val_loss=0.0000 scale=2.0000 norm=6.1124
[iter 200] loss=2.2748 val_loss=0.0000 scale=2.0000 norm=6.0733
[iter 300] loss=1.8351 val_loss=0.0000 scale=4.0000 norm=12.1568
[iter 400] loss=1.3638 val_loss=0.0000 scale=4.0000 norm=12.1584
[iter 0] loss=1.2534 val_loss=0.0000 scale=1.0000 norm=0.9046
[iter 100] loss=1.0102 val_loss=0.0000 scale=2.0000 norm=1.5969
[iter 200] loss=0.9527 val_loss=0.0000 scale=2.0000 norm=1.6279
[iter 300] loss=0.9144 val_loss=0.0000 scale=2.0000 norm=1.6371
[iter 400] loss=0.8693 val_loss=0.0000 scale=4.0000 norm=3.2774
[iter 0] loss=1.4004 val_loss=0.0000 scale=2.0000 norm=1.9799
[iter 100] loss=1.0701 val_loss=0.0000 scale=2.0000 norm=1.6950
[iter 200] loss=0.9238 val_loss=0.0000 scale=2.0000 norm=1.6805
[iter 300] loss=0.7933 val_loss=0.0000 scale=4.0000 norm=3.3734
[iter 400] loss=0.6192 val_loss=0.0000 scale=4.0000 norm=3.3777
[iter 0] loss=2.7906 val_loss=0.0000 scale=2.0000 norm=6.3336
[iter 100] loss=2.4734 val_loss=0.0000 scale=2.0000 norm=5.6805
[iter 200] loss=2.1741 val_loss=0.0000 scale=4.0000 norm=11.3005
[iter 300] loss=1.6654 val_loss=0.0000 scale=4.0000 norm=11.3076
[iter 400] loss=1.1531 val_loss=0.0000 scale=4.0000 norm=11.3084
[iter 0] loss=1.3472 val_loss=0.0000 scale=1.0000 norm=0.9569
[iter 100] loss=1.0987 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 200] loss=1.0385 val_loss=0.0000 scale=2.0000 norm=1.7009
[iter 300] loss=1.0059 val_loss=0.0000 scale=2.0000 norm=1.7106
[iter 400] loss=0.9770 val_loss=0.0000 scale=2.0000 norm=1.7130
[iter 0] loss=1.3050 val_loss=0.0000 scale=2.0000 norm=1.8775
[iter 100] loss=0.9943 val_loss=0.0000 scale=2.0000 norm=1.6221
[iter 200] loss=0.8523 val_loss=0.0000 scale=2.0000 norm=1.6199
[iter 300] loss=0.7074 val_loss=0.0000 scale=4.0000 norm=3.2616
[iter 400] loss=0.5351 val_loss=0.0000 scale=4.0000 norm=3.2642
[iter 0] loss=2.8434 val_loss=0.0000 scale=2.0000 norm=7.0188
[iter 100] loss=2.5383 val_loss=0.0000 scale=2.0000 norm=6.2359
[iter 200] loss=2.2455 val_loss=0.0000 scale=4.0000 norm=12.3840
[iter 300] loss=1.7685 val_loss=0.0000 scale=4.0000 norm=12.3900
[iter 400] loss=1.2924 val_loss=0.0000 scale=4.0000 norm=12.3908
[iter 0] loss=1.3202 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0501 val_loss=0.0000 scale=2.0000 norm=1.6394
[iter 200] loss=0.9849 val_loss=0.0000 scale=2.0000 norm=1.6647
[iter 300] loss=0.9459 val_loss=0.0000 scale=2.0000 norm=1.6747
[iter 400] loss=0.9040 val_loss=0.0000 scale=2.0000 norm=1.6768
[iter 0] loss=1.3569 val_loss=0.0000 scale=1.0000 norm=0.9670
[iter 100] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6622
[iter 200] loss=0.8712 val_loss=0.0000 scale=2.0000 norm=1.6543
[iter 300] loss=0.7389 val_loss=0.0000 scale=4.0000 norm=3.3446
[iter 400] loss=0.5721 val_loss=0.0000 scale=4.0000 norm=3.3489
[iter 0] loss=2.8478 val_loss=0.0000 scale=2.0000 norm=7.0856
[iter 100] loss=2.5188 val_loss=0.0000 scale=2.0000 norm=6.1712
[iter 200] loss=2.2188 val_loss=0.0000 scale=4.0000 norm=12.2248
[iter 300] loss=1.7181 val_loss=0.0000 scale=4.0000 norm=12.2280
[iter 400] loss=1.2168 val_loss=0.0000 scale=4.0000 norm=12.2287
[iter 0] loss=1.2623 val_loss=0.0000 scale=1.0000 norm=0.9083
[iter 100] loss=1.0256 val_loss=0.0000 scale=2.0000 norm=1.6201
[iter 200] loss=0.9742 val_loss=0.0000 scale=2.0000 norm=1.6449
[iter 300] loss=0.9415 val_loss=0.0000 scale=2.0000 norm=1.6526
[iter 400] loss=0.9055 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=0.9900
[iter 100] loss=1.0638 val_loss=0.0000 scale=2.0000 norm=1.7066
[iter 200] loss=0.9054 val_loss=0.0000 scale=2.0000 norm=1.6891
[iter 300] loss=0.7778 val_loss=0.0000 scale=4.0000 norm=3.3903
[iter 400] loss=0.6009 val_loss=0.0000 scale=4.0000 norm=3.3914
[iter 0] loss=2.8504 val_loss=0.0000 scale=2.0000 norm=7.1258
[iter 100] loss=2.5682 val_loss=0.0000 scale=2.0000 norm=6.3642
[iter 200] loss=2.3259 val_loss=0.0000 scale=2.0000 norm=6.3138
[iter 300] loss=1.8774 val_loss=0.0000 scale=4.0000 norm=12.6271
[iter 400] loss=1.4313 val_loss=0.0000 scale=4.0000 norm=12.6279
[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=0.9091
[iter 100] loss=1.0194 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 200] loss=0.9546 val_loss=0.0000 scale=2.0000 norm=1.6400
[iter 300] loss=0.9102 val_loss=0.0000 scale=4.0000 norm=3.3067
[iter 400] loss=0.8570 val_loss=0.0000 scale=4.0000 norm=3.3101
[iter 0] loss=1.3826 val_loss=0.0000 scale=2.0000 norm=1.9608
[iter 100] loss=1.0744 val_loss=0.0000 scale=2.0000 norm=1.7127
[iter 200] loss=0.9397 val_loss=0.0000 scale=2.0000 norm=1.6995
[iter 300] loss=0.8012 val_loss=0.0000 scale=4.0000 norm=3.4047
[iter 400] loss=0.6403 val_loss=0.0000 scale=4.0000 norm=3.4069
[iter 0] loss=2.8328 val_loss=0.0000 scale=2.0000 norm=6.8945
[iter 100] loss=2.5236 val_loss=0.0000 scale=2.0000 norm=6.1119
[iter 200] loss=2.2642 val_loss=0.0000 scale=2.0000 norm=6.0571
[iter 300] loss=1.8017 val_loss=0.0000 scale=4.0000 norm=12.1195
[iter 400] loss=1.3009 val_loss=0.0000 scale=4.0000 norm=12.1211
[iter 0] loss=1.2441 val_loss=0.0000 scale=1.0000 norm=0.9004
[iter 100] loss=1.0044 val_loss=0.0000 scale=2.0000 norm=1.6064
[iter 200] loss=0.9462 val_loss=0.0000 scale=2.0000 norm=1.6389
[iter 300] loss=0.9088 val_loss=0.0000 scale=2.0000 norm=1.6496
[iter 400] loss=0.8692 val_loss=0.0000 scale=2.0000 norm=1.6514
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0550
[iter 100] loss=1.1249 val_loss=0.0000 scale=2.0000 norm=1.7851
[iter 200] loss=0.9669 val_loss=0.0000 scale=2.0000 norm=1.7803
[iter 300] loss=0.7978 val_loss=0.0000 scale=4.0000 norm=3.5772
[iter 400] loss=0.6050 val_loss=0.0000 scale=4.0000 norm=3.5714
[iter 0] loss=2.8149 val_loss=0.0000 scale=2.0000 norm=6.6187
[iter 100] loss=2.4812 val_loss=0.0000 scale=2.0000 norm=5.8966
[iter 200] loss=2.1397 val_loss=0.0000 scale=4.0000 norm=11.7337
[iter 300] loss=1.6210 val_loss=0.0000 scale=4.0000 norm=11.7373
[iter 400] loss=1.1077 val_loss=0.0000 scale=4.0000 norm=11.7382
[iter 0] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=0.9362
[iter 100] loss=1.0602 val_loss=0.0000 scale=2.0000 norm=1.6615
[iter 200] loss=0.9868 val_loss=0.0000 scale=2.0000 norm=1.6830
[iter 300] loss=0.9435 val_loss=0.0000 scale=4.0000 norm=3.3823
[iter 400] loss=0.8817 val_loss=0.0000 scale=4.0000 norm=3.3884
[iter 0] loss=1.3435 val_loss=0.0000 scale=2.0000 norm=1.9213
[iter 100] loss=0.9925 val_loss=0.0000 scale=2.0000 norm=1.6244
[iter 200] loss=0.8366 val_loss=0.0000 scale=2.0000 norm=1.6227
[iter 300] loss=0.6692 val_loss=0.0000 scale=4.0000 norm=3.2530
[iter 400] loss=0.4873 val_loss=0.0000 scale=4.0000 norm=3.2479
[iter 0] loss=2.8188 val_loss=0.0000 scale=2.0000 norm=6.7092
[iter 100] loss=2.5208 val_loss=0.0000 scale=2.0000 norm=6.0236
[iter 200] loss=2.2051 val_loss=0.0000 scale=4.0000 norm=12.0153
[iter 300] loss=1.7280 val_loss=0.0000 scale=4.0000 norm=12.0179
[iter 400] loss=0.9352 val_loss=0.0000 scale=8.0000 norm=24.0362
[iter 0] loss=1.2729 val_loss=0.0000 scale=1.0000 norm=0.9145
[iter 100] loss=1.0199 val_loss=0.0000 scale=2.0000 norm=1.6169
[iter 200] loss=0.9636 val_loss=0.0000 scale=2.0000 norm=1.6486
[iter 300] loss=0.9358 val_loss=0.0000 scale=2.0000 norm=1.6620
[iter 400] loss=0.9098 val_loss=0.0000 scale=2.0000 norm=1.6644
[iter 0] loss=1.3675 val_loss=0.0000 scale=2.0000 norm=1.9472
[iter 100] loss=1.0297 val_loss=0.0000 scale=2.0000 norm=1.6806
[iter 200] loss=0.8680 val_loss=0.0000 scale=2.0000 norm=1.6695
[iter 300] loss=0.6928 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 400] loss=0.5109 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 0] loss=2.8016 val_loss=0.0000 scale=2.0000 norm=6.5394
[iter 100] loss=2.4890 val_loss=0.0000 scale=2.0000 norm=5.8782
[iter 200] loss=2.1748 val_loss=0.0000 scale=4.0000 norm=11.7116
[iter 300] loss=1.6844 val_loss=0.0000 scale=4.0000 norm=11.7174
[iter 400] loss=1.1901 val_loss=0.0000 scale=4.0000 norm=11.7180
[iter 0] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=0.9372
[iter 100] loss=1.0508 val_loss=0.0000 scale=2.0000 norm=1.6382
[iter 200] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.6555
[iter 300] loss=0.8687 val_loss=0.0000 scale=4.0000 norm=3.3296
[iter 400] loss=0.7272 val_loss=0.0000 scale=4.0000 norm=3.3318
[iter 0] loss=1.3703 val_loss=0.0000 scale=1.0000 norm=0.9735
[iter 100] loss=1.0408 val_loss=0.0000 scale=2.0000 norm=1.6562
[iter 200] loss=0.8991 val_loss=0.0000 scale=2.0000 norm=1.6445
[iter 300] loss=0.7785 val_loss=0.0000 scale=4.0000 norm=3.3052
[iter 400] loss=0.6163 val_loss=0.0000 scale=4.0000 norm=3.3117
[iter 0] loss=2.8038 val_loss=0.0000 scale=2.0000 norm=6.5481
[iter 100] loss=2.5104 val_loss=0.0000 scale=2.0000 norm=5.8493
[iter 200] loss=2.2714 val_loss=0.0000 scale=2.0000 norm=5.8051
[iter 300] loss=1.8563 val_loss=0.0000 scale=4.0000 norm=11.6109
[iter 400] loss=1.4007 val_loss=0.0000 scale=4.0000 norm=11.6110
[iter 0] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=0.9370
[iter 100] loss=1.0172 val_loss=0.0000 scale=2.0000 norm=1.6018
[iter 200] loss=0.9099 val_loss=0.0000 scale=2.0000 norm=1.6171
[iter 300] loss=0.8243 val_loss=0.0000 scale=2.0000 norm=1.6290
[iter 400] loss=0.7172 val_loss=0.0000 scale=4.0000 norm=3.2628
[iter 0] loss=1.3571 val_loss=0.0000 scale=2.0000 norm=1.9347
[iter 100] loss=1.0670 val_loss=0.0000 scale=2.0000 norm=1.7047
[iter 200] loss=0.9402 val_loss=0.0000 scale=2.0000 norm=1.7046
[iter 300] loss=0.8317 val_loss=0.0000 scale=4.0000 norm=3.4253
[iter 400] loss=0.6765 val_loss=0.0000 scale=4.0000 norm=3.4313
[iter 0] loss=2.8250 val_loss=0.0000 scale=2.0000 norm=6.8125
[iter 100] loss=2.4892 val_loss=0.0000 scale=2.0000 norm=6.0109
[iter 200] loss=2.1737 val_loss=0.0000 scale=4.0000 norm=11.9535
[iter 300] loss=1.6481 val_loss=0.0000 scale=4.0000 norm=11.9528
[iter 400] loss=1.1288 val_loss=0.0000 scale=4.0000 norm=11.9531
[iter 0] loss=1.2604 val_loss=0.0000 scale=1.0000 norm=0.9080
[iter 100] loss=1.0396 val_loss=0.0000 scale=2.0000 norm=1.6185
[iter 200] loss=0.9806 val_loss=0.0000 scale=2.0000 norm=1.6431
[iter 300] loss=0.9388 val_loss=0.0000 scale=2.0000 norm=1.6524
[iter 400] loss=0.8909 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 0] loss=1.4348 val_loss=0.0000 scale=1.0000 norm=1.0085
[iter 100] loss=1.0537 val_loss=0.0000 scale=2.0000 norm=1.6815
[iter 200] loss=0.9128 val_loss=0.0000 scale=2.0000 norm=1.6831
[iter 300] loss=0.7961 val_loss=0.0000 scale=4.0000 norm=3.3855
[iter 400] loss=0.6352 val_loss=0.0000 scale=4.0000 norm=3.3835
[iter 0] loss=2.7725 val_loss=0.0000 scale=2.0000 norm=6.1605
[iter 100] loss=2.4581 val_loss=0.0000 scale=2.0000 norm=5.5531
[iter 200] loss=2.1858 val_loss=0.0000 scale=2.0000 norm=5.5242
[iter 300] loss=1.7385 val_loss=0.0000 scale=4.0000 norm=11.0616
[iter 400] loss=1.2555 val_loss=0.0000 scale=4.0000 norm=11.0638
[iter 0] loss=1.2519 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0091 val_loss=0.0000 scale=2.0000 norm=1.5848
[iter 200] loss=0.9486 val_loss=0.0000 scale=2.0000 norm=1.6151
[iter 300] loss=0.9111 val_loss=0.0000 scale=2.0000 norm=1.6235
[iter 400] loss=0.8745 val_loss=0.0000 scale=2.0000 norm=1.6256
[iter 0] loss=1.3513 val_loss=0.0000 scale=2.0000 norm=1.9289
[iter 100] loss=1.0483 val_loss=0.0000 scale=2.0000 norm=1.6896
[iter 200] loss=0.9055 val_loss=0.0000 scale=2.0000 norm=1.6866
[iter 300] loss=0.7701 val_loss=0.0000 scale=4.0000 norm=3.3983
[iter 400] loss=0.6097 val_loss=0.0000 scale=4.0000 norm=3.4031
[iter 0] loss=2.7907 val_loss=0.0000 scale=2.0000 norm=6.4321
[iter 100] loss=2.4655 val_loss=0.0000 scale=2.0000 norm=5.7532
[iter 200] loss=2.1125 val_loss=0.0000 scale=4.0000 norm=11.4576
[iter 300] loss=1.5893 val_loss=0.0000 scale=4.0000 norm=11.4652
[iter 400] loss=1.0657 val_loss=0.0000 scale=4.0000 norm=11.4658
[iter 0] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=0.9430
[iter 100] loss=1.0456 val_loss=0.0000 scale=2.0000 norm=1.6500
[iter 200] loss=0.9287 val_loss=0.0000 scale=2.0000 norm=1.6640
[iter 300] loss=0.8276 val_loss=0.0000 scale=2.0000 norm=1.6714
[iter 400] loss=0.7133 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 0] loss=1.3545 val_loss=0.0000 scale=1.0000 norm=0.9669
[iter 100] loss=1.0444 val_loss=0.0000 scale=2.0000 norm=1.6716
[iter 200] loss=0.9054 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 300] loss=0.7925 val_loss=0.0000 scale=4.0000 norm=3.3367
[iter 400] loss=0.6349 val_loss=0.0000 scale=4.0000 norm=3.3260
[iter 0] loss=2.8016 val_loss=0.0000 scale=2.0000 norm=6.4951
[iter 100] loss=2.5014 val_loss=0.0000 scale=2.0000 norm=5.8058
[iter 200] loss=2.2524 val_loss=0.0000 scale=4.0000 norm=11.5346
[iter 300] loss=1.7941 val_loss=0.0000 scale=4.0000 norm=11.5428
[iter 400] loss=1.3184 val_loss=0.0000 scale=4.0000 norm=11.5442
[iter 0] loss=1.2247 val_loss=0.0000 scale=1.0000 norm=0.8869
[iter 100] loss=0.9804 val_loss=0.0000 scale=2.0000 norm=1.5880
[iter 200] loss=0.9173 val_loss=0.0000 scale=2.0000 norm=1.6152
[iter 300] loss=0.8813 val_loss=0.0000 scale=2.0000 norm=1.6232
[iter 400] loss=0.8392 val_loss=0.0000 scale=4.0000 norm=3.2547
[iter 0] loss=1.3487 val_loss=0.0000 scale=1.0000 norm=0.9639
[iter 100] loss=1.0229 val_loss=0.0000 scale=2.0000 norm=1.6658
[iter 200] loss=0.8795 val_loss=0.0000 scale=2.0000 norm=1.6530
[iter 300] loss=0.7401 val_loss=0.0000 scale=4.0000 norm=3.3120
[iter 400] loss=0.5969 val_loss=0.0000 scale=4.0000 norm=3.3171
[iter 0] loss=2.7909 val_loss=0.0000 scale=2.0000 norm=6.4074
[iter 100] loss=2.4875 val_loss=0.0000 scale=2.0000 norm=5.8102
[iter 200] loss=2.1630 val_loss=0.0000 scale=4.0000 norm=11.5846
[iter 300] loss=1.7035 val_loss=0.0000 scale=4.0000 norm=11.5887
[iter 400] loss=1.1198 val_loss=0.0000 scale=8.0000 norm=23.1781
[iter 0] loss=1.3199 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0393 val_loss=0.0000 scale=2.0000 norm=1.6406
[iter 200] loss=0.9455 val_loss=0.0000 scale=2.0000 norm=1.6648
[iter 300] loss=0.8594 val_loss=0.0000 scale=2.0000 norm=1.6753
[iter 400] loss=0.7202 val_loss=0.0000 scale=4.0000 norm=3.3516
[iter 0] loss=1.3652 val_loss=0.0000 scale=2.0000 norm=1.9423
[iter 100] loss=1.0713 val_loss=0.0000 scale=2.0000 norm=1.7075
[iter 200] loss=0.9353 val_loss=0.0000 scale=2.0000 norm=1.6947
[iter 300] loss=0.7862 val_loss=0.0000 scale=4.0000 norm=3.3942
[iter 400] loss=0.6192 val_loss=0.0000 scale=4.0000 norm=3.3960
[iter 0] loss=2.8172 val_loss=0.0000 scale=2.0000 norm=6.6655
[iter 100] loss=2.5115 val_loss=0.0000 scale=2.0000 norm=5.9932
[iter 200] loss=2.2144 val_loss=0.0000 scale=4.0000 norm=11.9176
[iter 300] loss=1.7427 val_loss=0.0000 scale=4.0000 norm=11.9278
[iter 400] loss=1.2657 val_loss=0.0000 scale=4.0000 norm=11.9287
[iter 0] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=0.9089
[iter 100] loss=0.9991 val_loss=0.0000 scale=2.0000 norm=1.5953
[iter 200] loss=0.9002 val_loss=0.0000 scale=2.0000 norm=1.6149
[iter 300] loss=0.8212 val_loss=0.0000 scale=2.0000 norm=1.6196
[iter 400] loss=0.7158 val_loss=0.0000 scale=4.0000 norm=3.2481
[iter 0] loss=1.3664 val_loss=0.0000 scale=2.0000 norm=1.9434
[iter 100] loss=1.0258 val_loss=0.0000 scale=2.0000 norm=1.6699
[iter 200] loss=0.8700 val_loss=0.0000 scale=2.0000 norm=1.6676
[iter 300] loss=0.6939 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 400] loss=0.5150 val_loss=0.0000 scale=4.0000 norm=3.3588
[iter 0] loss=2.8399 val_loss=0.0000 scale=2.0000 norm=6.9733
[iter 100] loss=2.5356 val_loss=0.0000 scale=2.0000 norm=6.2259
[iter 200] loss=2.2541 val_loss=0.0000 scale=4.0000 norm=12.3677
[iter 300] loss=1.7668 val_loss=0.0000 scale=4.0000 norm=12.3745
[iter 400] loss=1.2655 val_loss=0.0000 scale=4.0000 norm=12.3758
[iter 0] loss=1.2546 val_loss=0.0000 scale=1.0000 norm=0.9029
[iter 100] loss=0.9909 val_loss=0.0000 scale=2.0000 norm=1.5743
[iter 200] loss=0.9372 val_loss=0.0000 scale=2.0000 norm=1.6102
[iter 300] loss=0.9044 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 400] loss=0.8662 val_loss=0.0000 scale=2.0000 norm=1.6230
[iter 0] loss=1.4090 val_loss=0.0000 scale=2.0000 norm=1.9893
[iter 100] loss=1.0855 val_loss=0.0000 scale=2.0000 norm=1.7272
[iter 200] loss=0.9379 val_loss=0.0000 scale=2.0000 norm=1.7194
[iter 300] loss=0.7983 val_loss=0.0000 scale=4.0000 norm=3.4556
[iter 400] loss=0.6259 val_loss=0.0000 scale=4.0000 norm=3.4598
[iter 0] loss=2.8001 val_loss=0.0000 scale=2.0000 norm=6.4598
[iter 100] loss=2.4961 val_loss=0.0000 scale=2.0000 norm=5.8792
[iter 200] loss=2.1474 val_loss=0.0000 scale=4.0000 norm=11.7219
[iter 300] loss=1.6467 val_loss=0.0000 scale=4.0000 norm=11.7302
[iter 400] loss=1.1454 val_loss=0.0000 scale=4.0000 norm=11.7309
[iter 0] loss=1.2674 val_loss=0.0000 scale=1.0000 norm=0.9112
[iter 100] loss=1.0665 val_loss=0.0000 scale=2.0000 norm=1.6586
[iter 200] loss=1.0225 val_loss=0.0000 scale=2.0000 norm=1.6857
[iter 300] loss=0.9953 val_loss=0.0000 scale=2.0000 norm=1.6936
[iter 400] loss=0.9671 val_loss=0.0000 scale=2.0000 norm=1.6945
[iter 0] loss=1.3258 val_loss=0.0000 scale=1.0000 norm=0.9508
[iter 100] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6479
[iter 200] loss=0.8887 val_loss=0.0000 scale=2.0000 norm=1.6422
[iter 300] loss=0.7418 val_loss=0.0000 scale=4.0000 norm=3.3003
[iter 400] loss=0.5633 val_loss=0.0000 scale=4.0000 norm=3.3009
[iter 0] loss=2.8212 val_loss=0.0000 scale=2.0000 norm=6.7227
[iter 100] loss=2.5002 val_loss=0.0000 scale=2.0000 norm=6.0149
[iter 200] loss=2.1673 val_loss=0.0000 scale=4.0000 norm=11.9665
[iter 300] loss=1.6783 val_loss=0.0000 scale=4.0000 norm=11.9729
[iter 400] loss=1.1840 val_loss=0.0000 scale=4.0000 norm=11.9735
[iter 0] loss=1.2773 val_loss=0.0000 scale=1.0000 norm=0.9179
[iter 100] loss=1.0535 val_loss=0.0000 scale=2.0000 norm=1.6381
[iter 200] loss=1.0050 val_loss=0.0000 scale=2.0000 norm=1.6641
[iter 300] loss=0.9746 val_loss=0.0000 scale=2.0000 norm=1.6728
[iter 400] loss=0.9470 val_loss=0.0000 scale=2.0000 norm=1.6755
[iter 0] loss=1.3749 val_loss=0.0000 scale=2.0000 norm=1.9544
[iter 100] loss=1.0609 val_loss=0.0000 scale=2.0000 norm=1.6883
[iter 200] loss=0.9339 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 300] loss=0.8363 val_loss=0.0000 scale=4.0000 norm=3.3603
[iter 400] loss=0.7120 val_loss=0.0000 scale=4.0000 norm=3.3612
[iter 0] loss=2.8275 val_loss=0.0000 scale=2.0000 norm=6.8253
[iter 100] loss=2.5420 val_loss=0.0000 scale=2.0000 norm=6.0977
[iter 200] loss=2.2557 val_loss=0.0000 scale=4.0000 norm=12.1072
[iter 300] loss=1.8044 val_loss=0.0000 scale=4.0000 norm=12.1080
[iter 400] loss=1.3569 val_loss=0.0000 scale=4.0000 norm=12.1088
[iter 0] loss=1.2996 val_loss=0.0000 scale=1.0000 norm=0.9298
[iter 100] loss=0.9669 val_loss=0.0000 scale=2.0000 norm=1.5412
[iter 200] loss=0.9059 val_loss=0.0000 scale=2.0000 norm=1.5803
[iter 300] loss=0.8720 val_loss=0.0000 scale=2.0000 norm=1.5935
[iter 400] loss=0.8335 val_loss=0.0000 scale=2.0000 norm=1.5959
[iter 0] loss=1.4823 val_loss=0.0000 scale=1.0000 norm=1.0348
[iter 100] loss=1.1518 val_loss=0.0000 scale=2.0000 norm=1.7906
[iter 200] loss=1.0062 val_loss=0.0000 scale=2.0000 norm=1.7819
[iter 300] loss=0.8605 val_loss=0.0000 scale=4.0000 norm=3.5736
[iter 400] loss=0.6919 val_loss=0.0000 scale=4.0000 norm=3.5782
[iter 0] loss=2.8264 val_loss=0.0000 scale=2.0000 norm=6.8194
[iter 100] loss=2.5169 val_loss=0.0000 scale=2.0000 norm=6.1124
[iter 200] loss=2.2748 val_loss=0.0000 scale=2.0000 norm=6.0733
[iter 300] loss=1.8351 val_loss=0.0000 scale=4.0000 norm=12.1568
[iter 400] loss=1.3638 val_loss=0.0000 scale=4.0000 norm=12.1584
[iter 0] loss=1.2546 val_loss=0.0000 scale=1.0000 norm=0.9029
[iter 100] loss=0.9909 val_loss=0.0000 scale=2.0000 norm=1.5743
[iter 200] loss=0.9372 val_loss=0.0000 scale=2.0000 norm=1.6102
[iter 300] loss=0.9044 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 400] loss=0.8662 val_loss=0.0000 scale=2.0000 norm=1.6230
[iter 0] loss=1.4090 val_loss=0.0000 scale=2.0000 norm=1.9893
[iter 100] loss=1.0855 val_loss=0.0000 scale=2.0000 norm=1.7272
[iter 200] loss=0.9379 val_loss=0.0000 scale=2.0000 norm=1.7194
[iter 300] loss=0.7983 val_loss=0.0000 scale=4.0000 norm=3.4556
[iter 400] loss=0.6259 val_loss=0.0000 scale=4.0000 norm=3.4598
[iter 0] loss=2.8001 val_loss=0.0000 scale=2.0000 norm=6.4598
[iter 100] loss=2.4961 val_loss=0.0000 scale=2.0000 norm=5.8792
[iter 200] loss=2.1474 val_loss=0.0000 scale=4.0000 norm=11.7219
[iter 300] loss=1.6467 val_loss=0.0000 scale=4.0000 norm=11.7302
[iter 400] loss=1.1454 val_loss=0.0000 scale=4.0000 norm=11.7309
[iter 0] loss=1.2787 val_loss=0.0000 scale=1.0000 norm=0.9183
[iter 100] loss=1.0388 val_loss=0.0000 scale=2.0000 norm=1.6088
[iter 200] loss=0.9812 val_loss=0.0000 scale=2.0000 norm=1.6380
[iter 300] loss=0.9444 val_loss=0.0000 scale=2.0000 norm=1.6477
[iter 400] loss=0.9116 val_loss=0.0000 scale=2.0000 norm=1.6467
[iter 0] loss=1.4284 val_loss=0.0000 scale=1.0000 norm=1.0051
[iter 100] loss=1.1043 val_loss=0.0000 scale=2.0000 norm=1.7448
[iter 200] loss=0.9532 val_loss=0.0000 scale=2.0000 norm=1.7452
[iter 300] loss=0.7918 val_loss=0.0000 scale=4.0000 norm=3.5306
[iter 400] loss=0.6143 val_loss=0.0000 scale=4.0000 norm=3.5303
[iter 0] loss=2.8414 val_loss=0.0000 scale=2.0000 norm=7.0053
[iter 100] loss=2.5308 val_loss=0.0000 scale=2.0000 norm=6.1360
[iter 200] loss=2.2644 val_loss=0.0000 scale=4.0000 norm=12.1631
[iter 300] loss=1.7780 val_loss=0.0000 scale=4.0000 norm=12.1638
[iter 400] loss=1.2783 val_loss=0.0000 scale=4.0000 norm=12.1650
[iter 0] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=0.9402
[iter 100] loss=1.0712 val_loss=0.0000 scale=2.0000 norm=1.6565
[iter 200] loss=1.0059 val_loss=0.0000 scale=2.0000 norm=1.6890
[iter 300] loss=0.9645 val_loss=0.0000 scale=2.0000 norm=1.6976
[iter 400] loss=0.9212 val_loss=0.0000 scale=2.0000 norm=1.6999
[iter 0] loss=1.3368 val_loss=0.0000 scale=1.0000 norm=0.9551
[iter 100] loss=0.9926 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 200] loss=0.8379 val_loss=0.0000 scale=4.0000 norm=3.2160
[iter 300] loss=0.6700 val_loss=0.0000 scale=4.0000 norm=3.2428
[iter 400] loss=0.4795 val_loss=0.0000 scale=4.0000 norm=3.2434
[iter 0] loss=2.8212 val_loss=0.0000 scale=2.0000 norm=6.7590
[iter 100] loss=2.4858 val_loss=0.0000 scale=2.0000 norm=5.9744
[iter 200] loss=2.2071 val_loss=0.0000 scale=2.0000 norm=5.9264
[iter 300] loss=1.7260 val_loss=0.0000 scale=4.0000 norm=11.8598
[iter 400] loss=1.2130 val_loss=0.0000 scale=4.0000 norm=11.8614
[iter 0] loss=1.2764 val_loss=0.0000 scale=1.0000 norm=0.9169
[iter 100] loss=1.0389 val_loss=0.0000 scale=2.0000 norm=1.6330
[iter 200] loss=0.9729 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 300] loss=0.9286 val_loss=0.0000 scale=2.0000 norm=1.6643
[iter 400] loss=0.8903 val_loss=0.0000 scale=2.0000 norm=1.6666
[iter 0] loss=1.3441 val_loss=0.0000 scale=2.0000 norm=1.9210
[iter 100] loss=1.0008 val_loss=0.0000 scale=2.0000 norm=1.6328
[iter 200] loss=0.8411 val_loss=0.0000 scale=2.0000 norm=1.6210
[iter 300] loss=0.6953 val_loss=0.0000 scale=4.0000 norm=3.2624
[iter 400] loss=0.5190 val_loss=0.0000 scale=4.0000 norm=3.2591
[iter 0] loss=2.8108 val_loss=0.0000 scale=2.0000 norm=6.6583
[iter 100] loss=2.4984 val_loss=0.0000 scale=2.0000 norm=5.9682
[iter 200] loss=2.1510 val_loss=0.0000 scale=4.0000 norm=11.8836
[iter 300] loss=1.6323 val_loss=0.0000 scale=4.0000 norm=11.8915
[iter 400] loss=1.1078 val_loss=0.0000 scale=4.0000 norm=11.8922
[iter 0] loss=1.2821 val_loss=0.0000 scale=1.0000 norm=0.9197
[iter 100] loss=1.0044 val_loss=0.0000 scale=2.0000 norm=1.5821
[iter 200] loss=0.9490 val_loss=0.0000 scale=2.0000 norm=1.6144
[iter 300] loss=0.9167 val_loss=0.0000 scale=2.0000 norm=1.6255
[iter 400] loss=0.8852 val_loss=0.0000 scale=2.0000 norm=1.6272
[iter 0] loss=1.3859 val_loss=0.0000 scale=2.0000 norm=1.9646
[iter 100] loss=0.9927 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 200] loss=0.8177 val_loss=0.0000 scale=2.0000 norm=1.6173
[iter 300] loss=0.6774 val_loss=0.0000 scale=4.0000 norm=3.2543
[iter 400] loss=0.4856 val_loss=0.0000 scale=4.0000 norm=3.2604
[iter 0] loss=2.7999 val_loss=0.0000 scale=2.0000 norm=6.4988
[iter 100] loss=2.4528 val_loss=0.0000 scale=2.0000 norm=5.7320
[iter 200] loss=2.1913 val_loss=0.0000 scale=2.0000 norm=5.6882
[iter 300] loss=1.7339 val_loss=0.0000 scale=4.0000 norm=11.3867
[iter 400] loss=1.2210 val_loss=0.0000 scale=4.0000 norm=11.3893
[iter 0] loss=1.2787 val_loss=0.0000 scale=1.0000 norm=0.9183
[iter 100] loss=1.0388 val_loss=0.0000 scale=2.0000 norm=1.6088
[iter 200] loss=0.9812 val_loss=0.0000 scale=2.0000 norm=1.6380
[iter 300] loss=0.9444 val_loss=0.0000 scale=2.0000 norm=1.6477
[iter 400] loss=0.9116 val_loss=0.0000 scale=2.0000 norm=1.6467
[iter 0] loss=1.4284 val_loss=0.0000 scale=1.0000 norm=1.0051
[iter 100] loss=1.1043 val_loss=0.0000 scale=2.0000 norm=1.7448
[iter 200] loss=0.9532 val_loss=0.0000 scale=2.0000 norm=1.7452
[iter 300] loss=0.7918 val_loss=0.0000 scale=4.0000 norm=3.5306
[iter 400] loss=0.6143 val_loss=0.0000 scale=4.0000 norm=3.5303
[iter 0] loss=2.8414 val_loss=0.0000 scale=2.0000 norm=7.0053
[iter 100] loss=2.5309 val_loss=0.0000 scale=2.0000 norm=6.1361
[iter 200] loss=2.2645 val_loss=0.0000 scale=4.0000 norm=12.1631
[iter 300] loss=1.7759 val_loss=0.0000 scale=4.0000 norm=12.1638
[iter 400] loss=1.2661 val_loss=0.0000 scale=4.0000 norm=12.1651
[iter 0] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=0.9402
[iter 100] loss=1.0712 val_loss=0.0000 scale=2.0000 norm=1.6565
[iter 200] loss=1.0059 val_loss=0.0000 scale=2.0000 norm=1.6890
[iter 300] loss=0.9645 val_loss=0.0000 scale=2.0000 norm=1.6976
[iter 400] loss=0.9212 val_loss=0.0000 scale=2.0000 norm=1.6999
[iter 0] loss=1.3368 val_loss=0.0000 scale=1.0000 norm=0.9551
[iter 100] loss=0.9926 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 200] loss=0.8379 val_loss=0.0000 scale=4.0000 norm=3.2160
[iter 300] loss=0.6700 val_loss=0.0000 scale=4.0000 norm=3.2428
[iter 400] loss=0.4795 val_loss=0.0000 scale=4.0000 norm=3.2434
[iter 0] loss=2.8212 val_loss=0.0000 scale=2.0000 norm=6.7590
[iter 100] loss=2.4858 val_loss=0.0000 scale=2.0000 norm=5.9744
[iter 200] loss=2.2071 val_loss=0.0000 scale=2.0000 norm=5.9264
[iter 300] loss=1.7260 val_loss=0.0000 scale=4.0000 norm=11.8598
[iter 400] loss=1.2130 val_loss=0.0000 scale=4.0000 norm=11.8614
[iter 0] loss=1.2764 val_loss=0.0000 scale=1.0000 norm=0.9169
[iter 100] loss=1.0389 val_loss=0.0000 scale=2.0000 norm=1.6330
[iter 200] loss=0.9729 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 300] loss=0.9286 val_loss=0.0000 scale=2.0000 norm=1.6643
[iter 400] loss=0.8903 val_loss=0.0000 scale=2.0000 norm=1.6666
[iter 0] loss=1.3441 val_loss=0.0000 scale=2.0000 norm=1.9210
[iter 100] loss=1.0008 val_loss=0.0000 scale=2.0000 norm=1.6328
[iter 200] loss=0.8411 val_loss=0.0000 scale=2.0000 norm=1.6210
[iter 300] loss=0.6953 val_loss=0.0000 scale=4.0000 norm=3.2624
[iter 400] loss=0.5190 val_loss=0.0000 scale=4.0000 norm=3.2592
[iter 0] loss=2.8108 val_loss=0.0000 scale=2.0000 norm=6.6583
[iter 100] loss=2.4983 val_loss=0.0000 scale=2.0000 norm=5.9682
[iter 200] loss=2.1562 val_loss=0.0000 scale=4.0000 norm=11.8836
[iter 300] loss=1.6374 val_loss=0.0000 scale=4.0000 norm=11.8915
[iter 400] loss=1.1182 val_loss=0.0000 scale=4.0000 norm=11.8921
[iter 0] loss=1.2520 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0208 val_loss=0.0000 scale=2.0000 norm=1.5985
[iter 200] loss=0.9678 val_loss=0.0000 scale=2.0000 norm=1.6203
[iter 300] loss=0.9376 val_loss=0.0000 scale=2.0000 norm=1.6302
[iter 400] loss=0.9047 val_loss=0.0000 scale=4.0000 norm=3.2635
[iter 0] loss=1.3875 val_loss=0.0000 scale=2.0000 norm=1.9660
[iter 100] loss=1.0943 val_loss=0.0000 scale=2.0000 norm=1.7400
[iter 200] loss=0.9492 val_loss=0.0000 scale=2.0000 norm=1.7311
[iter 300] loss=0.7982 val_loss=0.0000 scale=4.0000 norm=3.4843
[iter 400] loss=0.6253 val_loss=0.0000 scale=4.0000 norm=3.4857
[iter 0] loss=2.8579 val_loss=0.0000 scale=2.0000 norm=7.2441
[iter 100] loss=2.5653 val_loss=0.0000 scale=2.0000 norm=6.4590
[iter 200] loss=2.2809 val_loss=0.0000 scale=4.0000 norm=12.8316
[iter 300] loss=1.8100 val_loss=0.0000 scale=4.0000 norm=12.8364
[iter 400] loss=1.3384 val_loss=0.0000 scale=4.0000 norm=12.8370
[iter 0] loss=1.3217 val_loss=0.0000 scale=1.0000 norm=0.9423
[iter 100] loss=1.0525 val_loss=0.0000 scale=2.0000 norm=1.6494
[iter 200] loss=0.9420 val_loss=0.0000 scale=2.0000 norm=1.6656
[iter 300] loss=0.8489 val_loss=0.0000 scale=2.0000 norm=1.6730
[iter 400] loss=0.6962 val_loss=0.0000 scale=4.0000 norm=3.3487
[iter 0] loss=1.3780 val_loss=0.0000 scale=2.0000 norm=1.9562
[iter 100] loss=1.0629 val_loss=0.0000 scale=2.0000 norm=1.6921
[iter 200] loss=0.9179 val_loss=0.0000 scale=2.0000 norm=1.6804
[iter 300] loss=0.7729 val_loss=0.0000 scale=4.0000 norm=3.3729
[iter 400] loss=0.5977 val_loss=0.0000 scale=4.0000 norm=3.3684
[iter 0] loss=2.8287 val_loss=0.0000 scale=2.0000 norm=6.8673
[iter 100] loss=2.5265 val_loss=0.0000 scale=2.0000 norm=6.1228
[iter 200] loss=2.2783 val_loss=0.0000 scale=4.0000 norm=12.1468
[iter 300] loss=1.8199 val_loss=0.0000 scale=4.0000 norm=12.1494
[iter 400] loss=1.3404 val_loss=0.0000 scale=4.0000 norm=12.1493
[iter 0] loss=1.2162 val_loss=0.0000 scale=1.0000 norm=0.8824
[iter 100] loss=0.9805 val_loss=0.0000 scale=2.0000 norm=1.5600
[iter 200] loss=0.9239 val_loss=0.0000 scale=2.0000 norm=1.5894
[iter 300] loss=0.8888 val_loss=0.0000 scale=2.0000 norm=1.6002
[iter 400] loss=0.8524 val_loss=0.0000 scale=2.0000 norm=1.6019
[iter 0] loss=1.4692 val_loss=0.0000 scale=1.0000 norm=1.0274
[iter 100] loss=1.1381 val_loss=0.0000 scale=2.0000 norm=1.7811
[iter 200] loss=0.9890 val_loss=0.0000 scale=2.0000 norm=1.7753
[iter 300] loss=0.8405 val_loss=0.0000 scale=4.0000 norm=3.5805
[iter 400] loss=0.6648 val_loss=0.0000 scale=4.0000 norm=3.5813
[iter 0] loss=2.8181 val_loss=0.0000 scale=2.0000 norm=6.7371
[iter 100] loss=2.4969 val_loss=0.0000 scale=2.0000 norm=5.9657
[iter 200] loss=2.2284 val_loss=0.0000 scale=2.0000 norm=5.9241
[iter 300] loss=1.7492 val_loss=0.0000 scale=4.0000 norm=11.8577
[iter 400] loss=1.2301 val_loss=0.0000 scale=4.0000 norm=11.8591
[iter 0] loss=1.2359 val_loss=0.0000 scale=1.0000 norm=0.8946
[iter 100] loss=1.0081 val_loss=0.0000 scale=2.0000 norm=1.6007
[iter 200] loss=0.9508 val_loss=0.0000 scale=2.0000 norm=1.6332
[iter 300] loss=0.9163 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 400] loss=0.8724 val_loss=0.0000 scale=4.0000 norm=3.3012
[iter 0] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=0.9463
[iter 100] loss=0.9979 val_loss=0.0000 scale=2.0000 norm=1.6343
[iter 200] loss=0.8561 val_loss=0.0000 scale=2.0000 norm=1.6229
[iter 300] loss=0.7334 val_loss=0.0000 scale=4.0000 norm=3.2633
[iter 400] loss=0.5687 val_loss=0.0000 scale=4.0000 norm=3.2615
[iter 0] loss=2.7903 val_loss=0.0000 scale=2.0000 norm=6.3676
[iter 100] loss=2.4711 val_loss=0.0000 scale=2.0000 norm=5.6449
[iter 200] loss=2.1624 val_loss=0.0000 scale=4.0000 norm=11.2375
[iter 300] loss=1.6479 val_loss=0.0000 scale=4.0000 norm=11.2436
[iter 400] loss=1.1140 val_loss=0.0000 scale=4.0000 norm=11.2442
[iter 0] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=0.9430
[iter 100] loss=1.0456 val_loss=0.0000 scale=2.0000 norm=1.6500
[iter 200] loss=0.9287 val_loss=0.0000 scale=2.0000 norm=1.6640
[iter 300] loss=0.8276 val_loss=0.0000 scale=2.0000 norm=1.6714
[iter 400] loss=0.7133 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 0] loss=1.3545 val_loss=0.0000 scale=1.0000 norm=0.9669
[iter 100] loss=1.0444 val_loss=0.0000 scale=2.0000 norm=1.6716
[iter 200] loss=0.9054 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 300] loss=0.7925 val_loss=0.0000 scale=4.0000 norm=3.3367
[iter 400] loss=0.6349 val_loss=0.0000 scale=4.0000 norm=3.3260
[iter 0] loss=2.8016 val_loss=0.0000 scale=2.0000 norm=6.4951
[iter 100] loss=2.5015 val_loss=0.0000 scale=2.0000 norm=5.8058
[iter 200] loss=2.2479 val_loss=0.0000 scale=2.0000 norm=5.7672
[iter 300] loss=1.7896 val_loss=0.0000 scale=4.0000 norm=11.5430
[iter 400] loss=1.3139 val_loss=0.0000 scale=4.0000 norm=11.5440
[iter 0] loss=1.2247 val_loss=0.0000 scale=1.0000 norm=0.8869
[iter 100] loss=0.9804 val_loss=0.0000 scale=2.0000 norm=1.5880
[iter 200] loss=0.9173 val_loss=0.0000 scale=2.0000 norm=1.6152
[iter 300] loss=0.8813 val_loss=0.0000 scale=2.0000 norm=1.6232
[iter 400] loss=0.8392 val_loss=0.0000 scale=4.0000 norm=3.2547
[iter 0] loss=1.3487 val_loss=0.0000 scale=1.0000 norm=0.9639
[iter 100] loss=1.0229 val_loss=0.0000 scale=2.0000 norm=1.6658
[iter 200] loss=0.8795 val_loss=0.0000 scale=2.0000 norm=1.6530
[iter 300] loss=0.7401 val_loss=0.0000 scale=4.0000 norm=3.3120
[iter 400] loss=0.5969 val_loss=0.0000 scale=4.0000 norm=3.3171
[iter 0] loss=2.7909 val_loss=0.0000 scale=2.0000 norm=6.4074
[iter 100] loss=2.4875 val_loss=0.0000 scale=2.0000 norm=5.8102
[iter 200] loss=2.1630 val_loss=0.0000 scale=4.0000 norm=11.5846
[iter 300] loss=1.7035 val_loss=0.0000 scale=4.0000 norm=11.5887
[iter 400] loss=1.1198 val_loss=0.0000 scale=8.0000 norm=23.1781
[iter 0] loss=1.3199 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0393 val_loss=0.0000 scale=2.0000 norm=1.6406
[iter 200] loss=0.9455 val_loss=0.0000 scale=2.0000 norm=1.6648
[iter 300] loss=0.8594 val_loss=0.0000 scale=2.0000 norm=1.6753
[iter 400] loss=0.7202 val_loss=0.0000 scale=4.0000 norm=3.3516
[iter 0] loss=1.3652 val_loss=0.0000 scale=2.0000 norm=1.9423
[iter 100] loss=1.0713 val_loss=0.0000 scale=2.0000 norm=1.7075
[iter 200] loss=0.9353 val_loss=0.0000 scale=2.0000 norm=1.6947
[iter 300] loss=0.7862 val_loss=0.0000 scale=4.0000 norm=3.3942
[iter 400] loss=0.6192 val_loss=0.0000 scale=4.0000 norm=3.3960
[iter 0] loss=2.8172 val_loss=0.0000 scale=2.0000 norm=6.6655
[iter 100] loss=2.5115 val_loss=0.0000 scale=2.0000 norm=5.9932
[iter 200] loss=2.2191 val_loss=0.0000 scale=4.0000 norm=11.9177
[iter 300] loss=1.7450 val_loss=0.0000 scale=4.0000 norm=11.9279
[iter 400] loss=1.2678 val_loss=0.0000 scale=4.0000 norm=11.9289
[iter 0] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=0.9089
[iter 100] loss=0.9991 val_loss=0.0000 scale=2.0000 norm=1.5953
[iter 200] loss=0.9002 val_loss=0.0000 scale=2.0000 norm=1.6149
[iter 300] loss=0.8212 val_loss=0.0000 scale=2.0000 norm=1.6196
[iter 400] loss=0.7158 val_loss=0.0000 scale=4.0000 norm=3.2481
[iter 0] loss=1.3664 val_loss=0.0000 scale=2.0000 norm=1.9434
[iter 100] loss=1.0258 val_loss=0.0000 scale=2.0000 norm=1.6699
[iter 200] loss=0.8700 val_loss=0.0000 scale=2.0000 norm=1.6676
[iter 300] loss=0.6939 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 400] loss=0.5150 val_loss=0.0000 scale=4.0000 norm=3.3588
[iter 0] loss=2.8399 val_loss=0.0000 scale=2.0000 norm=6.9733
[iter 100] loss=2.5356 val_loss=0.0000 scale=2.0000 norm=6.2259
[iter 200] loss=2.2541 val_loss=0.0000 scale=4.0000 norm=12.3677
[iter 300] loss=1.7668 val_loss=0.0000 scale=4.0000 norm=12.3745
[iter 400] loss=1.2656 val_loss=0.0000 scale=4.0000 norm=12.3758
[iter 0] loss=1.2520 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0208 val_loss=0.0000 scale=2.0000 norm=1.5985
[iter 200] loss=0.9678 val_loss=0.0000 scale=2.0000 norm=1.6203
[iter 300] loss=0.9376 val_loss=0.0000 scale=2.0000 norm=1.6302
[iter 400] loss=0.9047 val_loss=0.0000 scale=4.0000 norm=3.2635
[iter 0] loss=1.3875 val_loss=0.0000 scale=2.0000 norm=1.9660
[iter 100] loss=1.0943 val_loss=0.0000 scale=2.0000 norm=1.7400
[iter 200] loss=0.9492 val_loss=0.0000 scale=2.0000 norm=1.7311
[iter 300] loss=0.7982 val_loss=0.0000 scale=4.0000 norm=3.4843
[iter 400] loss=0.6253 val_loss=0.0000 scale=4.0000 norm=3.4857
[iter 0] loss=2.8579 val_loss=0.0000 scale=2.0000 norm=7.2441
[iter 100] loss=2.5653 val_loss=0.0000 scale=2.0000 norm=6.4590
[iter 200] loss=2.2809 val_loss=0.0000 scale=4.0000 norm=12.8316
[iter 300] loss=1.8100 val_loss=0.0000 scale=4.0000 norm=12.8364
[iter 400] loss=1.3384 val_loss=0.0000 scale=4.0000 norm=12.8370
[iter 0] loss=1.3217 val_loss=0.0000 scale=1.0000 norm=0.9423
[iter 100] loss=1.0525 val_loss=0.0000 scale=2.0000 norm=1.6494
[iter 200] loss=0.9420 val_loss=0.0000 scale=2.0000 norm=1.6656
[iter 300] loss=0.8489 val_loss=0.0000 scale=2.0000 norm=1.6730
[iter 400] loss=0.6962 val_loss=0.0000 scale=4.0000 norm=3.3487
[iter 0] loss=1.3780 val_loss=0.0000 scale=2.0000 norm=1.9562
[iter 100] loss=1.0629 val_loss=0.0000 scale=2.0000 norm=1.6921
[iter 200] loss=0.9179 val_loss=0.0000 scale=2.0000 norm=1.6804
[iter 300] loss=0.7729 val_loss=0.0000 scale=4.0000 norm=3.3729
[iter 400] loss=0.5977 val_loss=0.0000 scale=4.0000 norm=3.3684
[iter 0] loss=2.8287 val_loss=0.0000 scale=2.0000 norm=6.8673
[iter 100] loss=2.5265 val_loss=0.0000 scale=2.0000 norm=6.1228
[iter 200] loss=2.2783 val_loss=0.0000 scale=4.0000 norm=12.1468
[iter 300] loss=1.8199 val_loss=0.0000 scale=4.0000 norm=12.1494
[iter 400] loss=1.3404 val_loss=0.0000 scale=4.0000 norm=12.1493
[iter 0] loss=1.2162 val_loss=0.0000 scale=1.0000 norm=0.8824
[iter 100] loss=0.9805 val_loss=0.0000 scale=2.0000 norm=1.5600
[iter 200] loss=0.9239 val_loss=0.0000 scale=2.0000 norm=1.5894
[iter 300] loss=0.8888 val_loss=0.0000 scale=2.0000 norm=1.6002
[iter 400] loss=0.8524 val_loss=0.0000 scale=2.0000 norm=1.6019
[iter 0] loss=1.4692 val_loss=0.0000 scale=1.0000 norm=1.0274
[iter 100] loss=1.1381 val_loss=0.0000 scale=2.0000 norm=1.7811
[iter 200] loss=0.9890 val_loss=0.0000 scale=2.0000 norm=1.7753
[iter 300] loss=0.8405 val_loss=0.0000 scale=4.0000 norm=3.5805
[iter 400] loss=0.6648 val_loss=0.0000 scale=4.0000 norm=3.5813
[iter 0] loss=2.8181 val_loss=0.0000 scale=2.0000 norm=6.7371
[iter 100] loss=2.4969 val_loss=0.0000 scale=2.0000 norm=5.9657
[iter 200] loss=2.2284 val_loss=0.0000 scale=2.0000 norm=5.9241
[iter 300] loss=1.7492 val_loss=0.0000 scale=4.0000 norm=11.8577
[iter 400] loss=1.2301 val_loss=0.0000 scale=4.0000 norm=11.8591
[iter 0] loss=1.3236 val_loss=0.0000 scale=1.0000 norm=0.9435
[iter 100] loss=1.0919 val_loss=0.0000 scale=2.0000 norm=1.6829
[iter 200] loss=1.0370 val_loss=0.0000 scale=2.0000 norm=1.7044
[iter 300] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.7137
[iter 400] loss=0.9685 val_loss=0.0000 scale=2.0000 norm=1.7151
[iter 0] loss=1.4085 val_loss=0.0000 scale=2.0000 norm=1.9890
[iter 100] loss=1.0951 val_loss=0.0000 scale=2.0000 norm=1.7289
[iter 200] loss=0.9685 val_loss=0.0000 scale=2.0000 norm=1.7223
[iter 300] loss=0.8496 val_loss=0.0000 scale=4.0000 norm=3.4546
[iter 400] loss=0.7102 val_loss=0.0000 scale=4.0000 norm=3.4534
[iter 0] loss=2.8206 val_loss=0.0000 scale=2.0000 norm=6.7138
[iter 100] loss=2.5077 val_loss=0.0000 scale=2.0000 norm=5.9564
[iter 200] loss=2.2688 val_loss=0.0000 scale=2.0000 norm=5.9053
[iter 300] loss=1.8513 val_loss=0.0000 scale=4.0000 norm=11.8153
[iter 400] loss=1.4111 val_loss=0.0000 scale=4.0000 norm=11.8162
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9184
[iter 100] loss=1.0775 val_loss=0.0000 scale=2.0000 norm=1.6682
[iter 200] loss=1.0245 val_loss=0.0000 scale=2.0000 norm=1.6980
[iter 300] loss=0.9929 val_loss=0.0000 scale=2.0000 norm=1.7064
[iter 400] loss=0.9637 val_loss=0.0000 scale=2.0000 norm=1.7082
[iter 0] loss=1.3537 val_loss=0.0000 scale=1.0000 norm=0.9656
[iter 100] loss=1.0222 val_loss=0.0000 scale=2.0000 norm=1.6641
[iter 200] loss=0.8860 val_loss=0.0000 scale=2.0000 norm=1.6582
[iter 300] loss=0.7488 val_loss=0.0000 scale=4.0000 norm=3.3280
[iter 400] loss=0.5983 val_loss=0.0000 scale=4.0000 norm=3.3276
[iter 0] loss=2.8256 val_loss=0.0000 scale=2.0000 norm=6.8131
[iter 100] loss=2.5133 val_loss=0.0000 scale=2.0000 norm=6.0581
[iter 200] loss=2.1919 val_loss=0.0000 scale=4.0000 norm=12.0335
[iter 300] loss=1.7029 val_loss=0.0000 scale=4.0000 norm=12.0348
[iter 400] loss=1.2195 val_loss=0.0000 scale=4.0000 norm=12.0357
[iter 0] loss=1.3108 val_loss=0.0000 scale=1.0000 norm=0.9365
[iter 100] loss=0.9940 val_loss=0.0000 scale=2.0000 norm=1.5712
[iter 200] loss=0.9297 val_loss=0.0000 scale=2.0000 norm=1.6058
[iter 300] loss=0.8909 val_loss=0.0000 scale=2.0000 norm=1.6198
[iter 400] loss=0.8514 val_loss=0.0000 scale=2.0000 norm=1.6224
[iter 0] loss=1.4291 val_loss=0.0000 scale=2.0000 norm=2.0111
[iter 100] loss=1.0606 val_loss=0.0000 scale=2.0000 norm=1.7062
[iter 200] loss=0.8980 val_loss=0.0000 scale=2.0000 norm=1.6970
[iter 300] loss=0.7616 val_loss=0.0000 scale=4.0000 norm=3.4340
[iter 400] loss=0.5893 val_loss=0.0000 scale=4.0000 norm=3.4400
[iter 0] loss=2.8053 val_loss=0.0000 scale=2.0000 norm=6.5256
[iter 100] loss=2.4796 val_loss=0.0000 scale=2.0000 norm=5.7735
[iter 200] loss=2.1261 val_loss=0.0000 scale=4.0000 norm=11.4642
[iter 300] loss=1.6136 val_loss=0.0000 scale=4.0000 norm=11.4690
[iter 400] loss=1.0955 val_loss=0.0000 scale=4.0000 norm=11.4693
[iter 0] loss=1.3013 val_loss=0.0000 scale=1.0000 norm=0.9306
[iter 100] loss=1.0405 val_loss=0.0000 scale=2.0000 norm=1.6264
[iter 200] loss=0.9732 val_loss=0.0000 scale=2.0000 norm=1.6534
[iter 300] loss=0.9322 val_loss=0.0000 scale=2.0000 norm=1.6651
[iter 400] loss=0.8947 val_loss=0.0000 scale=2.0000 norm=1.6674
[iter 0] loss=1.3872 val_loss=0.0000 scale=2.0000 norm=1.9665
[iter 100] loss=1.0432 val_loss=0.0000 scale=2.0000 norm=1.6865
[iter 200] loss=0.8827 val_loss=0.0000 scale=2.0000 norm=1.6729
[iter 300] loss=0.7208 val_loss=0.0000 scale=4.0000 norm=3.3554
[iter 400] loss=0.5364 val_loss=0.0000 scale=4.0000 norm=3.3580
[iter 0] loss=2.8061 val_loss=0.0000 scale=2.0000 norm=6.5567
[iter 100] loss=2.4903 val_loss=0.0000 scale=2.0000 norm=5.8895
[iter 200] loss=2.2032 val_loss=0.0000 scale=4.0000 norm=11.7206
[iter 300] loss=1.7050 val_loss=0.0000 scale=4.0000 norm=11.7249
[iter 400] loss=1.2111 val_loss=0.0000 scale=4.0000 norm=11.7259
[iter 0] loss=1.2359 val_loss=0.0000 scale=1.0000 norm=0.8946
[iter 100] loss=1.0081 val_loss=0.0000 scale=2.0000 norm=1.6007
[iter 200] loss=0.9508 val_loss=0.0000 scale=2.0000 norm=1.6332
[iter 300] loss=0.9163 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 400] loss=0.8724 val_loss=0.0000 scale=4.0000 norm=3.3012
[iter 0] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=0.9463
[iter 100] loss=0.9979 val_loss=0.0000 scale=2.0000 norm=1.6343
[iter 200] loss=0.8561 val_loss=0.0000 scale=2.0000 norm=1.6229
[iter 300] loss=0.7334 val_loss=0.0000 scale=4.0000 norm=3.2633
[iter 400] loss=0.5687 val_loss=0.0000 scale=4.0000 norm=3.2615
[iter 0] loss=2.7903 val_loss=0.0000 scale=2.0000 norm=6.3676
[iter 100] loss=2.4711 val_loss=0.0000 scale=2.0000 norm=5.6449
[iter 200] loss=2.1624 val_loss=0.0000 scale=4.0000 norm=11.2375
[iter 300] loss=1.6479 val_loss=0.0000 scale=4.0000 norm=11.2436
[iter 400] loss=1.1140 val_loss=0.0000 scale=4.0000 norm=11.2442

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n14>
Subject: Job 304961: <mordred_NGB_Robust Scaler_multimodal Rh_20250120> in cluster <Hazel> Done

Job <mordred_NGB_Robust Scaler_multimodal Rh_20250120> was submitted from host <c207n01> by user <sdehgha2> in cluster <Hazel> at Tue Jan 21 18:17:15 2025
Job was executed on host(s) <6*c207n14>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Jan 21 18:17:17 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Tue Jan 21 18:17:17 2025
Terminated at Tue Jan 21 18:37:15 2025
Results reported at Tue Jan 21 18:37:15 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 6
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Robust Scaler_multimodal Rh_20250120"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "RRU Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   5738.49 sec.
    Max Memory :                                 5 GB
    Average Memory :                             4.21 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               11.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   1207 sec.
    Turnaround time :                            1200 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250121.err> for stderr output of this job.

