Dimer
RRU Dimer
Average scores:	 r2: [ 0.004  0.052 -2.659]Â±[0.174 0.098 3.995]
[array([ 0.00357747,  0.0517641 , -2.65863598]), array([8.64617919e+00, 1.53852667e+02, 1.02186653e+05]), array([6.39387726e+00, 1.01109321e+02, 2.46440154e+04])]
Dimer
Filename: (Mordred)_NGB_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh_without_log/Dimer/(Mordred)_NGB_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh_without_log/Dimer/(Mordred)_NGB_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh_without_log/Dimer/(Mordred)_NGB_Robust Scaler_shape.json
Done Saving scores!
[iter 0] loss=1.3108 val_loss=0.0000 scale=1.0000 norm=0.9365
[iter 100] loss=0.9993 val_loss=0.0000 scale=2.0000 norm=1.5717
[iter 200] loss=0.9357 val_loss=0.0000 scale=2.0000 norm=1.6060
[iter 300] loss=0.8997 val_loss=0.0000 scale=2.0000 norm=1.6190
[iter 400] loss=0.8661 val_loss=0.0000 scale=4.0000 norm=3.2434
[iter 0] loss=1.4291 val_loss=0.0000 scale=2.0000 norm=2.0111
[iter 100] loss=1.0707 val_loss=0.0000 scale=2.0000 norm=1.7065
[iter 200] loss=0.9103 val_loss=0.0000 scale=2.0000 norm=1.6925
[iter 300] loss=0.8106 val_loss=0.0000 scale=2.0000 norm=1.7063
[iter 400] loss=0.7160 val_loss=0.0000 scale=2.0000 norm=1.7169
[iter 0] loss=13.4586 val_loss=0.0000 scale=2.0000 norm=59840.7698
[iter 100] loss=11.7036 val_loss=0.0000 scale=4.0000 norm=109860.9487
[iter 200] loss=10.4267 val_loss=0.0000 scale=4.0000 norm=109781.3413
[iter 300] loss=9.6421 val_loss=0.0000 scale=8.0000 norm=219531.8265
[iter 400] loss=8.7436 val_loss=0.0000 scale=4.0000 norm=109765.1602
[iter 0] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=0.9402
[iter 100] loss=1.0715 val_loss=0.0000 scale=2.0000 norm=1.6594
[iter 200] loss=1.0093 val_loss=0.0000 scale=2.0000 norm=1.6909
[iter 300] loss=0.9698 val_loss=0.0000 scale=2.0000 norm=1.6995
[iter 400] loss=0.9325 val_loss=0.0000 scale=2.0000 norm=1.7009
[iter 0] loss=1.3368 val_loss=0.0000 scale=1.0000 norm=0.9551
[iter 100] loss=1.0019 val_loss=0.0000 scale=2.0000 norm=1.6083
[iter 200] loss=0.8499 val_loss=0.0000 scale=2.0000 norm=1.6034
[iter 300] loss=0.7418 val_loss=0.0000 scale=2.0000 norm=1.6150
[iter 400] loss=0.6343 val_loss=0.0000 scale=4.0000 norm=3.2371
[iter 0] loss=12.2579 val_loss=0.0000 scale=2.0000 norm=23878.1143
[iter 100] loss=10.8842 val_loss=0.0000 scale=4.0000 norm=44515.0419
[iter 200] loss=10.0059 val_loss=0.0000 scale=4.0000 norm=44409.7631
[iter 300] loss=9.3417 val_loss=0.0000 scale=8.0000 norm=88802.8082
[iter 400] loss=8.4907 val_loss=0.0000 scale=4.0000 norm=44401.1791
[iter 0] loss=1.2674 val_loss=0.0000 scale=1.0000 norm=0.9112
[iter 100] loss=1.0662 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 200] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6878
[iter 300] loss=0.9978 val_loss=0.0000 scale=2.0000 norm=1.6955
[iter 400] loss=0.9679 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 0] loss=1.3258 val_loss=0.0000 scale=2.0000 norm=1.9016
[iter 100] loss=1.0257 val_loss=0.0000 scale=2.0000 norm=1.6484
[iter 200] loss=0.8896 val_loss=0.0000 scale=2.0000 norm=1.6399
[iter 300] loss=0.7865 val_loss=0.0000 scale=4.0000 norm=3.2952
[iter 400] loss=0.6791 val_loss=0.0000 scale=2.0000 norm=1.6498
[iter 0] loss=13.4600 val_loss=0.0000 scale=2.0000 norm=60949.3788
[iter 100] loss=11.7265 val_loss=0.0000 scale=4.0000 norm=113797.0623
[iter 200] loss=10.5769 val_loss=0.0000 scale=4.0000 norm=113732.5015
[iter 300] loss=9.8757 val_loss=0.0000 scale=8.0000 norm=227438.9571
[iter 400] loss=8.8712 val_loss=0.0000 scale=8.0000 norm=227437.6786
[iter 0] loss=1.2162 val_loss=0.0000 scale=1.0000 norm=0.8824
[iter 100] loss=0.9820 val_loss=0.0000 scale=2.0000 norm=1.5616
[iter 200] loss=0.9276 val_loss=0.0000 scale=2.0000 norm=1.5885
[iter 300] loss=0.8978 val_loss=0.0000 scale=2.0000 norm=1.6000
[iter 400] loss=0.8701 val_loss=0.0000 scale=2.0000 norm=1.6015
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0549
[iter 100] loss=1.1467 val_loss=0.0000 scale=2.0000 norm=1.7823
[iter 200] loss=1.0054 val_loss=0.0000 scale=2.0000 norm=1.7739
[iter 300] loss=0.8971 val_loss=0.0000 scale=2.0000 norm=1.7834
[iter 400] loss=0.7938 val_loss=0.0000 scale=2.0000 norm=1.7885
[iter 0] loss=13.4188 val_loss=0.0000 scale=2.0000 norm=48410.1325
[iter 100] loss=11.6399 val_loss=0.0000 scale=4.0000 norm=93887.3163
[iter 200] loss=10.5391 val_loss=0.0000 scale=4.0000 norm=93860.8105
[iter 300] loss=9.8249 val_loss=0.0000 scale=8.0000 norm=187704.0424
[iter 400] loss=8.7728 val_loss=0.0000 scale=8.0000 norm=187702.6406
[iter 0] loss=1.2821 val_loss=0.0000 scale=1.0000 norm=0.9197
[iter 100] loss=1.0107 val_loss=0.0000 scale=2.0000 norm=1.5829
[iter 200] loss=0.9539 val_loss=0.0000 scale=2.0000 norm=1.6137
[iter 300] loss=0.9258 val_loss=0.0000 scale=2.0000 norm=1.6258
[iter 400] loss=0.8987 val_loss=0.0000 scale=2.0000 norm=1.6268
[iter 0] loss=1.3859 val_loss=0.0000 scale=1.0000 norm=0.9823
[iter 100] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.6291
[iter 200] loss=0.8320 val_loss=0.0000 scale=2.0000 norm=1.6159
[iter 300] loss=0.7114 val_loss=0.0000 scale=2.0000 norm=1.6243
[iter 400] loss=0.5952 val_loss=0.0000 scale=2.0000 norm=1.6292
[iter 0] loss=11.2645 val_loss=0.0000 scale=2.0000 norm=13721.0140
[iter 100] loss=10.4779 val_loss=0.0000 scale=2.0000 norm=12820.0370
[iter 200] loss=9.6886 val_loss=0.0000 scale=4.0000 norm=25358.6890
[iter 300] loss=9.1486 val_loss=0.0000 scale=4.0000 norm=25344.5320
[iter 400] loss=8.5618 val_loss=0.0000 scale=4.0000 norm=25343.8647
[iter 0] loss=1.2247 val_loss=0.0000 scale=1.0000 norm=0.8869
[iter 100] loss=0.9850 val_loss=0.0000 scale=2.0000 norm=1.5884
[iter 200] loss=0.9192 val_loss=0.0000 scale=2.0000 norm=1.6129
[iter 300] loss=0.8816 val_loss=0.0000 scale=2.0000 norm=1.6191
[iter 400] loss=0.8580 val_loss=0.0000 scale=2.0000 norm=1.6250
[iter 0] loss=1.3487 val_loss=0.0000 scale=1.0000 norm=0.9639
[iter 100] loss=1.0250 val_loss=0.0000 scale=2.0000 norm=1.6668
[iter 200] loss=0.8868 val_loss=0.0000 scale=2.0000 norm=1.6526
[iter 300] loss=0.7885 val_loss=0.0000 scale=2.0000 norm=1.6540
[iter 400] loss=0.6747 val_loss=0.0000 scale=4.0000 norm=3.3082
[iter 0] loss=11.8665 val_loss=0.0000 scale=2.0000 norm=18875.6366
[iter 100] loss=10.7020 val_loss=0.0000 scale=4.0000 norm=36718.3028
[iter 200] loss=9.8647 val_loss=0.0000 scale=4.0000 norm=36686.9667
[iter 300] loss=9.2277 val_loss=0.0000 scale=8.0000 norm=73365.2466
[iter 400] loss=8.2964 val_loss=0.0000 scale=8.0000 norm=73365.2926
[iter 0] loss=1.3013 val_loss=0.0000 scale=1.0000 norm=0.9306
[iter 100] loss=1.0408 val_loss=0.0000 scale=2.0000 norm=1.6283
[iter 200] loss=0.9789 val_loss=0.0000 scale=2.0000 norm=1.6535
[iter 300] loss=0.9393 val_loss=0.0000 scale=2.0000 norm=1.6651
[iter 400] loss=0.8992 val_loss=0.0000 scale=2.0000 norm=1.6666
[iter 0] loss=1.3872 val_loss=0.0000 scale=2.0000 norm=1.9665
[iter 100] loss=1.0493 val_loss=0.0000 scale=2.0000 norm=1.6875
[iter 200] loss=0.8973 val_loss=0.0000 scale=2.0000 norm=1.6719
[iter 300] loss=0.7913 val_loss=0.0000 scale=4.0000 norm=3.3431
[iter 400] loss=0.6705 val_loss=0.0000 scale=2.0000 norm=1.6766
[iter 0] loss=13.4574 val_loss=0.0000 scale=2.0000 norm=58338.7171
[iter 100] loss=11.7115 val_loss=0.0000 scale=4.0000 norm=111228.3143
[iter 200] loss=10.5336 val_loss=0.0000 scale=4.0000 norm=111216.7050
[iter 300] loss=9.7895 val_loss=0.0000 scale=8.0000 norm=222415.1416
[iter 400] loss=8.7894 val_loss=0.0000 scale=8.0000 norm=222414.4097
[iter 0] loss=1.2534 val_loss=0.0000 scale=1.0000 norm=0.9046
[iter 100] loss=1.0132 val_loss=0.0000 scale=2.0000 norm=1.5963
[iter 200] loss=0.9577 val_loss=0.0000 scale=2.0000 norm=1.6262
[iter 300] loss=0.9238 val_loss=0.0000 scale=2.0000 norm=1.6345
[iter 400] loss=0.8926 val_loss=0.0000 scale=2.0000 norm=1.6362
[iter 0] loss=1.4004 val_loss=0.0000 scale=2.0000 norm=1.9799
[iter 100] loss=1.0805 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 200] loss=0.9358 val_loss=0.0000 scale=2.0000 norm=1.6809
[iter 300] loss=0.8339 val_loss=0.0000 scale=2.0000 norm=1.6850
[iter 400] loss=0.7159 val_loss=0.0000 scale=2.0000 norm=1.6894
[iter 0] loss=13.4605 val_loss=0.0000 scale=2.0000 norm=61923.2394
[iter 100] loss=11.7686 val_loss=0.0000 scale=4.0000 norm=114392.9057
[iter 200] loss=10.6264 val_loss=0.0000 scale=4.0000 norm=114304.6380
[iter 300] loss=9.9234 val_loss=0.0000 scale=8.0000 norm=228594.8267
[iter 400] loss=8.8805 val_loss=0.0000 scale=8.0000 norm=228593.8375
[iter 0] loss=1.2548 val_loss=0.0000 scale=1.0000 norm=0.9050
[iter 100] loss=0.9649 val_loss=0.0000 scale=2.0000 norm=1.5464
[iter 200] loss=0.8691 val_loss=0.0000 scale=2.0000 norm=1.5707
[iter 300] loss=0.7834 val_loss=0.0000 scale=4.0000 norm=3.1592
[iter 400] loss=0.6969 val_loss=0.0000 scale=2.0000 norm=1.5802
[iter 0] loss=1.4331 val_loss=0.0000 scale=2.0000 norm=2.0154
[iter 100] loss=1.1054 val_loss=0.0000 scale=2.0000 norm=1.7389
[iter 200] loss=0.9588 val_loss=0.0000 scale=2.0000 norm=1.7307
[iter 300] loss=0.8565 val_loss=0.0000 scale=2.0000 norm=1.7336
[iter 400] loss=0.7581 val_loss=0.0000 scale=4.0000 norm=3.4794
[iter 0] loss=13.4601 val_loss=0.0000 scale=2.0000 norm=61418.5986
[iter 100] loss=11.8068 val_loss=0.0000 scale=4.0000 norm=114524.0005
[iter 200] loss=10.6845 val_loss=0.0000 scale=4.0000 norm=114491.7804
[iter 300] loss=9.9291 val_loss=0.0000 scale=8.0000 norm=228969.5420
[iter 400] loss=8.9141 val_loss=0.0000 scale=8.0000 norm=228969.4391
[iter 0] loss=1.2519 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0116 val_loss=0.0000 scale=2.0000 norm=1.5855
[iter 200] loss=0.9555 val_loss=0.0000 scale=2.0000 norm=1.6143
[iter 300] loss=0.9197 val_loss=0.0000 scale=2.0000 norm=1.6243
[iter 400] loss=0.8842 val_loss=0.0000 scale=2.0000 norm=1.6258
[iter 0] loss=1.3513 val_loss=0.0000 scale=2.0000 norm=1.9289
[iter 100] loss=1.0553 val_loss=0.0000 scale=2.0000 norm=1.6897
[iter 200] loss=0.9193 val_loss=0.0000 scale=2.0000 norm=1.6881
[iter 300] loss=0.8247 val_loss=0.0000 scale=2.0000 norm=1.6929
[iter 400] loss=0.7072 val_loss=0.0000 scale=2.0000 norm=1.7001
[iter 0] loss=12.2411 val_loss=0.0000 scale=2.0000 norm=20949.1319
[iter 100] loss=10.7512 val_loss=0.0000 scale=4.0000 norm=40325.2607
[iter 200] loss=9.8088 val_loss=0.0000 scale=4.0000 norm=40286.7647
[iter 300] loss=9.0279 val_loss=0.0000 scale=8.0000 norm=80563.6903
[iter 400] loss=7.9565 val_loss=0.0000 scale=8.0000 norm=80563.7225
[iter 0] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=0.9362
[iter 100] loss=1.0620 val_loss=0.0000 scale=2.0000 norm=1.6600
[iter 200] loss=0.9924 val_loss=0.0000 scale=2.0000 norm=1.6811
[iter 300] loss=0.9480 val_loss=0.0000 scale=2.0000 norm=1.6921
[iter 400] loss=0.9103 val_loss=0.0000 scale=4.0000 norm=3.3942
[iter 0] loss=1.3435 val_loss=0.0000 scale=2.0000 norm=1.9213
[iter 100] loss=0.9969 val_loss=0.0000 scale=2.0000 norm=1.6251
[iter 200] loss=0.8426 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 300] loss=0.7135 val_loss=0.0000 scale=2.0000 norm=1.6251
[iter 400] loss=0.5649 val_loss=0.0000 scale=2.0000 norm=1.6199
[iter 0] loss=13.4578 val_loss=0.0000 scale=2.0000 norm=58187.7039
[iter 100] loss=11.6973 val_loss=0.0000 scale=4.0000 norm=110479.9667
[iter 200] loss=10.5240 val_loss=0.0000 scale=4.0000 norm=110462.2250
[iter 300] loss=9.8196 val_loss=0.0000 scale=8.0000 norm=220910.2207
[iter 400] loss=8.8813 val_loss=0.0000 scale=8.0000 norm=220909.3133
[iter 0] loss=1.3472 val_loss=0.0000 scale=1.0000 norm=0.9569
[iter 100] loss=1.0969 val_loss=0.0000 scale=2.0000 norm=1.6752
[iter 200] loss=1.0409 val_loss=0.0000 scale=2.0000 norm=1.7022
[iter 300] loss=1.0103 val_loss=0.0000 scale=2.0000 norm=1.7132
[iter 400] loss=0.9766 val_loss=0.0000 scale=4.0000 norm=3.4305
[iter 0] loss=1.3050 val_loss=0.0000 scale=2.0000 norm=1.8775
[iter 100] loss=1.0018 val_loss=0.0000 scale=2.0000 norm=1.6214
[iter 200] loss=0.8649 val_loss=0.0000 scale=2.0000 norm=1.6162
[iter 300] loss=0.7591 val_loss=0.0000 scale=2.0000 norm=1.6286
[iter 400] loss=0.6444 val_loss=0.0000 scale=4.0000 norm=3.2624
[iter 0] loss=13.4587 val_loss=0.0000 scale=2.0000 norm=61239.9286
[iter 100] loss=11.7382 val_loss=0.0000 scale=4.0000 norm=113491.6958
[iter 200] loss=10.5961 val_loss=0.0000 scale=4.0000 norm=113474.6891
[iter 300] loss=9.9101 val_loss=0.0000 scale=8.0000 norm=226928.2882
[iter 400] loss=8.9333 val_loss=0.0000 scale=8.0000 norm=226926.4958
[iter 0] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=0.9372
[iter 100] loss=1.0489 val_loss=0.0000 scale=2.0000 norm=1.6387
[iter 200] loss=0.9567 val_loss=0.0000 scale=2.0000 norm=1.6564
[iter 300] loss=0.8692 val_loss=0.0000 scale=4.0000 norm=3.3265
[iter 400] loss=0.7836 val_loss=0.0000 scale=2.0000 norm=1.6625
[iter 0] loss=1.3703 val_loss=0.0000 scale=1.0000 norm=0.9735
[iter 100] loss=1.0532 val_loss=0.0000 scale=2.0000 norm=1.6570
[iter 200] loss=0.9194 val_loss=0.0000 scale=2.0000 norm=1.6461
[iter 300] loss=0.8214 val_loss=0.0000 scale=2.0000 norm=1.6485
[iter 400] loss=0.7282 val_loss=0.0000 scale=2.0000 norm=1.6543
[iter 0] loss=13.4568 val_loss=0.0000 scale=2.0000 norm=57335.5236
[iter 100] loss=11.7058 val_loss=0.0000 scale=4.0000 norm=108620.0989
[iter 200] loss=10.5664 val_loss=0.0000 scale=4.0000 norm=108626.6454
[iter 300] loss=9.8642 val_loss=0.0000 scale=8.0000 norm=217242.5685
[iter 400] loss=8.9327 val_loss=0.0000 scale=8.0000 norm=217241.8617
[iter 0] loss=1.2546 val_loss=0.0000 scale=1.0000 norm=0.9029
[iter 100] loss=0.9917 val_loss=0.0000 scale=2.0000 norm=1.5744
[iter 200] loss=0.9387 val_loss=0.0000 scale=2.0000 norm=1.6113
[iter 300] loss=0.9077 val_loss=0.0000 scale=2.0000 norm=1.6220
[iter 400] loss=0.8748 val_loss=0.0000 scale=4.0000 norm=3.2478
[iter 0] loss=1.4090 val_loss=0.0000 scale=2.0000 norm=1.9893
[iter 100] loss=1.0927 val_loss=0.0000 scale=2.0000 norm=1.7285
[iter 200] loss=0.9529 val_loss=0.0000 scale=2.0000 norm=1.7187
[iter 300] loss=0.8509 val_loss=0.0000 scale=4.0000 norm=3.4434
[iter 400] loss=0.7516 val_loss=0.0000 scale=2.0000 norm=1.7281
[iter 0] loss=13.4597 val_loss=0.0000 scale=2.0000 norm=60278.9903
[iter 100] loss=11.7324 val_loss=0.0000 scale=4.0000 norm=112588.4724
[iter 200] loss=10.5866 val_loss=0.0000 scale=4.0000 norm=112567.7265
[iter 300] loss=9.8473 val_loss=0.0000 scale=8.0000 norm=225112.0591
[iter 400] loss=8.8443 val_loss=0.0000 scale=8.0000 norm=225110.3628
[iter 0] loss=1.2548 val_loss=0.0000 scale=1.0000 norm=0.9050
[iter 100] loss=0.9649 val_loss=0.0000 scale=2.0000 norm=1.5464
[iter 200] loss=0.8691 val_loss=0.0000 scale=2.0000 norm=1.5707
[iter 300] loss=0.7834 val_loss=0.0000 scale=4.0000 norm=3.1592
[iter 400] loss=0.6969 val_loss=0.0000 scale=2.0000 norm=1.5802
[iter 0] loss=1.4331 val_loss=0.0000 scale=2.0000 norm=2.0154
[iter 100] loss=1.1054 val_loss=0.0000 scale=2.0000 norm=1.7389
[iter 200] loss=0.9588 val_loss=0.0000 scale=2.0000 norm=1.7307
[iter 300] loss=0.8565 val_loss=0.0000 scale=2.0000 norm=1.7336
[iter 400] loss=0.7581 val_loss=0.0000 scale=4.0000 norm=3.4794
[iter 0] loss=13.4601 val_loss=0.0000 scale=2.0000 norm=61418.5986
[iter 100] loss=11.8068 val_loss=0.0000 scale=4.0000 norm=114524.0005
[iter 200] loss=10.6845 val_loss=0.0000 scale=4.0000 norm=114491.7804
[iter 300] loss=9.9291 val_loss=0.0000 scale=8.0000 norm=228969.5420
[iter 400] loss=8.9141 val_loss=0.0000 scale=8.0000 norm=228969.4391
[iter 0] loss=1.2441 val_loss=0.0000 scale=1.0000 norm=0.9004
[iter 100] loss=1.0093 val_loss=0.0000 scale=2.0000 norm=1.6054
[iter 200] loss=0.9531 val_loss=0.0000 scale=2.0000 norm=1.6402
[iter 300] loss=0.9195 val_loss=0.0000 scale=2.0000 norm=1.6504
[iter 400] loss=0.8842 val_loss=0.0000 scale=2.0000 norm=1.6519
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0550
[iter 100] loss=1.1329 val_loss=0.0000 scale=2.0000 norm=1.7849
[iter 200] loss=0.9799 val_loss=0.0000 scale=2.0000 norm=1.7793
[iter 300] loss=0.8571 val_loss=0.0000 scale=2.0000 norm=1.7819
[iter 400] loss=0.7322 val_loss=0.0000 scale=2.0000 norm=1.7818
[iter 0] loss=13.4604 val_loss=0.0000 scale=2.0000 norm=61696.3465
[iter 100] loss=11.7724 val_loss=0.0000 scale=4.0000 norm=115603.7874
[iter 200] loss=10.6359 val_loss=0.0000 scale=4.0000 norm=115514.6012
[iter 300] loss=9.8974 val_loss=0.0000 scale=8.0000 norm=231007.4732
[iter 400] loss=8.8655 val_loss=0.0000 scale=8.0000 norm=231005.7667
[iter 0] loss=1.2546 val_loss=0.0000 scale=1.0000 norm=0.9029
[iter 100] loss=0.9917 val_loss=0.0000 scale=2.0000 norm=1.5744
[iter 200] loss=0.9387 val_loss=0.0000 scale=2.0000 norm=1.6113
[iter 300] loss=0.9077 val_loss=0.0000 scale=2.0000 norm=1.6220
[iter 400] loss=0.8748 val_loss=0.0000 scale=4.0000 norm=3.2478
[iter 0] loss=1.4090 val_loss=0.0000 scale=2.0000 norm=1.9893
[iter 100] loss=1.0927 val_loss=0.0000 scale=2.0000 norm=1.7285
[iter 200] loss=0.9529 val_loss=0.0000 scale=2.0000 norm=1.7187
[iter 300] loss=0.8509 val_loss=0.0000 scale=4.0000 norm=3.4434
[iter 400] loss=0.7516 val_loss=0.0000 scale=2.0000 norm=1.7281
[iter 0] loss=13.4597 val_loss=0.0000 scale=2.0000 norm=60278.9903
[iter 100] loss=11.7324 val_loss=0.0000 scale=4.0000 norm=112588.4724
[iter 200] loss=10.5866 val_loss=0.0000 scale=4.0000 norm=112567.7265
[iter 300] loss=9.8473 val_loss=0.0000 scale=8.0000 norm=225112.0591
[iter 400] loss=8.8443 val_loss=0.0000 scale=8.0000 norm=225110.3628
[iter 0] loss=1.3108 val_loss=0.0000 scale=1.0000 norm=0.9365
[iter 100] loss=0.9993 val_loss=0.0000 scale=2.0000 norm=1.5717
[iter 200] loss=0.9357 val_loss=0.0000 scale=2.0000 norm=1.6060
[iter 300] loss=0.8997 val_loss=0.0000 scale=2.0000 norm=1.6190
[iter 400] loss=0.8661 val_loss=0.0000 scale=4.0000 norm=3.2434
[iter 0] loss=1.4291 val_loss=0.0000 scale=2.0000 norm=2.0111
[iter 100] loss=1.0707 val_loss=0.0000 scale=2.0000 norm=1.7065
[iter 200] loss=0.9103 val_loss=0.0000 scale=2.0000 norm=1.6925
[iter 300] loss=0.8106 val_loss=0.0000 scale=2.0000 norm=1.7063
[iter 400] loss=0.7160 val_loss=0.0000 scale=2.0000 norm=1.7169
[iter 0] loss=13.4586 val_loss=0.0000 scale=2.0000 norm=59840.7698
[iter 100] loss=11.7036 val_loss=0.0000 scale=4.0000 norm=109860.9487
[iter 200] loss=10.4267 val_loss=0.0000 scale=4.0000 norm=109781.3413
[iter 300] loss=9.6421 val_loss=0.0000 scale=8.0000 norm=219531.8265
[iter 400] loss=8.7436 val_loss=0.0000 scale=4.0000 norm=109765.1615
[iter 0] loss=1.3202 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0556 val_loss=0.0000 scale=2.0000 norm=1.6373
[iter 200] loss=0.9917 val_loss=0.0000 scale=2.0000 norm=1.6667
[iter 300] loss=0.9507 val_loss=0.0000 scale=2.0000 norm=1.6758
[iter 400] loss=0.9131 val_loss=0.0000 scale=2.0000 norm=1.6799
[iter 0] loss=1.3569 val_loss=0.0000 scale=1.0000 norm=0.9670
[iter 100] loss=1.0278 val_loss=0.0000 scale=2.0000 norm=1.6611
[iter 200] loss=0.8824 val_loss=0.0000 scale=2.0000 norm=1.6499
[iter 300] loss=0.7887 val_loss=0.0000 scale=4.0000 norm=3.3305
[iter 400] loss=0.6945 val_loss=0.0000 scale=2.0000 norm=1.6743
[iter 0] loss=13.4601 val_loss=0.0000 scale=2.0000 norm=61140.5327
[iter 100] loss=11.7289 val_loss=0.0000 scale=4.0000 norm=113090.6362
[iter 200] loss=10.5693 val_loss=0.0000 scale=4.0000 norm=113060.9336
[iter 300] loss=9.8282 val_loss=0.0000 scale=8.0000 norm=226105.3560
[iter 400] loss=8.8234 val_loss=0.0000 scale=8.0000 norm=226105.0675
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9178
[iter 100] loss=1.0396 val_loss=0.0000 scale=2.0000 norm=1.6143
[iter 200] loss=0.9904 val_loss=0.0000 scale=2.0000 norm=1.6464
[iter 300] loss=0.9610 val_loss=0.0000 scale=4.0000 norm=3.3158
[iter 400] loss=0.9295 val_loss=0.0000 scale=2.0000 norm=1.6597
[iter 0] loss=1.3920 val_loss=0.0000 scale=1.0000 norm=0.9856
[iter 100] loss=1.0408 val_loss=0.0000 scale=2.0000 norm=1.6860
[iter 200] loss=0.8918 val_loss=0.0000 scale=2.0000 norm=1.6817
[iter 300] loss=0.7876 val_loss=0.0000 scale=4.0000 norm=3.3962
[iter 400] loss=0.6758 val_loss=0.0000 scale=2.0000 norm=1.7026
[iter 0] loss=12.2351 val_loss=0.0000 scale=2.0000 norm=20907.5541
[iter 100] loss=10.6948 val_loss=0.0000 scale=4.0000 norm=40079.8703
[iter 200] loss=9.7464 val_loss=0.0000 scale=4.0000 norm=40013.4770
[iter 300] loss=8.9157 val_loss=0.0000 scale=8.0000 norm=80015.6345
[iter 400] loss=7.8649 val_loss=0.0000 scale=8.0000 norm=80015.6725
[iter 0] loss=1.2729 val_loss=0.0000 scale=1.0000 norm=0.9145
[iter 100] loss=1.0173 val_loss=0.0000 scale=2.0000 norm=1.6181
[iter 200] loss=0.9630 val_loss=0.0000 scale=2.0000 norm=1.6483
[iter 300] loss=0.9367 val_loss=0.0000 scale=2.0000 norm=1.6632
[iter 400] loss=0.9118 val_loss=0.0000 scale=2.0000 norm=1.6655
[iter 0] loss=1.3675 val_loss=0.0000 scale=2.0000 norm=1.9472
[iter 100] loss=1.0375 val_loss=0.0000 scale=2.0000 norm=1.6795
[iter 200] loss=0.8819 val_loss=0.0000 scale=2.0000 norm=1.6687
[iter 300] loss=0.7668 val_loss=0.0000 scale=2.0000 norm=1.6746
[iter 400] loss=0.6415 val_loss=0.0000 scale=2.0000 norm=1.6787
[iter 0] loss=12.2464 val_loss=0.0000 scale=2.0000 norm=21739.0699
[iter 100] loss=10.7512 val_loss=0.0000 scale=4.0000 norm=42316.0614
[iter 200] loss=9.8512 val_loss=0.0000 scale=4.0000 norm=42271.6360
[iter 300] loss=9.2679 val_loss=0.0000 scale=8.0000 norm=84531.7308
[iter 400] loss=8.5792 val_loss=0.0000 scale=8.0000 norm=84530.9583
[iter 0] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=0.9089
[iter 100] loss=1.0088 val_loss=0.0000 scale=2.0000 norm=1.5960
[iter 200] loss=0.9144 val_loss=0.0000 scale=2.0000 norm=1.6158
[iter 300] loss=0.8368 val_loss=0.0000 scale=4.0000 norm=3.2463
[iter 400] loss=0.7502 val_loss=0.0000 scale=2.0000 norm=1.6251
[iter 0] loss=1.3664 val_loss=0.0000 scale=2.0000 norm=1.9434
[iter 100] loss=1.0314 val_loss=0.0000 scale=2.0000 norm=1.6698
[iter 200] loss=0.8786 val_loss=0.0000 scale=2.0000 norm=1.6630
[iter 300] loss=0.7650 val_loss=0.0000 scale=4.0000 norm=3.3494
[iter 400] loss=0.6117 val_loss=0.0000 scale=4.0000 norm=3.3548
[iter 0] loss=13.4224 val_loss=0.0000 scale=2.0000 norm=53011.7863
[iter 100] loss=11.7129 val_loss=0.0000 scale=4.0000 norm=98897.0640
[iter 200] loss=10.6141 val_loss=0.0000 scale=4.0000 norm=98857.9649
[iter 300] loss=9.8782 val_loss=0.0000 scale=8.0000 norm=197698.9826
[iter 400] loss=8.8748 val_loss=0.0000 scale=8.0000 norm=197697.9586
[iter 0] loss=1.2520 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0181 val_loss=0.0000 scale=2.0000 norm=1.5975
[iter 200] loss=0.9687 val_loss=0.0000 scale=2.0000 norm=1.6183
[iter 300] loss=0.9408 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 400] loss=0.9160 val_loss=0.0000 scale=2.0000 norm=1.6300
[iter 0] loss=1.3875 val_loss=0.0000 scale=2.0000 norm=1.9660
[iter 100] loss=1.1015 val_loss=0.0000 scale=2.0000 norm=1.7399
[iter 200] loss=0.9645 val_loss=0.0000 scale=2.0000 norm=1.7311
[iter 300] loss=0.8642 val_loss=0.0000 scale=4.0000 norm=3.4825
[iter 400] loss=0.7063 val_loss=0.0000 scale=4.0000 norm=3.4921
[iter 0] loss=13.4585 val_loss=0.0000 scale=2.0000 norm=59699.4861
[iter 100] loss=11.7243 val_loss=0.0000 scale=4.0000 norm=112385.4245
[iter 200] loss=10.6004 val_loss=0.0000 scale=4.0000 norm=112322.2256
[iter 300] loss=9.9414 val_loss=0.0000 scale=8.0000 norm=224611.4818
[iter 400] loss=9.0209 val_loss=0.0000 scale=8.0000 norm=224608.6368
[iter 0] loss=1.3217 val_loss=0.0000 scale=1.0000 norm=0.9423
[iter 100] loss=1.0595 val_loss=0.0000 scale=2.0000 norm=1.6514
[iter 200] loss=0.9534 val_loss=0.0000 scale=2.0000 norm=1.6689
[iter 300] loss=0.8495 val_loss=0.0000 scale=2.0000 norm=1.6728
[iter 400] loss=0.7300 val_loss=0.0000 scale=2.0000 norm=1.6744
[iter 0] loss=1.3780 val_loss=0.0000 scale=2.0000 norm=1.9562
[iter 100] loss=1.0666 val_loss=0.0000 scale=2.0000 norm=1.6932
[iter 200] loss=0.9258 val_loss=0.0000 scale=2.0000 norm=1.6810
[iter 300] loss=0.8001 val_loss=0.0000 scale=4.0000 norm=3.3670
[iter 400] loss=0.6292 val_loss=0.0000 scale=4.0000 norm=3.3717
[iter 0] loss=13.4456 val_loss=0.0000 scale=2.0000 norm=54036.4493
[iter 100] loss=11.6828 val_loss=0.0000 scale=4.0000 norm=102992.3038
[iter 200] loss=10.5676 val_loss=0.0000 scale=4.0000 norm=102995.8536
[iter 300] loss=9.8393 val_loss=0.0000 scale=8.0000 norm=205979.3751
[iter 400] loss=8.8864 val_loss=0.0000 scale=8.0000 norm=205978.9430
[iter 0] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=0.9372
[iter 100] loss=1.0489 val_loss=0.0000 scale=2.0000 norm=1.6387
[iter 200] loss=0.9567 val_loss=0.0000 scale=2.0000 norm=1.6564
[iter 300] loss=0.8692 val_loss=0.0000 scale=4.0000 norm=3.3265
[iter 400] loss=0.7836 val_loss=0.0000 scale=2.0000 norm=1.6625
[iter 0] loss=1.3703 val_loss=0.0000 scale=1.0000 norm=0.9735
[iter 100] loss=1.0532 val_loss=0.0000 scale=2.0000 norm=1.6570
[iter 200] loss=0.9194 val_loss=0.0000 scale=2.0000 norm=1.6461
[iter 300] loss=0.8214 val_loss=0.0000 scale=2.0000 norm=1.6485
[iter 400] loss=0.7282 val_loss=0.0000 scale=2.0000 norm=1.6543
[iter 0] loss=13.4568 val_loss=0.0000 scale=2.0000 norm=57335.5236
[iter 100] loss=11.7058 val_loss=0.0000 scale=4.0000 norm=108620.0989
[iter 200] loss=10.5664 val_loss=0.0000 scale=4.0000 norm=108626.6454
[iter 300] loss=9.8642 val_loss=0.0000 scale=8.0000 norm=217242.5685
[iter 400] loss=8.9327 val_loss=0.0000 scale=8.0000 norm=217241.8623
[iter 0] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=0.9370
[iter 100] loss=1.0109 val_loss=0.0000 scale=2.0000 norm=1.6028
[iter 200] loss=0.9037 val_loss=0.0000 scale=2.0000 norm=1.6227
[iter 300] loss=0.8110 val_loss=0.0000 scale=4.0000 norm=3.2663
[iter 400] loss=0.6700 val_loss=0.0000 scale=2.0000 norm=1.6337
[iter 0] loss=1.3571 val_loss=0.0000 scale=2.0000 norm=1.9347
[iter 100] loss=1.0754 val_loss=0.0000 scale=2.0000 norm=1.7054
[iter 200] loss=0.9539 val_loss=0.0000 scale=2.0000 norm=1.7036
[iter 300] loss=0.8710 val_loss=0.0000 scale=2.0000 norm=1.7112
[iter 400] loss=0.7850 val_loss=0.0000 scale=4.0000 norm=3.4321
[iter 0] loss=13.4219 val_loss=0.0000 scale=2.0000 norm=51251.8923
[iter 100] loss=11.6842 val_loss=0.0000 scale=4.0000 norm=96967.8037
[iter 200] loss=10.5397 val_loss=0.0000 scale=4.0000 norm=96949.9967
[iter 300] loss=9.7665 val_loss=0.0000 scale=8.0000 norm=193881.4513
[iter 400] loss=8.7269 val_loss=0.0000 scale=8.0000 norm=193880.5714
[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=0.9091
[iter 100] loss=1.0224 val_loss=0.0000 scale=2.0000 norm=1.6074
[iter 200] loss=0.9596 val_loss=0.0000 scale=2.0000 norm=1.6402
[iter 300] loss=0.9181 val_loss=0.0000 scale=2.0000 norm=1.6505
[iter 400] loss=0.8795 val_loss=0.0000 scale=2.0000 norm=1.6531
[iter 0] loss=1.3826 val_loss=0.0000 scale=1.0000 norm=0.9804
[iter 100] loss=1.0818 val_loss=0.0000 scale=2.0000 norm=1.7116
[iter 200] loss=0.9502 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 300] loss=0.8568 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 400] loss=0.7723 val_loss=0.0000 scale=2.0000 norm=1.6998
[iter 0] loss=13.4569 val_loss=0.0000 scale=2.0000 norm=57516.1521
[iter 100] loss=11.6840 val_loss=0.0000 scale=4.0000 norm=109641.2795
[iter 200] loss=10.5258 val_loss=0.0000 scale=4.0000 norm=109624.5934
[iter 300] loss=9.8538 val_loss=0.0000 scale=4.0000 norm=109610.9948
[iter 400] loss=9.0005 val_loss=0.0000 scale=8.0000 norm=219218.2351
[iter 0] loss=1.2996 val_loss=0.0000 scale=1.0000 norm=0.9298
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.5421
[iter 200] loss=0.9100 val_loss=0.0000 scale=2.0000 norm=1.5816
[iter 300] loss=0.8770 val_loss=0.0000 scale=4.0000 norm=3.1907
[iter 400] loss=0.8446 val_loss=0.0000 scale=2.0000 norm=1.5972
[iter 0] loss=1.4823 val_loss=0.0000 scale=2.0000 norm=2.0697
[iter 100] loss=1.1600 val_loss=0.0000 scale=2.0000 norm=1.7919
[iter 200] loss=1.0213 val_loss=0.0000 scale=2.0000 norm=1.7795
[iter 300] loss=0.9222 val_loss=0.0000 scale=4.0000 norm=3.5578
[iter 400] loss=0.8067 val_loss=0.0000 scale=2.0000 norm=1.7869
[iter 0] loss=13.4584 val_loss=0.0000 scale=2.0000 norm=59369.0193
[iter 100] loss=11.7086 val_loss=0.0000 scale=4.0000 norm=110226.3378
[iter 200] loss=10.4856 val_loss=0.0000 scale=4.0000 norm=110172.8392
[iter 300] loss=9.8092 val_loss=0.0000 scale=4.0000 norm=110160.1572
[iter 400] loss=8.8584 val_loss=0.0000 scale=8.0000 norm=220318.8159
[iter 0] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=0.9430
[iter 100] loss=1.0494 val_loss=0.0000 scale=2.0000 norm=1.6522
[iter 200] loss=0.9342 val_loss=0.0000 scale=2.0000 norm=1.6652
[iter 300] loss=0.8300 val_loss=0.0000 scale=2.0000 norm=1.6740
[iter 400] loss=0.7235 val_loss=0.0000 scale=4.0000 norm=3.3510
[iter 0] loss=1.3545 val_loss=0.0000 scale=1.0000 norm=0.9669
[iter 100] loss=1.0568 val_loss=0.0000 scale=2.0000 norm=1.6725
[iter 200] loss=0.9231 val_loss=0.0000 scale=2.0000 norm=1.6627
[iter 300] loss=0.8326 val_loss=0.0000 scale=2.0000 norm=1.6661
[iter 400] loss=0.7465 val_loss=0.0000 scale=2.0000 norm=1.6673
[iter 0] loss=13.4579 val_loss=0.0000 scale=2.0000 norm=59724.5886
[iter 100] loss=11.7205 val_loss=0.0000 scale=4.0000 norm=112329.1604
[iter 200] loss=10.5650 val_loss=0.0000 scale=4.0000 norm=112278.7378
[iter 300] loss=9.8653 val_loss=0.0000 scale=8.0000 norm=224532.1230
[iter 400] loss=8.9008 val_loss=0.0000 scale=8.0000 norm=224530.6721
[iter 0] loss=1.2674 val_loss=0.0000 scale=1.0000 norm=0.9112
[iter 100] loss=1.0662 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 200] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6878
[iter 300] loss=0.9978 val_loss=0.0000 scale=2.0000 norm=1.6955
[iter 400] loss=0.9679 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 0] loss=1.3258 val_loss=0.0000 scale=2.0000 norm=1.9016
[iter 100] loss=1.0257 val_loss=0.0000 scale=2.0000 norm=1.6484
[iter 200] loss=0.8896 val_loss=0.0000 scale=2.0000 norm=1.6399
[iter 300] loss=0.7865 val_loss=0.0000 scale=4.0000 norm=3.2952
[iter 400] loss=0.6791 val_loss=0.0000 scale=2.0000 norm=1.6498
[iter 0] loss=13.4600 val_loss=0.0000 scale=2.0000 norm=60949.3788
[iter 100] loss=11.7265 val_loss=0.0000 scale=4.0000 norm=113797.0623
[iter 200] loss=10.5769 val_loss=0.0000 scale=4.0000 norm=113732.5015
[iter 300] loss=9.8757 val_loss=0.0000 scale=8.0000 norm=227438.9571
[iter 400] loss=8.8712 val_loss=0.0000 scale=8.0000 norm=227437.6800
[iter 0] loss=1.2880 val_loss=0.0000 scale=1.0000 norm=0.9224
[iter 100] loss=1.0308 val_loss=0.0000 scale=2.0000 norm=1.6242
[iter 200] loss=0.9644 val_loss=0.0000 scale=2.0000 norm=1.6502
[iter 300] loss=0.9237 val_loss=0.0000 scale=4.0000 norm=3.3221
[iter 400] loss=0.8834 val_loss=0.0000 scale=2.0000 norm=1.6658
[iter 0] loss=1.3802 val_loss=0.0000 scale=2.0000 norm=1.9592
[iter 100] loss=1.0403 val_loss=0.0000 scale=2.0000 norm=1.6620
[iter 200] loss=0.9015 val_loss=0.0000 scale=2.0000 norm=1.6522
[iter 300] loss=0.8031 val_loss=0.0000 scale=2.0000 norm=1.6539
[iter 400] loss=0.6923 val_loss=0.0000 scale=4.0000 norm=3.3130
[iter 0] loss=13.4221 val_loss=0.0000 scale=2.0000 norm=52155.6181
[iter 100] loss=11.6563 val_loss=0.0000 scale=4.0000 norm=97740.7427
[iter 200] loss=10.5101 val_loss=0.0000 scale=4.0000 norm=97662.0298
[iter 300] loss=9.8244 val_loss=0.0000 scale=8.0000 norm=195297.5148
[iter 400] loss=9.0754 val_loss=0.0000 scale=4.0000 norm=97648.0060
[iter 0] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=0.9362
[iter 100] loss=1.0620 val_loss=0.0000 scale=2.0000 norm=1.6600
[iter 200] loss=0.9924 val_loss=0.0000 scale=2.0000 norm=1.6811
[iter 300] loss=0.9480 val_loss=0.0000 scale=2.0000 norm=1.6921
[iter 400] loss=0.9103 val_loss=0.0000 scale=4.0000 norm=3.3942
[iter 0] loss=1.3435 val_loss=0.0000 scale=2.0000 norm=1.9213
[iter 100] loss=0.9969 val_loss=0.0000 scale=2.0000 norm=1.6251
[iter 200] loss=0.8426 val_loss=0.0000 scale=2.0000 norm=1.6215
[iter 300] loss=0.7148 val_loss=0.0000 scale=2.0000 norm=1.6252
[iter 400] loss=0.5677 val_loss=0.0000 scale=4.0000 norm=3.2396
[iter 0] loss=13.4578 val_loss=0.0000 scale=2.0000 norm=58187.7039
[iter 100] loss=11.6973 val_loss=0.0000 scale=4.0000 norm=110479.9667
[iter 200] loss=10.5240 val_loss=0.0000 scale=4.0000 norm=110462.2250
[iter 300] loss=9.8196 val_loss=0.0000 scale=8.0000 norm=220910.2207
[iter 400] loss=8.8812 val_loss=0.0000 scale=8.0000 norm=220909.3114
[iter 0] loss=1.3472 val_loss=0.0000 scale=1.0000 norm=0.9569
[iter 100] loss=1.0969 val_loss=0.0000 scale=2.0000 norm=1.6752
[iter 200] loss=1.0409 val_loss=0.0000 scale=2.0000 norm=1.7022
[iter 300] loss=1.0103 val_loss=0.0000 scale=2.0000 norm=1.7132
[iter 400] loss=0.9766 val_loss=0.0000 scale=4.0000 norm=3.4305
[iter 0] loss=1.3050 val_loss=0.0000 scale=2.0000 norm=1.8775
[iter 100] loss=1.0018 val_loss=0.0000 scale=2.0000 norm=1.6214
[iter 200] loss=0.8649 val_loss=0.0000 scale=2.0000 norm=1.6162
[iter 300] loss=0.7591 val_loss=0.0000 scale=2.0000 norm=1.6286
[iter 400] loss=0.6444 val_loss=0.0000 scale=4.0000 norm=3.2624
[iter 0] loss=13.4587 val_loss=0.0000 scale=2.0000 norm=61239.9286
[iter 100] loss=11.7382 val_loss=0.0000 scale=4.0000 norm=113491.6958
[iter 200] loss=10.5961 val_loss=0.0000 scale=4.0000 norm=113474.6891
[iter 300] loss=9.9101 val_loss=0.0000 scale=8.0000 norm=226928.2882
[iter 400] loss=8.9333 val_loss=0.0000 scale=8.0000 norm=226926.4942
[iter 0] loss=1.2359 val_loss=0.0000 scale=1.0000 norm=0.8946
[iter 100] loss=1.0101 val_loss=0.0000 scale=2.0000 norm=1.6050
[iter 200] loss=0.9550 val_loss=0.0000 scale=2.0000 norm=1.6328
[iter 300] loss=0.9188 val_loss=0.0000 scale=2.0000 norm=1.6440
[iter 400] loss=0.8926 val_loss=0.0000 scale=2.0000 norm=1.6492
[iter 0] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=0.9463
[iter 100] loss=1.0072 val_loss=0.0000 scale=2.0000 norm=1.6366
[iter 200] loss=0.8666 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 300] loss=0.7639 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 400] loss=0.6675 val_loss=0.0000 scale=2.0000 norm=1.6326
[iter 0] loss=13.4591 val_loss=0.0000 scale=2.0000 norm=58691.7659
[iter 100] loss=11.7151 val_loss=0.0000 scale=4.0000 norm=112893.1977
[iter 200] loss=10.5515 val_loss=0.0000 scale=4.0000 norm=112891.5036
[iter 300] loss=9.8055 val_loss=0.0000 scale=8.0000 norm=225763.7672
[iter 400] loss=8.7666 val_loss=0.0000 scale=8.0000 norm=225762.6201
[iter 0] loss=1.2534 val_loss=0.0000 scale=1.0000 norm=0.9046
[iter 100] loss=1.0132 val_loss=0.0000 scale=2.0000 norm=1.5963
[iter 200] loss=0.9577 val_loss=0.0000 scale=2.0000 norm=1.6262
[iter 300] loss=0.9238 val_loss=0.0000 scale=2.0000 norm=1.6345
[iter 400] loss=0.8926 val_loss=0.0000 scale=2.0000 norm=1.6362
[iter 0] loss=1.4004 val_loss=0.0000 scale=2.0000 norm=1.9799
[iter 100] loss=1.0805 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 200] loss=0.9358 val_loss=0.0000 scale=2.0000 norm=1.6809
[iter 300] loss=0.8339 val_loss=0.0000 scale=2.0000 norm=1.6850
[iter 400] loss=0.7159 val_loss=0.0000 scale=2.0000 norm=1.6894
[iter 0] loss=13.4605 val_loss=0.0000 scale=2.0000 norm=61923.2394
[iter 100] loss=11.7686 val_loss=0.0000 scale=4.0000 norm=114392.9057
[iter 200] loss=10.6264 val_loss=0.0000 scale=4.0000 norm=114304.6380
[iter 300] loss=9.9234 val_loss=0.0000 scale=8.0000 norm=228594.8267
[iter 400] loss=8.8805 val_loss=0.0000 scale=8.0000 norm=228593.8395
[iter 0] loss=1.2773 val_loss=0.0000 scale=1.0000 norm=0.9179
[iter 100] loss=1.0564 val_loss=0.0000 scale=2.0000 norm=1.6369
[iter 200] loss=1.0084 val_loss=0.0000 scale=2.0000 norm=1.6634
[iter 300] loss=0.9790 val_loss=0.0000 scale=2.0000 norm=1.6727
[iter 400] loss=0.9533 val_loss=0.0000 scale=2.0000 norm=1.6760
[iter 0] loss=1.3749 val_loss=0.0000 scale=2.0000 norm=1.9544
[iter 100] loss=1.0587 val_loss=0.0000 scale=2.0000 norm=1.6875
[iter 200] loss=0.9353 val_loss=0.0000 scale=2.0000 norm=1.6775
[iter 300] loss=0.8336 val_loss=0.0000 scale=4.0000 norm=3.3657
[iter 400] loss=0.7112 val_loss=0.0000 scale=4.0000 norm=3.3615
[iter 0] loss=13.4605 val_loss=0.0000 scale=2.0000 norm=61975.1128
[iter 100] loss=11.7725 val_loss=0.0000 scale=4.0000 norm=114478.8086
[iter 200] loss=10.5300 val_loss=0.0000 scale=4.0000 norm=114414.2165
[iter 300] loss=9.8653 val_loss=0.0000 scale=4.0000 norm=114403.9085
[iter 400] loss=9.1986 val_loss=0.0000 scale=4.0000 norm=114402.6526
[iter 0] loss=1.3236 val_loss=0.0000 scale=1.0000 norm=0.9435
[iter 100] loss=1.0934 val_loss=0.0000 scale=2.0000 norm=1.6802
[iter 200] loss=1.0381 val_loss=0.0000 scale=2.0000 norm=1.7047
[iter 300] loss=1.0037 val_loss=0.0000 scale=2.0000 norm=1.7143
[iter 400] loss=0.9640 val_loss=0.0000 scale=4.0000 norm=3.4323
[iter 0] loss=1.4085 val_loss=0.0000 scale=1.0000 norm=0.9945
[iter 100] loss=1.1023 val_loss=0.0000 scale=2.0000 norm=1.7298
[iter 200] loss=0.9757 val_loss=0.0000 scale=2.0000 norm=1.7190
[iter 300] loss=0.8944 val_loss=0.0000 scale=2.0000 norm=1.7239
[iter 400] loss=0.8147 val_loss=0.0000 scale=2.0000 norm=1.7288
[iter 0] loss=13.4454 val_loss=0.0000 scale=2.0000 norm=55593.6653
[iter 100] loss=11.6965 val_loss=0.0000 scale=4.0000 norm=104849.5712
[iter 200] loss=10.5749 val_loss=0.0000 scale=4.0000 norm=104835.9966
[iter 300] loss=9.8905 val_loss=0.0000 scale=8.0000 norm=209661.2628
[iter 400] loss=8.9848 val_loss=0.0000 scale=8.0000 norm=209660.7560
[iter 0] loss=1.2623 val_loss=0.0000 scale=1.0000 norm=0.9083
[iter 100] loss=1.0263 val_loss=0.0000 scale=2.0000 norm=1.6178
[iter 200] loss=0.9768 val_loss=0.0000 scale=2.0000 norm=1.6460
[iter 300] loss=0.9478 val_loss=0.0000 scale=2.0000 norm=1.6542
[iter 400] loss=0.9188 val_loss=0.0000 scale=2.0000 norm=1.6553
[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=0.9900
[iter 100] loss=1.0734 val_loss=0.0000 scale=2.0000 norm=1.7074
[iter 200] loss=0.9194 val_loss=0.0000 scale=2.0000 norm=1.6891
[iter 300] loss=0.8118 val_loss=0.0000 scale=4.0000 norm=3.3848
[iter 400] loss=0.6710 val_loss=0.0000 scale=4.0000 norm=3.3911
[iter 0] loss=13.4584 val_loss=0.0000 scale=2.0000 norm=61070.1560
[iter 100] loss=11.7691 val_loss=0.0000 scale=4.0000 norm=114129.3807
[iter 200] loss=10.6610 val_loss=0.0000 scale=4.0000 norm=114139.6397
[iter 300] loss=9.9815 val_loss=0.0000 scale=8.0000 norm=228267.6453
[iter 400] loss=9.0794 val_loss=0.0000 scale=8.0000 norm=228266.6408
[iter 0] loss=1.2604 val_loss=0.0000 scale=1.0000 norm=0.9080
[iter 100] loss=1.0411 val_loss=0.0000 scale=2.0000 norm=1.6226
[iter 200] loss=0.9859 val_loss=0.0000 scale=2.0000 norm=1.6476
[iter 300] loss=0.9467 val_loss=0.0000 scale=2.0000 norm=1.6547
[iter 400] loss=0.8960 val_loss=0.0000 scale=4.0000 norm=3.3116
[iter 0] loss=1.4348 val_loss=0.0000 scale=1.0000 norm=1.0085
[iter 100] loss=1.0593 val_loss=0.0000 scale=2.0000 norm=1.6821
[iter 200] loss=0.9211 val_loss=0.0000 scale=2.0000 norm=1.6808
[iter 300] loss=0.8254 val_loss=0.0000 scale=2.0000 norm=1.6884
[iter 400] loss=0.7192 val_loss=0.0000 scale=2.0000 norm=1.6903
[iter 0] loss=13.4220 val_loss=0.0000 scale=2.0000 norm=51255.8325
[iter 100] loss=11.6535 val_loss=0.0000 scale=4.0000 norm=97020.2259
[iter 200] loss=10.4984 val_loss=0.0000 scale=4.0000 norm=96985.8019
[iter 300] loss=9.8219 val_loss=0.0000 scale=8.0000 norm=193958.1484
[iter 400] loss=8.8355 val_loss=0.0000 scale=8.0000 norm=193957.3364
[iter 0] loss=1.2764 val_loss=0.0000 scale=1.0000 norm=0.9169
[iter 100] loss=1.0446 val_loss=0.0000 scale=2.0000 norm=1.6359
[iter 200] loss=0.9854 val_loss=0.0000 scale=2.0000 norm=1.6626
[iter 300] loss=0.9468 val_loss=0.0000 scale=2.0000 norm=1.6673
[iter 400] loss=0.9102 val_loss=0.0000 scale=2.0000 norm=1.6689
[iter 0] loss=1.3441 val_loss=0.0000 scale=2.0000 norm=1.9210
[iter 100] loss=1.0079 val_loss=0.0000 scale=2.0000 norm=1.6354
[iter 200] loss=0.8574 val_loss=0.0000 scale=2.0000 norm=1.6250
[iter 300] loss=0.7369 val_loss=0.0000 scale=2.0000 norm=1.6260
[iter 400] loss=0.6113 val_loss=0.0000 scale=4.0000 norm=3.2522
[iter 0] loss=12.3731 val_loss=0.0000 scale=2.0000 norm=26534.1702
[iter 100] loss=10.9091 val_loss=0.0000 scale=4.0000 norm=51059.8643
[iter 200] loss=9.9430 val_loss=0.0000 scale=4.0000 norm=50982.9332
[iter 300] loss=9.3542 val_loss=0.0000 scale=8.0000 norm=101942.6131
[iter 400] loss=8.7134 val_loss=0.0000 scale=8.0000 norm=101940.4665
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9184
[iter 100] loss=1.0806 val_loss=0.0000 scale=2.0000 norm=1.6685
[iter 200] loss=1.0293 val_loss=0.0000 scale=2.0000 norm=1.6974
[iter 300] loss=1.0011 val_loss=0.0000 scale=2.0000 norm=1.7071
[iter 400] loss=0.9714 val_loss=0.0000 scale=2.0000 norm=1.7091
[iter 0] loss=1.3537 val_loss=0.0000 scale=2.0000 norm=1.9311
[iter 100] loss=1.0272 val_loss=0.0000 scale=2.0000 norm=1.6634
[iter 200] loss=0.8954 val_loss=0.0000 scale=2.0000 norm=1.6530
[iter 300] loss=0.8060 val_loss=0.0000 scale=2.0000 norm=1.6588
[iter 400] loss=0.7127 val_loss=0.0000 scale=2.0000 norm=1.6615
[iter 0] loss=13.4602 val_loss=0.0000 scale=2.0000 norm=60915.9477
[iter 100] loss=11.7655 val_loss=0.0000 scale=4.0000 norm=113272.5021
[iter 200] loss=10.4804 val_loss=0.0000 scale=4.0000 norm=113205.7337
[iter 300] loss=9.7712 val_loss=0.0000 scale=4.0000 norm=113194.0102
[iter 400] loss=8.8114 val_loss=0.0000 scale=8.0000 norm=226386.3900
[iter 0] loss=1.2787 val_loss=0.0000 scale=1.0000 norm=0.9183
[iter 100] loss=1.0387 val_loss=0.0000 scale=2.0000 norm=1.6111
[iter 200] loss=0.9869 val_loss=0.0000 scale=2.0000 norm=1.6396
[iter 300] loss=0.9533 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 400] loss=0.9236 val_loss=0.0000 scale=2.0000 norm=1.6470
[iter 0] loss=1.4284 val_loss=0.0000 scale=2.0000 norm=2.0102
[iter 100] loss=1.1100 val_loss=0.0000 scale=2.0000 norm=1.7447
[iter 200] loss=0.9652 val_loss=0.0000 scale=2.0000 norm=1.7429
[iter 300] loss=0.8623 val_loss=0.0000 scale=2.0000 norm=1.7582
[iter 400] loss=0.7494 val_loss=0.0000 scale=2.0000 norm=1.7667
[iter 0] loss=13.4578 val_loss=0.0000 scale=2.0000 norm=59874.0300
[iter 100] loss=11.7681 val_loss=0.0000 scale=4.0000 norm=113741.8720
[iter 200] loss=10.6393 val_loss=0.0000 scale=4.0000 norm=113756.7290
[iter 300] loss=9.9144 val_loss=0.0000 scale=8.0000 norm=227491.5428
[iter 400] loss=8.9024 val_loss=0.0000 scale=8.0000 norm=227489.7736
[iter 0] loss=1.2441 val_loss=0.0000 scale=1.0000 norm=0.9004
[iter 100] loss=1.0093 val_loss=0.0000 scale=2.0000 norm=1.6054
[iter 200] loss=0.9531 val_loss=0.0000 scale=2.0000 norm=1.6402
[iter 300] loss=0.9195 val_loss=0.0000 scale=2.0000 norm=1.6504
[iter 400] loss=0.8842 val_loss=0.0000 scale=2.0000 norm=1.6519
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0550
[iter 100] loss=1.1329 val_loss=0.0000 scale=2.0000 norm=1.7849
[iter 200] loss=0.9799 val_loss=0.0000 scale=2.0000 norm=1.7793
[iter 300] loss=0.8571 val_loss=0.0000 scale=2.0000 norm=1.7819
[iter 400] loss=0.7322 val_loss=0.0000 scale=2.0000 norm=1.7818
[iter 0] loss=13.4604 val_loss=0.0000 scale=2.0000 norm=61696.3465
[iter 100] loss=11.7724 val_loss=0.0000 scale=4.0000 norm=115603.7874
[iter 200] loss=10.6359 val_loss=0.0000 scale=4.0000 norm=115514.6012
[iter 300] loss=9.8974 val_loss=0.0000 scale=8.0000 norm=231007.4732
[iter 400] loss=8.8647 val_loss=0.0000 scale=8.0000 norm=231005.7674
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9184
[iter 100] loss=1.0806 val_loss=0.0000 scale=2.0000 norm=1.6685
[iter 200] loss=1.0293 val_loss=0.0000 scale=2.0000 norm=1.6974
[iter 300] loss=1.0011 val_loss=0.0000 scale=2.0000 norm=1.7071
[iter 400] loss=0.9714 val_loss=0.0000 scale=2.0000 norm=1.7091
[iter 0] loss=1.3537 val_loss=0.0000 scale=2.0000 norm=1.9311
[iter 100] loss=1.0272 val_loss=0.0000 scale=2.0000 norm=1.6634
[iter 200] loss=0.8954 val_loss=0.0000 scale=2.0000 norm=1.6530
[iter 300] loss=0.8060 val_loss=0.0000 scale=2.0000 norm=1.6588
[iter 400] loss=0.7127 val_loss=0.0000 scale=2.0000 norm=1.6615
[iter 0] loss=13.4602 val_loss=0.0000 scale=2.0000 norm=60915.9477
[iter 100] loss=11.7655 val_loss=0.0000 scale=4.0000 norm=113272.5021
[iter 200] loss=10.4804 val_loss=0.0000 scale=4.0000 norm=113205.7337
[iter 300] loss=9.7712 val_loss=0.0000 scale=4.0000 norm=113194.0102
[iter 400] loss=8.8114 val_loss=0.0000 scale=8.0000 norm=226386.3888
[iter 0] loss=1.2787 val_loss=0.0000 scale=1.0000 norm=0.9183
[iter 100] loss=1.0387 val_loss=0.0000 scale=2.0000 norm=1.6111
[iter 200] loss=0.9869 val_loss=0.0000 scale=2.0000 norm=1.6396
[iter 300] loss=0.9533 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 400] loss=0.9236 val_loss=0.0000 scale=2.0000 norm=1.6470
[iter 0] loss=1.4284 val_loss=0.0000 scale=2.0000 norm=2.0102
[iter 100] loss=1.1100 val_loss=0.0000 scale=2.0000 norm=1.7447
[iter 200] loss=0.9652 val_loss=0.0000 scale=2.0000 norm=1.7429
[iter 300] loss=0.8623 val_loss=0.0000 scale=2.0000 norm=1.7582
[iter 400] loss=0.7494 val_loss=0.0000 scale=2.0000 norm=1.7667
[iter 0] loss=13.4578 val_loss=0.0000 scale=2.0000 norm=59874.0300
[iter 100] loss=11.7681 val_loss=0.0000 scale=4.0000 norm=113741.8720
[iter 200] loss=10.6393 val_loss=0.0000 scale=4.0000 norm=113756.7290
[iter 300] loss=9.9144 val_loss=0.0000 scale=8.0000 norm=227491.5428
[iter 400] loss=8.9024 val_loss=0.0000 scale=8.0000 norm=227489.7743
[iter 0] loss=1.3199 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0369 val_loss=0.0000 scale=2.0000 norm=1.6437
[iter 200] loss=0.9494 val_loss=0.0000 scale=2.0000 norm=1.6677
[iter 300] loss=0.8740 val_loss=0.0000 scale=2.0000 norm=1.6755
[iter 400] loss=0.7823 val_loss=0.0000 scale=2.0000 norm=1.6761
[iter 0] loss=1.3652 val_loss=0.0000 scale=2.0000 norm=1.9423
[iter 100] loss=1.0792 val_loss=0.0000 scale=2.0000 norm=1.7078
[iter 200] loss=0.9431 val_loss=0.0000 scale=2.0000 norm=1.6916
[iter 300] loss=0.8457 val_loss=0.0000 scale=2.0000 norm=1.6926
[iter 400] loss=0.7326 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 0] loss=13.4598 val_loss=0.0000 scale=2.0000 norm=60783.1902
[iter 100] loss=11.7236 val_loss=0.0000 scale=4.0000 norm=113414.7119
[iter 200] loss=10.5828 val_loss=0.0000 scale=4.0000 norm=113349.9044
[iter 300] loss=9.9008 val_loss=0.0000 scale=8.0000 norm=226670.7322
[iter 400] loss=8.9327 val_loss=0.0000 scale=8.0000 norm=226668.4259
[iter 0] loss=1.2247 val_loss=0.0000 scale=1.0000 norm=0.8869
[iter 100] loss=0.9850 val_loss=0.0000 scale=2.0000 norm=1.5884
[iter 200] loss=0.9192 val_loss=0.0000 scale=2.0000 norm=1.6129
[iter 300] loss=0.8816 val_loss=0.0000 scale=2.0000 norm=1.6191
[iter 400] loss=0.8580 val_loss=0.0000 scale=2.0000 norm=1.6250
[iter 0] loss=1.3487 val_loss=0.0000 scale=1.0000 norm=0.9639
[iter 100] loss=1.0250 val_loss=0.0000 scale=2.0000 norm=1.6668
[iter 200] loss=0.8868 val_loss=0.0000 scale=2.0000 norm=1.6526
[iter 300] loss=0.7885 val_loss=0.0000 scale=2.0000 norm=1.6540
[iter 400] loss=0.6747 val_loss=0.0000 scale=4.0000 norm=3.3082
[iter 0] loss=11.8665 val_loss=0.0000 scale=2.0000 norm=18875.6366
[iter 100] loss=10.7020 val_loss=0.0000 scale=4.0000 norm=36718.3028
[iter 200] loss=9.8647 val_loss=0.0000 scale=4.0000 norm=36686.9667
[iter 300] loss=9.2277 val_loss=0.0000 scale=8.0000 norm=73365.2466
[iter 400] loss=8.2964 val_loss=0.0000 scale=8.0000 norm=73365.2925
[iter 0] loss=1.3013 val_loss=0.0000 scale=1.0000 norm=0.9306
[iter 100] loss=1.0408 val_loss=0.0000 scale=2.0000 norm=1.6283
[iter 200] loss=0.9789 val_loss=0.0000 scale=2.0000 norm=1.6535
[iter 300] loss=0.9393 val_loss=0.0000 scale=2.0000 norm=1.6651
[iter 400] loss=0.8992 val_loss=0.0000 scale=2.0000 norm=1.6666
[iter 0] loss=1.3872 val_loss=0.0000 scale=2.0000 norm=1.9665
[iter 100] loss=1.0493 val_loss=0.0000 scale=2.0000 norm=1.6875
[iter 200] loss=0.8973 val_loss=0.0000 scale=2.0000 norm=1.6719
[iter 300] loss=0.7913 val_loss=0.0000 scale=4.0000 norm=3.3431
[iter 400] loss=0.6705 val_loss=0.0000 scale=2.0000 norm=1.6766
[iter 0] loss=13.4574 val_loss=0.0000 scale=2.0000 norm=58338.7171
[iter 100] loss=11.7115 val_loss=0.0000 scale=4.0000 norm=111228.3143
[iter 200] loss=10.5336 val_loss=0.0000 scale=4.0000 norm=111216.7050
[iter 300] loss=9.7895 val_loss=0.0000 scale=8.0000 norm=222415.1416
[iter 400] loss=8.7894 val_loss=0.0000 scale=8.0000 norm=222414.4087
[iter 0] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=0.9089
[iter 100] loss=1.0088 val_loss=0.0000 scale=2.0000 norm=1.5960
[iter 200] loss=0.9144 val_loss=0.0000 scale=2.0000 norm=1.6158
[iter 300] loss=0.8368 val_loss=0.0000 scale=4.0000 norm=3.2463
[iter 400] loss=0.7502 val_loss=0.0000 scale=2.0000 norm=1.6251
[iter 0] loss=1.3664 val_loss=0.0000 scale=2.0000 norm=1.9434
[iter 100] loss=1.0314 val_loss=0.0000 scale=2.0000 norm=1.6698
[iter 200] loss=0.8786 val_loss=0.0000 scale=2.0000 norm=1.6630
[iter 300] loss=0.7650 val_loss=0.0000 scale=4.0000 norm=3.3494
[iter 400] loss=0.6117 val_loss=0.0000 scale=4.0000 norm=3.3548
[iter 0] loss=13.4224 val_loss=0.0000 scale=2.0000 norm=53011.7863
[iter 100] loss=11.7129 val_loss=0.0000 scale=4.0000 norm=98897.0640
[iter 200] loss=10.6141 val_loss=0.0000 scale=4.0000 norm=98857.9649
[iter 300] loss=9.8782 val_loss=0.0000 scale=8.0000 norm=197698.9826
[iter 400] loss=8.8748 val_loss=0.0000 scale=8.0000 norm=197697.9594
[iter 0] loss=1.2821 val_loss=0.0000 scale=1.0000 norm=0.9197
[iter 100] loss=1.0107 val_loss=0.0000 scale=2.0000 norm=1.5829
[iter 200] loss=0.9539 val_loss=0.0000 scale=2.0000 norm=1.6137
[iter 300] loss=0.9258 val_loss=0.0000 scale=2.0000 norm=1.6258
[iter 400] loss=0.8987 val_loss=0.0000 scale=2.0000 norm=1.6268
[iter 0] loss=1.3859 val_loss=0.0000 scale=1.0000 norm=0.9823
[iter 100] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.6291
[iter 200] loss=0.8320 val_loss=0.0000 scale=2.0000 norm=1.6159
[iter 300] loss=0.7114 val_loss=0.0000 scale=2.0000 norm=1.6243
[iter 400] loss=0.5952 val_loss=0.0000 scale=2.0000 norm=1.6292
[iter 0] loss=11.2645 val_loss=0.0000 scale=2.0000 norm=13721.0140
[iter 100] loss=10.4779 val_loss=0.0000 scale=2.0000 norm=12820.0370
[iter 200] loss=9.6886 val_loss=0.0000 scale=4.0000 norm=25358.6890
[iter 300] loss=9.1486 val_loss=0.0000 scale=4.0000 norm=25344.5320
[iter 400] loss=8.5618 val_loss=0.0000 scale=4.0000 norm=25343.8647
[iter 0] loss=1.2729 val_loss=0.0000 scale=1.0000 norm=0.9145
[iter 100] loss=1.0173 val_loss=0.0000 scale=2.0000 norm=1.6181
[iter 200] loss=0.9630 val_loss=0.0000 scale=2.0000 norm=1.6483
[iter 300] loss=0.9367 val_loss=0.0000 scale=2.0000 norm=1.6632
[iter 400] loss=0.9118 val_loss=0.0000 scale=2.0000 norm=1.6655
[iter 0] loss=1.3675 val_loss=0.0000 scale=2.0000 norm=1.9472
[iter 100] loss=1.0375 val_loss=0.0000 scale=2.0000 norm=1.6795
[iter 200] loss=0.8819 val_loss=0.0000 scale=2.0000 norm=1.6687
[iter 300] loss=0.7668 val_loss=0.0000 scale=2.0000 norm=1.6746
[iter 400] loss=0.6415 val_loss=0.0000 scale=2.0000 norm=1.6787
[iter 0] loss=12.2464 val_loss=0.0000 scale=2.0000 norm=21739.0699
[iter 100] loss=10.7512 val_loss=0.0000 scale=4.0000 norm=42316.0614
[iter 200] loss=9.8512 val_loss=0.0000 scale=4.0000 norm=42271.6360
[iter 300] loss=9.2679 val_loss=0.0000 scale=8.0000 norm=84531.7308
[iter 400] loss=8.5792 val_loss=0.0000 scale=4.0000 norm=42265.4785
[iter 0] loss=1.2519 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0116 val_loss=0.0000 scale=2.0000 norm=1.5855
[iter 200] loss=0.9555 val_loss=0.0000 scale=2.0000 norm=1.6143
[iter 300] loss=0.9197 val_loss=0.0000 scale=2.0000 norm=1.6243
[iter 400] loss=0.8842 val_loss=0.0000 scale=2.0000 norm=1.6258
[iter 0] loss=1.3513 val_loss=0.0000 scale=2.0000 norm=1.9289
[iter 100] loss=1.0553 val_loss=0.0000 scale=2.0000 norm=1.6897
[iter 200] loss=0.9193 val_loss=0.0000 scale=2.0000 norm=1.6881
[iter 300] loss=0.8247 val_loss=0.0000 scale=2.0000 norm=1.6929
[iter 400] loss=0.7072 val_loss=0.0000 scale=2.0000 norm=1.7001
[iter 0] loss=12.2411 val_loss=0.0000 scale=2.0000 norm=20949.1319
[iter 100] loss=10.7512 val_loss=0.0000 scale=4.0000 norm=40325.2607
[iter 200] loss=9.8088 val_loss=0.0000 scale=4.0000 norm=40286.7647
[iter 300] loss=9.0279 val_loss=0.0000 scale=8.0000 norm=80563.6903
[iter 400] loss=7.9565 val_loss=0.0000 scale=8.0000 norm=80563.7229
[iter 0] loss=1.2520 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0181 val_loss=0.0000 scale=2.0000 norm=1.5975
[iter 200] loss=0.9687 val_loss=0.0000 scale=2.0000 norm=1.6183
[iter 300] loss=0.9408 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 400] loss=0.9160 val_loss=0.0000 scale=2.0000 norm=1.6300
[iter 0] loss=1.3875 val_loss=0.0000 scale=2.0000 norm=1.9660
[iter 100] loss=1.1015 val_loss=0.0000 scale=2.0000 norm=1.7399
[iter 200] loss=0.9645 val_loss=0.0000 scale=2.0000 norm=1.7311
[iter 300] loss=0.8642 val_loss=0.0000 scale=4.0000 norm=3.4825
[iter 400] loss=0.7063 val_loss=0.0000 scale=4.0000 norm=3.4921
[iter 0] loss=13.4585 val_loss=0.0000 scale=2.0000 norm=59699.4861
[iter 100] loss=11.7243 val_loss=0.0000 scale=4.0000 norm=112385.4245
[iter 200] loss=10.6004 val_loss=0.0000 scale=4.0000 norm=112322.2256
[iter 300] loss=9.9414 val_loss=0.0000 scale=8.0000 norm=224611.4818
[iter 400] loss=9.0503 val_loss=0.0000 scale=4.0000 norm=112304.3215
[iter 0] loss=1.2604 val_loss=0.0000 scale=1.0000 norm=0.9080
[iter 100] loss=1.0411 val_loss=0.0000 scale=2.0000 norm=1.6226
[iter 200] loss=0.9859 val_loss=0.0000 scale=2.0000 norm=1.6476
[iter 300] loss=0.9467 val_loss=0.0000 scale=2.0000 norm=1.6547
[iter 400] loss=0.8960 val_loss=0.0000 scale=4.0000 norm=3.3116
[iter 0] loss=1.4348 val_loss=0.0000 scale=1.0000 norm=1.0085
[iter 100] loss=1.0593 val_loss=0.0000 scale=2.0000 norm=1.6821
[iter 200] loss=0.9211 val_loss=0.0000 scale=2.0000 norm=1.6809
[iter 300] loss=0.8254 val_loss=0.0000 scale=2.0000 norm=1.6884
[iter 400] loss=0.7175 val_loss=0.0000 scale=2.0000 norm=1.6904
[iter 0] loss=13.4220 val_loss=0.0000 scale=2.0000 norm=51255.8325
[iter 100] loss=11.6535 val_loss=0.0000 scale=4.0000 norm=97020.8700
[iter 200] loss=10.4984 val_loss=0.0000 scale=4.0000 norm=96985.9387
[iter 300] loss=9.8219 val_loss=0.0000 scale=4.0000 norm=96979.1058
[iter 400] loss=8.8404 val_loss=0.0000 scale=8.0000 norm=193957.3349
[iter 0] loss=1.3202 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0556 val_loss=0.0000 scale=2.0000 norm=1.6373
[iter 200] loss=0.9917 val_loss=0.0000 scale=2.0000 norm=1.6667
[iter 300] loss=0.9507 val_loss=0.0000 scale=2.0000 norm=1.6758
[iter 400] loss=0.9131 val_loss=0.0000 scale=2.0000 norm=1.6799
[iter 0] loss=1.3569 val_loss=0.0000 scale=1.0000 norm=0.9670
[iter 100] loss=1.0278 val_loss=0.0000 scale=2.0000 norm=1.6611
[iter 200] loss=0.8824 val_loss=0.0000 scale=2.0000 norm=1.6499
[iter 300] loss=0.7887 val_loss=0.0000 scale=4.0000 norm=3.3305
[iter 400] loss=0.6945 val_loss=0.0000 scale=2.0000 norm=1.6743
[iter 0] loss=13.4601 val_loss=0.0000 scale=2.0000 norm=61140.5327
[iter 100] loss=11.7289 val_loss=0.0000 scale=4.0000 norm=113090.6362
[iter 200] loss=10.5693 val_loss=0.0000 scale=4.0000 norm=113060.9336
[iter 300] loss=9.8282 val_loss=0.0000 scale=8.0000 norm=226105.3560
[iter 400] loss=8.8234 val_loss=0.0000 scale=8.0000 norm=226105.0677
[iter 0] loss=1.3217 val_loss=0.0000 scale=1.0000 norm=0.9423
[iter 100] loss=1.0595 val_loss=0.0000 scale=2.0000 norm=1.6514
[iter 200] loss=0.9534 val_loss=0.0000 scale=2.0000 norm=1.6689
[iter 300] loss=0.8495 val_loss=0.0000 scale=2.0000 norm=1.6728
[iter 400] loss=0.7300 val_loss=0.0000 scale=2.0000 norm=1.6744
[iter 0] loss=1.3780 val_loss=0.0000 scale=2.0000 norm=1.9562
[iter 100] loss=1.0666 val_loss=0.0000 scale=2.0000 norm=1.6932
[iter 200] loss=0.9258 val_loss=0.0000 scale=2.0000 norm=1.6810
[iter 300] loss=0.8001 val_loss=0.0000 scale=4.0000 norm=3.3670
[iter 400] loss=0.6292 val_loss=0.0000 scale=4.0000 norm=3.3717
[iter 0] loss=13.4456 val_loss=0.0000 scale=2.0000 norm=54036.4493
[iter 100] loss=11.6828 val_loss=0.0000 scale=4.0000 norm=102992.3038
[iter 200] loss=10.5676 val_loss=0.0000 scale=4.0000 norm=102995.8536
[iter 300] loss=9.8393 val_loss=0.0000 scale=8.0000 norm=205979.3751
[iter 400] loss=8.8864 val_loss=0.0000 scale=8.0000 norm=205978.9431
[iter 0] loss=1.2773 val_loss=0.0000 scale=1.0000 norm=0.9179
[iter 100] loss=1.0564 val_loss=0.0000 scale=2.0000 norm=1.6369
[iter 200] loss=1.0084 val_loss=0.0000 scale=2.0000 norm=1.6634
[iter 300] loss=0.9790 val_loss=0.0000 scale=2.0000 norm=1.6727
[iter 400] loss=0.9533 val_loss=0.0000 scale=2.0000 norm=1.6760
[iter 0] loss=1.3749 val_loss=0.0000 scale=2.0000 norm=1.9544
[iter 100] loss=1.0587 val_loss=0.0000 scale=2.0000 norm=1.6875
[iter 200] loss=0.9353 val_loss=0.0000 scale=2.0000 norm=1.6775
[iter 300] loss=0.8336 val_loss=0.0000 scale=4.0000 norm=3.3657
[iter 400] loss=0.7112 val_loss=0.0000 scale=4.0000 norm=3.3615
[iter 0] loss=13.4605 val_loss=0.0000 scale=2.0000 norm=61975.1128
[iter 100] loss=11.7725 val_loss=0.0000 scale=4.0000 norm=114478.8086
[iter 200] loss=10.5300 val_loss=0.0000 scale=4.0000 norm=114414.2165
[iter 300] loss=9.8653 val_loss=0.0000 scale=4.0000 norm=114403.9085
[iter 400] loss=9.1986 val_loss=0.0000 scale=4.0000 norm=114402.6526
[iter 0] loss=1.3236 val_loss=0.0000 scale=1.0000 norm=0.9435
[iter 100] loss=1.0934 val_loss=0.0000 scale=2.0000 norm=1.6802
[iter 200] loss=1.0381 val_loss=0.0000 scale=2.0000 norm=1.7047
[iter 300] loss=1.0037 val_loss=0.0000 scale=2.0000 norm=1.7143
[iter 400] loss=0.9640 val_loss=0.0000 scale=4.0000 norm=3.4323
[iter 0] loss=1.4085 val_loss=0.0000 scale=1.0000 norm=0.9945
[iter 100] loss=1.1023 val_loss=0.0000 scale=2.0000 norm=1.7298
[iter 200] loss=0.9756 val_loss=0.0000 scale=2.0000 norm=1.7189
[iter 300] loss=0.8941 val_loss=0.0000 scale=2.0000 norm=1.7248
[iter 400] loss=0.8175 val_loss=0.0000 scale=2.0000 norm=1.7294
[iter 0] loss=13.4454 val_loss=0.0000 scale=2.0000 norm=55593.6653
[iter 100] loss=11.6965 val_loss=0.0000 scale=4.0000 norm=104849.5712
[iter 200] loss=10.5749 val_loss=0.0000 scale=4.0000 norm=104835.9966
[iter 300] loss=9.8905 val_loss=0.0000 scale=8.0000 norm=209661.2628
[iter 400] loss=8.9848 val_loss=0.0000 scale=8.0000 norm=209660.7563
[iter 0] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=0.9430
[iter 100] loss=1.0494 val_loss=0.0000 scale=2.0000 norm=1.6522
[iter 200] loss=0.9342 val_loss=0.0000 scale=2.0000 norm=1.6652
[iter 300] loss=0.8300 val_loss=0.0000 scale=2.0000 norm=1.6740
[iter 400] loss=0.7235 val_loss=0.0000 scale=4.0000 norm=3.3510
[iter 0] loss=1.3545 val_loss=0.0000 scale=1.0000 norm=0.9669
[iter 100] loss=1.0568 val_loss=0.0000 scale=2.0000 norm=1.6725
[iter 200] loss=0.9231 val_loss=0.0000 scale=2.0000 norm=1.6627
[iter 300] loss=0.8326 val_loss=0.0000 scale=2.0000 norm=1.6661
[iter 400] loss=0.7465 val_loss=0.0000 scale=2.0000 norm=1.6673
[iter 0] loss=13.4579 val_loss=0.0000 scale=2.0000 norm=59724.5886
[iter 100] loss=11.7205 val_loss=0.0000 scale=4.0000 norm=112329.1604
[iter 200] loss=10.5650 val_loss=0.0000 scale=4.0000 norm=112278.7378
[iter 300] loss=9.8653 val_loss=0.0000 scale=8.0000 norm=224532.1230
[iter 400] loss=8.9008 val_loss=0.0000 scale=8.0000 norm=224530.6715
[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=0.9091
[iter 100] loss=1.0224 val_loss=0.0000 scale=2.0000 norm=1.6074
[iter 200] loss=0.9596 val_loss=0.0000 scale=2.0000 norm=1.6402
[iter 300] loss=0.9181 val_loss=0.0000 scale=2.0000 norm=1.6505
[iter 400] loss=0.8795 val_loss=0.0000 scale=2.0000 norm=1.6531
[iter 0] loss=1.3826 val_loss=0.0000 scale=1.0000 norm=0.9804
[iter 100] loss=1.0818 val_loss=0.0000 scale=2.0000 norm=1.7116
[iter 200] loss=0.9502 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 300] loss=0.8568 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 400] loss=0.7723 val_loss=0.0000 scale=2.0000 norm=1.6998
[iter 0] loss=13.4569 val_loss=0.0000 scale=2.0000 norm=57516.1521
[iter 100] loss=11.6840 val_loss=0.0000 scale=4.0000 norm=109641.2795
[iter 200] loss=10.5258 val_loss=0.0000 scale=4.0000 norm=109624.5934
[iter 300] loss=9.8538 val_loss=0.0000 scale=4.0000 norm=109610.9948
[iter 400] loss=8.9954 val_loss=0.0000 scale=8.0000 norm=219218.2363
[iter 0] loss=1.2880 val_loss=0.0000 scale=1.0000 norm=0.9224
[iter 100] loss=1.0308 val_loss=0.0000 scale=2.0000 norm=1.6242
[iter 200] loss=0.9644 val_loss=0.0000 scale=2.0000 norm=1.6502
[iter 300] loss=0.9237 val_loss=0.0000 scale=4.0000 norm=3.3221
[iter 400] loss=0.8834 val_loss=0.0000 scale=2.0000 norm=1.6658
[iter 0] loss=1.3802 val_loss=0.0000 scale=2.0000 norm=1.9592
[iter 100] loss=1.0403 val_loss=0.0000 scale=2.0000 norm=1.6620
[iter 200] loss=0.9015 val_loss=0.0000 scale=2.0000 norm=1.6522
[iter 300] loss=0.8031 val_loss=0.0000 scale=2.0000 norm=1.6539
[iter 400] loss=0.6923 val_loss=0.0000 scale=4.0000 norm=3.3130
[iter 0] loss=13.4221 val_loss=0.0000 scale=2.0000 norm=52155.6181
[iter 100] loss=11.6563 val_loss=0.0000 scale=4.0000 norm=97740.7427
[iter 200] loss=10.5101 val_loss=0.0000 scale=4.0000 norm=97662.0298
[iter 300] loss=9.8244 val_loss=0.0000 scale=8.0000 norm=195297.5148
[iter 400] loss=9.0754 val_loss=0.0000 scale=4.0000 norm=97648.0060
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9178
[iter 100] loss=1.0396 val_loss=0.0000 scale=2.0000 norm=1.6143
[iter 200] loss=0.9904 val_loss=0.0000 scale=2.0000 norm=1.6464
[iter 300] loss=0.9610 val_loss=0.0000 scale=4.0000 norm=3.3158
[iter 400] loss=0.9295 val_loss=0.0000 scale=2.0000 norm=1.6597
[iter 0] loss=1.3920 val_loss=0.0000 scale=1.0000 norm=0.9856
[iter 100] loss=1.0409 val_loss=0.0000 scale=2.0000 norm=1.6860
[iter 200] loss=0.8918 val_loss=0.0000 scale=2.0000 norm=1.6817
[iter 300] loss=0.7876 val_loss=0.0000 scale=4.0000 norm=3.3962
[iter 400] loss=0.6758 val_loss=0.0000 scale=2.0000 norm=1.7026
[iter 0] loss=12.2351 val_loss=0.0000 scale=2.0000 norm=20907.5541
[iter 100] loss=10.6948 val_loss=0.0000 scale=4.0000 norm=40079.8703
[iter 200] loss=9.7464 val_loss=0.0000 scale=4.0000 norm=40013.4770
[iter 300] loss=8.9157 val_loss=0.0000 scale=8.0000 norm=80015.6345
[iter 400] loss=7.8649 val_loss=0.0000 scale=8.0000 norm=80015.6721
[iter 0] loss=1.3199 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0369 val_loss=0.0000 scale=2.0000 norm=1.6437
[iter 200] loss=0.9494 val_loss=0.0000 scale=2.0000 norm=1.6677
[iter 300] loss=0.8740 val_loss=0.0000 scale=2.0000 norm=1.6755
[iter 400] loss=0.7823 val_loss=0.0000 scale=2.0000 norm=1.6761
[iter 0] loss=1.3652 val_loss=0.0000 scale=2.0000 norm=1.9423
[iter 100] loss=1.0792 val_loss=0.0000 scale=2.0000 norm=1.7078
[iter 200] loss=0.9431 val_loss=0.0000 scale=2.0000 norm=1.6916
[iter 300] loss=0.8457 val_loss=0.0000 scale=2.0000 norm=1.6926
[iter 400] loss=0.7326 val_loss=0.0000 scale=2.0000 norm=1.6967
[iter 0] loss=13.4598 val_loss=0.0000 scale=2.0000 norm=60783.1902
[iter 100] loss=11.7236 val_loss=0.0000 scale=4.0000 norm=113414.7119
[iter 200] loss=10.5828 val_loss=0.0000 scale=4.0000 norm=113349.9044
[iter 300] loss=9.9008 val_loss=0.0000 scale=8.0000 norm=226670.7322
[iter 400] loss=8.9328 val_loss=0.0000 scale=8.0000 norm=226668.4278
[iter 0] loss=1.2996 val_loss=0.0000 scale=1.0000 norm=0.9298
[iter 100] loss=0.9690 val_loss=0.0000 scale=2.0000 norm=1.5421
[iter 200] loss=0.9100 val_loss=0.0000 scale=2.0000 norm=1.5816
[iter 300] loss=0.8770 val_loss=0.0000 scale=4.0000 norm=3.1907
[iter 400] loss=0.8446 val_loss=0.0000 scale=2.0000 norm=1.5972
[iter 0] loss=1.4823 val_loss=0.0000 scale=2.0000 norm=2.0697
[iter 100] loss=1.1600 val_loss=0.0000 scale=2.0000 norm=1.7919
[iter 200] loss=1.0213 val_loss=0.0000 scale=2.0000 norm=1.7795
[iter 300] loss=0.9222 val_loss=0.0000 scale=4.0000 norm=3.5578
[iter 400] loss=0.8067 val_loss=0.0000 scale=2.0000 norm=1.7869
[iter 0] loss=13.4584 val_loss=0.0000 scale=2.0000 norm=59369.0193
[iter 100] loss=11.7086 val_loss=0.0000 scale=4.0000 norm=110226.3378
[iter 200] loss=10.4856 val_loss=0.0000 scale=4.0000 norm=110172.8392
[iter 300] loss=9.8092 val_loss=0.0000 scale=4.0000 norm=110160.1572
[iter 400] loss=8.8584 val_loss=0.0000 scale=8.0000 norm=220318.8162
[iter 0] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=0.9370
[iter 100] loss=1.0109 val_loss=0.0000 scale=2.0000 norm=1.6028
[iter 200] loss=0.9037 val_loss=0.0000 scale=2.0000 norm=1.6227
[iter 300] loss=0.8110 val_loss=0.0000 scale=4.0000 norm=3.2663
[iter 400] loss=0.6700 val_loss=0.0000 scale=2.0000 norm=1.6337
[iter 0] loss=1.3571 val_loss=0.0000 scale=2.0000 norm=1.9347
[iter 100] loss=1.0754 val_loss=0.0000 scale=2.0000 norm=1.7054
[iter 200] loss=0.9539 val_loss=0.0000 scale=2.0000 norm=1.7036
[iter 300] loss=0.8710 val_loss=0.0000 scale=2.0000 norm=1.7112
[iter 400] loss=0.7850 val_loss=0.0000 scale=4.0000 norm=3.4321
[iter 0] loss=13.4219 val_loss=0.0000 scale=2.0000 norm=51251.8923
[iter 100] loss=11.6842 val_loss=0.0000 scale=4.0000 norm=96967.8037
[iter 200] loss=10.5397 val_loss=0.0000 scale=4.0000 norm=96949.9967
[iter 300] loss=9.7665 val_loss=0.0000 scale=8.0000 norm=193881.4513
[iter 400] loss=8.7269 val_loss=0.0000 scale=8.0000 norm=193880.5714
[iter 0] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=0.9402
[iter 100] loss=1.0715 val_loss=0.0000 scale=2.0000 norm=1.6594
[iter 200] loss=1.0093 val_loss=0.0000 scale=2.0000 norm=1.6909
[iter 300] loss=0.9698 val_loss=0.0000 scale=2.0000 norm=1.6995
[iter 400] loss=0.9325 val_loss=0.0000 scale=2.0000 norm=1.7009
[iter 0] loss=1.3368 val_loss=0.0000 scale=1.0000 norm=0.9551
[iter 100] loss=1.0019 val_loss=0.0000 scale=2.0000 norm=1.6083
[iter 200] loss=0.8499 val_loss=0.0000 scale=2.0000 norm=1.6034
[iter 300] loss=0.7418 val_loss=0.0000 scale=2.0000 norm=1.6150
[iter 400] loss=0.6343 val_loss=0.0000 scale=4.0000 norm=3.2371
[iter 0] loss=12.2579 val_loss=0.0000 scale=2.0000 norm=23878.1143
[iter 100] loss=10.8842 val_loss=0.0000 scale=4.0000 norm=44515.0419
[iter 200] loss=10.0059 val_loss=0.0000 scale=4.0000 norm=44409.7631
[iter 300] loss=9.3417 val_loss=0.0000 scale=8.0000 norm=88802.8082
[iter 400] loss=8.4907 val_loss=0.0000 scale=4.0000 norm=44401.1791
[iter 0] loss=1.2359 val_loss=0.0000 scale=1.0000 norm=0.8946
[iter 100] loss=1.0101 val_loss=0.0000 scale=2.0000 norm=1.6050
[iter 200] loss=0.9550 val_loss=0.0000 scale=2.0000 norm=1.6328
[iter 300] loss=0.9188 val_loss=0.0000 scale=2.0000 norm=1.6440
[iter 400] loss=0.8926 val_loss=0.0000 scale=2.0000 norm=1.6492
[iter 0] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=0.9463
[iter 100] loss=1.0072 val_loss=0.0000 scale=2.0000 norm=1.6366
[iter 200] loss=0.8666 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 300] loss=0.7639 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 400] loss=0.6675 val_loss=0.0000 scale=2.0000 norm=1.6326
[iter 0] loss=13.4591 val_loss=0.0000 scale=2.0000 norm=58691.7659
[iter 100] loss=11.7151 val_loss=0.0000 scale=4.0000 norm=112893.1977
[iter 200] loss=10.5515 val_loss=0.0000 scale=4.0000 norm=112891.5036
[iter 300] loss=9.8055 val_loss=0.0000 scale=8.0000 norm=225763.7672
[iter 400] loss=8.7666 val_loss=0.0000 scale=8.0000 norm=225762.6207
[iter 0] loss=1.2623 val_loss=0.0000 scale=1.0000 norm=0.9083
[iter 100] loss=1.0263 val_loss=0.0000 scale=2.0000 norm=1.6178
[iter 200] loss=0.9768 val_loss=0.0000 scale=2.0000 norm=1.6460
[iter 300] loss=0.9478 val_loss=0.0000 scale=2.0000 norm=1.6542
[iter 400] loss=0.9188 val_loss=0.0000 scale=2.0000 norm=1.6553
[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=0.9900
[iter 100] loss=1.0734 val_loss=0.0000 scale=2.0000 norm=1.7074
[iter 200] loss=0.9194 val_loss=0.0000 scale=2.0000 norm=1.6891
[iter 300] loss=0.8118 val_loss=0.0000 scale=4.0000 norm=3.3848
[iter 400] loss=0.6710 val_loss=0.0000 scale=4.0000 norm=3.3911
[iter 0] loss=13.4584 val_loss=0.0000 scale=2.0000 norm=61070.1560
[iter 100] loss=11.7691 val_loss=0.0000 scale=4.0000 norm=114129.3807
[iter 200] loss=10.6610 val_loss=0.0000 scale=4.0000 norm=114139.6397
[iter 300] loss=9.9815 val_loss=0.0000 scale=8.0000 norm=228267.6453
[iter 400] loss=9.0794 val_loss=0.0000 scale=8.0000 norm=228266.6408
[iter 0] loss=1.2162 val_loss=0.0000 scale=1.0000 norm=0.8824
[iter 100] loss=0.9820 val_loss=0.0000 scale=2.0000 norm=1.5616
[iter 200] loss=0.9276 val_loss=0.0000 scale=2.0000 norm=1.5885
[iter 300] loss=0.8978 val_loss=0.0000 scale=2.0000 norm=1.6000
[iter 400] loss=0.8701 val_loss=0.0000 scale=2.0000 norm=1.6015
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0549
[iter 100] loss=1.1467 val_loss=0.0000 scale=2.0000 norm=1.7823
[iter 200] loss=1.0054 val_loss=0.0000 scale=2.0000 norm=1.7739
[iter 300] loss=0.8971 val_loss=0.0000 scale=2.0000 norm=1.7834
[iter 400] loss=0.7938 val_loss=0.0000 scale=2.0000 norm=1.7885
[iter 0] loss=13.4188 val_loss=0.0000 scale=2.0000 norm=48410.1325
[iter 100] loss=11.6399 val_loss=0.0000 scale=4.0000 norm=93887.3163
[iter 200] loss=10.5391 val_loss=0.0000 scale=4.0000 norm=93860.8105
[iter 300] loss=9.8249 val_loss=0.0000 scale=8.0000 norm=187704.0424
[iter 400] loss=8.7728 val_loss=0.0000 scale=8.0000 norm=187702.6412
[iter 0] loss=1.2764 val_loss=0.0000 scale=1.0000 norm=0.9169
[iter 100] loss=1.0446 val_loss=0.0000 scale=2.0000 norm=1.6359
[iter 200] loss=0.9851 val_loss=0.0000 scale=2.0000 norm=1.6627
[iter 300] loss=0.9464 val_loss=0.0000 scale=2.0000 norm=1.6675
[iter 400] loss=0.9080 val_loss=0.0000 scale=2.0000 norm=1.6691
[iter 0] loss=1.3441 val_loss=0.0000 scale=2.0000 norm=1.9210
[iter 100] loss=1.0079 val_loss=0.0000 scale=2.0000 norm=1.6354
[iter 200] loss=0.8574 val_loss=0.0000 scale=2.0000 norm=1.6250
[iter 300] loss=0.7369 val_loss=0.0000 scale=2.0000 norm=1.6260
[iter 400] loss=0.6113 val_loss=0.0000 scale=4.0000 norm=3.2522
[iter 0] loss=12.3731 val_loss=0.0000 scale=2.0000 norm=26534.1702
[iter 100] loss=10.9091 val_loss=0.0000 scale=4.0000 norm=51059.8643
[iter 200] loss=9.9430 val_loss=0.0000 scale=4.0000 norm=50982.9332
[iter 300] loss=9.3542 val_loss=0.0000 scale=8.0000 norm=101942.6131
[iter 400] loss=8.7134 val_loss=0.0000 scale=8.0000 norm=101940.4665

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n11>
Subject: Job 315452: <mordred_NGB_Robust Scaler_multimodal Rh_20250123> in cluster <Hazel> Done

Job <mordred_NGB_Robust Scaler_multimodal Rh_20250123> was submitted from host <c037n01> by user <sdehgha2> in cluster <Hazel> at Thu Jan 23 14:12:10 2025
Job was executed on host(s) <6*c202n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Jan 23 14:12:11 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Jan 23 14:12:11 2025
Terminated at Thu Jan 23 14:23:40 2025
Results reported at Thu Jan 23 14:23:40 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 6
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Robust Scaler_multimodal Rh_20250123"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250123.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250123.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3201.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.28 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               13.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   694 sec.
    Turnaround time :                            690 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250123.err> for stderr output of this job.

Average scores:	 r2: [-0.004  0.052 -2.663]Â±[0.176 0.098 4.   ]
[array([-0.00445377,  0.05168983, -2.66292619]), array([8.67888576e+00, 1.53832317e+02, 1.02200969e+05]), array([6.41943440e+00, 1.01071586e+02, 2.47120756e+04])]
RRU Dimer
Filename: (Mordred)_NGB_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh_without_log/RRU Dimer/(Mordred)_NGB_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh_without_log/RRU Dimer/(Mordred)_NGB_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh_without_log/RRU Dimer/(Mordred)_NGB_Robust Scaler_shape.json
Done Saving scores!
[iter 0] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=0.9370
[iter 100] loss=1.0172 val_loss=0.0000 scale=2.0000 norm=1.6018
[iter 200] loss=0.9099 val_loss=0.0000 scale=2.0000 norm=1.6171
[iter 300] loss=0.8243 val_loss=0.0000 scale=2.0000 norm=1.6290
[iter 400] loss=0.7172 val_loss=0.0000 scale=4.0000 norm=3.2628
[iter 0] loss=1.3571 val_loss=0.0000 scale=2.0000 norm=1.9347
[iter 100] loss=1.0670 val_loss=0.0000 scale=2.0000 norm=1.7047
[iter 200] loss=0.9402 val_loss=0.0000 scale=2.0000 norm=1.7046
[iter 300] loss=0.8317 val_loss=0.0000 scale=4.0000 norm=3.4253
[iter 400] loss=0.6765 val_loss=0.0000 scale=4.0000 norm=3.4313
[iter 0] loss=13.4219 val_loss=0.0000 scale=2.0000 norm=51251.8923
[iter 100] loss=11.6842 val_loss=0.0000 scale=4.0000 norm=96961.4752
[iter 200] loss=10.5388 val_loss=0.0000 scale=4.0000 norm=96946.6401
[iter 300] loss=9.7424 val_loss=0.0000 scale=8.0000 norm=193880.8630
[iter 400] loss=8.7036 val_loss=0.0000 scale=8.0000 norm=193880.5673
[iter 0] loss=1.3472 val_loss=0.0000 scale=1.0000 norm=0.9569
[iter 100] loss=1.0987 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 200] loss=1.0385 val_loss=0.0000 scale=2.0000 norm=1.7009
[iter 300] loss=1.0059 val_loss=0.0000 scale=2.0000 norm=1.7106
[iter 400] loss=0.9770 val_loss=0.0000 scale=2.0000 norm=1.7130
[iter 0] loss=1.3050 val_loss=0.0000 scale=2.0000 norm=1.8775
[iter 100] loss=0.9943 val_loss=0.0000 scale=2.0000 norm=1.6221
[iter 200] loss=0.8523 val_loss=0.0000 scale=2.0000 norm=1.6199
[iter 300] loss=0.7074 val_loss=0.0000 scale=4.0000 norm=3.2616
[iter 400] loss=0.5351 val_loss=0.0000 scale=4.0000 norm=3.2642
[iter 0] loss=13.4587 val_loss=0.0000 scale=2.0000 norm=61239.9286
[iter 100] loss=11.7381 val_loss=0.0000 scale=4.0000 norm=113489.4378
[iter 200] loss=10.5923 val_loss=0.0000 scale=4.0000 norm=113474.5008
[iter 300] loss=9.8829 val_loss=0.0000 scale=8.0000 norm=226927.1256
[iter 400] loss=8.9303 val_loss=0.0000 scale=8.0000 norm=226926.5014
[iter 0] loss=1.3202 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0501 val_loss=0.0000 scale=2.0000 norm=1.6394
[iter 200] loss=0.9849 val_loss=0.0000 scale=2.0000 norm=1.6647
[iter 300] loss=0.9459 val_loss=0.0000 scale=2.0000 norm=1.6747
[iter 400] loss=0.9040 val_loss=0.0000 scale=2.0000 norm=1.6768
[iter 0] loss=1.3569 val_loss=0.0000 scale=1.0000 norm=0.9670
[iter 100] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6622
[iter 200] loss=0.8712 val_loss=0.0000 scale=2.0000 norm=1.6543
[iter 300] loss=0.7389 val_loss=0.0000 scale=4.0000 norm=3.3446
[iter 400] loss=0.5721 val_loss=0.0000 scale=4.0000 norm=3.3489
[iter 0] loss=13.4601 val_loss=0.0000 scale=2.0000 norm=61140.5327
[iter 100] loss=11.7288 val_loss=0.0000 scale=4.0000 norm=113075.5865
[iter 200] loss=10.5670 val_loss=0.0000 scale=4.0000 norm=113059.7739
[iter 300] loss=9.8163 val_loss=0.0000 scale=8.0000 norm=226105.2622
[iter 400] loss=8.8132 val_loss=0.0000 scale=8.0000 norm=226105.0646
[iter 0] loss=1.2534 val_loss=0.0000 scale=1.0000 norm=0.9046
[iter 100] loss=1.0102 val_loss=0.0000 scale=2.0000 norm=1.5969
[iter 200] loss=0.9527 val_loss=0.0000 scale=2.0000 norm=1.6279
[iter 300] loss=0.9144 val_loss=0.0000 scale=2.0000 norm=1.6371
[iter 400] loss=0.8693 val_loss=0.0000 scale=4.0000 norm=3.2774
[iter 0] loss=1.4004 val_loss=0.0000 scale=2.0000 norm=1.9799
[iter 100] loss=1.0701 val_loss=0.0000 scale=2.0000 norm=1.6950
[iter 200] loss=0.9238 val_loss=0.0000 scale=2.0000 norm=1.6805
[iter 300] loss=0.7933 val_loss=0.0000 scale=4.0000 norm=3.3734
[iter 400] loss=0.6192 val_loss=0.0000 scale=4.0000 norm=3.3777
[iter 0] loss=13.4605 val_loss=0.0000 scale=2.0000 norm=61923.2394
[iter 100] loss=11.7687 val_loss=0.0000 scale=4.0000 norm=114454.8792
[iter 200] loss=10.6229 val_loss=0.0000 scale=4.0000 norm=114312.0603
[iter 300] loss=9.9313 val_loss=0.0000 scale=4.0000 norm=114298.0427
[iter 400] loss=8.9191 val_loss=0.0000 scale=8.0000 norm=228593.8326
[iter 0] loss=1.2548 val_loss=0.0000 scale=1.0000 norm=0.9050
[iter 100] loss=0.9591 val_loss=0.0000 scale=2.0000 norm=1.5440
[iter 200] loss=0.8639 val_loss=0.0000 scale=2.0000 norm=1.5702
[iter 300] loss=0.7755 val_loss=0.0000 scale=2.0000 norm=1.5797
[iter 400] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.5809
[iter 0] loss=1.4331 val_loss=0.0000 scale=2.0000 norm=2.0154
[iter 100] loss=1.0984 val_loss=0.0000 scale=2.0000 norm=1.7385
[iter 200] loss=0.9489 val_loss=0.0000 scale=2.0000 norm=1.7313
[iter 300] loss=0.7975 val_loss=0.0000 scale=4.0000 norm=3.4795
[iter 400] loss=0.6342 val_loss=0.0000 scale=4.0000 norm=3.4810
[iter 0] loss=13.4601 val_loss=0.0000 scale=2.0000 norm=61418.5986
[iter 100] loss=11.8069 val_loss=0.0000 scale=4.0000 norm=114598.4364
[iter 200] loss=10.6810 val_loss=0.0000 scale=4.0000 norm=114500.4176
[iter 300] loss=9.9322 val_loss=0.0000 scale=8.0000 norm=228969.9981
[iter 400] loss=8.9420 val_loss=0.0000 scale=8.0000 norm=228969.4330
[iter 0] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=0.9372
[iter 100] loss=1.0508 val_loss=0.0000 scale=2.0000 norm=1.6382
[iter 200] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.6555
[iter 300] loss=0.8687 val_loss=0.0000 scale=4.0000 norm=3.3296
[iter 400] loss=0.7272 val_loss=0.0000 scale=4.0000 norm=3.3318
[iter 0] loss=1.3703 val_loss=0.0000 scale=1.0000 norm=0.9735
[iter 100] loss=1.0408 val_loss=0.0000 scale=2.0000 norm=1.6562
[iter 200] loss=0.8991 val_loss=0.0000 scale=2.0000 norm=1.6445
[iter 300] loss=0.7785 val_loss=0.0000 scale=4.0000 norm=3.3052
[iter 400] loss=0.6163 val_loss=0.0000 scale=4.0000 norm=3.3117
[iter 0] loss=13.4568 val_loss=0.0000 scale=2.0000 norm=57335.5236
[iter 100] loss=11.7059 val_loss=0.0000 scale=4.0000 norm=108676.2970
[iter 200] loss=10.5621 val_loss=0.0000 scale=4.0000 norm=108632.2202
[iter 300] loss=9.8933 val_loss=0.0000 scale=8.0000 norm=217242.6482
[iter 400] loss=8.9856 val_loss=0.0000 scale=8.0000 norm=217241.8598
[iter 0] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=0.9362
[iter 100] loss=1.0602 val_loss=0.0000 scale=2.0000 norm=1.6615
[iter 200] loss=0.9868 val_loss=0.0000 scale=2.0000 norm=1.6830
[iter 300] loss=0.9435 val_loss=0.0000 scale=4.0000 norm=3.3823
[iter 400] loss=0.8817 val_loss=0.0000 scale=4.0000 norm=3.3884
[iter 0] loss=1.3435 val_loss=0.0000 scale=2.0000 norm=1.9213
[iter 100] loss=0.9925 val_loss=0.0000 scale=2.0000 norm=1.6244
[iter 200] loss=0.8366 val_loss=0.0000 scale=2.0000 norm=1.6227
[iter 300] loss=0.6692 val_loss=0.0000 scale=4.0000 norm=3.2530
[iter 400] loss=0.4873 val_loss=0.0000 scale=4.0000 norm=3.2479
[iter 0] loss=13.4578 val_loss=0.0000 scale=2.0000 norm=58187.7039
[iter 100] loss=11.6973 val_loss=0.0000 scale=4.0000 norm=110470.8853
[iter 200] loss=10.5282 val_loss=0.0000 scale=4.0000 norm=110467.3870
[iter 300] loss=9.8200 val_loss=0.0000 scale=8.0000 norm=220909.7379
[iter 400] loss=8.8646 val_loss=0.0000 scale=8.0000 norm=220909.3202
[iter 0] loss=1.2729 val_loss=0.0000 scale=1.0000 norm=0.9145
[iter 100] loss=1.0199 val_loss=0.0000 scale=2.0000 norm=1.6169
[iter 200] loss=0.9636 val_loss=0.0000 scale=2.0000 norm=1.6486
[iter 300] loss=0.9358 val_loss=0.0000 scale=2.0000 norm=1.6620
[iter 400] loss=0.9098 val_loss=0.0000 scale=2.0000 norm=1.6644
[iter 0] loss=1.3675 val_loss=0.0000 scale=2.0000 norm=1.9472
[iter 100] loss=1.0297 val_loss=0.0000 scale=2.0000 norm=1.6806
[iter 200] loss=0.8680 val_loss=0.0000 scale=2.0000 norm=1.6695
[iter 300] loss=0.6928 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 400] loss=0.5109 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 0] loss=12.2464 val_loss=0.0000 scale=2.0000 norm=21739.0699
[iter 100] loss=10.7450 val_loss=0.0000 scale=4.0000 norm=42350.0841
[iter 200] loss=9.8457 val_loss=0.0000 scale=4.0000 norm=42270.9879
[iter 300] loss=9.1051 val_loss=0.0000 scale=8.0000 norm=84530.8515
[iter 400] loss=8.1260 val_loss=0.0000 scale=8.0000 norm=84530.9780
[iter 0] loss=1.2880 val_loss=0.0000 scale=1.0000 norm=0.9224
[iter 100] loss=1.0260 val_loss=0.0000 scale=2.0000 norm=1.6254
[iter 200] loss=0.9585 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 300] loss=0.9145 val_loss=0.0000 scale=4.0000 norm=3.3295
[iter 400] loss=0.8600 val_loss=0.0000 scale=4.0000 norm=3.3352
[iter 0] loss=1.3802 val_loss=0.0000 scale=2.0000 norm=1.9592
[iter 100] loss=1.0319 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 200] loss=0.8864 val_loss=0.0000 scale=2.0000 norm=1.6513
[iter 300] loss=0.7410 val_loss=0.0000 scale=4.0000 norm=3.3185
[iter 400] loss=0.5811 val_loss=0.0000 scale=4.0000 norm=3.3123
[iter 0] loss=13.4221 val_loss=0.0000 scale=2.0000 norm=52155.6181
[iter 100] loss=11.6563 val_loss=0.0000 scale=4.0000 norm=97735.4318
[iter 200] loss=10.5123 val_loss=0.0000 scale=4.0000 norm=97662.5177
[iter 300] loss=9.8168 val_loss=0.0000 scale=8.0000 norm=195296.7467
[iter 400] loss=8.8734 val_loss=0.0000 scale=8.0000 norm=195296.0065
[iter 0] loss=1.2623 val_loss=0.0000 scale=1.0000 norm=0.9083
[iter 100] loss=1.0256 val_loss=0.0000 scale=2.0000 norm=1.6201
[iter 200] loss=0.9742 val_loss=0.0000 scale=2.0000 norm=1.6449
[iter 300] loss=0.9415 val_loss=0.0000 scale=2.0000 norm=1.6526
[iter 400] loss=0.9055 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=0.9900
[iter 100] loss=1.0638 val_loss=0.0000 scale=2.0000 norm=1.7066
[iter 200] loss=0.9054 val_loss=0.0000 scale=2.0000 norm=1.6891
[iter 300] loss=0.7778 val_loss=0.0000 scale=4.0000 norm=3.3903
[iter 400] loss=0.6009 val_loss=0.0000 scale=4.0000 norm=3.3914
[iter 0] loss=13.4584 val_loss=0.0000 scale=2.0000 norm=61070.1560
[iter 100] loss=11.7692 val_loss=0.0000 scale=4.0000 norm=114199.5477
[iter 200] loss=10.6570 val_loss=0.0000 scale=4.0000 norm=114149.7727
[iter 300] loss=10.0071 val_loss=0.0000 scale=8.0000 norm=228267.6062
[iter 400] loss=9.1141 val_loss=0.0000 scale=8.0000 norm=228266.6405
[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=0.9091
[iter 100] loss=1.0194 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 200] loss=0.9546 val_loss=0.0000 scale=2.0000 norm=1.6400
[iter 300] loss=0.9102 val_loss=0.0000 scale=4.0000 norm=3.3067
[iter 400] loss=0.8570 val_loss=0.0000 scale=4.0000 norm=3.3101
[iter 0] loss=1.3826 val_loss=0.0000 scale=2.0000 norm=1.9608
[iter 100] loss=1.0744 val_loss=0.0000 scale=2.0000 norm=1.7127
[iter 200] loss=0.9397 val_loss=0.0000 scale=2.0000 norm=1.6995
[iter 300] loss=0.8012 val_loss=0.0000 scale=4.0000 norm=3.4047
[iter 400] loss=0.6403 val_loss=0.0000 scale=4.0000 norm=3.4069
[iter 0] loss=13.4569 val_loss=0.0000 scale=2.0000 norm=57516.1521
[iter 100] loss=11.6840 val_loss=0.0000 scale=4.0000 norm=109649.3699
[iter 200] loss=10.5210 val_loss=0.0000 scale=4.0000 norm=109625.4435
[iter 300] loss=9.8182 val_loss=0.0000 scale=8.0000 norm=219219.5888
[iter 400] loss=8.8150 val_loss=0.0000 scale=8.0000 norm=219218.2551
[iter 0] loss=1.2441 val_loss=0.0000 scale=1.0000 norm=0.9004
[iter 100] loss=1.0044 val_loss=0.0000 scale=2.0000 norm=1.6064
[iter 200] loss=0.9462 val_loss=0.0000 scale=2.0000 norm=1.6389
[iter 300] loss=0.9088 val_loss=0.0000 scale=2.0000 norm=1.6496
[iter 400] loss=0.8692 val_loss=0.0000 scale=2.0000 norm=1.6514
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0550
[iter 100] loss=1.1249 val_loss=0.0000 scale=2.0000 norm=1.7851
[iter 200] loss=0.9669 val_loss=0.0000 scale=2.0000 norm=1.7803
[iter 300] loss=0.7978 val_loss=0.0000 scale=4.0000 norm=3.5772
[iter 400] loss=0.6050 val_loss=0.0000 scale=4.0000 norm=3.5714
[iter 0] loss=13.4604 val_loss=0.0000 scale=2.0000 norm=61696.3465
[iter 100] loss=11.7724 val_loss=0.0000 scale=4.0000 norm=115603.1181
[iter 200] loss=10.6318 val_loss=0.0000 scale=4.0000 norm=115521.6017
[iter 300] loss=9.9570 val_loss=0.0000 scale=4.0000 norm=115504.5855
[iter 400] loss=8.9395 val_loss=0.0000 scale=8.0000 norm=231005.7506
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9178
[iter 100] loss=1.0395 val_loss=0.0000 scale=2.0000 norm=1.6154
[iter 200] loss=0.9883 val_loss=0.0000 scale=2.0000 norm=1.6466
[iter 300] loss=0.9581 val_loss=0.0000 scale=2.0000 norm=1.6570
[iter 400] loss=0.9146 val_loss=0.0000 scale=4.0000 norm=3.3192
[iter 0] loss=1.3920 val_loss=0.0000 scale=1.0000 norm=0.9856
[iter 100] loss=1.0360 val_loss=0.0000 scale=2.0000 norm=1.6852
[iter 200] loss=0.8835 val_loss=0.0000 scale=2.0000 norm=1.6824
[iter 300] loss=0.7421 val_loss=0.0000 scale=4.0000 norm=3.4070
[iter 400] loss=0.5811 val_loss=0.0000 scale=4.0000 norm=3.4111
[iter 0] loss=12.2351 val_loss=0.0000 scale=2.0000 norm=20907.5541
[iter 100] loss=10.6889 val_loss=0.0000 scale=4.0000 norm=40136.1259
[iter 200] loss=9.7421 val_loss=0.0000 scale=4.0000 norm=40014.7946
[iter 300] loss=8.9699 val_loss=0.0000 scale=8.0000 norm=80015.6493
[iter 400] loss=7.9311 val_loss=0.0000 scale=8.0000 norm=80015.6726
[iter 0] loss=1.2604 val_loss=0.0000 scale=1.0000 norm=0.9080
[iter 100] loss=1.0396 val_loss=0.0000 scale=2.0000 norm=1.6185
[iter 200] loss=0.9806 val_loss=0.0000 scale=2.0000 norm=1.6431
[iter 300] loss=0.9388 val_loss=0.0000 scale=2.0000 norm=1.6524
[iter 400] loss=0.8909 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 0] loss=1.4348 val_loss=0.0000 scale=1.0000 norm=1.0085
[iter 100] loss=1.0537 val_loss=0.0000 scale=2.0000 norm=1.6815
[iter 200] loss=0.9128 val_loss=0.0000 scale=2.0000 norm=1.6831
[iter 300] loss=0.7961 val_loss=0.0000 scale=4.0000 norm=3.3855
[iter 400] loss=0.6352 val_loss=0.0000 scale=4.0000 norm=3.3835
[iter 0] loss=13.4220 val_loss=0.0000 scale=2.0000 norm=51255.8325
[iter 100] loss=11.6535 val_loss=0.0000 scale=4.0000 norm=97081.9612
[iter 200] loss=10.5009 val_loss=0.0000 scale=4.0000 norm=96990.8031
[iter 300] loss=9.8252 val_loss=0.0000 scale=4.0000 norm=96979.3579
[iter 400] loss=8.8886 val_loss=0.0000 scale=8.0000 norm=193957.3334
[iter 0] loss=1.2519 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0091 val_loss=0.0000 scale=2.0000 norm=1.5848
[iter 200] loss=0.9486 val_loss=0.0000 scale=2.0000 norm=1.6151
[iter 300] loss=0.9111 val_loss=0.0000 scale=2.0000 norm=1.6235
[iter 400] loss=0.8745 val_loss=0.0000 scale=2.0000 norm=1.6256
[iter 0] loss=1.3513 val_loss=0.0000 scale=2.0000 norm=1.9289
[iter 100] loss=1.0483 val_loss=0.0000 scale=2.0000 norm=1.6896
[iter 200] loss=0.9055 val_loss=0.0000 scale=2.0000 norm=1.6866
[iter 300] loss=0.7701 val_loss=0.0000 scale=4.0000 norm=3.3983
[iter 400] loss=0.6097 val_loss=0.0000 scale=4.0000 norm=3.4031
[iter 0] loss=12.2411 val_loss=0.0000 scale=2.0000 norm=20949.1319
[iter 100] loss=10.7508 val_loss=0.0000 scale=4.0000 norm=40313.9928
[iter 200] loss=9.8077 val_loss=0.0000 scale=4.0000 norm=40284.3086
[iter 300] loss=8.9487 val_loss=0.0000 scale=8.0000 norm=80563.6673
[iter 400] loss=7.9011 val_loss=0.0000 scale=8.0000 norm=80563.7239
[iter 0] loss=1.2534 val_loss=0.0000 scale=1.0000 norm=0.9046
[iter 100] loss=1.0102 val_loss=0.0000 scale=2.0000 norm=1.5969
[iter 200] loss=0.9527 val_loss=0.0000 scale=2.0000 norm=1.6279
[iter 300] loss=0.9144 val_loss=0.0000 scale=2.0000 norm=1.6371
[iter 400] loss=0.8693 val_loss=0.0000 scale=4.0000 norm=3.2774
[iter 0] loss=1.4004 val_loss=0.0000 scale=2.0000 norm=1.9799
[iter 100] loss=1.0701 val_loss=0.0000 scale=2.0000 norm=1.6950
[iter 200] loss=0.9238 val_loss=0.0000 scale=2.0000 norm=1.6805
[iter 300] loss=0.7933 val_loss=0.0000 scale=4.0000 norm=3.3734
[iter 400] loss=0.6192 val_loss=0.0000 scale=4.0000 norm=3.3777
[iter 0] loss=13.4605 val_loss=0.0000 scale=2.0000 norm=61923.2394
[iter 100] loss=11.7687 val_loss=0.0000 scale=4.0000 norm=114454.8792
[iter 200] loss=10.6229 val_loss=0.0000 scale=4.0000 norm=114312.0603
[iter 300] loss=9.9313 val_loss=0.0000 scale=4.0000 norm=114298.0427
[iter 400] loss=8.9191 val_loss=0.0000 scale=8.0000 norm=228593.8310
[iter 0] loss=1.3472 val_loss=0.0000 scale=1.0000 norm=0.9569
[iter 100] loss=1.0987 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 200] loss=1.0385 val_loss=0.0000 scale=2.0000 norm=1.7009
[iter 300] loss=1.0059 val_loss=0.0000 scale=2.0000 norm=1.7106
[iter 400] loss=0.9770 val_loss=0.0000 scale=2.0000 norm=1.7130
[iter 0] loss=1.3050 val_loss=0.0000 scale=2.0000 norm=1.8775
[iter 100] loss=0.9943 val_loss=0.0000 scale=2.0000 norm=1.6221
[iter 200] loss=0.8523 val_loss=0.0000 scale=2.0000 norm=1.6199
[iter 300] loss=0.7074 val_loss=0.0000 scale=4.0000 norm=3.2616
[iter 400] loss=0.5351 val_loss=0.0000 scale=4.0000 norm=3.2642
[iter 0] loss=13.4587 val_loss=0.0000 scale=2.0000 norm=61239.9286
[iter 100] loss=11.7381 val_loss=0.0000 scale=4.0000 norm=113489.4378
[iter 200] loss=10.5923 val_loss=0.0000 scale=4.0000 norm=113474.5008
[iter 300] loss=9.8829 val_loss=0.0000 scale=8.0000 norm=226927.1256
[iter 400] loss=8.9303 val_loss=0.0000 scale=8.0000 norm=226926.5014
[iter 0] loss=1.3202 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0501 val_loss=0.0000 scale=2.0000 norm=1.6394
[iter 200] loss=0.9849 val_loss=0.0000 scale=2.0000 norm=1.6647
[iter 300] loss=0.9459 val_loss=0.0000 scale=2.0000 norm=1.6747
[iter 400] loss=0.9040 val_loss=0.0000 scale=2.0000 norm=1.6768
[iter 0] loss=1.3569 val_loss=0.0000 scale=1.0000 norm=0.9670
[iter 100] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6622
[iter 200] loss=0.8712 val_loss=0.0000 scale=2.0000 norm=1.6543
[iter 300] loss=0.7389 val_loss=0.0000 scale=4.0000 norm=3.3446
[iter 400] loss=0.5721 val_loss=0.0000 scale=4.0000 norm=3.3489
[iter 0] loss=13.4601 val_loss=0.0000 scale=2.0000 norm=61140.5327
[iter 100] loss=11.7288 val_loss=0.0000 scale=4.0000 norm=113075.5865
[iter 200] loss=10.5670 val_loss=0.0000 scale=4.0000 norm=113059.7739
[iter 300] loss=9.8163 val_loss=0.0000 scale=8.0000 norm=226105.2622
[iter 400] loss=8.8132 val_loss=0.0000 scale=8.0000 norm=226105.0632
[iter 0] loss=1.3118 val_loss=0.0000 scale=1.0000 norm=0.9370
[iter 100] loss=1.0172 val_loss=0.0000 scale=2.0000 norm=1.6018
[iter 200] loss=0.9099 val_loss=0.0000 scale=2.0000 norm=1.6171
[iter 300] loss=0.8243 val_loss=0.0000 scale=2.0000 norm=1.6290
[iter 400] loss=0.7172 val_loss=0.0000 scale=4.0000 norm=3.2628
[iter 0] loss=1.3571 val_loss=0.0000 scale=2.0000 norm=1.9347
[iter 100] loss=1.0670 val_loss=0.0000 scale=2.0000 norm=1.7047
[iter 200] loss=0.9402 val_loss=0.0000 scale=2.0000 norm=1.7046
[iter 300] loss=0.8317 val_loss=0.0000 scale=4.0000 norm=3.4253
[iter 400] loss=0.6765 val_loss=0.0000 scale=4.0000 norm=3.4313
[iter 0] loss=13.4219 val_loss=0.0000 scale=2.0000 norm=51251.8923
[iter 100] loss=11.6842 val_loss=0.0000 scale=4.0000 norm=96961.4752
[iter 200] loss=10.5388 val_loss=0.0000 scale=4.0000 norm=96946.6401
[iter 300] loss=9.7424 val_loss=0.0000 scale=8.0000 norm=193880.8630
[iter 400] loss=8.7036 val_loss=0.0000 scale=8.0000 norm=193880.5668
[iter 0] loss=1.2641 val_loss=0.0000 scale=1.0000 norm=0.9091
[iter 100] loss=1.0194 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 200] loss=0.9546 val_loss=0.0000 scale=2.0000 norm=1.6400
[iter 300] loss=0.9102 val_loss=0.0000 scale=4.0000 norm=3.3067
[iter 400] loss=0.8570 val_loss=0.0000 scale=4.0000 norm=3.3101
[iter 0] loss=1.3826 val_loss=0.0000 scale=2.0000 norm=1.9608
[iter 100] loss=1.0744 val_loss=0.0000 scale=2.0000 norm=1.7127
[iter 200] loss=0.9397 val_loss=0.0000 scale=2.0000 norm=1.6995
[iter 300] loss=0.8012 val_loss=0.0000 scale=4.0000 norm=3.4047
[iter 400] loss=0.6403 val_loss=0.0000 scale=4.0000 norm=3.4069
[iter 0] loss=13.4569 val_loss=0.0000 scale=2.0000 norm=57516.1521
[iter 100] loss=11.6840 val_loss=0.0000 scale=4.0000 norm=109649.3699
[iter 200] loss=10.5210 val_loss=0.0000 scale=4.0000 norm=109625.4435
[iter 300] loss=9.8182 val_loss=0.0000 scale=8.0000 norm=219219.5888
[iter 400] loss=8.8150 val_loss=0.0000 scale=8.0000 norm=219218.2558
[iter 0] loss=1.2441 val_loss=0.0000 scale=1.0000 norm=0.9004
[iter 100] loss=1.0044 val_loss=0.0000 scale=2.0000 norm=1.6064
[iter 200] loss=0.9462 val_loss=0.0000 scale=2.0000 norm=1.6389
[iter 300] loss=0.9088 val_loss=0.0000 scale=2.0000 norm=1.6496
[iter 400] loss=0.8692 val_loss=0.0000 scale=2.0000 norm=1.6514
[iter 0] loss=1.4692 val_loss=0.0000 scale=2.0000 norm=2.0550
[iter 100] loss=1.1249 val_loss=0.0000 scale=2.0000 norm=1.7851
[iter 200] loss=0.9669 val_loss=0.0000 scale=2.0000 norm=1.7803
[iter 300] loss=0.7978 val_loss=0.0000 scale=4.0000 norm=3.5772
[iter 400] loss=0.6050 val_loss=0.0000 scale=4.0000 norm=3.5714
[iter 0] loss=13.4604 val_loss=0.0000 scale=2.0000 norm=61696.3465
[iter 100] loss=11.7724 val_loss=0.0000 scale=4.0000 norm=115603.1181
[iter 200] loss=10.6318 val_loss=0.0000 scale=4.0000 norm=115521.6017
[iter 300] loss=9.9570 val_loss=0.0000 scale=4.0000 norm=115504.5855
[iter 400] loss=8.9395 val_loss=0.0000 scale=8.0000 norm=231005.7522
[iter 0] loss=1.3112 val_loss=0.0000 scale=1.0000 norm=0.9362
[iter 100] loss=1.0602 val_loss=0.0000 scale=2.0000 norm=1.6615
[iter 200] loss=0.9868 val_loss=0.0000 scale=2.0000 norm=1.6830
[iter 300] loss=0.9435 val_loss=0.0000 scale=4.0000 norm=3.3823
[iter 400] loss=0.8817 val_loss=0.0000 scale=4.0000 norm=3.3884
[iter 0] loss=1.3435 val_loss=0.0000 scale=2.0000 norm=1.9213
[iter 100] loss=0.9925 val_loss=0.0000 scale=2.0000 norm=1.6244
[iter 200] loss=0.8366 val_loss=0.0000 scale=2.0000 norm=1.6227
[iter 300] loss=0.6692 val_loss=0.0000 scale=4.0000 norm=3.2530
[iter 400] loss=0.4873 val_loss=0.0000 scale=4.0000 norm=3.2479
[iter 0] loss=13.4578 val_loss=0.0000 scale=2.0000 norm=58187.7039
[iter 100] loss=11.6973 val_loss=0.0000 scale=4.0000 norm=110470.8853
[iter 200] loss=10.5282 val_loss=0.0000 scale=4.0000 norm=110467.3870
[iter 300] loss=9.8200 val_loss=0.0000 scale=8.0000 norm=220909.7379
[iter 400] loss=8.8646 val_loss=0.0000 scale=8.0000 norm=220909.3204
[iter 0] loss=1.2729 val_loss=0.0000 scale=1.0000 norm=0.9145
[iter 100] loss=1.0199 val_loss=0.0000 scale=2.0000 norm=1.6169
[iter 200] loss=0.9636 val_loss=0.0000 scale=2.0000 norm=1.6486
[iter 300] loss=0.9358 val_loss=0.0000 scale=2.0000 norm=1.6620
[iter 400] loss=0.9098 val_loss=0.0000 scale=2.0000 norm=1.6644
[iter 0] loss=1.3675 val_loss=0.0000 scale=2.0000 norm=1.9472
[iter 100] loss=1.0297 val_loss=0.0000 scale=2.0000 norm=1.6806
[iter 200] loss=0.8680 val_loss=0.0000 scale=2.0000 norm=1.6695
[iter 300] loss=0.6928 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 400] loss=0.5109 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 0] loss=12.2464 val_loss=0.0000 scale=2.0000 norm=21739.0699
[iter 100] loss=10.7450 val_loss=0.0000 scale=4.0000 norm=42350.0841
[iter 200] loss=9.8457 val_loss=0.0000 scale=4.0000 norm=42270.9879
[iter 300] loss=9.1051 val_loss=0.0000 scale=8.0000 norm=84530.8515
[iter 400] loss=8.1260 val_loss=0.0000 scale=8.0000 norm=84530.9777
[iter 0] loss=1.3141 val_loss=0.0000 scale=1.0000 norm=0.9372
[iter 100] loss=1.0508 val_loss=0.0000 scale=2.0000 norm=1.6382
[iter 200] loss=0.9547 val_loss=0.0000 scale=2.0000 norm=1.6555
[iter 300] loss=0.8694 val_loss=0.0000 scale=4.0000 norm=3.3296
[iter 400] loss=0.7287 val_loss=0.0000 scale=4.0000 norm=3.3318
[iter 0] loss=1.3703 val_loss=0.0000 scale=1.0000 norm=0.9735
[iter 100] loss=1.0408 val_loss=0.0000 scale=2.0000 norm=1.6562
[iter 200] loss=0.8991 val_loss=0.0000 scale=2.0000 norm=1.6445
[iter 300] loss=0.7785 val_loss=0.0000 scale=4.0000 norm=3.3052
[iter 400] loss=0.6163 val_loss=0.0000 scale=4.0000 norm=3.3117
[iter 0] loss=13.4568 val_loss=0.0000 scale=2.0000 norm=57335.5236
[iter 100] loss=11.7059 val_loss=0.0000 scale=4.0000 norm=108676.2970
[iter 200] loss=10.5621 val_loss=0.0000 scale=4.0000 norm=108632.2202
[iter 300] loss=9.8933 val_loss=0.0000 scale=8.0000 norm=217242.6482
[iter 400] loss=8.9856 val_loss=0.0000 scale=8.0000 norm=217241.8603
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9184
[iter 100] loss=1.0775 val_loss=0.0000 scale=2.0000 norm=1.6682
[iter 200] loss=1.0245 val_loss=0.0000 scale=2.0000 norm=1.6980
[iter 300] loss=0.9929 val_loss=0.0000 scale=2.0000 norm=1.7064
[iter 400] loss=0.9637 val_loss=0.0000 scale=2.0000 norm=1.7082
[iter 0] loss=1.3537 val_loss=0.0000 scale=1.0000 norm=0.9656
[iter 100] loss=1.0222 val_loss=0.0000 scale=2.0000 norm=1.6641
[iter 200] loss=0.8860 val_loss=0.0000 scale=2.0000 norm=1.6582
[iter 300] loss=0.7488 val_loss=0.0000 scale=4.0000 norm=3.3280
[iter 400] loss=0.5983 val_loss=0.0000 scale=4.0000 norm=3.3276
[iter 0] loss=13.4602 val_loss=0.0000 scale=2.0000 norm=60915.9477
[iter 100] loss=11.7655 val_loss=0.0000 scale=4.0000 norm=113258.7072
[iter 200] loss=10.4839 val_loss=0.0000 scale=4.0000 norm=113202.2858
[iter 300] loss=9.7754 val_loss=0.0000 scale=4.0000 norm=113193.7057
[iter 400] loss=8.8243 val_loss=0.0000 scale=8.0000 norm=226386.3849
[iter 0] loss=1.3108 val_loss=0.0000 scale=1.0000 norm=0.9365
[iter 100] loss=0.9940 val_loss=0.0000 scale=2.0000 norm=1.5712
[iter 200] loss=0.9297 val_loss=0.0000 scale=2.0000 norm=1.6058
[iter 300] loss=0.8909 val_loss=0.0000 scale=2.0000 norm=1.6198
[iter 400] loss=0.8514 val_loss=0.0000 scale=2.0000 norm=1.6224
[iter 0] loss=1.4291 val_loss=0.0000 scale=2.0000 norm=2.0111
[iter 100] loss=1.0607 val_loss=0.0000 scale=2.0000 norm=1.7063
[iter 200] loss=0.8980 val_loss=0.0000 scale=2.0000 norm=1.6971
[iter 300] loss=0.7634 val_loss=0.0000 scale=4.0000 norm=3.4337
[iter 400] loss=0.5911 val_loss=0.0000 scale=4.0000 norm=3.4401
[iter 0] loss=13.4586 val_loss=0.0000 scale=2.0000 norm=59840.7698
[iter 100] loss=11.7036 val_loss=0.0000 scale=4.0000 norm=109866.0132
[iter 200] loss=10.4226 val_loss=0.0000 scale=4.0000 norm=109782.6713
[iter 300] loss=9.6623 val_loss=0.0000 scale=8.0000 norm=219531.3807
[iter 400] loss=8.6233 val_loss=0.0000 scale=8.0000 norm=219530.3247
[iter 0] loss=1.3013 val_loss=0.0000 scale=1.0000 norm=0.9306
[iter 100] loss=1.0405 val_loss=0.0000 scale=2.0000 norm=1.6264
[iter 200] loss=0.9732 val_loss=0.0000 scale=2.0000 norm=1.6534
[iter 300] loss=0.9322 val_loss=0.0000 scale=2.0000 norm=1.6651
[iter 400] loss=0.8947 val_loss=0.0000 scale=2.0000 norm=1.6674
[iter 0] loss=1.3872 val_loss=0.0000 scale=2.0000 norm=1.9665
[iter 100] loss=1.0432 val_loss=0.0000 scale=2.0000 norm=1.6865
[iter 200] loss=0.8827 val_loss=0.0000 scale=2.0000 norm=1.6729
[iter 300] loss=0.7208 val_loss=0.0000 scale=4.0000 norm=3.3554
[iter 400] loss=0.5364 val_loss=0.0000 scale=4.0000 norm=3.3580
[iter 0] loss=13.4574 val_loss=0.0000 scale=2.0000 norm=58338.7171
[iter 100] loss=11.7114 val_loss=0.0000 scale=4.0000 norm=111220.5419
[iter 200] loss=10.5347 val_loss=0.0000 scale=4.0000 norm=111217.1615
[iter 300] loss=9.8112 val_loss=0.0000 scale=8.0000 norm=222415.1824
[iter 400] loss=8.8229 val_loss=0.0000 scale=8.0000 norm=222414.4104
[iter 0] loss=1.2764 val_loss=0.0000 scale=1.0000 norm=0.9169
[iter 100] loss=1.0389 val_loss=0.0000 scale=2.0000 norm=1.6330
[iter 200] loss=0.9729 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 300] loss=0.9286 val_loss=0.0000 scale=2.0000 norm=1.6643
[iter 400] loss=0.8903 val_loss=0.0000 scale=2.0000 norm=1.6666
[iter 0] loss=1.3441 val_loss=0.0000 scale=2.0000 norm=1.9210
[iter 100] loss=1.0008 val_loss=0.0000 scale=2.0000 norm=1.6328
[iter 200] loss=0.8411 val_loss=0.0000 scale=2.0000 norm=1.6210
[iter 300] loss=0.6953 val_loss=0.0000 scale=4.0000 norm=3.2624
[iter 400] loss=0.5190 val_loss=0.0000 scale=4.0000 norm=3.2591
[iter 0] loss=12.3731 val_loss=0.0000 scale=2.0000 norm=26534.1702
[iter 100] loss=10.9088 val_loss=0.0000 scale=4.0000 norm=51067.2032
[iter 200] loss=9.9399 val_loss=0.0000 scale=4.0000 norm=50980.4812
[iter 300] loss=9.3061 val_loss=0.0000 scale=8.0000 norm=101940.9647
[iter 400] loss=8.2673 val_loss=0.0000 scale=8.0000 norm=101940.4666
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9184
[iter 100] loss=1.0775 val_loss=0.0000 scale=2.0000 norm=1.6682
[iter 200] loss=1.0245 val_loss=0.0000 scale=2.0000 norm=1.6980
[iter 300] loss=0.9929 val_loss=0.0000 scale=2.0000 norm=1.7064
[iter 400] loss=0.9637 val_loss=0.0000 scale=2.0000 norm=1.7082
[iter 0] loss=1.3537 val_loss=0.0000 scale=1.0000 norm=0.9656
[iter 100] loss=1.0222 val_loss=0.0000 scale=2.0000 norm=1.6641
[iter 200] loss=0.8860 val_loss=0.0000 scale=2.0000 norm=1.6582
[iter 300] loss=0.7488 val_loss=0.0000 scale=4.0000 norm=3.3280
[iter 400] loss=0.5983 val_loss=0.0000 scale=4.0000 norm=3.3276
[iter 0] loss=13.4602 val_loss=0.0000 scale=2.0000 norm=60915.9477
[iter 100] loss=11.7655 val_loss=0.0000 scale=4.0000 norm=113258.7072
[iter 200] loss=10.4839 val_loss=0.0000 scale=4.0000 norm=113202.2858
[iter 300] loss=9.7754 val_loss=0.0000 scale=4.0000 norm=113193.7057
[iter 400] loss=8.8243 val_loss=0.0000 scale=8.0000 norm=226386.3841
[iter 0] loss=1.3108 val_loss=0.0000 scale=1.0000 norm=0.9365
[iter 100] loss=0.9940 val_loss=0.0000 scale=2.0000 norm=1.5712
[iter 200] loss=0.9297 val_loss=0.0000 scale=2.0000 norm=1.6058
[iter 300] loss=0.8909 val_loss=0.0000 scale=2.0000 norm=1.6198
[iter 400] loss=0.8514 val_loss=0.0000 scale=2.0000 norm=1.6224
[iter 0] loss=1.4291 val_loss=0.0000 scale=2.0000 norm=2.0111
[iter 100] loss=1.0607 val_loss=0.0000 scale=2.0000 norm=1.7062
[iter 200] loss=0.8980 val_loss=0.0000 scale=2.0000 norm=1.6971
[iter 300] loss=0.7609 val_loss=0.0000 scale=4.0000 norm=3.4339
[iter 400] loss=0.5885 val_loss=0.0000 scale=4.0000 norm=3.4401
[iter 0] loss=13.4586 val_loss=0.0000 scale=2.0000 norm=59840.7698
[iter 100] loss=11.7036 val_loss=0.0000 scale=4.0000 norm=109866.0132
[iter 200] loss=10.4226 val_loss=0.0000 scale=4.0000 norm=109782.6713
[iter 300] loss=9.6623 val_loss=0.0000 scale=8.0000 norm=219531.3807
[iter 400] loss=8.6233 val_loss=0.0000 scale=8.0000 norm=219530.3237
[iter 0] loss=1.3013 val_loss=0.0000 scale=1.0000 norm=0.9306
[iter 100] loss=1.0405 val_loss=0.0000 scale=2.0000 norm=1.6264
[iter 200] loss=0.9732 val_loss=0.0000 scale=2.0000 norm=1.6534
[iter 300] loss=0.9322 val_loss=0.0000 scale=2.0000 norm=1.6651
[iter 400] loss=0.8947 val_loss=0.0000 scale=2.0000 norm=1.6674
[iter 0] loss=1.3872 val_loss=0.0000 scale=2.0000 norm=1.9665
[iter 100] loss=1.0432 val_loss=0.0000 scale=2.0000 norm=1.6865
[iter 200] loss=0.8827 val_loss=0.0000 scale=2.0000 norm=1.6729
[iter 300] loss=0.7208 val_loss=0.0000 scale=4.0000 norm=3.3554
[iter 400] loss=0.5364 val_loss=0.0000 scale=4.0000 norm=3.3580
[iter 0] loss=13.4574 val_loss=0.0000 scale=2.0000 norm=58338.7171
[iter 100] loss=11.7114 val_loss=0.0000 scale=4.0000 norm=111220.5419
[iter 200] loss=10.5347 val_loss=0.0000 scale=4.0000 norm=111217.1615
[iter 300] loss=9.8112 val_loss=0.0000 scale=8.0000 norm=222415.1824
[iter 400] loss=8.8229 val_loss=0.0000 scale=8.0000 norm=222414.4113
[iter 0] loss=1.2359 val_loss=0.0000 scale=1.0000 norm=0.8946
[iter 100] loss=1.0081 val_loss=0.0000 scale=2.0000 norm=1.6007
[iter 200] loss=0.9508 val_loss=0.0000 scale=2.0000 norm=1.6332
[iter 300] loss=0.9163 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 400] loss=0.8724 val_loss=0.0000 scale=4.0000 norm=3.3012
[iter 0] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=0.9463
[iter 100] loss=0.9979 val_loss=0.0000 scale=2.0000 norm=1.6343
[iter 200] loss=0.8561 val_loss=0.0000 scale=2.0000 norm=1.6229
[iter 300] loss=0.7334 val_loss=0.0000 scale=4.0000 norm=3.2633
[iter 400] loss=0.5687 val_loss=0.0000 scale=4.0000 norm=3.2615
[iter 0] loss=13.4591 val_loss=0.0000 scale=2.0000 norm=58691.7659
[iter 100] loss=11.7150 val_loss=0.0000 scale=4.0000 norm=112885.3993
[iter 200] loss=10.5467 val_loss=0.0000 scale=4.0000 norm=112889.3131
[iter 300] loss=9.8073 val_loss=0.0000 scale=8.0000 norm=225763.1192
[iter 400] loss=8.7803 val_loss=0.0000 scale=8.0000 norm=225762.6182
[iter 0] loss=1.2623 val_loss=0.0000 scale=1.0000 norm=0.9083
[iter 100] loss=1.0256 val_loss=0.0000 scale=2.0000 norm=1.6201
[iter 200] loss=0.9742 val_loss=0.0000 scale=2.0000 norm=1.6449
[iter 300] loss=0.9415 val_loss=0.0000 scale=2.0000 norm=1.6526
[iter 400] loss=0.9055 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 0] loss=1.4001 val_loss=0.0000 scale=1.0000 norm=0.9900
[iter 100] loss=1.0638 val_loss=0.0000 scale=2.0000 norm=1.7066
[iter 200] loss=0.9055 val_loss=0.0000 scale=2.0000 norm=1.6891
[iter 300] loss=0.7778 val_loss=0.0000 scale=4.0000 norm=3.3903
[iter 400] loss=0.6009 val_loss=0.0000 scale=4.0000 norm=3.3914
[iter 0] loss=13.4584 val_loss=0.0000 scale=2.0000 norm=61070.1560
[iter 100] loss=11.7692 val_loss=0.0000 scale=4.0000 norm=114199.5477
[iter 200] loss=10.6570 val_loss=0.0000 scale=4.0000 norm=114149.7727
[iter 300] loss=10.0071 val_loss=0.0000 scale=8.0000 norm=228267.6062
[iter 400] loss=9.1141 val_loss=0.0000 scale=8.0000 norm=228266.6391
[iter 0] loss=1.2548 val_loss=0.0000 scale=1.0000 norm=0.9050
[iter 100] loss=0.9591 val_loss=0.0000 scale=2.0000 norm=1.5440
[iter 200] loss=0.8639 val_loss=0.0000 scale=2.0000 norm=1.5702
[iter 300] loss=0.7755 val_loss=0.0000 scale=2.0000 norm=1.5797
[iter 400] loss=0.6511 val_loss=0.0000 scale=2.0000 norm=1.5809
[iter 0] loss=1.4331 val_loss=0.0000 scale=2.0000 norm=2.0154
[iter 100] loss=1.0984 val_loss=0.0000 scale=2.0000 norm=1.7385
[iter 200] loss=0.9489 val_loss=0.0000 scale=2.0000 norm=1.7313
[iter 300] loss=0.7975 val_loss=0.0000 scale=4.0000 norm=3.4795
[iter 400] loss=0.6342 val_loss=0.0000 scale=4.0000 norm=3.4810
[iter 0] loss=13.4601 val_loss=0.0000 scale=2.0000 norm=61418.5986
[iter 100] loss=11.8069 val_loss=0.0000 scale=4.0000 norm=114598.4364
[iter 200] loss=10.6810 val_loss=0.0000 scale=4.0000 norm=114500.4176
[iter 300] loss=9.9322 val_loss=0.0000 scale=8.0000 norm=228969.9981
[iter 400] loss=8.9419 val_loss=0.0000 scale=8.0000 norm=228969.4328
[iter 0] loss=1.2880 val_loss=0.0000 scale=1.0000 norm=0.9224
[iter 100] loss=1.0260 val_loss=0.0000 scale=2.0000 norm=1.6254
[iter 200] loss=0.9585 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 300] loss=0.9145 val_loss=0.0000 scale=4.0000 norm=3.3295
[iter 400] loss=0.8600 val_loss=0.0000 scale=4.0000 norm=3.3352
[iter 0] loss=1.3802 val_loss=0.0000 scale=2.0000 norm=1.9592
[iter 100] loss=1.0319 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 200] loss=0.8864 val_loss=0.0000 scale=2.0000 norm=1.6513
[iter 300] loss=0.7410 val_loss=0.0000 scale=4.0000 norm=3.3185
[iter 400] loss=0.5811 val_loss=0.0000 scale=4.0000 norm=3.3123
[iter 0] loss=13.4221 val_loss=0.0000 scale=2.0000 norm=52155.6181
[iter 100] loss=11.6563 val_loss=0.0000 scale=4.0000 norm=97735.4318
[iter 200] loss=10.5123 val_loss=0.0000 scale=4.0000 norm=97662.5177
[iter 300] loss=9.8168 val_loss=0.0000 scale=8.0000 norm=195296.7467
[iter 400] loss=8.8734 val_loss=0.0000 scale=8.0000 norm=195296.0068
[iter 0] loss=1.2546 val_loss=0.0000 scale=1.0000 norm=0.9029
[iter 100] loss=0.9909 val_loss=0.0000 scale=2.0000 norm=1.5743
[iter 200] loss=0.9372 val_loss=0.0000 scale=2.0000 norm=1.6102
[iter 300] loss=0.9044 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 400] loss=0.8662 val_loss=0.0000 scale=2.0000 norm=1.6230
[iter 0] loss=1.4090 val_loss=0.0000 scale=2.0000 norm=1.9893
[iter 100] loss=1.0855 val_loss=0.0000 scale=2.0000 norm=1.7272
[iter 200] loss=0.9379 val_loss=0.0000 scale=2.0000 norm=1.7194
[iter 300] loss=0.7983 val_loss=0.0000 scale=4.0000 norm=3.4556
[iter 400] loss=0.6259 val_loss=0.0000 scale=4.0000 norm=3.4598
[iter 0] loss=13.4597 val_loss=0.0000 scale=2.0000 norm=60278.9903
[iter 100] loss=11.7323 val_loss=0.0000 scale=4.0000 norm=112590.4231
[iter 200] loss=10.5837 val_loss=0.0000 scale=4.0000 norm=112565.3367
[iter 300] loss=9.8505 val_loss=0.0000 scale=8.0000 norm=225110.9563
[iter 400] loss=8.8474 val_loss=0.0000 scale=8.0000 norm=225110.3622
[iter 0] loss=1.2247 val_loss=0.0000 scale=1.0000 norm=0.8869
[iter 100] loss=0.9804 val_loss=0.0000 scale=2.0000 norm=1.5880
[iter 200] loss=0.9173 val_loss=0.0000 scale=2.0000 norm=1.6152
[iter 300] loss=0.8813 val_loss=0.0000 scale=2.0000 norm=1.6232
[iter 400] loss=0.8392 val_loss=0.0000 scale=4.0000 norm=3.2547
[iter 0] loss=1.3487 val_loss=0.0000 scale=1.0000 norm=0.9639
[iter 100] loss=1.0229 val_loss=0.0000 scale=2.0000 norm=1.6658
[iter 200] loss=0.8795 val_loss=0.0000 scale=2.0000 norm=1.6530
[iter 300] loss=0.7401 val_loss=0.0000 scale=4.0000 norm=3.3120
[iter 400] loss=0.5969 val_loss=0.0000 scale=4.0000 norm=3.3171
[iter 0] loss=11.8665 val_loss=0.0000 scale=2.0000 norm=18875.6366
[iter 100] loss=10.7208 val_loss=0.0000 scale=4.0000 norm=36718.8857
[iter 200] loss=9.8768 val_loss=0.0000 scale=4.0000 norm=36687.4094
[iter 300] loss=9.1956 val_loss=0.0000 scale=8.0000 norm=73365.2176
[iter 400] loss=8.2762 val_loss=0.0000 scale=8.0000 norm=73365.2933
[iter 0] loss=1.3199 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0393 val_loss=0.0000 scale=2.0000 norm=1.6406
[iter 200] loss=0.9455 val_loss=0.0000 scale=2.0000 norm=1.6648
[iter 300] loss=0.8594 val_loss=0.0000 scale=2.0000 norm=1.6753
[iter 400] loss=0.7202 val_loss=0.0000 scale=4.0000 norm=3.3516
[iter 0] loss=1.3652 val_loss=0.0000 scale=2.0000 norm=1.9423
[iter 100] loss=1.0713 val_loss=0.0000 scale=2.0000 norm=1.7075
[iter 200] loss=0.9353 val_loss=0.0000 scale=2.0000 norm=1.6947
[iter 300] loss=0.7862 val_loss=0.0000 scale=4.0000 norm=3.3942
[iter 400] loss=0.6192 val_loss=0.0000 scale=4.0000 norm=3.3960
[iter 0] loss=13.4598 val_loss=0.0000 scale=2.0000 norm=60783.1902
[iter 100] loss=11.7236 val_loss=0.0000 scale=4.0000 norm=113417.7737
[iter 200] loss=10.5789 val_loss=0.0000 scale=4.0000 norm=113350.9027
[iter 300] loss=9.8909 val_loss=0.0000 scale=8.0000 norm=226669.7642
[iter 400] loss=8.9354 val_loss=0.0000 scale=8.0000 norm=226668.4360
[iter 0] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=0.9089
[iter 100] loss=0.9991 val_loss=0.0000 scale=2.0000 norm=1.5953
[iter 200] loss=0.9002 val_loss=0.0000 scale=2.0000 norm=1.6149
[iter 300] loss=0.8212 val_loss=0.0000 scale=2.0000 norm=1.6196
[iter 400] loss=0.7158 val_loss=0.0000 scale=4.0000 norm=3.2481
[iter 0] loss=1.3664 val_loss=0.0000 scale=2.0000 norm=1.9434
[iter 100] loss=1.0258 val_loss=0.0000 scale=2.0000 norm=1.6699
[iter 200] loss=0.8700 val_loss=0.0000 scale=2.0000 norm=1.6676
[iter 300] loss=0.6939 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 400] loss=0.5150 val_loss=0.0000 scale=4.0000 norm=3.3588
[iter 0] loss=13.4224 val_loss=0.0000 scale=2.0000 norm=53011.7863
[iter 100] loss=11.7130 val_loss=0.0000 scale=4.0000 norm=98973.8160
[iter 200] loss=10.6096 val_loss=0.0000 scale=4.0000 norm=98866.4234
[iter 300] loss=9.9347 val_loss=0.0000 scale=8.0000 norm=197699.5280
[iter 400] loss=8.9316 val_loss=0.0000 scale=8.0000 norm=197697.9573
[iter 0] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=0.9430
[iter 100] loss=1.0456 val_loss=0.0000 scale=2.0000 norm=1.6500
[iter 200] loss=0.9287 val_loss=0.0000 scale=2.0000 norm=1.6640
[iter 300] loss=0.8276 val_loss=0.0000 scale=2.0000 norm=1.6714
[iter 400] loss=0.7133 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 0] loss=1.3545 val_loss=0.0000 scale=1.0000 norm=0.9669
[iter 100] loss=1.0444 val_loss=0.0000 scale=2.0000 norm=1.6716
[iter 200] loss=0.9054 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 300] loss=0.7925 val_loss=0.0000 scale=4.0000 norm=3.3367
[iter 400] loss=0.6349 val_loss=0.0000 scale=4.0000 norm=3.3260
[iter 0] loss=13.4579 val_loss=0.0000 scale=2.0000 norm=59724.5886
[iter 100] loss=11.7205 val_loss=0.0000 scale=4.0000 norm=112320.5992
[iter 200] loss=10.5610 val_loss=0.0000 scale=4.0000 norm=112275.6089
[iter 300] loss=9.8433 val_loss=0.0000 scale=8.0000 norm=224531.2015
[iter 400] loss=8.8908 val_loss=0.0000 scale=8.0000 norm=224530.6681
[iter 0] loss=1.2674 val_loss=0.0000 scale=1.0000 norm=0.9112
[iter 100] loss=1.0665 val_loss=0.0000 scale=2.0000 norm=1.6586
[iter 200] loss=1.0225 val_loss=0.0000 scale=2.0000 norm=1.6857
[iter 300] loss=0.9953 val_loss=0.0000 scale=2.0000 norm=1.6936
[iter 400] loss=0.9671 val_loss=0.0000 scale=2.0000 norm=1.6945
[iter 0] loss=1.3258 val_loss=0.0000 scale=1.0000 norm=0.9508
[iter 100] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6479
[iter 200] loss=0.8887 val_loss=0.0000 scale=2.0000 norm=1.6422
[iter 300] loss=0.7418 val_loss=0.0000 scale=4.0000 norm=3.3003
[iter 400] loss=0.5633 val_loss=0.0000 scale=4.0000 norm=3.3009
[iter 0] loss=13.4600 val_loss=0.0000 scale=2.0000 norm=60949.3788
[iter 100] loss=11.7265 val_loss=0.0000 scale=4.0000 norm=113792.5813
[iter 200] loss=10.5727 val_loss=0.0000 scale=4.0000 norm=113729.3607
[iter 300] loss=9.8593 val_loss=0.0000 scale=8.0000 norm=227438.7026
[iter 400] loss=8.8798 val_loss=0.0000 scale=8.0000 norm=227437.6799
[iter 0] loss=1.2773 val_loss=0.0000 scale=1.0000 norm=0.9179
[iter 100] loss=1.0535 val_loss=0.0000 scale=2.0000 norm=1.6381
[iter 200] loss=1.0050 val_loss=0.0000 scale=2.0000 norm=1.6641
[iter 300] loss=0.9746 val_loss=0.0000 scale=2.0000 norm=1.6728
[iter 400] loss=0.9470 val_loss=0.0000 scale=2.0000 norm=1.6755
[iter 0] loss=1.3749 val_loss=0.0000 scale=2.0000 norm=1.9544
[iter 100] loss=1.0609 val_loss=0.0000 scale=2.0000 norm=1.6883
[iter 200] loss=0.9339 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 300] loss=0.8363 val_loss=0.0000 scale=4.0000 norm=3.3603
[iter 400] loss=0.7120 val_loss=0.0000 scale=4.0000 norm=3.3612
[iter 0] loss=13.4605 val_loss=0.0000 scale=2.0000 norm=61975.1128
[iter 100] loss=11.7725 val_loss=0.0000 scale=4.0000 norm=114482.5712
[iter 200] loss=10.5263 val_loss=0.0000 scale=4.0000 norm=114410.4060
[iter 300] loss=9.8125 val_loss=0.0000 scale=8.0000 norm=228805.6938
[iter 400] loss=8.9168 val_loss=0.0000 scale=8.0000 norm=228805.2660
[iter 0] loss=1.2996 val_loss=0.0000 scale=1.0000 norm=0.9298
[iter 100] loss=0.9669 val_loss=0.0000 scale=2.0000 norm=1.5412
[iter 200] loss=0.9059 val_loss=0.0000 scale=2.0000 norm=1.5803
[iter 300] loss=0.8720 val_loss=0.0000 scale=2.0000 norm=1.5935
[iter 400] loss=0.8335 val_loss=0.0000 scale=2.0000 norm=1.5959
[iter 0] loss=1.4823 val_loss=0.0000 scale=1.0000 norm=1.0348
[iter 100] loss=1.1518 val_loss=0.0000 scale=2.0000 norm=1.7906
[iter 200] loss=1.0062 val_loss=0.0000 scale=2.0000 norm=1.7819
[iter 300] loss=0.8605 val_loss=0.0000 scale=4.0000 norm=3.5736
[iter 400] loss=0.6919 val_loss=0.0000 scale=4.0000 norm=3.5782
[iter 0] loss=13.4584 val_loss=0.0000 scale=2.0000 norm=59369.0193
[iter 100] loss=11.7086 val_loss=0.0000 scale=4.0000 norm=110209.1565
[iter 200] loss=10.4882 val_loss=0.0000 scale=4.0000 norm=110172.0634
[iter 300] loss=9.8165 val_loss=0.0000 scale=4.0000 norm=110160.0082
[iter 400] loss=8.8785 val_loss=0.0000 scale=8.0000 norm=220318.8209
[iter 0] loss=1.2810 val_loss=0.0000 scale=1.0000 norm=0.9178
[iter 100] loss=1.0395 val_loss=0.0000 scale=2.0000 norm=1.6154
[iter 200] loss=0.9883 val_loss=0.0000 scale=2.0000 norm=1.6466
[iter 300] loss=0.9581 val_loss=0.0000 scale=2.0000 norm=1.6570
[iter 400] loss=0.9146 val_loss=0.0000 scale=4.0000 norm=3.3192
[iter 0] loss=1.3920 val_loss=0.0000 scale=1.0000 norm=0.9856
[iter 100] loss=1.0360 val_loss=0.0000 scale=2.0000 norm=1.6852
[iter 200] loss=0.8835 val_loss=0.0000 scale=2.0000 norm=1.6824
[iter 300] loss=0.7421 val_loss=0.0000 scale=4.0000 norm=3.4070
[iter 400] loss=0.5811 val_loss=0.0000 scale=4.0000 norm=3.4111
[iter 0] loss=12.2351 val_loss=0.0000 scale=2.0000 norm=20907.5541
[iter 100] loss=10.6889 val_loss=0.0000 scale=4.0000 norm=40136.1259
[iter 200] loss=9.7421 val_loss=0.0000 scale=4.0000 norm=40014.7946
[iter 300] loss=8.9699 val_loss=0.0000 scale=8.0000 norm=80015.6493
[iter 400] loss=7.9311 val_loss=0.0000 scale=8.0000 norm=80015.6726
[iter 0] loss=1.2604 val_loss=0.0000 scale=1.0000 norm=0.9080
[iter 100] loss=1.0396 val_loss=0.0000 scale=2.0000 norm=1.6185
[iter 200] loss=0.9806 val_loss=0.0000 scale=2.0000 norm=1.6431
[iter 300] loss=0.9388 val_loss=0.0000 scale=2.0000 norm=1.6524
[iter 400] loss=0.8909 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 0] loss=1.4348 val_loss=0.0000 scale=1.0000 norm=1.0085
[iter 100] loss=1.0537 val_loss=0.0000 scale=2.0000 norm=1.6815
[iter 200] loss=0.9128 val_loss=0.0000 scale=2.0000 norm=1.6831
[iter 300] loss=0.7961 val_loss=0.0000 scale=4.0000 norm=3.3855
[iter 400] loss=0.6352 val_loss=0.0000 scale=4.0000 norm=3.3835
[iter 0] loss=13.4220 val_loss=0.0000 scale=2.0000 norm=51255.8325
[iter 100] loss=11.6535 val_loss=0.0000 scale=4.0000 norm=97081.9612
[iter 200] loss=10.5009 val_loss=0.0000 scale=4.0000 norm=96990.8031
[iter 300] loss=9.8252 val_loss=0.0000 scale=4.0000 norm=96979.3579
[iter 400] loss=8.8886 val_loss=0.0000 scale=8.0000 norm=193957.3311
[iter 0] loss=1.2519 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0091 val_loss=0.0000 scale=2.0000 norm=1.5848
[iter 200] loss=0.9486 val_loss=0.0000 scale=2.0000 norm=1.6151
[iter 300] loss=0.9111 val_loss=0.0000 scale=2.0000 norm=1.6235
[iter 400] loss=0.8745 val_loss=0.0000 scale=2.0000 norm=1.6256
[iter 0] loss=1.3513 val_loss=0.0000 scale=2.0000 norm=1.9289
[iter 100] loss=1.0483 val_loss=0.0000 scale=2.0000 norm=1.6896
[iter 200] loss=0.9055 val_loss=0.0000 scale=2.0000 norm=1.6866
[iter 300] loss=0.7701 val_loss=0.0000 scale=4.0000 norm=3.3983
[iter 400] loss=0.6097 val_loss=0.0000 scale=4.0000 norm=3.4031
[iter 0] loss=12.2411 val_loss=0.0000 scale=2.0000 norm=20949.1319
[iter 100] loss=10.7508 val_loss=0.0000 scale=4.0000 norm=40313.9928
[iter 200] loss=9.8077 val_loss=0.0000 scale=4.0000 norm=40284.3086
[iter 300] loss=8.9487 val_loss=0.0000 scale=8.0000 norm=80563.6673
[iter 400] loss=7.9011 val_loss=0.0000 scale=8.0000 norm=80563.7239
[iter 0] loss=1.2520 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0208 val_loss=0.0000 scale=2.0000 norm=1.5985
[iter 200] loss=0.9678 val_loss=0.0000 scale=2.0000 norm=1.6203
[iter 300] loss=0.9376 val_loss=0.0000 scale=2.0000 norm=1.6302
[iter 400] loss=0.9047 val_loss=0.0000 scale=4.0000 norm=3.2635
[iter 0] loss=1.3875 val_loss=0.0000 scale=2.0000 norm=1.9660
[iter 100] loss=1.0943 val_loss=0.0000 scale=2.0000 norm=1.7400
[iter 200] loss=0.9492 val_loss=0.0000 scale=2.0000 norm=1.7311
[iter 300] loss=0.7982 val_loss=0.0000 scale=4.0000 norm=3.4843
[iter 400] loss=0.6253 val_loss=0.0000 scale=4.0000 norm=3.4857
[iter 0] loss=13.4585 val_loss=0.0000 scale=2.0000 norm=59699.4861
[iter 100] loss=11.7243 val_loss=0.0000 scale=4.0000 norm=112383.1066
[iter 200] loss=10.5962 val_loss=0.0000 scale=4.0000 norm=112322.6895
[iter 300] loss=9.9372 val_loss=0.0000 scale=8.0000 norm=224610.1064
[iter 400] loss=8.9937 val_loss=0.0000 scale=8.0000 norm=224608.6344
[iter 0] loss=1.3217 val_loss=0.0000 scale=1.0000 norm=0.9423
[iter 100] loss=1.0525 val_loss=0.0000 scale=2.0000 norm=1.6494
[iter 200] loss=0.9420 val_loss=0.0000 scale=2.0000 norm=1.6656
[iter 300] loss=0.8489 val_loss=0.0000 scale=2.0000 norm=1.6730
[iter 400] loss=0.6962 val_loss=0.0000 scale=4.0000 norm=3.3487
[iter 0] loss=1.3780 val_loss=0.0000 scale=2.0000 norm=1.9562
[iter 100] loss=1.0629 val_loss=0.0000 scale=2.0000 norm=1.6921
[iter 200] loss=0.9179 val_loss=0.0000 scale=2.0000 norm=1.6804
[iter 300] loss=0.7729 val_loss=0.0000 scale=4.0000 norm=3.3729
[iter 400] loss=0.5977 val_loss=0.0000 scale=4.0000 norm=3.3684
[iter 0] loss=13.4456 val_loss=0.0000 scale=2.0000 norm=54036.4493
[iter 100] loss=11.6829 val_loss=0.0000 scale=4.0000 norm=103052.7395
[iter 200] loss=10.5642 val_loss=0.0000 scale=4.0000 norm=103001.5770
[iter 300] loss=9.8660 val_loss=0.0000 scale=8.0000 norm=205979.3985
[iter 400] loss=8.9107 val_loss=0.0000 scale=8.0000 norm=205978.9449
[iter 0] loss=1.2162 val_loss=0.0000 scale=1.0000 norm=0.8824
[iter 100] loss=0.9805 val_loss=0.0000 scale=2.0000 norm=1.5600
[iter 200] loss=0.9239 val_loss=0.0000 scale=2.0000 norm=1.5894
[iter 300] loss=0.8888 val_loss=0.0000 scale=2.0000 norm=1.6002
[iter 400] loss=0.8524 val_loss=0.0000 scale=2.0000 norm=1.6019
[iter 0] loss=1.4692 val_loss=0.0000 scale=1.0000 norm=1.0274
[iter 100] loss=1.1381 val_loss=0.0000 scale=2.0000 norm=1.7811
[iter 200] loss=0.9890 val_loss=0.0000 scale=2.0000 norm=1.7753
[iter 300] loss=0.8405 val_loss=0.0000 scale=4.0000 norm=3.5805
[iter 400] loss=0.6648 val_loss=0.0000 scale=4.0000 norm=3.5813
[iter 0] loss=13.4188 val_loss=0.0000 scale=2.0000 norm=48410.1325
[iter 100] loss=11.6398 val_loss=0.0000 scale=4.0000 norm=93895.2529
[iter 200] loss=10.5353 val_loss=0.0000 scale=4.0000 norm=93866.6138
[iter 300] loss=9.8481 val_loss=0.0000 scale=8.0000 norm=187704.6563
[iter 400] loss=8.8067 val_loss=0.0000 scale=8.0000 norm=187702.6359
[iter 0] loss=1.2359 val_loss=0.0000 scale=1.0000 norm=0.8946
[iter 100] loss=1.0081 val_loss=0.0000 scale=2.0000 norm=1.6007
[iter 200] loss=0.9508 val_loss=0.0000 scale=2.0000 norm=1.6332
[iter 300] loss=0.9163 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 400] loss=0.8724 val_loss=0.0000 scale=4.0000 norm=3.3012
[iter 0] loss=1.3174 val_loss=0.0000 scale=1.0000 norm=0.9463
[iter 100] loss=0.9979 val_loss=0.0000 scale=2.0000 norm=1.6343
[iter 200] loss=0.8561 val_loss=0.0000 scale=2.0000 norm=1.6229
[iter 300] loss=0.7334 val_loss=0.0000 scale=4.0000 norm=3.2633
[iter 400] loss=0.5687 val_loss=0.0000 scale=4.0000 norm=3.2615
[iter 0] loss=13.4591 val_loss=0.0000 scale=2.0000 norm=58691.7659
[iter 100] loss=11.7150 val_loss=0.0000 scale=4.0000 norm=112885.3993
[iter 200] loss=10.5467 val_loss=0.0000 scale=4.0000 norm=112889.3131
[iter 300] loss=9.8073 val_loss=0.0000 scale=8.0000 norm=225763.1192
[iter 400] loss=8.7803 val_loss=0.0000 scale=8.0000 norm=225762.6190
[iter 0] loss=1.3195 val_loss=0.0000 scale=1.0000 norm=0.9430
[iter 100] loss=1.0456 val_loss=0.0000 scale=2.0000 norm=1.6500
[iter 200] loss=0.9287 val_loss=0.0000 scale=2.0000 norm=1.6640
[iter 300] loss=0.8276 val_loss=0.0000 scale=2.0000 norm=1.6714
[iter 400] loss=0.7133 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 0] loss=1.3545 val_loss=0.0000 scale=1.0000 norm=0.9669
[iter 100] loss=1.0444 val_loss=0.0000 scale=2.0000 norm=1.6716
[iter 200] loss=0.9054 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 300] loss=0.7925 val_loss=0.0000 scale=4.0000 norm=3.3367
[iter 400] loss=0.6349 val_loss=0.0000 scale=4.0000 norm=3.3260
[iter 0] loss=13.4579 val_loss=0.0000 scale=2.0000 norm=59724.5886
[iter 100] loss=11.7205 val_loss=0.0000 scale=4.0000 norm=112320.5992
[iter 200] loss=10.5610 val_loss=0.0000 scale=4.0000 norm=112275.6089
[iter 300] loss=9.8433 val_loss=0.0000 scale=8.0000 norm=224531.2015
[iter 400] loss=8.8908 val_loss=0.0000 scale=8.0000 norm=224530.6688
[iter 0] loss=1.2787 val_loss=0.0000 scale=1.0000 norm=0.9183
[iter 100] loss=1.0388 val_loss=0.0000 scale=2.0000 norm=1.6088
[iter 200] loss=0.9812 val_loss=0.0000 scale=2.0000 norm=1.6380
[iter 300] loss=0.9444 val_loss=0.0000 scale=2.0000 norm=1.6477
[iter 400] loss=0.9116 val_loss=0.0000 scale=2.0000 norm=1.6467
[iter 0] loss=1.4284 val_loss=0.0000 scale=1.0000 norm=1.0051
[iter 100] loss=1.1043 val_loss=0.0000 scale=2.0000 norm=1.7448
[iter 200] loss=0.9532 val_loss=0.0000 scale=2.0000 norm=1.7452
[iter 300] loss=0.7918 val_loss=0.0000 scale=4.0000 norm=3.5306
[iter 400] loss=0.6143 val_loss=0.0000 scale=4.0000 norm=3.5303
[iter 0] loss=13.4578 val_loss=0.0000 scale=2.0000 norm=59874.0300
[iter 100] loss=11.7682 val_loss=0.0000 scale=4.0000 norm=113807.2969
[iter 200] loss=10.6345 val_loss=0.0000 scale=4.0000 norm=113764.4577
[iter 300] loss=9.9505 val_loss=0.0000 scale=8.0000 norm=227491.4587
[iter 400] loss=8.9503 val_loss=0.0000 scale=8.0000 norm=227489.7707
[iter 0] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=0.9402
[iter 100] loss=1.0712 val_loss=0.0000 scale=2.0000 norm=1.6565
[iter 200] loss=1.0059 val_loss=0.0000 scale=2.0000 norm=1.6890
[iter 300] loss=0.9645 val_loss=0.0000 scale=2.0000 norm=1.6976
[iter 400] loss=0.9212 val_loss=0.0000 scale=2.0000 norm=1.6999
[iter 0] loss=1.3368 val_loss=0.0000 scale=1.0000 norm=0.9551
[iter 100] loss=0.9926 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 200] loss=0.8379 val_loss=0.0000 scale=4.0000 norm=3.2160
[iter 300] loss=0.6700 val_loss=0.0000 scale=4.0000 norm=3.2428
[iter 400] loss=0.4795 val_loss=0.0000 scale=4.0000 norm=3.2434
[iter 0] loss=12.2579 val_loss=0.0000 scale=2.0000 norm=23878.1143
[iter 100] loss=10.8832 val_loss=0.0000 scale=4.0000 norm=44510.3285
[iter 200] loss=10.0020 val_loss=0.0000 scale=4.0000 norm=44407.1419
[iter 300] loss=9.2080 val_loss=0.0000 scale=8.0000 norm=88802.3639
[iter 400] loss=8.1812 val_loss=0.0000 scale=8.0000 norm=88802.3743
[iter 0] loss=1.3236 val_loss=0.0000 scale=1.0000 norm=0.9435
[iter 100] loss=1.0919 val_loss=0.0000 scale=2.0000 norm=1.6829
[iter 200] loss=1.0370 val_loss=0.0000 scale=2.0000 norm=1.7044
[iter 300] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.7137
[iter 400] loss=0.9685 val_loss=0.0000 scale=2.0000 norm=1.7151
[iter 0] loss=1.4085 val_loss=0.0000 scale=2.0000 norm=1.9890
[iter 100] loss=1.0951 val_loss=0.0000 scale=2.0000 norm=1.7289
[iter 200] loss=0.9685 val_loss=0.0000 scale=2.0000 norm=1.7223
[iter 300] loss=0.8496 val_loss=0.0000 scale=4.0000 norm=3.4546
[iter 400] loss=0.7102 val_loss=0.0000 scale=4.0000 norm=3.4534
[iter 0] loss=13.4454 val_loss=0.0000 scale=2.0000 norm=55593.6653
[iter 100] loss=11.6966 val_loss=0.0000 scale=4.0000 norm=104919.4176
[iter 200] loss=10.5726 val_loss=0.0000 scale=4.0000 norm=104840.7856
[iter 300] loss=9.8960 val_loss=0.0000 scale=8.0000 norm=209661.6764
[iter 400] loss=9.0138 val_loss=0.0000 scale=8.0000 norm=209660.7506
[iter 0] loss=1.2821 val_loss=0.0000 scale=1.0000 norm=0.9197
[iter 100] loss=1.0044 val_loss=0.0000 scale=2.0000 norm=1.5821
[iter 200] loss=0.9490 val_loss=0.0000 scale=2.0000 norm=1.6144
[iter 300] loss=0.9167 val_loss=0.0000 scale=2.0000 norm=1.6255
[iter 400] loss=0.8852 val_loss=0.0000 scale=2.0000 norm=1.6272
[iter 0] loss=1.3859 val_loss=0.0000 scale=2.0000 norm=1.9646
[iter 100] loss=0.9927 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 200] loss=0.8177 val_loss=0.0000 scale=2.0000 norm=1.6173
[iter 300] loss=0.6774 val_loss=0.0000 scale=4.0000 norm=3.2543
[iter 400] loss=0.4856 val_loss=0.0000 scale=4.0000 norm=3.2604
[iter 0] loss=11.2645 val_loss=0.0000 scale=2.0000 norm=13721.0140
[iter 100] loss=10.4744 val_loss=0.0000 scale=2.0000 norm=12814.1071
[iter 200] loss=9.6834 val_loss=0.0000 scale=4.0000 norm=25354.9553
[iter 300] loss=9.1416 val_loss=0.0000 scale=4.0000 norm=25343.9863
[iter 400] loss=8.2328 val_loss=0.0000 scale=8.0000 norm=50687.7006
[iter 0] loss=1.2674 val_loss=0.0000 scale=1.0000 norm=0.9112
[iter 100] loss=1.0665 val_loss=0.0000 scale=2.0000 norm=1.6586
[iter 200] loss=1.0225 val_loss=0.0000 scale=2.0000 norm=1.6857
[iter 300] loss=0.9953 val_loss=0.0000 scale=2.0000 norm=1.6936
[iter 400] loss=0.9671 val_loss=0.0000 scale=2.0000 norm=1.6945
[iter 0] loss=1.3258 val_loss=0.0000 scale=1.0000 norm=0.9508
[iter 100] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.6479
[iter 200] loss=0.8887 val_loss=0.0000 scale=2.0000 norm=1.6422
[iter 300] loss=0.7418 val_loss=0.0000 scale=4.0000 norm=3.3003
[iter 400] loss=0.5633 val_loss=0.0000 scale=4.0000 norm=3.3009
[iter 0] loss=13.4600 val_loss=0.0000 scale=2.0000 norm=60949.3788
[iter 100] loss=11.7265 val_loss=0.0000 scale=4.0000 norm=113792.5813
[iter 200] loss=10.5727 val_loss=0.0000 scale=4.0000 norm=113729.3607
[iter 300] loss=9.8593 val_loss=0.0000 scale=8.0000 norm=227438.7026
[iter 400] loss=8.8798 val_loss=0.0000 scale=8.0000 norm=227437.6800
[iter 0] loss=1.2773 val_loss=0.0000 scale=1.0000 norm=0.9179
[iter 100] loss=1.0535 val_loss=0.0000 scale=2.0000 norm=1.6381
[iter 200] loss=1.0050 val_loss=0.0000 scale=2.0000 norm=1.6641
[iter 300] loss=0.9746 val_loss=0.0000 scale=2.0000 norm=1.6728
[iter 400] loss=0.9470 val_loss=0.0000 scale=2.0000 norm=1.6755
[iter 0] loss=1.3749 val_loss=0.0000 scale=2.0000 norm=1.9544
[iter 100] loss=1.0609 val_loss=0.0000 scale=2.0000 norm=1.6883
[iter 200] loss=0.9339 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 300] loss=0.8363 val_loss=0.0000 scale=4.0000 norm=3.3603
[iter 400] loss=0.7120 val_loss=0.0000 scale=4.0000 norm=3.3612
[iter 0] loss=13.4605 val_loss=0.0000 scale=2.0000 norm=61975.1128
[iter 100] loss=11.7725 val_loss=0.0000 scale=4.0000 norm=114482.5712
[iter 200] loss=10.5263 val_loss=0.0000 scale=4.0000 norm=114410.4060
[iter 300] loss=9.8125 val_loss=0.0000 scale=8.0000 norm=228805.6938
[iter 400] loss=8.9168 val_loss=0.0000 scale=8.0000 norm=228805.2658
[iter 0] loss=1.2996 val_loss=0.0000 scale=1.0000 norm=0.9298
[iter 100] loss=0.9669 val_loss=0.0000 scale=2.0000 norm=1.5412
[iter 200] loss=0.9059 val_loss=0.0000 scale=2.0000 norm=1.5803
[iter 300] loss=0.8720 val_loss=0.0000 scale=2.0000 norm=1.5935
[iter 400] loss=0.8335 val_loss=0.0000 scale=2.0000 norm=1.5959
[iter 0] loss=1.4823 val_loss=0.0000 scale=1.0000 norm=1.0348
[iter 100] loss=1.1518 val_loss=0.0000 scale=2.0000 norm=1.7906
[iter 200] loss=1.0062 val_loss=0.0000 scale=2.0000 norm=1.7819
[iter 300] loss=0.8605 val_loss=0.0000 scale=4.0000 norm=3.5736
[iter 400] loss=0.6919 val_loss=0.0000 scale=4.0000 norm=3.5782
[iter 0] loss=13.4584 val_loss=0.0000 scale=2.0000 norm=59369.0193
[iter 100] loss=11.7086 val_loss=0.0000 scale=4.0000 norm=110209.1565
[iter 200] loss=10.4882 val_loss=0.0000 scale=4.0000 norm=110172.0634
[iter 300] loss=9.8165 val_loss=0.0000 scale=4.0000 norm=110160.0082
[iter 400] loss=8.8785 val_loss=0.0000 scale=8.0000 norm=220318.8224
[iter 0] loss=1.2520 val_loss=0.0000 scale=1.0000 norm=0.9026
[iter 100] loss=1.0208 val_loss=0.0000 scale=2.0000 norm=1.5985
[iter 200] loss=0.9678 val_loss=0.0000 scale=2.0000 norm=1.6203
[iter 300] loss=0.9376 val_loss=0.0000 scale=2.0000 norm=1.6302
[iter 400] loss=0.9047 val_loss=0.0000 scale=4.0000 norm=3.2635
[iter 0] loss=1.3875 val_loss=0.0000 scale=2.0000 norm=1.9660
[iter 100] loss=1.0943 val_loss=0.0000 scale=2.0000 norm=1.7400
[iter 200] loss=0.9492 val_loss=0.0000 scale=2.0000 norm=1.7311
[iter 300] loss=0.7982 val_loss=0.0000 scale=4.0000 norm=3.4843
[iter 400] loss=0.6253 val_loss=0.0000 scale=4.0000 norm=3.4857
[iter 0] loss=13.4585 val_loss=0.0000 scale=2.0000 norm=59699.4861
[iter 100] loss=11.7243 val_loss=0.0000 scale=4.0000 norm=112383.1066
[iter 200] loss=10.5962 val_loss=0.0000 scale=4.0000 norm=112322.6895
[iter 300] loss=9.9372 val_loss=0.0000 scale=8.0000 norm=224610.1064
[iter 400] loss=8.9937 val_loss=0.0000 scale=8.0000 norm=224608.6347
[iter 0] loss=1.2247 val_loss=0.0000 scale=1.0000 norm=0.8869
[iter 100] loss=0.9804 val_loss=0.0000 scale=2.0000 norm=1.5880
[iter 200] loss=0.9173 val_loss=0.0000 scale=2.0000 norm=1.6152
[iter 300] loss=0.8813 val_loss=0.0000 scale=2.0000 norm=1.6232
[iter 400] loss=0.8392 val_loss=0.0000 scale=4.0000 norm=3.2547
[iter 0] loss=1.3487 val_loss=0.0000 scale=1.0000 norm=0.9639
[iter 100] loss=1.0229 val_loss=0.0000 scale=2.0000 norm=1.6658
[iter 200] loss=0.8795 val_loss=0.0000 scale=2.0000 norm=1.6530
[iter 300] loss=0.7401 val_loss=0.0000 scale=4.0000 norm=3.3120
[iter 400] loss=0.5969 val_loss=0.0000 scale=4.0000 norm=3.3171
[iter 0] loss=11.8665 val_loss=0.0000 scale=2.0000 norm=18875.6366
[iter 100] loss=10.7208 val_loss=0.0000 scale=4.0000 norm=36718.8857
[iter 200] loss=9.8768 val_loss=0.0000 scale=4.0000 norm=36687.4094
[iter 300] loss=9.1956 val_loss=0.0000 scale=8.0000 norm=73365.2176
[iter 400] loss=8.2762 val_loss=0.0000 scale=8.0000 norm=73365.2934
[iter 0] loss=1.3199 val_loss=0.0000 scale=1.0000 norm=0.9413
[iter 100] loss=1.0393 val_loss=0.0000 scale=2.0000 norm=1.6406
[iter 200] loss=0.9455 val_loss=0.0000 scale=2.0000 norm=1.6648
[iter 300] loss=0.8594 val_loss=0.0000 scale=2.0000 norm=1.6753
[iter 400] loss=0.7202 val_loss=0.0000 scale=4.0000 norm=3.3516
[iter 0] loss=1.3652 val_loss=0.0000 scale=2.0000 norm=1.9423
[iter 100] loss=1.0713 val_loss=0.0000 scale=2.0000 norm=1.7075
[iter 200] loss=0.9353 val_loss=0.0000 scale=2.0000 norm=1.6947
[iter 300] loss=0.7862 val_loss=0.0000 scale=4.0000 norm=3.3942
[iter 400] loss=0.6192 val_loss=0.0000 scale=4.0000 norm=3.3960
[iter 0] loss=13.4598 val_loss=0.0000 scale=2.0000 norm=60783.1902
[iter 100] loss=11.7236 val_loss=0.0000 scale=4.0000 norm=113417.7737
[iter 200] loss=10.5789 val_loss=0.0000 scale=4.0000 norm=113350.9027
[iter 300] loss=9.8909 val_loss=0.0000 scale=8.0000 norm=226669.7642
[iter 400] loss=8.9354 val_loss=0.0000 scale=8.0000 norm=226668.4360
[iter 0] loss=1.2647 val_loss=0.0000 scale=1.0000 norm=0.9089
[iter 100] loss=0.9991 val_loss=0.0000 scale=2.0000 norm=1.5953
[iter 200] loss=0.9002 val_loss=0.0000 scale=2.0000 norm=1.6149
[iter 300] loss=0.8212 val_loss=0.0000 scale=2.0000 norm=1.6196
[iter 400] loss=0.7158 val_loss=0.0000 scale=4.0000 norm=3.2481
[iter 0] loss=1.3664 val_loss=0.0000 scale=2.0000 norm=1.9434
[iter 100] loss=1.0258 val_loss=0.0000 scale=2.0000 norm=1.6699
[iter 200] loss=0.8700 val_loss=0.0000 scale=2.0000 norm=1.6676
[iter 300] loss=0.6939 val_loss=0.0000 scale=4.0000 norm=3.3550
[iter 400] loss=0.5150 val_loss=0.0000 scale=4.0000 norm=3.3588
[iter 0] loss=13.4224 val_loss=0.0000 scale=2.0000 norm=53011.7863
[iter 100] loss=11.7130 val_loss=0.0000 scale=4.0000 norm=98973.8160
[iter 200] loss=10.6096 val_loss=0.0000 scale=4.0000 norm=98866.4234
[iter 300] loss=9.9347 val_loss=0.0000 scale=8.0000 norm=197699.5280
[iter 400] loss=8.9316 val_loss=0.0000 scale=8.0000 norm=197697.9576
[iter 0] loss=1.2821 val_loss=0.0000 scale=1.0000 norm=0.9197
[iter 100] loss=1.0044 val_loss=0.0000 scale=2.0000 norm=1.5821
[iter 200] loss=0.9490 val_loss=0.0000 scale=2.0000 norm=1.6144
[iter 300] loss=0.9167 val_loss=0.0000 scale=2.0000 norm=1.6255
[iter 400] loss=0.8852 val_loss=0.0000 scale=2.0000 norm=1.6272
[iter 0] loss=1.3859 val_loss=0.0000 scale=2.0000 norm=1.9646
[iter 100] loss=0.9927 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 200] loss=0.8177 val_loss=0.0000 scale=2.0000 norm=1.6173
[iter 300] loss=0.6774 val_loss=0.0000 scale=4.0000 norm=3.2543
[iter 400] loss=0.4856 val_loss=0.0000 scale=4.0000 norm=3.2604
[iter 0] loss=11.2645 val_loss=0.0000 scale=2.0000 norm=13721.0140
[iter 100] loss=10.4744 val_loss=0.0000 scale=2.0000 norm=12814.1071
[iter 200] loss=9.6834 val_loss=0.0000 scale=4.0000 norm=25354.9553
[iter 300] loss=9.1416 val_loss=0.0000 scale=4.0000 norm=25343.9863
[iter 400] loss=8.2328 val_loss=0.0000 scale=8.0000 norm=50687.7006
[iter 0] loss=1.2787 val_loss=0.0000 scale=1.0000 norm=0.9183
[iter 100] loss=1.0388 val_loss=0.0000 scale=2.0000 norm=1.6088
[iter 200] loss=0.9812 val_loss=0.0000 scale=2.0000 norm=1.6380
[iter 300] loss=0.9444 val_loss=0.0000 scale=2.0000 norm=1.6477
[iter 400] loss=0.9116 val_loss=0.0000 scale=2.0000 norm=1.6467
[iter 0] loss=1.4284 val_loss=0.0000 scale=1.0000 norm=1.0051
[iter 100] loss=1.1043 val_loss=0.0000 scale=2.0000 norm=1.7448
[iter 200] loss=0.9532 val_loss=0.0000 scale=2.0000 norm=1.7452
[iter 300] loss=0.7918 val_loss=0.0000 scale=4.0000 norm=3.5306
[iter 400] loss=0.6143 val_loss=0.0000 scale=4.0000 norm=3.5303
[iter 0] loss=13.4578 val_loss=0.0000 scale=2.0000 norm=59874.0300
[iter 100] loss=11.7682 val_loss=0.0000 scale=4.0000 norm=113807.2969
[iter 200] loss=10.6345 val_loss=0.0000 scale=4.0000 norm=113764.4577
[iter 300] loss=9.9505 val_loss=0.0000 scale=8.0000 norm=227491.4587
[iter 400] loss=8.9503 val_loss=0.0000 scale=8.0000 norm=227489.7704
[iter 0] loss=1.3192 val_loss=0.0000 scale=1.0000 norm=0.9402
[iter 100] loss=1.0712 val_loss=0.0000 scale=2.0000 norm=1.6565
[iter 200] loss=1.0059 val_loss=0.0000 scale=2.0000 norm=1.6890
[iter 300] loss=0.9645 val_loss=0.0000 scale=2.0000 norm=1.6976
[iter 400] loss=0.9212 val_loss=0.0000 scale=2.0000 norm=1.6999
[iter 0] loss=1.3368 val_loss=0.0000 scale=1.0000 norm=0.9551
[iter 100] loss=0.9926 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 200] loss=0.8379 val_loss=0.0000 scale=4.0000 norm=3.2160
[iter 300] loss=0.6700 val_loss=0.0000 scale=4.0000 norm=3.2428
[iter 400] loss=0.4795 val_loss=0.0000 scale=4.0000 norm=3.2434
[iter 0] loss=12.2579 val_loss=0.0000 scale=2.0000 norm=23878.1143
[iter 100] loss=10.8832 val_loss=0.0000 scale=4.0000 norm=44510.3285
[iter 200] loss=10.0020 val_loss=0.0000 scale=4.0000 norm=44407.1419
[iter 300] loss=9.2080 val_loss=0.0000 scale=8.0000 norm=88802.3639
[iter 400] loss=8.1812 val_loss=0.0000 scale=8.0000 norm=88802.3742
[iter 0] loss=1.3236 val_loss=0.0000 scale=1.0000 norm=0.9435
[iter 100] loss=1.0919 val_loss=0.0000 scale=2.0000 norm=1.6829
[iter 200] loss=1.0370 val_loss=0.0000 scale=2.0000 norm=1.7044
[iter 300] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.7137
[iter 400] loss=0.9685 val_loss=0.0000 scale=2.0000 norm=1.7151
[iter 0] loss=1.4085 val_loss=0.0000 scale=2.0000 norm=1.9890
[iter 100] loss=1.0951 val_loss=0.0000 scale=2.0000 norm=1.7289
[iter 200] loss=0.9685 val_loss=0.0000 scale=2.0000 norm=1.7223
[iter 300] loss=0.8496 val_loss=0.0000 scale=4.0000 norm=3.4546
[iter 400] loss=0.7102 val_loss=0.0000 scale=4.0000 norm=3.4534
[iter 0] loss=13.4454 val_loss=0.0000 scale=2.0000 norm=55593.6653
[iter 100] loss=11.6966 val_loss=0.0000 scale=4.0000 norm=104919.4176
[iter 200] loss=10.5726 val_loss=0.0000 scale=4.0000 norm=104840.7856
[iter 300] loss=9.8960 val_loss=0.0000 scale=8.0000 norm=209661.6764
[iter 400] loss=9.0138 val_loss=0.0000 scale=8.0000 norm=209660.7502
[iter 0] loss=1.2546 val_loss=0.0000 scale=1.0000 norm=0.9029
[iter 100] loss=0.9909 val_loss=0.0000 scale=2.0000 norm=1.5743
[iter 200] loss=0.9372 val_loss=0.0000 scale=2.0000 norm=1.6102
[iter 300] loss=0.9044 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 400] loss=0.8662 val_loss=0.0000 scale=2.0000 norm=1.6230
[iter 0] loss=1.4090 val_loss=0.0000 scale=2.0000 norm=1.9893
[iter 100] loss=1.0855 val_loss=0.0000 scale=2.0000 norm=1.7272
[iter 200] loss=0.9379 val_loss=0.0000 scale=2.0000 norm=1.7194
[iter 300] loss=0.7983 val_loss=0.0000 scale=4.0000 norm=3.4556
[iter 400] loss=0.6259 val_loss=0.0000 scale=4.0000 norm=3.4598
[iter 0] loss=13.4597 val_loss=0.0000 scale=2.0000 norm=60278.9903
[iter 100] loss=11.7323 val_loss=0.0000 scale=4.0000 norm=112590.4231
[iter 200] loss=10.5837 val_loss=0.0000 scale=4.0000 norm=112565.3367
[iter 300] loss=9.8505 val_loss=0.0000 scale=8.0000 norm=225110.9563
[iter 400] loss=8.8474 val_loss=0.0000 scale=8.0000 norm=225110.3614
[iter 0] loss=1.3217 val_loss=0.0000 scale=1.0000 norm=0.9423
[iter 100] loss=1.0525 val_loss=0.0000 scale=2.0000 norm=1.6494
[iter 200] loss=0.9420 val_loss=0.0000 scale=2.0000 norm=1.6656
[iter 300] loss=0.8489 val_loss=0.0000 scale=2.0000 norm=1.6730
[iter 400] loss=0.6962 val_loss=0.0000 scale=4.0000 norm=3.3487
[iter 0] loss=1.3780 val_loss=0.0000 scale=2.0000 norm=1.9562
[iter 100] loss=1.0629 val_loss=0.0000 scale=2.0000 norm=1.6921
[iter 200] loss=0.9179 val_loss=0.0000 scale=2.0000 norm=1.6804
[iter 300] loss=0.7729 val_loss=0.0000 scale=4.0000 norm=3.3729
[iter 400] loss=0.5977 val_loss=0.0000 scale=4.0000 norm=3.3684
[iter 0] loss=13.4456 val_loss=0.0000 scale=2.0000 norm=54036.4493
[iter 100] loss=11.6829 val_loss=0.0000 scale=4.0000 norm=103052.7395
[iter 200] loss=10.5642 val_loss=0.0000 scale=4.0000 norm=103001.5770
[iter 300] loss=9.8660 val_loss=0.0000 scale=8.0000 norm=205979.3985
[iter 400] loss=8.9107 val_loss=0.0000 scale=8.0000 norm=205978.9456
[iter 0] loss=1.2162 val_loss=0.0000 scale=1.0000 norm=0.8824
[iter 100] loss=0.9805 val_loss=0.0000 scale=2.0000 norm=1.5600
[iter 200] loss=0.9239 val_loss=0.0000 scale=2.0000 norm=1.5894
[iter 300] loss=0.8888 val_loss=0.0000 scale=2.0000 norm=1.6002
[iter 400] loss=0.8524 val_loss=0.0000 scale=2.0000 norm=1.6019
[iter 0] loss=1.4692 val_loss=0.0000 scale=1.0000 norm=1.0274
[iter 100] loss=1.1381 val_loss=0.0000 scale=2.0000 norm=1.7811
[iter 200] loss=0.9890 val_loss=0.0000 scale=2.0000 norm=1.7753
[iter 300] loss=0.8405 val_loss=0.0000 scale=4.0000 norm=3.5805
[iter 400] loss=0.6648 val_loss=0.0000 scale=4.0000 norm=3.5813
[iter 0] loss=13.4188 val_loss=0.0000 scale=2.0000 norm=48410.1325
[iter 100] loss=11.6398 val_loss=0.0000 scale=4.0000 norm=93895.2529
[iter 200] loss=10.5353 val_loss=0.0000 scale=4.0000 norm=93866.6138
[iter 300] loss=9.8481 val_loss=0.0000 scale=8.0000 norm=187704.6563
[iter 400] loss=8.8067 val_loss=0.0000 scale=8.0000 norm=187702.6329
[iter 0] loss=1.2764 val_loss=0.0000 scale=1.0000 norm=0.9169
[iter 100] loss=1.0389 val_loss=0.0000 scale=2.0000 norm=1.6330
[iter 200] loss=0.9729 val_loss=0.0000 scale=2.0000 norm=1.6608
[iter 300] loss=0.9286 val_loss=0.0000 scale=2.0000 norm=1.6643
[iter 400] loss=0.8903 val_loss=0.0000 scale=2.0000 norm=1.6666
[iter 0] loss=1.3441 val_loss=0.0000 scale=2.0000 norm=1.9210
[iter 100] loss=1.0008 val_loss=0.0000 scale=2.0000 norm=1.6328
[iter 200] loss=0.8411 val_loss=0.0000 scale=2.0000 norm=1.6210
[iter 300] loss=0.6953 val_loss=0.0000 scale=4.0000 norm=3.2624
[iter 400] loss=0.5190 val_loss=0.0000 scale=4.0000 norm=3.2592
[iter 0] loss=12.3731 val_loss=0.0000 scale=2.0000 norm=26534.1702
[iter 100] loss=10.9088 val_loss=0.0000 scale=4.0000 norm=51067.2032
[iter 200] loss=9.9399 val_loss=0.0000 scale=4.0000 norm=50980.4812
[iter 300] loss=9.3061 val_loss=0.0000 scale=8.0000 norm=101940.9647
[iter 400] loss=8.2673 val_loss=0.0000 scale=8.0000 norm=101940.4668

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n06>
Subject: Job 315453: <mordred_NGB_Robust Scaler_multimodal Rh_20250123> in cluster <Hazel> Done

Job <mordred_NGB_Robust Scaler_multimodal Rh_20250123> was submitted from host <c037n01> by user <sdehgha2> in cluster <Hazel> at Thu Jan 23 14:12:10 2025
Job was executed on host(s) <6*c202n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Jan 23 14:12:11 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Jan 23 14:12:11 2025
Terminated at Thu Jan 23 14:31:26 2025
Results reported at Thu Jan 23 14:31:26 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
          
#BSUB -n 6
#BSUB -W 40:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "mordred_NGB_Robust Scaler_multimodal Rh_20250123"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250123.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250123.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_only.py mordred              --regressor_type NGB              --target "multimodal Rh"              --oligo_type "RRU Dimer"              --transform_type "Robust Scaler"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   5471.14 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.00 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               13.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   1157 sec.
    Turnaround time :                            1156 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/mordred_NGB_Robust Scaler_multimodal Rh_20250123.err> for stderr output of this job.

