


OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.031741510716110304), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0018373490398805416), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1510), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0018373490398805416), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1510), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.004866083656078861), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1446), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.005375310315577804), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 216), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002475020409163446), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 600), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002746874203004447), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 848), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.06013114684311058), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 299), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.016932395983188887), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1850), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0031914960255496794), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 604), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00444568694857147), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 351), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.09922697533722438), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 60), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.08066062278353024), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 77), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.03252108800594495), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 153), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.009067096135225546), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 766), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.07052484699266225), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 59), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.008356668479529508), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 529), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002456264065330874), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1606), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.09939793013929618), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 161), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02185286030861459), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 89), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02808287184621455), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 102), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.007868614520574049), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02951873479954331), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 97), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02821857380742718), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 98), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.023455588662312368), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 132), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.1), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 53), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.020419414951108602), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.00789537938371002), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 267), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0021399160816263966), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 901), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.013935277771160584), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 189), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.012384420043302933), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 181), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.09954460009279503), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 53), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.007898999231098368), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 569), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.1), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.03277951825394108), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 179), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__verbose', False)])


Average scores:	 r2: [ 1.380000e-01  2.830000e-01 -2.690456e+04]±[1.19000000e-01 1.20000000e-01 1.56841616e+05]
[array([ 1.37791592e-01,  2.83087915e-01, -2.69045603e+04]), array([8.07455829e+00, 1.33525021e+02, 4.63597796e+05]), array([5.18423044e+00, 8.58684857e+01, 6.39475278e+04])]
Dimer_scaler
Filename: (ECFP3.count.512-PDI-Mw-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Dimer_scaler/(ECFP3.count.512-PDI-Mw-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Dimer_scaler/(ECFP3.count.512-PDI-Mw-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/Dimer_scaler/(ECFP3.count.512-PDI-Mw-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c017n01>
Subject: Job 310888: <NGB_multimodal Rh_ECFP_Robust Scaler_Dimer_20250122> in cluster <Hazel> Done

Job <NGB_multimodal Rh_ECFP_Robust Scaler_Dimer_20250122> was submitted from host <c004n03> by user <sdehgha2> in cluster <Hazel> at Wed Jan 22 17:39:35 2025
Job was executed on host(s) <6*c017n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Jan 22 17:39:36 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed Jan 22 17:39:36 2025
Terminated at Thu Jan 23 07:23:49 2025
Results reported at Thu Jan 23 07:23:49 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 6
#BSUB -W 72:05
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "NGB_multimodal Rh_ECFP_Robust Scaler_Dimer_20250122"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/NGB_multimodal Rh_ECFP_Robust Scaler_Dimer_3_count_20250122.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/NGB_multimodal Rh_ECFP_Robust Scaler_Dimer_3_count_20250122.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py --target_features "multimodal Rh"                                       --representation "ECFP"                                       --regressor_type "NGB"                                       --transform_type "Robust Scaler"                                       --radius "3"                                       --vector "count"                                       --oligomer_representation "Dimer"                                       --numerical_feats 'Mn (g/mol)' 'PDI' 'Mw (g/mol)' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' "polymer dP" "polymer dD" "polymer dH" 'solvent dP' 'solvent dD' 'solvent dH'                                       --columns_to_impute "PDI" "Temperature SANS/SLS/DLS/SEC (K)" "Concentration (mg/ml)"                                       --special_impute 'Mw (g/mol)'                                       --imputer mean 




------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   229455.00 sec.
    Max Memory :                                 8 GB
    Average Memory :                             7.52 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               8.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   49456 sec.
    Turnaround time :                            49454 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/NGB_multimodal Rh_ECFP_Robust Scaler_Dimer_3_count_20250122.err> for stderr output of this job.

