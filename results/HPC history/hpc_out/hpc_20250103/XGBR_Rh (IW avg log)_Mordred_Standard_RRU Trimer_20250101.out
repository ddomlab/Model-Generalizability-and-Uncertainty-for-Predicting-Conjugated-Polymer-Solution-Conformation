


OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002282994454833569), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04180763826911515), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 56), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.030757825682949366), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 96), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07248212786795039), ('regressor__regressor__max_depth', 37), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01435998048209492), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 88), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00813608923977813), ('regressor__regressor__max_depth', 245), ('regressor__regressor__n_estimators', 58), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0034880979650613072), ('regressor__regressor__max_depth', 3258), ('regressor__regressor__n_estimators', 367), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006010052302291517), ('regressor__regressor__max_depth', 1220), ('regressor__regressor__n_estimators', 80), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005302819517808), ('regressor__regressor__max_depth', 865), ('regressor__regressor__n_estimators', 885), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0033024094583670076), ('regressor__regressor__max_depth', 50), ('regressor__regressor__n_estimators', 252), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010065324516346358), ('regressor__regressor__max_depth', 9659), ('regressor__regressor__n_estimators', 759), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005495145051857602), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 944), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008300107246139594), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 628), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03814621917434846), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 75), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014001562839408137), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 101), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013121340277331396), ('regressor__regressor__max_depth', 3473), ('regressor__regressor__n_estimators', 98), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014163769170436746), ('regressor__regressor__max_depth', 58), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.016701944159378843), ('regressor__regressor__max_depth', 22), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014154835945955147), ('regressor__regressor__max_depth', 2080), ('regressor__regressor__n_estimators', 65), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006003298369531399), ('regressor__regressor__max_depth', 157), ('regressor__regressor__n_estimators', 133), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002265570386350473), ('regressor__regressor__max_depth', 181), ('regressor__regressor__n_estimators', 579), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026660791642048664), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0428839069891835), ('regressor__regressor__max_depth', 236), ('regressor__regressor__n_estimators', 102), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.051504519044183704), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07135859495465752), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 107), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0018272799337950574), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 701), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1545), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010137386588904241), ('regressor__regressor__max_depth', 88), ('regressor__regressor__n_estimators', 711), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01382346654889201), ('regressor__regressor__max_depth', 1668), ('regressor__regressor__n_estimators', 64), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01687856627865949), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 63), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0019600809544327467), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 376), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002959468265267479), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 362), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.010163852873865814), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 262), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001126894525522175), ('regressor__regressor__max_depth', 1489), ('regressor__regressor__n_estimators', 457), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 133), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.33±0.35	 r2: -37.37±174.96
RRU Trimer_scaler
Filename: (Mordred-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_mean_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Trimer_scaler/(Mordred-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_mean_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Trimer_scaler/(Mordred-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_mean_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/RRU Trimer_scaler/(Mordred-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_mean_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n01>
Subject: Job 109720: <XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer> in cluster <Hazel> Done

Job <XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer> was submitted from host <c207n01> by user <sdehgha2> in cluster <Hazel> at Wed Jan  1 13:39:58 2025
Job was executed on host(s) <2*c200n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Jan  1 13:40:55 2025
                            <2*c200n02>
                            <2*c200n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed Jan  1 13:40:55 2025
Terminated at Thu Jan  2 06:25:37 2025
Results reported at Thu Jan  2 06:25:37 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 6
#BSUB -W 72:05
#BSUB -R span[ptile=2]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer_20250101.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer_20250101.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py --target_features "Rh (IW avg log)"                                       --representation "Mordred"                                       --regressor_type "XGBR"                                       --transform_type "Standard"                                       --oligomer_representation "RRU Trimer"                                       --numerical_feats 'Mn (g/mol)' 'PDI' 'Mw (g/mol)' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' 'solvent dP' 'solvent dD' 'solvent dH'                                       --columns_to_impute "PDI" "Temperature SANS/SLS/DLS/SEC (K)" "Concentration (mg/ml)"                                       --special_impute 'Mw (g/mol)'                                       --imputer mean 




------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   163561.00 sec.
    Max Memory :                                 7 GB
    Average Memory :                             5.80 GB
    Total Requested Memory :                     48.00 GB
    Delta Memory :                               41.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   60309 sec.
    Turnaround time :                            60339 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer_20250101.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n02>
Subject: Job 138848: <XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer> in cluster <Hazel> Exited

Job <XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Thu Jan  2 11:40:50 2025
Job was executed on host(s) <2*c203n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Thu Jan  2 11:40:51 2025
                            <2*c203n04>
                            <2*c203n08>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Thu Jan  2 11:40:51 2025
Terminated at Thu Jan  2 11:41:27 2025
Results reported at Thu Jan  2 11:41:27 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 6
#BSUB -W 72:05
#BSUB -R span[ptile=2]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer_20250101.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer_20250101.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py --target_features "Rh (IW avg log)"                                       --representation "Mordred"                                       --regressor_type "XGBR"                                       --transform_type "Standard"                                       --oligomer_representation "RRU Trimer"                                       --numerical_feats 'Mn (g/mol)' 'PDI' 'Mw (g/mol)' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' 'solvent dP' 'solvent dD' 'solvent dH'                                       --columns_to_impute "PDI" "Temperature SANS/SLS/DLS/SEC (K)" "Concentration (mg/ml)"                                       --special_impute 'Mw (g/mol)'                                       --imputer mean 




------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   2.77 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     48.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   44 sec.
    Turnaround time :                            37 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_Mordred_Standard_RRU Trimer_20250101.err> for stderr output of this job.

