


OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04561410334381917), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 93), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04287419909032855), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02152964762891338), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 130), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005531342693771952), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1120), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02625064115261706), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 53), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00813608923977813), ('regressor__regressor__max_depth', 245), ('regressor__regressor__n_estimators', 58), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015241617928832547), ('regressor__regressor__max_depth', 7675), ('regressor__regressor__n_estimators', 1021), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006013332157840858), ('regressor__regressor__max_depth', 1211), ('regressor__regressor__n_estimators', 80), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006840357881031932), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 730), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0033088778453911963), ('regressor__regressor__max_depth', 2210), ('regressor__regressor__n_estimators', 252), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010056496020280788), ('regressor__regressor__max_depth', 2790), ('regressor__regressor__n_estimators', 759), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.012878266949459621), ('regressor__regressor__max_depth', 6298), ('regressor__regressor__n_estimators', 422), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02597723607667774), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0565561477453766), ('regressor__regressor__max_depth', 27), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01399836121259343), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 97), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013121340277331396), ('regressor__regressor__max_depth', 3473), ('regressor__regressor__n_estimators', 98), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013978993462480093), ('regressor__regressor__max_depth', 653), ('regressor__regressor__n_estimators', 60), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01670543226413511), ('regressor__regressor__max_depth', 22), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0141553998318881), ('regressor__regressor__max_depth', 2071), ('regressor__regressor__n_estimators', 65), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.016651324642419148), ('regressor__regressor__max_depth', 22), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0017353431141977193), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 775), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02610378859700933), ('regressor__regressor__max_depth', 34), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0428839069891835), ('regressor__regressor__max_depth', 236), ('regressor__regressor__n_estimators', 102), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013351464778181547), ('regressor__regressor__max_depth', 943), ('regressor__regressor__n_estimators', 197), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.015267469511518165), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 134), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008539970725353976), ('regressor__regressor__max_depth', 583), ('regressor__regressor__n_estimators', 138), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03263507027006712), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 65), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001026127989394692), ('regressor__regressor__max_depth', 38), ('regressor__regressor__n_estimators', 697), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01386754353744942), ('regressor__regressor__max_depth', 1687), ('regressor__regressor__n_estimators', 64), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01835217136361782), ('regressor__regressor__max_depth', 791), ('regressor__regressor__n_estimators', 58), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004173196229015342), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 166), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002959468265267479), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 362), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0013311525262441888), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001126894525522175), ('regressor__regressor__max_depth', 1489), ('regressor__regressor__n_estimators', 457), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.36±0.36	 r2: -33.88±183.82
Trimer_scaler
Filename: (ECFP3.count.512-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer_scaler/(ECFP3.count.512-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer_scaler/(ECFP3.count.512-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer_scaler/(ECFP3.count.512-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c003n02>
Subject: Job 109673: <XGBR_Rh (IW avg log)_ECFP_Standard_Trimer> in cluster <Hazel> Done

Job <XGBR_Rh (IW avg log)_ECFP_Standard_Trimer> was submitted from host <c010n01> by user <sdehgha2> in cluster <Hazel> at Wed Jan  1 13:39:25 2025
Job was executed on host(s) <2*c003n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Jan  1 13:39:26 2025
                            <2*c006n03>
                            <2*c004n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed Jan  1 13:39:26 2025
Terminated at Wed Jan  1 15:35:20 2025
Results reported at Wed Jan  1 15:35:20 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 6
#BSUB -W 72:05
#BSUB -R span[ptile=2]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "XGBR_Rh (IW avg log)_ECFP_Standard_Trimer"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_ECFP_Standard_Trimer_3_count_20250101.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_ECFP_Standard_Trimer_3_count_20250101.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py --target_features "Rh (IW avg log)"                                       --representation "ECFP"                                       --regressor_type "XGBR"                                       --transform_type "Standard"                                       --radius "3"                                       --vector "count"                                       --oligomer_representation "Trimer"                                       --numerical_feats 'Mn (g/mol)' 'PDI' 'Mw (g/mol)' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' 'solvent dP' 'solvent dD' 'solvent dH'                                       --columns_to_impute "PDI" "Temperature SANS/SLS/DLS/SEC (K)" "Concentration (mg/ml)"                                       --special_impute 'Mw (g/mol)'                                       --imputer mean 




------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   23137.10 sec.
    Max Memory :                                 11 GB
    Average Memory :                             10.03 GB
    Total Requested Memory :                     48.00 GB
    Delta Memory :                               37.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   6970 sec.
    Turnaround time :                            6955 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_ECFP_Standard_Trimer_3_count_20250101.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04561410334381917), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 93), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04287419909032855), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02152964762891338), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 130), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005531342693771952), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1120), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02625064115261706), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 53), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00813608923977813), ('regressor__regressor__max_depth', 245), ('regressor__regressor__n_estimators', 58), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015241617928832547), ('regressor__regressor__max_depth', 7675), ('regressor__regressor__n_estimators', 1021), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006013332157840858), ('regressor__regressor__max_depth', 1211), ('regressor__regressor__n_estimators', 80), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006840357881031932), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 730), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0033088778453911963), ('regressor__regressor__max_depth', 2210), ('regressor__regressor__n_estimators', 252), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010056496020280788), ('regressor__regressor__max_depth', 2790), ('regressor__regressor__n_estimators', 759), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.012878266949459621), ('regressor__regressor__max_depth', 6298), ('regressor__regressor__n_estimators', 422), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02597723607667774), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0565561477453766), ('regressor__regressor__max_depth', 27), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01399836121259343), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 97), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013121340277331396), ('regressor__regressor__max_depth', 3473), ('regressor__regressor__n_estimators', 98), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013978993462480093), ('regressor__regressor__max_depth', 653), ('regressor__regressor__n_estimators', 60), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01670543226413511), ('regressor__regressor__max_depth', 22), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0141553998318881), ('regressor__regressor__max_depth', 2071), ('regressor__regressor__n_estimators', 65), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.016651324642419148), ('regressor__regressor__max_depth', 22), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0017353431141977193), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 775), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02610378859700933), ('regressor__regressor__max_depth', 34), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0428839069891835), ('regressor__regressor__max_depth', 236), ('regressor__regressor__n_estimators', 102), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013351464778181547), ('regressor__regressor__max_depth', 943), ('regressor__regressor__n_estimators', 197), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.015267469511518165), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 134), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008539970725353976), ('regressor__regressor__max_depth', 583), ('regressor__regressor__n_estimators', 138), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03263507027006712), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 65), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001026127989394692), ('regressor__regressor__max_depth', 38), ('regressor__regressor__n_estimators', 697), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01386754353744942), ('regressor__regressor__max_depth', 1687), ('regressor__regressor__n_estimators', 64), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01835217136361782), ('regressor__regressor__max_depth', 791), ('regressor__regressor__n_estimators', 58), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004173196229015342), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 166), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002959468265267479), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 362), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0013311525262441888), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001126894525522175), ('regressor__regressor__max_depth', 1489), ('regressor__regressor__n_estimators', 457), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.36±0.36	 r2: -33.88±183.82
Trimer_scaler
Filename: (ECFP3.count.512-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer_scaler/(ECFP3.count.512-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer_scaler/(ECFP3.count.512-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_Standard_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rh/Trimer_scaler/(ECFP3.count.512-PDI-Mw-concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_Standard_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c022n02>
Subject: Job 111296: <XGBR_Rh (IW avg log)_ECFP_Standard_Trimer> in cluster <Hazel> Done

Job <XGBR_Rh (IW avg log)_ECFP_Standard_Trimer> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Wed Jan  1 16:21:12 2025
Job was executed on host(s) <2*c022n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Jan  1 16:21:13 2025
                            <2*c014n03>
                            <2*c018n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Wed Jan  1 16:21:13 2025
Terminated at Wed Jan  1 18:18:23 2025
Results reported at Wed Jan  1 18:18:23 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 6
#BSUB -W 72:05
#BSUB -R span[ptile=2]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "XGBR_Rh (IW avg log)_ECFP_Standard_Trimer"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_ECFP_Standard_Trimer_3_count_20250101.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_ECFP_Standard_Trimer_3_count_20250101.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py --target_features "Rh (IW avg log)"                                       --representation "ECFP"                                       --regressor_type "XGBR"                                       --transform_type "Standard"                                       --radius "3"                                       --vector "count"                                       --oligomer_representation "Trimer"                                       --numerical_feats 'Mn (g/mol)' 'PDI' 'Mw (g/mol)' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' 'solvent dP' 'solvent dD' 'solvent dH'                                       --columns_to_impute "PDI" "Temperature SANS/SLS/DLS/SEC (K)" "Concentration (mg/ml)"                                       --special_impute 'Mw (g/mol)'                                       --imputer mean 




------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   24013.00 sec.
    Max Memory :                                 11 GB
    Average Memory :                             10.02 GB
    Total Requested Memory :                     48.00 GB
    Delta Memory :                               37.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   7054 sec.
    Turnaround time :                            7031 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/XGBR_Rh (IW avg log)_ECFP_Standard_Trimer_3_count_20250101.err> for stderr output of this job.

