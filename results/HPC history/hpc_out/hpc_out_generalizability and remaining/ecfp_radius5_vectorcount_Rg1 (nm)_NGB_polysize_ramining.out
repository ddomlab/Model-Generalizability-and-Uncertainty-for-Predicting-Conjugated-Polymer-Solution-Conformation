polymer unit :RRU Trimer with rep of ECFP5 and count



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
polymer unit :RRU Dimer with rep of ECFP5 and count



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
polymer unit :RRU Monomer with rep of ECFP5 and count



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009637157114309157), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1982), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009967732732385716), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1984), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009948925114103666), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1960), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009948187733532588), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1756), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00017503965053291583), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 946), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009871500349309598), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1984), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000982099586876546), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1995), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009984804295696643), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1995), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009896288115508636), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1999), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004941115922424022), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000987817189277156), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1998), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000989452706472453), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1967), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1548), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009984937680321783), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1996), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009984937680321783), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1996), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009980420704087882), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1980), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00099654403232923), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1911), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004964087778526683), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 714), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007600275106570869), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1340), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009747198908343955), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1992), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009896288115508636), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1999), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005541071252361559), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1780), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000987817189277156), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1998), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009929670267685429), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1990), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009627259059832255), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1728), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009929670267685429), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1990), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008881220813407089), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1794), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000996865050113866), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1971), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000993668274725941), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1984), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2051)}
Average scores:	 r: 0.91±0.08	 r2: 0.79±0.16
RRU Trimer_scaler
Filename: (ECFP5.count.2048-Mw-PDI)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer_scaler/(ECFP5.count.2048-Mw-PDI)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer_scaler/(ECFP5.count.2048-Mw-PDI)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer_scaler/(ECFP5.count.2048-Mw-PDI)_NGB_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n10>
Subject: Job 961326: <ecfp_radius5_vectorcount_NGB_Rg1 (nm)_polysize_ramining> in cluster <Hazel> Done

Job <ecfp_radius5_vectorcount_NGB_Rg1 (nm)_polysize_ramining> was submitted from host <c202n02> by user <sdehgha2> in cluster <Hazel> at Sun Oct 27 16:19:11 2024
Job was executed on host(s) <4*c205n10>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Oct 27 16:19:12 2024
                            <4*c205n06>
</home/sdehgha2> was used as the home directory.
</gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Sun Oct 27 16:19:12 2024
Terminated at Mon Oct 28 03:59:16 2024
Results reported at Mon Oct 28 03:59:16 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius5_vectorcount_NGB_Rg1 (nm)_polysize_ramining" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius5_vectorcount_Rg1 (nm)_NGB_polysize_ramining.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius5_vectorcount_Rg1 (nm)_NGB_polysize_ramining.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Trimer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   194085.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.93 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   42027 sec.
    Turnaround time :                            42005 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius5_vectorcount_Rg1 (nm)_NGB_polysize_ramining.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009940226801542346), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1899), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009804904933675969), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1999), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009730469951536886), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1987), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009980420704087882), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1980), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0002062206312391463), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1135), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000996333440502042), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1962), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009995295625309598), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1977), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 895), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000996358615255205), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1916), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009984804295696643), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1995), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000994357803290673), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1964), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009986516004384157), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1974), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009816387557166119), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1986), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000987817189277156), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1998), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009896288115508636), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1999), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000977774879236757), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1927), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009980387152175902), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1996), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009837792814502045), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1965), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009929829897830052), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1900), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000984797601520593), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1989), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009875538871215834), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1998), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2051)}
Average scores:	 r: 0.91±0.08	 r2: 0.79±0.16
RRU Dimer_scaler
Filename: (ECFP5.count.2048-Mw-PDI)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer_scaler/(ECFP5.count.2048-Mw-PDI)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer_scaler/(ECFP5.count.2048-Mw-PDI)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer_scaler/(ECFP5.count.2048-Mw-PDI)_NGB_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n14>
Subject: Job 961325: <ecfp_radius5_vectorcount_NGB_Rg1 (nm)_polysize_ramining> in cluster <Hazel> Done

Job <ecfp_radius5_vectorcount_NGB_Rg1 (nm)_polysize_ramining> was submitted from host <c202n02> by user <sdehgha2> in cluster <Hazel> at Sun Oct 27 16:19:11 2024
Job was executed on host(s) <4*c205n14>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Oct 27 16:19:12 2024
                            <4*c205n12>
</home/sdehgha2> was used as the home directory.
</gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Sun Oct 27 16:19:12 2024
Terminated at Mon Oct 28 04:24:31 2024
Results reported at Mon Oct 28 04:24:31 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius5_vectorcount_NGB_Rg1 (nm)_polysize_ramining" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius5_vectorcount_Rg1 (nm)_NGB_polysize_ramining.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius5_vectorcount_Rg1 (nm)_NGB_polysize_ramining.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Dimer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   197398.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.94 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   43537 sec.
    Turnaround time :                            43520 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius5_vectorcount_Rg1 (nm)_NGB_polysize_ramining.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009650822146517801), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007838730381544568), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1555), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009757250080201435), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1999), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009875538871215834), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1998), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 2051)}
Average scores:	 r: 0.91±0.08	 r2: 0.79±0.16
RRU Monomer_scaler
Filename: (ECFP5.count.2048-Mw-PDI)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer_scaler/(ECFP5.count.2048-Mw-PDI)_NGB_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer_scaler/(ECFP5.count.2048-Mw-PDI)_NGB_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer_scaler/(ECFP5.count.2048-Mw-PDI)_NGB_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n08>
Subject: Job 961324: <ecfp_radius5_vectorcount_NGB_Rg1 (nm)_polysize_ramining> in cluster <Hazel> Done

Job <ecfp_radius5_vectorcount_NGB_Rg1 (nm)_polysize_ramining> was submitted from host <c202n02> by user <sdehgha2> in cluster <Hazel> at Sun Oct 27 16:19:11 2024
Job was executed on host(s) <4*c202n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Oct 27 16:19:12 2024
                            <4*c202n04>
</home/sdehgha2> was used as the home directory.
</gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Sun Oct 27 16:19:12 2024
Terminated at Mon Oct 28 08:01:20 2024
Results reported at Mon Oct 28 08:01:20 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius5_vectorcount_NGB_Rg1 (nm)_polysize_ramining" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius5_vectorcount_Rg1 (nm)_NGB_polysize_ramining.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius5_vectorcount_Rg1 (nm)_NGB_polysize_ramining.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py ecfp --regressor_type NGB --radius 5 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   238830.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.96 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   56531 sec.
    Turnaround time :                            56529 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius5_vectorcount_Rg1 (nm)_NGB_polysize_ramining.err> for stderr output of this job.

