polymer representation: Dimer
polymer representation: RRU Dimer
polymer representation: RRU Monomer
polymer representation: Trimer
polymer representation: RRU Trimer
Dimer_scaler
Filename: (Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean_hypOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer_scaler/(Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean_hypOFF_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n08>
Subject: Job 856546: <structure_numerical_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_numerical_mordred_NGB_generalizibility> was submitted from host <c012n03> by user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:11:50 2024
Job was executed on host(s) <4*c207n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:11:50 2024
                            <4*c207n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Wed Oct 23 12:11:50 2024
Terminated at Wed Oct 23 12:21:04 2024
Results reported at Wed Oct 23 12:21:04 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 5:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_numerical_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_numerical.py mordred --regressor_type XGBR --target "Rg1 (nm)" --oligo_type "Dimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   376.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.10 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   562 sec.
    Turnaround time :                            554 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err> for stderr output of this job.

RRU Trimer_scaler
Filename: (Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean_hypOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer_scaler/(Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean_hypOFF_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n01>
Subject: Job 856551: <structure_numerical_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_numerical_mordred_NGB_generalizibility> was submitted from host <c012n03> by user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:12:10 2024
Job was executed on host(s) <4*c205n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:12:11 2024
                            <4*c205n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Wed Oct 23 12:12:11 2024
Terminated at Wed Oct 23 12:40:21 2024
Results reported at Wed Oct 23 12:40:21 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 5:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_numerical_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_numerical.py mordred --regressor_type XGBR --target "Rg1 (nm)" --oligo_type "RRU Trimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   591.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             0.90 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   1719 sec.
    Turnaround time :                            1691 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err> for stderr output of this job.

RRU Monomer_scaler
Filename: (Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean_hypOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer_scaler/(Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean_hypOFF_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n05>
Subject: Job 856548: <structure_numerical_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_numerical_mordred_NGB_generalizibility> was submitted from host <c012n03> by user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:12:03 2024
Job was executed on host(s) <4*c207n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:12:04 2024
                            <4*c207n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Wed Oct 23 12:12:04 2024
Terminated at Wed Oct 23 12:41:00 2024
Results reported at Wed Oct 23 12:41:00 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 5:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_numerical_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_numerical.py mordred --regressor_type XGBR --target "Rg1 (nm)" --oligo_type "RRU Monomer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   618.53 sec.
    Max Memory :                                 3 GB
    Average Memory :                             0.95 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   1749 sec.
    Turnaround time :                            1737 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err> for stderr output of this job.

RRU Dimer_scaler
Filename: (Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean_hypOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer_scaler/(Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean_hypOFF_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n03>
Subject: Job 856550: <structure_numerical_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_numerical_mordred_NGB_generalizibility> was submitted from host <c012n03> by user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:12:07 2024
Job was executed on host(s) <4*c203n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:12:07 2024
                            <4*c203n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Wed Oct 23 12:12:07 2024
Terminated at Wed Oct 23 12:41:18 2024
Results reported at Wed Oct 23 12:41:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 5:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_numerical_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_numerical.py mordred --regressor_type XGBR --target "Rg1 (nm)" --oligo_type "RRU Dimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   627.11 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.05 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   1770 sec.
    Turnaround time :                            1751 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err> for stderr output of this job.

Trimer_scaler
Filename: (Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean_hypOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer_scaler/(Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean_hypOFF_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n01>
Subject: Job 856547: <structure_numerical_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_numerical_mordred_NGB_generalizibility> was submitted from host <c012n03> by user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:11:57 2024
Job was executed on host(s) <4*c207n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:11:58 2024
                            <4*c207n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Wed Oct 23 12:11:58 2024
Terminated at Wed Oct 23 12:41:33 2024
Results reported at Wed Oct 23 12:41:33 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 5:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_numerical_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_numerical.py mordred --regressor_type XGBR --target "Rg1 (nm)" --oligo_type "Trimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   380.38 sec.
    Max Memory :                                 2 GB
    Average Memory :                             0.84 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   1784 sec.
    Turnaround time :                            1776 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err> for stderr output of this job.

polymer representation: Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01430529118455623), ('regressor__regressor__max_depth', 20), ('regressor__regressor__n_estimators', 79), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.019072281174117627), ('regressor__regressor__max_depth', 1904), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008540185281656983), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 270), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002603842840978718), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 392), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.010270362938541056), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 256), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0025988842127393445), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 163), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0022144938375607435), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 962), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0022453036608347555), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1547), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.021396505042840134), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 97), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0014850622702072358), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1060), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.023220759406062788), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 127), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.030340774542420006), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 130), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03249104530636686), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00783660766345155), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 393), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1066), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0019712461999166994), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 713), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06749703097328429), ('regressor__regressor__max_depth', 468), ('regressor__regressor__n_estimators', 1230), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.015147879514261818), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 80), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01925381696724183), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 158), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006486184023173054), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 222), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0035428807151259806), ('regressor__regressor__max_depth', 1781), ('regressor__regressor__n_estimators', 462), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005330687259365276), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 184), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05326344736142681), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005179973338393859), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 753), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.012330203256152496), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 118), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014327589132414357), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 69), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0064032458994905984), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 349), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05149511600840251), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 59), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04098492721571369), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 57), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006324591116217399), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 461), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004055284976921472), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 304), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09386253595756698), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0018636892683183879), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005947921235979719), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 573), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016666829238873877), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])


Monomer_scaler
Filename: (Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer_scaler/(Mordred-Mw-PDI-temperature-concentration-solvent dD-solvent dH-solvent dP)_XGBR_mean_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c012n03>
Subject: Job 856545: <structure_numerical_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_numerical_mordred_NGB_generalizibility> was submitted from host <c012n03> by user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:11:46 2024
Job was executed on host(s) <4*c012n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Wed Oct 23 12:11:47 2024
                            <4*c013n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Wed Oct 23 12:11:47 2024
Terminated at Wed Oct 23 15:52:59 2024
Results reported at Wed Oct 23 15:52:59 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 5:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_numerical_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_numerical.py mordred --regressor_type XGBR --target "Rg1 (nm)" --oligo_type "Monomer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   36769.03 sec.
    Max Memory :                                 6 GB
    Average Memory :                             3.96 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               58.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   13299 sec.
    Turnaround time :                            13273 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_numerical_mordred_generalizability_XGBR_wo_hypo.err> for stderr output of this job.

