Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Monomer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Dimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
Trimer



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6
RRU Trimer
Dimer
Trimer
RRU Dimer
RRU Monomer
Monomer
RRU Monomer
Filename: (Mordred)_NGB_hypOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(Mordred)_NGB_hypOFF_generalizability_scores.json
Done Saving scores!
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0012 val_loss=0.0000 scale=2.0000 norm=1.5957
[iter 200] loss=0.7365 val_loss=0.0000 scale=2.0000 norm=1.5895
[iter 300] loss=0.4069 val_loss=0.0000 scale=4.0000 norm=3.1680
[iter 400] loss=0.1619 val_loss=0.0000 scale=4.0000 norm=3.0556
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1633 val_loss=0.0000 scale=2.0000 norm=1.8733
[iter 200] loss=0.8688 val_loss=0.0000 scale=4.0000 norm=3.7637
[iter 300] loss=0.3648 val_loss=0.0000 scale=8.0000 norm=7.5297
[iter 400] loss=-0.6192 val_loss=0.0000 scale=16.0000 norm=15.0595
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0157 val_loss=0.0000 scale=2.0000 norm=1.5847
[iter 200] loss=0.7807 val_loss=0.0000 scale=4.0000 norm=3.1346
[iter 300] loss=0.4478 val_loss=0.0000 scale=4.0000 norm=3.1081
[iter 400] loss=0.2549 val_loss=0.0000 scale=4.0000 norm=3.1647
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.3175
[iter 200] loss=-0.2334 val_loss=0.0000 scale=2.0000 norm=1.2639
[iter 300] loss=-0.8684 val_loss=0.0000 scale=4.0000 norm=2.5376
[iter 400] loss=-1.7033 val_loss=0.0000 scale=8.0000 norm=4.9721
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9654 val_loss=0.0000 scale=2.0000 norm=1.5744
[iter 200] loss=0.6610 val_loss=0.0000 scale=2.0000 norm=1.5626
[iter 300] loss=0.2321 val_loss=0.0000 scale=4.0000 norm=3.1094
[iter 400] loss=-0.0843 val_loss=0.0000 scale=4.0000 norm=3.0427
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=2.0000 norm=1.5701
[iter 200] loss=0.7524 val_loss=0.0000 scale=2.0000 norm=1.5584
[iter 300] loss=0.4374 val_loss=0.0000 scale=4.0000 norm=3.0932
[iter 400] loss=0.2229 val_loss=0.0000 scale=4.0000 norm=3.0457
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9392 val_loss=0.0000 scale=2.0000 norm=1.5696
[iter 200] loss=0.6833 val_loss=0.0000 scale=2.0000 norm=1.5618
[iter 300] loss=0.3278 val_loss=0.0000 scale=4.0000 norm=3.1238
[iter 400] loss=0.0348 val_loss=0.0000 scale=4.0000 norm=3.0799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6310 val_loss=0.0000 scale=2.0000 norm=1.3721
[iter 200] loss=0.2143 val_loss=0.0000 scale=4.0000 norm=2.6093
[iter 300] loss=-0.2981 val_loss=0.0000 scale=4.0000 norm=2.6618
[iter 400] loss=-0.7634 val_loss=0.0000 scale=8.0000 norm=4.9863
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8970 val_loss=0.0000 scale=2.0000 norm=1.5607
[iter 200] loss=0.5414 val_loss=0.0000 scale=4.0000 norm=3.0579
[iter 300] loss=-0.0644 val_loss=0.0000 scale=4.0000 norm=3.0232
[iter 400] loss=-1.1264 val_loss=0.0000 scale=8.0000 norm=6.0466
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8543 val_loss=0.0000 scale=2.0000 norm=1.6122
[iter 200] loss=0.4151 val_loss=0.0000 scale=4.0000 norm=3.2085
[iter 300] loss=-0.0602 val_loss=0.0000 scale=4.0000 norm=3.1637
[iter 400] loss=-0.3929 val_loss=0.0000 scale=4.0000 norm=3.0548
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0297 val_loss=0.0000 scale=2.0000 norm=1.6085
[iter 200] loss=0.7909 val_loss=0.0000 scale=4.0000 norm=3.1573
[iter 300] loss=0.4448 val_loss=0.0000 scale=4.0000 norm=3.1030
[iter 400] loss=0.2130 val_loss=0.0000 scale=8.0000 norm=6.2116
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0092 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 200] loss=0.6696 val_loss=0.0000 scale=4.0000 norm=3.2022
[iter 300] loss=0.2614 val_loss=0.0000 scale=4.0000 norm=3.1653
[iter 400] loss=-0.0190 val_loss=0.0000 scale=4.0000 norm=3.1477
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0619 val_loss=0.0000 scale=2.0000 norm=1.6499
[iter 200] loss=0.8186 val_loss=0.0000 scale=4.0000 norm=3.2915
[iter 300] loss=0.4810 val_loss=0.0000 scale=4.0000 norm=3.1993
[iter 400] loss=0.2740 val_loss=0.0000 scale=4.0000 norm=3.1448
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0198 val_loss=0.0000 scale=2.0000 norm=1.5932
[iter 200] loss=0.7140 val_loss=0.0000 scale=4.0000 norm=3.1541
[iter 300] loss=0.3551 val_loss=0.0000 scale=4.0000 norm=3.0443
[iter 400] loss=0.1440 val_loss=0.0000 scale=4.0000 norm=2.9875
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9958 val_loss=0.0000 scale=2.0000 norm=1.5920
[iter 200] loss=0.6970 val_loss=0.0000 scale=4.0000 norm=3.1627
[iter 300] loss=0.3562 val_loss=0.0000 scale=4.0000 norm=3.0884
[iter 400] loss=0.1496 val_loss=0.0000 scale=4.0000 norm=2.9778
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0889 val_loss=0.0000 scale=2.0000 norm=1.6447
[iter 200] loss=0.9206 val_loss=0.0000 scale=2.0000 norm=1.6369
[iter 300] loss=0.6779 val_loss=0.0000 scale=4.0000 norm=3.2605
[iter 400] loss=0.5233 val_loss=0.0000 scale=4.0000 norm=3.2179
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0212 val_loss=0.0000 scale=2.0000 norm=1.6026
[iter 200] loss=0.7392 val_loss=0.0000 scale=4.0000 norm=3.1748
[iter 300] loss=0.3715 val_loss=0.0000 scale=4.0000 norm=3.0937
[iter 400] loss=0.0990 val_loss=0.0000 scale=8.0000 norm=6.1566
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0646 val_loss=0.0000 scale=2.0000 norm=1.6387
[iter 200] loss=0.8540 val_loss=0.0000 scale=2.0000 norm=1.6295
[iter 300] loss=0.5540 val_loss=0.0000 scale=4.0000 norm=3.2337
[iter 400] loss=0.2703 val_loss=0.0000 scale=4.0000 norm=3.1690
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7133 val_loss=0.0000 scale=2.0000 norm=1.3003
[iter 200] loss=0.3442 val_loss=0.0000 scale=4.0000 norm=2.6880
[iter 300] loss=-0.1828 val_loss=0.0000 scale=4.0000 norm=2.6566
[iter 400] loss=-0.6904 val_loss=0.0000 scale=4.0000 norm=2.6733
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0399 val_loss=0.0000 scale=2.0000 norm=1.6199
[iter 200] loss=0.7685 val_loss=0.0000 scale=4.0000 norm=3.2110
[iter 300] loss=0.3970 val_loss=0.0000 scale=4.0000 norm=3.1595
[iter 400] loss=0.1262 val_loss=0.0000 scale=4.0000 norm=3.1007
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0331 val_loss=0.0000 scale=2.0000 norm=1.5965
[iter 200] loss=0.8231 val_loss=0.0000 scale=4.0000 norm=3.1551
[iter 300] loss=0.5391 val_loss=0.0000 scale=4.0000 norm=3.1364
[iter 400] loss=0.3862 val_loss=0.0000 scale=4.0000 norm=3.1016
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8261 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 200] loss=0.3049 val_loss=0.0000 scale=4.0000 norm=3.2241
[iter 300] loss=-0.3026 val_loss=0.0000 scale=4.0000 norm=3.1618
[iter 400] loss=-0.8570 val_loss=0.0000 scale=4.0000 norm=3.0816
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0248 val_loss=0.0000 scale=2.0000 norm=1.5856
[iter 200] loss=0.8277 val_loss=0.0000 scale=2.0000 norm=1.5595
[iter 300] loss=0.5516 val_loss=0.0000 scale=4.0000 norm=3.0873
[iter 400] loss=0.4123 val_loss=0.0000 scale=4.0000 norm=3.0910
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0103 val_loss=0.0000 scale=2.0000 norm=1.5857
[iter 200] loss=0.7768 val_loss=0.0000 scale=2.0000 norm=1.5664
[iter 300] loss=0.4416 val_loss=0.0000 scale=4.0000 norm=3.0968
[iter 400] loss=0.2647 val_loss=0.0000 scale=4.0000 norm=3.0614
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1198 val_loss=0.0000 scale=2.0000 norm=1.7053
[iter 200] loss=0.8632 val_loss=0.0000 scale=4.0000 norm=3.3777
[iter 300] loss=0.5293 val_loss=0.0000 scale=4.0000 norm=3.3039
[iter 400] loss=0.1458 val_loss=0.0000 scale=8.0000 norm=6.4525
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0130 val_loss=0.0000 scale=2.0000 norm=1.5878
[iter 200] loss=0.7897 val_loss=0.0000 scale=2.0000 norm=1.5682
[iter 300] loss=0.4675 val_loss=0.0000 scale=4.0000 norm=3.1188
[iter 400] loss=0.2847 val_loss=0.0000 scale=4.0000 norm=3.0866
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9883 val_loss=0.0000 scale=2.0000 norm=1.6063
[iter 200] loss=0.6762 val_loss=0.0000 scale=4.0000 norm=3.1861
[iter 300] loss=0.2905 val_loss=0.0000 scale=4.0000 norm=3.1732
[iter 400] loss=-0.0877 val_loss=0.0000 scale=4.0000 norm=3.0741
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0140 val_loss=0.0000 scale=2.0000 norm=1.5936
[iter 200] loss=0.7958 val_loss=0.0000 scale=2.0000 norm=1.5946
[iter 300] loss=0.4591 val_loss=0.0000 scale=4.0000 norm=3.1492
[iter 400] loss=0.2403 val_loss=0.0000 scale=4.0000 norm=3.1495
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9859 val_loss=0.0000 scale=2.0000 norm=1.5703
[iter 200] loss=0.7353 val_loss=0.0000 scale=4.0000 norm=3.1010
[iter 300] loss=0.3908 val_loss=0.0000 scale=4.0000 norm=3.0867
[iter 400] loss=0.2050 val_loss=0.0000 scale=4.0000 norm=3.0616
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0358 val_loss=0.0000 scale=2.0000 norm=1.6341
[iter 200] loss=0.7497 val_loss=0.0000 scale=4.0000 norm=3.2197
[iter 300] loss=0.3558 val_loss=0.0000 scale=4.0000 norm=3.1990
[iter 400] loss=0.0461 val_loss=0.0000 scale=4.0000 norm=3.1331
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0472 val_loss=0.0000 scale=2.0000 norm=1.6124
[iter 200] loss=0.8215 val_loss=0.0000 scale=2.0000 norm=1.5885
[iter 300] loss=0.5460 val_loss=0.0000 scale=4.0000 norm=3.1193
[iter 400] loss=0.2798 val_loss=0.0000 scale=4.0000 norm=3.0851
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0547 val_loss=0.0000 scale=2.0000 norm=1.6256
[iter 200] loss=0.8254 val_loss=0.0000 scale=4.0000 norm=3.2328
[iter 300] loss=0.4974 val_loss=0.0000 scale=4.0000 norm=3.2078
[iter 400] loss=0.3048 val_loss=0.0000 scale=4.0000 norm=3.1590
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0270 val_loss=0.0000 scale=2.0000 norm=1.5984
[iter 200] loss=0.7790 val_loss=0.0000 scale=4.0000 norm=3.1781
[iter 300] loss=0.4261 val_loss=0.0000 scale=4.0000 norm=3.1441
[iter 400] loss=0.2284 val_loss=0.0000 scale=4.0000 norm=3.0867
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9476 val_loss=0.0000 scale=2.0000 norm=1.6439
[iter 200] loss=0.5548 val_loss=0.0000 scale=4.0000 norm=3.2429
[iter 300] loss=0.0913 val_loss=0.0000 scale=4.0000 norm=3.1960
[iter 400] loss=-0.2460 val_loss=0.0000 scale=4.0000 norm=3.0753
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0429 val_loss=0.0000 scale=2.0000 norm=1.6686
[iter 200] loss=0.6958 val_loss=0.0000 scale=4.0000 norm=3.3336
[iter 300] loss=0.1358 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-0.9442 val_loss=0.0000 scale=16.0000 norm=13.3438
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5863 val_loss=0.0000 scale=2.0000 norm=1.1503
[iter 200] loss=0.2166 val_loss=0.0000 scale=2.0000 norm=1.1021
[iter 300] loss=-0.1534 val_loss=0.0000 scale=4.0000 norm=2.2001
[iter 400] loss=-0.4616 val_loss=0.0000 scale=4.0000 norm=2.1871
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8196 val_loss=0.0000 scale=2.0000 norm=1.5550
[iter 200] loss=0.3286 val_loss=0.0000 scale=4.0000 norm=3.1048
[iter 300] loss=-0.2557 val_loss=0.0000 scale=4.0000 norm=3.0754
[iter 400] loss=-0.7064 val_loss=0.0000 scale=4.0000 norm=2.9966
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0145 val_loss=0.0000 scale=2.0000 norm=1.6016
[iter 200] loss=0.7778 val_loss=0.0000 scale=4.0000 norm=3.1708
[iter 300] loss=0.4419 val_loss=0.0000 scale=4.0000 norm=3.1317
[iter 400] loss=0.2377 val_loss=0.0000 scale=4.0000 norm=3.0590
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0002 val_loss=0.0000 scale=2.0000 norm=1.5664
[iter 200] loss=0.7778 val_loss=0.0000 scale=2.0000 norm=1.5463
[iter 300] loss=0.4939 val_loss=0.0000 scale=4.0000 norm=3.0734
[iter 400] loss=0.2942 val_loss=0.0000 scale=4.0000 norm=3.0183
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9239 val_loss=0.0000 scale=2.0000 norm=1.5195
[iter 200] loss=0.6348 val_loss=0.0000 scale=4.0000 norm=2.9898
[iter 300] loss=0.2626 val_loss=0.0000 scale=4.0000 norm=2.9555
[iter 400] loss=0.0018 val_loss=0.0000 scale=4.0000 norm=2.9447
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0299 val_loss=0.0000 scale=2.0000 norm=1.6022
[iter 200] loss=0.8075 val_loss=0.0000 scale=4.0000 norm=3.1977
[iter 300] loss=0.4688 val_loss=0.0000 scale=4.0000 norm=3.1711
[iter 400] loss=0.2664 val_loss=0.0000 scale=4.0000 norm=3.1543
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0722 val_loss=0.0000 scale=2.0000 norm=1.6237
[iter 200] loss=0.8828 val_loss=0.0000 scale=2.0000 norm=1.6109
[iter 300] loss=0.6029 val_loss=0.0000 scale=4.0000 norm=3.1887
[iter 400] loss=0.3899 val_loss=0.0000 scale=4.0000 norm=3.1087
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0404 val_loss=0.0000 scale=2.0000 norm=1.6167
[iter 200] loss=0.8049 val_loss=0.0000 scale=2.0000 norm=1.6034
[iter 300] loss=0.4385 val_loss=0.0000 scale=4.0000 norm=3.1827
[iter 400] loss=0.2024 val_loss=0.0000 scale=4.0000 norm=3.1486
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0520 val_loss=0.0000 scale=2.0000 norm=1.6267
[iter 200] loss=0.8227 val_loss=0.0000 scale=2.0000 norm=1.6067
[iter 300] loss=0.5206 val_loss=0.0000 scale=4.0000 norm=3.1617
[iter 400] loss=0.2614 val_loss=0.0000 scale=2.0000 norm=1.5445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0350 val_loss=0.0000 scale=2.0000 norm=1.6108
[iter 200] loss=0.8063 val_loss=0.0000 scale=4.0000 norm=3.1819
[iter 300] loss=0.4859 val_loss=0.0000 scale=4.0000 norm=3.1290
[iter 400] loss=0.2774 val_loss=0.0000 scale=4.0000 norm=3.1000
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7865 val_loss=0.0000 scale=2.0000 norm=1.4126
[iter 200] loss=0.5534 val_loss=0.0000 scale=4.0000 norm=2.9962
[iter 300] loss=0.2261 val_loss=0.0000 scale=4.0000 norm=3.0371
[iter 400] loss=-0.3632 val_loss=0.0000 scale=8.0000 norm=6.0770
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8644 val_loss=0.0000 scale=2.0000 norm=1.5893
[iter 200] loss=0.4033 val_loss=0.0000 scale=4.0000 norm=3.1137
[iter 300] loss=-0.2404 val_loss=0.0000 scale=8.0000 norm=6.2680
[iter 400] loss=-0.8468 val_loss=0.0000 scale=4.0000 norm=3.0899
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0255 val_loss=0.0000 scale=2.0000 norm=1.5854
[iter 200] loss=0.8139 val_loss=0.0000 scale=2.0000 norm=1.5697
[iter 300] loss=0.5473 val_loss=0.0000 scale=2.0000 norm=1.5469
[iter 400] loss=0.4523 val_loss=0.0000 scale=4.0000 norm=3.0801
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9961 val_loss=0.0000 scale=2.0000 norm=1.5930
[iter 200] loss=0.7066 val_loss=0.0000 scale=4.0000 norm=3.1504
[iter 300] loss=0.2636 val_loss=0.0000 scale=4.0000 norm=3.1254
[iter 400] loss=-0.0361 val_loss=0.0000 scale=4.0000 norm=3.0471
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9068 val_loss=0.0000 scale=2.0000 norm=1.5106
[iter 200] loss=0.6710 val_loss=0.0000 scale=2.0000 norm=1.5392
[iter 300] loss=0.2934 val_loss=0.0000 scale=4.0000 norm=3.0689
[iter 400] loss=-0.0330 val_loss=0.0000 scale=4.0000 norm=3.0226
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0016 val_loss=0.0000 scale=2.0000 norm=1.6051
[iter 200] loss=0.7290 val_loss=0.0000 scale=4.0000 norm=3.1903
[iter 300] loss=0.3743 val_loss=0.0000 scale=4.0000 norm=3.1560
[iter 400] loss=0.1402 val_loss=0.0000 scale=4.0000 norm=3.0809
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9760 val_loss=0.0000 scale=2.0000 norm=1.5741
[iter 200] loss=0.7226 val_loss=0.0000 scale=2.0000 norm=1.5396
[iter 300] loss=0.3844 val_loss=0.0000 scale=4.0000 norm=3.0125
[iter 400] loss=0.2009 val_loss=0.0000 scale=4.0000 norm=2.9353
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0432 val_loss=0.0000 scale=2.0000 norm=1.6197
[iter 200] loss=0.8211 val_loss=0.0000 scale=4.0000 norm=3.1997
[iter 300] loss=0.5115 val_loss=0.0000 scale=4.0000 norm=3.1432
[iter 400] loss=0.3606 val_loss=0.0000 scale=4.0000 norm=3.0960
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0403 val_loss=0.0000 scale=2.0000 norm=1.5890
[iter 200] loss=0.8407 val_loss=0.0000 scale=2.0000 norm=1.5763
[iter 300] loss=0.5667 val_loss=0.0000 scale=4.0000 norm=3.1287
[iter 400] loss=0.4062 val_loss=0.0000 scale=4.0000 norm=3.1059
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1181 val_loss=0.0000 scale=2.0000 norm=1.6826
[iter 200] loss=0.9502 val_loss=0.0000 scale=4.0000 norm=3.3442
[iter 300] loss=0.7077 val_loss=0.0000 scale=4.0000 norm=3.3045
[iter 400] loss=0.5385 val_loss=0.0000 scale=4.0000 norm=3.2662
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0294 val_loss=0.0000 scale=2.0000 norm=1.6206
[iter 200] loss=0.7289 val_loss=0.0000 scale=4.0000 norm=3.2180
[iter 300] loss=0.3793 val_loss=0.0000 scale=4.0000 norm=3.1538
[iter 400] loss=0.1484 val_loss=0.0000 scale=4.0000 norm=3.1124
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8652 val_loss=0.0000 scale=2.0000 norm=1.5256
[iter 200] loss=0.5309 val_loss=0.0000 scale=2.0000 norm=1.5128
[iter 300] loss=0.0492 val_loss=0.0000 scale=4.0000 norm=2.9534
[iter 400] loss=-0.3338 val_loss=0.0000 scale=4.0000 norm=2.9198
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9920 val_loss=0.0000 scale=2.0000 norm=1.5703
[iter 200] loss=0.7453 val_loss=0.0000 scale=2.0000 norm=1.5530
[iter 300] loss=0.4029 val_loss=0.0000 scale=4.0000 norm=3.1209
[iter 400] loss=0.1222 val_loss=0.0000 scale=4.0000 norm=3.0758
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8316 val_loss=0.0000 scale=2.0000 norm=1.4286
[iter 200] loss=0.5624 val_loss=0.0000 scale=2.0000 norm=1.4457
[iter 300] loss=0.2760 val_loss=0.0000 scale=4.0000 norm=2.8645
[iter 400] loss=0.0066 val_loss=0.0000 scale=4.0000 norm=2.7800
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9605 val_loss=0.0000 scale=2.0000 norm=1.5387
[iter 200] loss=0.7498 val_loss=0.0000 scale=4.0000 norm=3.1875
[iter 300] loss=0.4137 val_loss=0.0000 scale=8.0000 norm=6.4023
[iter 400] loss=-0.2463 val_loss=0.0000 scale=16.0000 norm=12.8051
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0077 val_loss=0.0000 scale=2.0000 norm=1.6079
[iter 200] loss=0.7597 val_loss=0.0000 scale=4.0000 norm=3.1925
[iter 300] loss=0.4315 val_loss=0.0000 scale=4.0000 norm=3.1847
[iter 400] loss=0.2363 val_loss=0.0000 scale=4.0000 norm=3.1239
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0520 val_loss=0.0000 scale=2.0000 norm=1.6320
[iter 200] loss=0.7866 val_loss=0.0000 scale=4.0000 norm=3.2612
[iter 300] loss=0.3963 val_loss=0.0000 scale=4.0000 norm=3.2434
[iter 400] loss=0.0966 val_loss=0.0000 scale=4.0000 norm=3.2222
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9709 val_loss=0.0000 scale=2.0000 norm=1.5808
[iter 200] loss=0.6001 val_loss=0.0000 scale=4.0000 norm=3.1595
[iter 300] loss=0.0860 val_loss=0.0000 scale=4.0000 norm=3.1627
[iter 400] loss=-0.3731 val_loss=0.0000 scale=4.0000 norm=3.0802
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0007 val_loss=0.0000 scale=2.0000 norm=1.5890
[iter 200] loss=0.7307 val_loss=0.0000 scale=4.0000 norm=3.1666
[iter 300] loss=0.3391 val_loss=0.0000 scale=4.0000 norm=3.1194
[iter 400] loss=0.0961 val_loss=0.0000 scale=4.0000 norm=3.0881
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0143 val_loss=0.0000 scale=2.0000 norm=1.5750
[iter 200] loss=0.7909 val_loss=0.0000 scale=2.0000 norm=1.5586
[iter 300] loss=0.4916 val_loss=0.0000 scale=4.0000 norm=3.1084
[iter 400] loss=0.3013 val_loss=0.0000 scale=4.0000 norm=3.0979
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9891 val_loss=0.0000 scale=2.0000 norm=1.5646
[iter 200] loss=0.7584 val_loss=0.0000 scale=2.0000 norm=1.5470
[iter 300] loss=0.4418 val_loss=0.0000 scale=4.0000 norm=3.0739
[iter 400] loss=0.2421 val_loss=0.0000 scale=4.0000 norm=3.0257
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9589 val_loss=0.0000 scale=2.0000 norm=1.5269
[iter 200] loss=0.7094 val_loss=0.0000 scale=2.0000 norm=1.5045
[iter 300] loss=0.3649 val_loss=0.0000 scale=4.0000 norm=2.9681
[iter 400] loss=0.0839 val_loss=0.0000 scale=4.0000 norm=2.9514
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1122 val_loss=0.0000 scale=2.0000 norm=1.6678
[iter 200] loss=0.9585 val_loss=0.0000 scale=2.0000 norm=1.6674
[iter 300] loss=0.7571 val_loss=0.0000 scale=4.0000 norm=3.3276
[iter 400] loss=0.5014 val_loss=0.0000 scale=8.0000 norm=6.6676
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4375 val_loss=0.0000 scale=2.0000 norm=1.1901
[iter 200] loss=-0.1530 val_loss=0.0000 scale=4.0000 norm=2.4690
[iter 300] loss=-0.8569 val_loss=0.0000 scale=4.0000 norm=2.5415
[iter 400] loss=-2.2289 val_loss=0.0000 scale=8.0000 norm=5.0862
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1054 val_loss=0.0000 scale=2.0000 norm=1.6499
[iter 200] loss=0.9517 val_loss=0.0000 scale=2.0000 norm=1.6386
[iter 300] loss=0.7283 val_loss=0.0000 scale=4.0000 norm=3.2258
[iter 400] loss=0.5843 val_loss=0.0000 scale=4.0000 norm=3.2032
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9656 val_loss=0.0000 scale=2.0000 norm=1.5459
[iter 200] loss=0.7032 val_loss=0.0000 scale=2.0000 norm=1.5300
[iter 300] loss=0.3351 val_loss=0.0000 scale=4.0000 norm=3.0302
[iter 400] loss=0.0966 val_loss=0.0000 scale=4.0000 norm=2.9886
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0188 val_loss=0.0000 scale=2.0000 norm=1.5839
[iter 200] loss=0.7837 val_loss=0.0000 scale=2.0000 norm=1.5694
[iter 300] loss=0.4397 val_loss=0.0000 scale=4.0000 norm=3.1387
[iter 400] loss=0.1861 val_loss=0.0000 scale=4.0000 norm=3.0768
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6631 val_loss=0.0000 scale=2.0000 norm=1.2792
[iter 200] loss=0.3134 val_loss=0.0000 scale=2.0000 norm=1.2814
[iter 300] loss=-0.1377 val_loss=0.0000 scale=4.0000 norm=2.5260
[iter 400] loss=-0.5352 val_loss=0.0000 scale=4.0000 norm=2.5158
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0001 val_loss=0.0000 scale=2.0000 norm=1.5889
[iter 200] loss=0.6989 val_loss=0.0000 scale=4.0000 norm=3.1766
[iter 300] loss=0.3253 val_loss=0.0000 scale=4.0000 norm=3.1432
[iter 400] loss=0.1052 val_loss=0.0000 scale=4.0000 norm=3.1269
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0047 val_loss=0.0000 scale=2.0000 norm=1.5847
[iter 200] loss=0.7655 val_loss=0.0000 scale=4.0000 norm=3.1213
[iter 300] loss=0.4466 val_loss=0.0000 scale=4.0000 norm=3.1183
[iter 400] loss=0.2315 val_loss=0.0000 scale=4.0000 norm=3.0583
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0128 val_loss=0.0000 scale=2.0000 norm=1.5831
[iter 200] loss=0.7736 val_loss=0.0000 scale=2.0000 norm=1.5739
[iter 300] loss=0.4202 val_loss=0.0000 scale=4.0000 norm=3.0926
[iter 400] loss=0.2272 val_loss=0.0000 scale=2.0000 norm=1.5430
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0474 val_loss=0.0000 scale=2.0000 norm=1.6263
[iter 200] loss=0.8073 val_loss=0.0000 scale=4.0000 norm=3.2448
[iter 300] loss=0.4768 val_loss=0.0000 scale=4.0000 norm=3.1831
[iter 400] loss=0.3355 val_loss=0.0000 scale=2.0000 norm=1.5548
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0180 val_loss=0.0000 scale=2.0000 norm=1.5805
[iter 200] loss=0.8021 val_loss=0.0000 scale=2.0000 norm=1.5662
[iter 300] loss=0.5008 val_loss=0.0000 scale=4.0000 norm=3.1277
[iter 400] loss=0.3340 val_loss=0.0000 scale=4.0000 norm=3.1342
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0009 val_loss=0.0000 scale=2.0000 norm=1.5958
[iter 200] loss=0.7523 val_loss=0.0000 scale=4.0000 norm=3.1640
[iter 300] loss=0.3913 val_loss=0.0000 scale=4.0000 norm=3.1327
[iter 400] loss=0.1807 val_loss=0.0000 scale=4.0000 norm=3.1432
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9940 val_loss=0.0000 scale=2.0000 norm=1.5960
[iter 200] loss=0.7400 val_loss=0.0000 scale=2.0000 norm=1.5905
[iter 300] loss=0.3582 val_loss=0.0000 scale=4.0000 norm=3.1419
[iter 400] loss=0.1246 val_loss=0.0000 scale=4.0000 norm=3.1386
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9677 val_loss=0.0000 scale=2.0000 norm=1.5361
[iter 200] loss=0.7187 val_loss=0.0000 scale=2.0000 norm=1.5263
[iter 300] loss=0.3736 val_loss=0.0000 scale=4.0000 norm=3.0134
[iter 400] loss=0.1849 val_loss=0.0000 scale=4.0000 norm=2.9452
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0277 val_loss=0.0000 scale=2.0000 norm=1.6103
[iter 200] loss=0.7859 val_loss=0.0000 scale=4.0000 norm=3.1995
[iter 300] loss=0.4451 val_loss=0.0000 scale=4.0000 norm=3.1556
[iter 400] loss=0.2598 val_loss=0.0000 scale=4.0000 norm=3.0977
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8306 val_loss=0.0000 scale=2.0000 norm=1.4455
[iter 200] loss=0.4209 val_loss=0.0000 scale=4.0000 norm=2.9168
[iter 300] loss=-0.2122 val_loss=0.0000 scale=4.0000 norm=2.8186
[iter 400] loss=-1.1489 val_loss=0.0000 scale=8.0000 norm=5.4818
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9900 val_loss=0.0000 scale=2.0000 norm=1.5900
[iter 200] loss=0.7637 val_loss=0.0000 scale=2.0000 norm=1.5773
[iter 300] loss=0.4408 val_loss=0.0000 scale=4.0000 norm=3.1492
[iter 400] loss=0.1905 val_loss=0.0000 scale=4.0000 norm=3.1007
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0185 val_loss=0.0000 scale=2.0000 norm=1.6009
[iter 200] loss=0.7581 val_loss=0.0000 scale=4.0000 norm=3.1566
[iter 300] loss=0.4233 val_loss=0.0000 scale=4.0000 norm=3.1289
[iter 400] loss=0.2561 val_loss=0.0000 scale=4.0000 norm=3.0954
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0022 val_loss=0.0000 scale=2.0000 norm=1.6065
[iter 200] loss=0.7675 val_loss=0.0000 scale=4.0000 norm=3.1708
[iter 300] loss=0.4647 val_loss=0.0000 scale=4.0000 norm=3.1749
[iter 400] loss=0.2370 val_loss=0.0000 scale=4.0000 norm=3.1038
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0404 val_loss=0.0000 scale=2.0000 norm=1.6171
[iter 200] loss=0.8121 val_loss=0.0000 scale=4.0000 norm=3.2104
[iter 300] loss=0.4643 val_loss=0.0000 scale=4.0000 norm=3.1852
[iter 400] loss=0.2384 val_loss=0.0000 scale=4.0000 norm=3.1869
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0119 val_loss=0.0000 scale=2.0000 norm=1.6398
[iter 200] loss=0.7960 val_loss=0.0000 scale=4.0000 norm=3.2199
[iter 300] loss=0.5087 val_loss=0.0000 scale=8.0000 norm=6.5049
[iter 400] loss=-0.0513 val_loss=0.0000 scale=16.0000 norm=13.0103
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0321 val_loss=0.0000 scale=2.0000 norm=1.6209
[iter 200] loss=0.8031 val_loss=0.0000 scale=4.0000 norm=3.2413
[iter 300] loss=0.4741 val_loss=0.0000 scale=4.0000 norm=3.2063
[iter 400] loss=0.3363 val_loss=0.0000 scale=4.0000 norm=3.1744
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0995 val_loss=0.0000 scale=2.0000 norm=1.6765
[iter 200] loss=0.8747 val_loss=0.0000 scale=4.0000 norm=3.3015
[iter 300] loss=0.6007 val_loss=0.0000 scale=4.0000 norm=3.2686
[iter 400] loss=0.2943 val_loss=0.0000 scale=8.0000 norm=6.4091
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0784 val_loss=0.0000 scale=2.0000 norm=1.6319
[iter 200] loss=0.8329 val_loss=0.0000 scale=4.0000 norm=3.2134
[iter 300] loss=0.5578 val_loss=0.0000 scale=4.0000 norm=3.1218
[iter 400] loss=0.1488 val_loss=0.0000 scale=8.0000 norm=6.2974
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9991 val_loss=0.0000 scale=2.0000 norm=1.5843
[iter 200] loss=0.7446 val_loss=0.0000 scale=4.0000 norm=3.1318
[iter 300] loss=0.4142 val_loss=0.0000 scale=4.0000 norm=3.0795
[iter 400] loss=0.2676 val_loss=0.0000 scale=4.0000 norm=3.1261
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0520 val_loss=0.0000 scale=2.0000 norm=1.6031
[iter 200] loss=0.8673 val_loss=0.0000 scale=2.0000 norm=1.5932
[iter 300] loss=0.6251 val_loss=0.0000 scale=4.0000 norm=3.1660
[iter 400] loss=0.4789 val_loss=0.0000 scale=4.0000 norm=3.0852
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0172 val_loss=0.0000 scale=2.0000 norm=1.5953
[iter 200] loss=0.7554 val_loss=0.0000 scale=4.0000 norm=3.1457
[iter 300] loss=0.3350 val_loss=0.0000 scale=4.0000 norm=3.1508
[iter 400] loss=-0.4227 val_loss=0.0000 scale=8.0000 norm=6.2995
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0344 val_loss=0.0000 scale=2.0000 norm=1.6191
[iter 200] loss=0.7948 val_loss=0.0000 scale=4.0000 norm=3.2372
[iter 300] loss=0.3944 val_loss=0.0000 scale=4.0000 norm=3.2075
[iter 400] loss=0.1230 val_loss=0.0000 scale=4.0000 norm=3.1545
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7648 val_loss=0.0000 scale=2.0000 norm=1.3383
[iter 200] loss=0.5141 val_loss=0.0000 scale=2.0000 norm=1.3838
[iter 300] loss=0.1709 val_loss=0.0000 scale=4.0000 norm=2.7566
[iter 400] loss=-0.1242 val_loss=0.0000 scale=4.0000 norm=2.6972
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0091 val_loss=0.0000 scale=2.0000 norm=1.5897
[iter 200] loss=0.7413 val_loss=0.0000 scale=4.0000 norm=3.1259
[iter 300] loss=0.3690 val_loss=0.0000 scale=4.0000 norm=3.0775
[iter 400] loss=0.0987 val_loss=0.0000 scale=4.0000 norm=2.9733
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0597 val_loss=0.0000 scale=2.0000 norm=1.6575
[iter 200] loss=0.7506 val_loss=0.0000 scale=4.0000 norm=3.2848
[iter 300] loss=0.3433 val_loss=0.0000 scale=4.0000 norm=3.2085
[iter 400] loss=0.0007 val_loss=0.0000 scale=8.0000 norm=6.3657
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9452 val_loss=0.0000 scale=2.0000 norm=1.5669
[iter 200] loss=0.5583 val_loss=0.0000 scale=4.0000 norm=3.0333
[iter 300] loss=0.1924 val_loss=0.0000 scale=4.0000 norm=2.9496
[iter 400] loss=-0.0201 val_loss=0.0000 scale=2.0000 norm=1.4484
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9580 val_loss=0.0000 scale=2.0000 norm=1.6013
[iter 200] loss=0.6908 val_loss=0.0000 scale=4.0000 norm=3.1563
[iter 300] loss=0.3681 val_loss=0.0000 scale=4.0000 norm=3.1572
[iter 400] loss=0.1042 val_loss=0.0000 scale=4.0000 norm=3.1076
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0306 val_loss=0.0000 scale=2.0000 norm=1.6109
[iter 200] loss=0.7959 val_loss=0.0000 scale=4.0000 norm=3.1996
[iter 300] loss=0.4635 val_loss=0.0000 scale=4.0000 norm=3.1566
[iter 400] loss=0.2801 val_loss=0.0000 scale=4.0000 norm=3.1364
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0811 val_loss=0.0000 scale=2.0000 norm=1.6179
[iter 200] loss=0.9031 val_loss=0.0000 scale=2.0000 norm=1.5979
[iter 300] loss=0.6286 val_loss=0.0000 scale=4.0000 norm=3.1571
[iter 400] loss=0.4345 val_loss=0.0000 scale=4.0000 norm=3.0714
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9785 val_loss=0.0000 scale=2.0000 norm=1.5633
[iter 200] loss=0.7017 val_loss=0.0000 scale=4.0000 norm=3.1163
[iter 300] loss=0.2868 val_loss=0.0000 scale=4.0000 norm=3.0507
[iter 400] loss=0.0142 val_loss=0.0000 scale=4.0000 norm=3.0200
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0332 val_loss=0.0000 scale=2.0000 norm=1.7085
[iter 200] loss=0.4992 val_loss=0.0000 scale=4.0000 norm=3.3739
[iter 300] loss=-0.1150 val_loss=0.0000 scale=8.0000 norm=6.4115
[iter 400] loss=-1.2991 val_loss=0.0000 scale=16.0000 norm=12.8159
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0166 val_loss=0.0000 scale=2.0000 norm=1.5861
[iter 200] loss=0.7932 val_loss=0.0000 scale=2.0000 norm=1.5678
[iter 300] loss=0.4841 val_loss=0.0000 scale=4.0000 norm=3.1203
[iter 400] loss=0.2830 val_loss=0.0000 scale=4.0000 norm=3.0683
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0163 val_loss=0.0000 scale=2.0000 norm=1.5970
[iter 200] loss=0.7829 val_loss=0.0000 scale=2.0000 norm=1.5836
[iter 300] loss=0.4651 val_loss=0.0000 scale=4.0000 norm=3.1419
[iter 400] loss=0.2739 val_loss=0.0000 scale=4.0000 norm=3.1726
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8419 val_loss=0.0000 scale=2.0000 norm=1.6270
[iter 200] loss=0.3599 val_loss=0.0000 scale=4.0000 norm=3.3314
[iter 300] loss=-0.4441 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-1.9801 val_loss=0.0000 scale=16.0000 norm=13.3436
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0750 val_loss=0.0000 scale=2.0000 norm=1.6346
[iter 200] loss=0.8925 val_loss=0.0000 scale=2.0000 norm=1.6255
[iter 300] loss=0.6299 val_loss=0.0000 scale=4.0000 norm=3.1917
[iter 400] loss=0.4934 val_loss=0.0000 scale=4.0000 norm=3.2021
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0494 val_loss=0.0000 scale=2.0000 norm=1.6297
[iter 200] loss=0.8261 val_loss=0.0000 scale=2.0000 norm=1.6232
[iter 300] loss=0.5151 val_loss=0.0000 scale=4.0000 norm=3.1907
[iter 400] loss=0.3727 val_loss=0.0000 scale=4.0000 norm=3.1649
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0114 val_loss=0.0000 scale=2.0000 norm=1.5973
[iter 200] loss=0.7315 val_loss=0.0000 scale=4.0000 norm=3.1733
[iter 300] loss=0.3927 val_loss=0.0000 scale=4.0000 norm=3.1327
[iter 400] loss=0.2408 val_loss=0.0000 scale=4.0000 norm=3.1681
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8895 val_loss=0.0000 scale=2.0000 norm=1.4819
[iter 200] loss=0.6571 val_loss=0.0000 scale=2.0000 norm=1.5098
[iter 300] loss=0.3248 val_loss=0.0000 scale=4.0000 norm=2.9663
[iter 400] loss=0.1407 val_loss=0.0000 scale=4.0000 norm=2.9785
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0269 val_loss=0.0000 scale=2.0000 norm=1.6088
[iter 200] loss=0.8144 val_loss=0.0000 scale=2.0000 norm=1.6054
[iter 300] loss=0.4872 val_loss=0.0000 scale=4.0000 norm=3.1535
[iter 400] loss=0.2901 val_loss=0.0000 scale=4.0000 norm=3.0943
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7349 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 200] loss=-0.1759 val_loss=0.0000 scale=4.0000 norm=3.0688
[iter 300] loss=-1.3658 val_loss=0.0000 scale=8.0000 norm=5.8055
[iter 400] loss=-3.5738 val_loss=0.0000 scale=16.0000 norm=11.6070
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0299 val_loss=0.0000 scale=2.0000 norm=1.6088
[iter 200] loss=0.7810 val_loss=0.0000 scale=4.0000 norm=3.1807
[iter 300] loss=0.4455 val_loss=0.0000 scale=4.0000 norm=3.1422
[iter 400] loss=0.2089 val_loss=0.0000 scale=8.0000 norm=6.3021
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0000 val_loss=0.0000 scale=2.0000 norm=1.5836
[iter 200] loss=0.7328 val_loss=0.0000 scale=4.0000 norm=3.1421
[iter 300] loss=0.3901 val_loss=0.0000 scale=4.0000 norm=3.0762
[iter 400] loss=0.2210 val_loss=0.0000 scale=4.0000 norm=3.0983
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0484 val_loss=0.0000 scale=2.0000 norm=1.6424
[iter 200] loss=0.8112 val_loss=0.0000 scale=4.0000 norm=3.2688
[iter 300] loss=0.4933 val_loss=0.0000 scale=4.0000 norm=3.2046
[iter 400] loss=0.3591 val_loss=0.0000 scale=4.0000 norm=3.2101
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9507 val_loss=0.0000 scale=2.0000 norm=1.5696
[iter 200] loss=0.5706 val_loss=0.0000 scale=4.0000 norm=3.1395
[iter 300] loss=0.0980 val_loss=0.0000 scale=4.0000 norm=2.9977
[iter 400] loss=-0.1635 val_loss=0.0000 scale=4.0000 norm=3.0060
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0064 val_loss=0.0000 scale=2.0000 norm=1.6011
[iter 200] loss=0.7443 val_loss=0.0000 scale=4.0000 norm=3.1911
[iter 300] loss=0.3501 val_loss=0.0000 scale=4.0000 norm=3.1590
[iter 400] loss=0.0976 val_loss=0.0000 scale=4.0000 norm=3.1634
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9858 val_loss=0.0000 scale=2.0000 norm=1.5676
[iter 200] loss=0.7396 val_loss=0.0000 scale=4.0000 norm=3.1102
[iter 300] loss=0.4068 val_loss=0.0000 scale=4.0000 norm=3.0947
[iter 400] loss=0.2117 val_loss=0.0000 scale=4.0000 norm=3.0422
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9597 val_loss=0.0000 scale=2.0000 norm=1.5537
[iter 200] loss=0.6761 val_loss=0.0000 scale=2.0000 norm=1.5294
[iter 300] loss=0.3166 val_loss=0.0000 scale=4.0000 norm=3.0104
[iter 400] loss=0.0237 val_loss=0.0000 scale=4.0000 norm=2.9216
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0365 val_loss=0.0000 scale=2.0000 norm=1.6107
[iter 200] loss=0.8305 val_loss=0.0000 scale=2.0000 norm=1.5874
[iter 300] loss=0.5494 val_loss=0.0000 scale=4.0000 norm=3.1251
[iter 400] loss=0.3287 val_loss=0.0000 scale=4.0000 norm=3.0668
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9905 val_loss=0.0000 scale=2.0000 norm=1.5699
[iter 200] loss=0.7027 val_loss=0.0000 scale=4.0000 norm=3.1383
[iter 300] loss=0.3407 val_loss=0.0000 scale=4.0000 norm=3.1035
[iter 400] loss=0.1174 val_loss=0.0000 scale=4.0000 norm=3.0228
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9818 val_loss=0.0000 scale=2.0000 norm=1.5687
[iter 200] loss=0.6995 val_loss=0.0000 scale=4.0000 norm=3.0671
[iter 300] loss=0.3597 val_loss=0.0000 scale=4.0000 norm=3.0082
[iter 400] loss=0.1084 val_loss=0.0000 scale=4.0000 norm=2.9512
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9575 val_loss=0.0000 scale=2.0000 norm=1.5669
[iter 200] loss=0.6260 val_loss=0.0000 scale=4.0000 norm=3.1315
[iter 300] loss=0.1862 val_loss=0.0000 scale=4.0000 norm=3.1153
[iter 400] loss=-0.1550 val_loss=0.0000 scale=4.0000 norm=3.0544
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9237 val_loss=0.0000 scale=2.0000 norm=1.6352
[iter 200] loss=0.5851 val_loss=0.0000 scale=4.0000 norm=3.3118
[iter 300] loss=0.0661 val_loss=0.0000 scale=4.0000 norm=3.2918
[iter 400] loss=-0.4683 val_loss=0.0000 scale=4.0000 norm=3.2765
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8727 val_loss=0.0000 scale=2.0000 norm=1.5373
[iter 200] loss=0.4740 val_loss=0.0000 scale=4.0000 norm=3.0410
[iter 300] loss=-0.0212 val_loss=0.0000 scale=4.0000 norm=3.0165
[iter 400] loss=-0.3626 val_loss=0.0000 scale=4.0000 norm=2.9477
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0515 val_loss=0.0000 scale=2.0000 norm=1.6722
[iter 200] loss=0.8253 val_loss=0.0000 scale=2.0000 norm=1.6632
[iter 300] loss=0.5554 val_loss=0.0000 scale=4.0000 norm=3.2524
[iter 400] loss=0.4210 val_loss=0.0000 scale=4.0000 norm=3.2082
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9079 val_loss=0.0000 scale=2.0000 norm=1.5572
[iter 200] loss=0.5612 val_loss=0.0000 scale=4.0000 norm=3.0512
[iter 300] loss=0.1344 val_loss=0.0000 scale=4.0000 norm=2.9404
[iter 400] loss=-0.1469 val_loss=0.0000 scale=4.0000 norm=2.8171
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9788 val_loss=0.0000 scale=2.0000 norm=1.6021
[iter 200] loss=0.6585 val_loss=0.0000 scale=4.0000 norm=3.1894
[iter 300] loss=0.3053 val_loss=0.0000 scale=4.0000 norm=3.1157
[iter 400] loss=0.1110 val_loss=0.0000 scale=4.0000 norm=3.0756
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6583 val_loss=0.0000 scale=2.0000 norm=1.2787
[iter 200] loss=0.4069 val_loss=0.0000 scale=2.0000 norm=1.3031
[iter 300] loss=0.1940 val_loss=0.0000 scale=4.0000 norm=2.6819
[iter 400] loss=-0.0517 val_loss=0.0000 scale=8.0000 norm=5.3463
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0152 val_loss=0.0000 scale=2.0000 norm=1.5980
[iter 200] loss=0.7672 val_loss=0.0000 scale=2.0000 norm=1.5903
[iter 300] loss=0.4303 val_loss=0.0000 scale=4.0000 norm=3.1454
[iter 400] loss=0.2153 val_loss=0.0000 scale=4.0000 norm=3.1504
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=2.0000 norm=1.5856
[iter 200] loss=0.7490 val_loss=0.0000 scale=2.0000 norm=1.5577
[iter 300] loss=0.4507 val_loss=0.0000 scale=4.0000 norm=3.0737
[iter 400] loss=0.2147 val_loss=0.0000 scale=4.0000 norm=2.9834
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0388 val_loss=0.0000 scale=2.0000 norm=1.6202
[iter 200] loss=0.7722 val_loss=0.0000 scale=4.0000 norm=3.2211
[iter 300] loss=0.4152 val_loss=0.0000 scale=4.0000 norm=3.1664
[iter 400] loss=0.2137 val_loss=0.0000 scale=4.0000 norm=3.0932
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0786 val_loss=0.0000 scale=2.0000 norm=1.6572
[iter 200] loss=0.7872 val_loss=0.0000 scale=4.0000 norm=3.2766
[iter 300] loss=0.4608 val_loss=0.0000 scale=4.0000 norm=3.1942
[iter 400] loss=0.2454 val_loss=0.0000 scale=8.0000 norm=6.2911
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9863 val_loss=0.0000 scale=2.0000 norm=1.5715
[iter 200] loss=0.7213 val_loss=0.0000 scale=4.0000 norm=3.1186
[iter 300] loss=0.3636 val_loss=0.0000 scale=4.0000 norm=3.0769
[iter 400] loss=0.1480 val_loss=0.0000 scale=4.0000 norm=3.0602
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6962 val_loss=0.0000 scale=2.0000 norm=1.5054
[iter 200] loss=0.1569 val_loss=0.0000 scale=4.0000 norm=2.8220
[iter 300] loss=-0.4385 val_loss=0.0000 scale=4.0000 norm=2.7348
[iter 400] loss=-0.8449 val_loss=0.0000 scale=4.0000 norm=2.7303
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1171 val_loss=0.0000 scale=2.0000 norm=1.6859
[iter 200] loss=0.9362 val_loss=0.0000 scale=2.0000 norm=1.6566
[iter 300] loss=0.6322 val_loss=0.0000 scale=4.0000 norm=3.3175
[iter 400] loss=0.3411 val_loss=0.0000 scale=4.0000 norm=3.2949
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9500 val_loss=0.0000 scale=2.0000 norm=1.6676
[iter 200] loss=0.4892 val_loss=0.0000 scale=4.0000 norm=3.3495
[iter 300] loss=-0.3106 val_loss=0.0000 scale=4.0000 norm=3.3524
[iter 400] loss=-1.1764 val_loss=0.0000 scale=4.0000 norm=3.3247
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0425 val_loss=0.0000 scale=2.0000 norm=1.6435
[iter 200] loss=0.8264 val_loss=0.0000 scale=4.0000 norm=3.2807
[iter 300] loss=0.4993 val_loss=0.0000 scale=4.0000 norm=3.2448
[iter 400] loss=0.3435 val_loss=0.0000 scale=4.0000 norm=3.1956
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0375 val_loss=0.0000 scale=2.0000 norm=1.6315
[iter 200] loss=0.8013 val_loss=0.0000 scale=2.0000 norm=1.6266
[iter 300] loss=0.4793 val_loss=0.0000 scale=4.0000 norm=3.1998
[iter 400] loss=0.3213 val_loss=0.0000 scale=4.0000 norm=3.2066
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9887 val_loss=0.0000 scale=2.0000 norm=1.6446
[iter 200] loss=0.4569 val_loss=0.0000 scale=4.0000 norm=3.2644
[iter 300] loss=-0.2038 val_loss=0.0000 scale=4.0000 norm=3.2013
[iter 400] loss=-0.7815 val_loss=0.0000 scale=4.0000 norm=3.1972
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9171 val_loss=0.0000 scale=2.0000 norm=1.6221
[iter 200] loss=0.6078 val_loss=0.0000 scale=4.0000 norm=3.3540
[iter 300] loss=0.1576 val_loss=0.0000 scale=4.0000 norm=3.3579
[iter 400] loss=-0.2096 val_loss=0.0000 scale=4.0000 norm=3.2346
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9696 val_loss=0.0000 scale=2.0000 norm=1.5777
[iter 200] loss=0.6746 val_loss=0.0000 scale=2.0000 norm=1.5607
[iter 300] loss=0.2312 val_loss=0.0000 scale=4.0000 norm=3.0773
[iter 400] loss=-0.0363 val_loss=0.0000 scale=4.0000 norm=2.9844
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0043 val_loss=0.0000 scale=2.0000 norm=1.6028
[iter 200] loss=0.6982 val_loss=0.0000 scale=4.0000 norm=3.1856
[iter 300] loss=0.3507 val_loss=0.0000 scale=4.0000 norm=3.1398
[iter 400] loss=0.1237 val_loss=0.0000 scale=2.0000 norm=1.5467
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8234 val_loss=0.0000 scale=2.0000 norm=1.5030
[iter 200] loss=0.5035 val_loss=0.0000 scale=2.0000 norm=1.5241
[iter 300] loss=0.0741 val_loss=0.0000 scale=4.0000 norm=2.9517
[iter 400] loss=-0.3386 val_loss=0.0000 scale=4.0000 norm=2.9397
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9339 val_loss=0.0000 scale=2.0000 norm=1.5535
[iter 200] loss=0.5599 val_loss=0.0000 scale=4.0000 norm=3.0702
[iter 300] loss=0.0225 val_loss=0.0000 scale=4.0000 norm=3.0077
[iter 400] loss=-0.4613 val_loss=0.0000 scale=4.0000 norm=2.9539
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9395 val_loss=0.0000 scale=2.0000 norm=1.5559
[iter 200] loss=0.7674 val_loss=0.0000 scale=2.0000 norm=1.5488
[iter 300] loss=0.5611 val_loss=0.0000 scale=4.0000 norm=3.0599
[iter 400] loss=0.2355 val_loss=0.0000 scale=8.0000 norm=6.1201
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0498 val_loss=0.0000 scale=2.0000 norm=1.6268
[iter 200] loss=0.8372 val_loss=0.0000 scale=4.0000 norm=3.2493
[iter 300] loss=0.5370 val_loss=0.0000 scale=4.0000 norm=3.1877
[iter 400] loss=0.4018 val_loss=0.0000 scale=4.0000 norm=3.1854
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0076 val_loss=0.0000 scale=2.0000 norm=1.5894
[iter 200] loss=0.7501 val_loss=0.0000 scale=4.0000 norm=3.1417
[iter 300] loss=0.4090 val_loss=0.0000 scale=4.0000 norm=3.1156
[iter 400] loss=0.2505 val_loss=0.0000 scale=4.0000 norm=3.1088
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5974 val_loss=0.0000 scale=2.0000 norm=1.1759
[iter 200] loss=0.2280 val_loss=0.0000 scale=2.0000 norm=1.1446
[iter 300] loss=-0.1512 val_loss=0.0000 scale=4.0000 norm=2.3407
[iter 400] loss=-0.4338 val_loss=0.0000 scale=4.0000 norm=2.2956
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9970 val_loss=0.0000 scale=2.0000 norm=1.5786
[iter 200] loss=0.7538 val_loss=0.0000 scale=2.0000 norm=1.5751
[iter 300] loss=0.3901 val_loss=0.0000 scale=4.0000 norm=3.1328
[iter 400] loss=0.1529 val_loss=0.0000 scale=4.0000 norm=3.0950
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6244 val_loss=0.0000 scale=2.0000 norm=1.1958
[iter 200] loss=0.2667 val_loss=0.0000 scale=2.0000 norm=1.2849
[iter 300] loss=-0.1980 val_loss=0.0000 scale=4.0000 norm=2.5824
[iter 400] loss=-0.6026 val_loss=0.0000 scale=8.0000 norm=4.9499
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8964 val_loss=0.0000 scale=2.0000 norm=1.5811
[iter 200] loss=0.6372 val_loss=0.0000 scale=2.0000 norm=1.6244
[iter 300] loss=0.2989 val_loss=0.0000 scale=4.0000 norm=3.1303
[iter 400] loss=-0.0845 val_loss=0.0000 scale=8.0000 norm=6.3115
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0117 val_loss=0.0000 scale=2.0000 norm=1.5987
[iter 200] loss=0.7504 val_loss=0.0000 scale=4.0000 norm=3.1547
[iter 300] loss=0.4310 val_loss=0.0000 scale=4.0000 norm=3.1007
[iter 400] loss=0.2559 val_loss=0.0000 scale=4.0000 norm=3.1672
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9999 val_loss=0.0000 scale=2.0000 norm=1.5860
[iter 200] loss=0.7566 val_loss=0.0000 scale=2.0000 norm=1.5841
[iter 300] loss=0.4257 val_loss=0.0000 scale=4.0000 norm=3.1242
[iter 400] loss=0.2552 val_loss=0.0000 scale=4.0000 norm=3.1134
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1225 val_loss=0.0000 scale=2.0000 norm=1.6698
[iter 200] loss=0.9594 val_loss=0.0000 scale=4.0000 norm=3.3088
[iter 300] loss=0.7196 val_loss=0.0000 scale=4.0000 norm=3.2620
[iter 400] loss=0.5636 val_loss=0.0000 scale=4.0000 norm=3.2196
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0654 val_loss=0.0000 scale=2.0000 norm=1.6700
[iter 200] loss=0.7415 val_loss=0.0000 scale=4.0000 norm=3.3184
[iter 300] loss=0.3755 val_loss=0.0000 scale=4.0000 norm=3.2899
[iter 400] loss=0.0591 val_loss=0.0000 scale=4.0000 norm=3.2215
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4758 val_loss=0.0000 scale=2.0000 norm=1.0500
[iter 200] loss=0.0729 val_loss=0.0000 scale=2.0000 norm=1.1270
[iter 300] loss=-0.3956 val_loss=0.0000 scale=4.0000 norm=2.2556
[iter 400] loss=-1.1922 val_loss=0.0000 scale=8.0000 norm=4.5003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9834 val_loss=0.0000 scale=2.0000 norm=1.5755
[iter 200] loss=0.7530 val_loss=0.0000 scale=2.0000 norm=1.5677
[iter 300] loss=0.4320 val_loss=0.0000 scale=4.0000 norm=3.1392
[iter 400] loss=0.2049 val_loss=0.0000 scale=4.0000 norm=3.1068
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0475 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 200] loss=0.8308 val_loss=0.0000 scale=4.0000 norm=3.4606
[iter 300] loss=0.4707 val_loss=0.0000 scale=8.0000 norm=6.9339
[iter 400] loss=-0.2373 val_loss=0.0000 scale=16.0000 norm=13.8680
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0623 val_loss=0.0000 scale=2.0000 norm=1.6426
[iter 200] loss=0.8437 val_loss=0.0000 scale=2.0000 norm=1.6447
[iter 300] loss=0.5343 val_loss=0.0000 scale=4.0000 norm=3.2399
[iter 400] loss=0.3179 val_loss=0.0000 scale=4.0000 norm=3.2384
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0351 val_loss=0.0000 scale=2.0000 norm=1.6180
[iter 200] loss=0.7885 val_loss=0.0000 scale=4.0000 norm=3.2258
[iter 300] loss=0.5092 val_loss=0.0000 scale=4.0000 norm=3.1673
[iter 400] loss=0.3712 val_loss=0.0000 scale=4.0000 norm=3.1392
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0104 val_loss=0.0000 scale=2.0000 norm=1.5748
[iter 200] loss=0.7708 val_loss=0.0000 scale=4.0000 norm=3.1227
[iter 300] loss=0.4011 val_loss=0.0000 scale=4.0000 norm=3.0564
[iter 400] loss=0.1984 val_loss=0.0000 scale=4.0000 norm=3.0563
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0568 val_loss=0.0000 scale=2.0000 norm=1.6145
[iter 200] loss=0.8521 val_loss=0.0000 scale=4.0000 norm=3.2015
[iter 300] loss=0.5389 val_loss=0.0000 scale=4.0000 norm=3.1407
[iter 400] loss=0.3529 val_loss=0.0000 scale=4.0000 norm=3.0856
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9884 val_loss=0.0000 scale=2.0000 norm=1.5692
[iter 200] loss=0.7461 val_loss=0.0000 scale=4.0000 norm=3.1340
[iter 300] loss=0.3893 val_loss=0.0000 scale=4.0000 norm=3.1183
[iter 400] loss=0.2023 val_loss=0.0000 scale=4.0000 norm=3.1283
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0546 val_loss=0.0000 scale=2.0000 norm=1.6626
[iter 200] loss=0.8358 val_loss=0.0000 scale=2.0000 norm=1.6541
[iter 300] loss=0.5457 val_loss=0.0000 scale=4.0000 norm=3.2555
[iter 400] loss=0.3741 val_loss=0.0000 scale=4.0000 norm=3.2100
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9705 val_loss=0.0000 scale=2.0000 norm=1.5515
[iter 200] loss=0.7295 val_loss=0.0000 scale=2.0000 norm=1.5248
[iter 300] loss=0.4257 val_loss=0.0000 scale=4.0000 norm=3.0142
[iter 400] loss=0.2051 val_loss=0.0000 scale=4.0000 norm=2.9424
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9150 val_loss=0.0000 scale=2.0000 norm=1.5396
[iter 200] loss=0.5925 val_loss=0.0000 scale=4.0000 norm=2.9963
[iter 300] loss=0.2294 val_loss=0.0000 scale=4.0000 norm=2.9683
[iter 400] loss=-0.0333 val_loss=0.0000 scale=8.0000 norm=5.9175
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0484 val_loss=0.0000 scale=2.0000 norm=1.6541
[iter 200] loss=0.7273 val_loss=0.0000 scale=4.0000 norm=3.3128
[iter 300] loss=0.2957 val_loss=0.0000 scale=4.0000 norm=3.2730
[iter 400] loss=-0.0140 val_loss=0.0000 scale=4.0000 norm=3.2163
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7417 val_loss=0.0000 scale=2.0000 norm=1.4641
[iter 200] loss=0.3527 val_loss=0.0000 scale=4.0000 norm=3.0540
[iter 300] loss=-0.1875 val_loss=0.0000 scale=4.0000 norm=3.0526
[iter 400] loss=-0.8845 val_loss=0.0000 scale=8.0000 norm=5.9929
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0353 val_loss=0.0000 scale=2.0000 norm=1.7343
[iter 200] loss=0.8080 val_loss=0.0000 scale=4.0000 norm=3.4555
[iter 300] loss=0.4357 val_loss=0.0000 scale=4.0000 norm=3.4193
[iter 400] loss=0.1004 val_loss=0.0000 scale=4.0000 norm=3.3686
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9913 val_loss=0.0000 scale=2.0000 norm=1.5691
[iter 200] loss=0.7456 val_loss=0.0000 scale=2.0000 norm=1.5581
[iter 300] loss=0.4870 val_loss=0.0000 scale=4.0000 norm=3.1073
[iter 400] loss=0.2385 val_loss=0.0000 scale=4.0000 norm=3.0765
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.6188
[iter 200] loss=0.7280 val_loss=0.0000 scale=4.0000 norm=3.1288
[iter 300] loss=0.4582 val_loss=0.0000 scale=4.0000 norm=3.0381
[iter 400] loss=0.2485 val_loss=0.0000 scale=8.0000 norm=5.9098
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0542 val_loss=0.0000 scale=2.0000 norm=1.6348
[iter 200] loss=0.7397 val_loss=0.0000 scale=4.0000 norm=3.2656
[iter 300] loss=0.4657 val_loss=0.0000 scale=4.0000 norm=3.2159
[iter 400] loss=0.3776 val_loss=0.0000 scale=2.0000 norm=1.5816
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0126 val_loss=0.0000 scale=2.0000 norm=1.5917
[iter 200] loss=0.7852 val_loss=0.0000 scale=2.0000 norm=1.5795
[iter 300] loss=0.4848 val_loss=0.0000 scale=4.0000 norm=3.1413
[iter 400] loss=0.2902 val_loss=0.0000 scale=4.0000 norm=3.1017
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9853 val_loss=0.0000 scale=2.0000 norm=1.5628
[iter 200] loss=0.7452 val_loss=0.0000 scale=2.0000 norm=1.5484
[iter 300] loss=0.4269 val_loss=0.0000 scale=4.0000 norm=3.0570
[iter 400] loss=0.2346 val_loss=0.0000 scale=4.0000 norm=3.0092
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0103 val_loss=0.0000 scale=2.0000 norm=1.5960
[iter 200] loss=0.7740 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 300] loss=0.4490 val_loss=0.0000 scale=4.0000 norm=3.1342
[iter 400] loss=0.2097 val_loss=0.0000 scale=4.0000 norm=3.0849
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0641 val_loss=0.0000 scale=2.0000 norm=1.6669
[iter 200] loss=0.8683 val_loss=0.0000 scale=2.0000 norm=1.6748
[iter 300] loss=0.6663 val_loss=0.0000 scale=2.0000 norm=1.6527
[iter 400] loss=0.5338 val_loss=0.0000 scale=4.0000 norm=3.2791
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9075 val_loss=0.0000 scale=2.0000 norm=1.5312
[iter 200] loss=0.5492 val_loss=0.0000 scale=4.0000 norm=2.9545
[iter 300] loss=0.0941 val_loss=0.0000 scale=4.0000 norm=2.7941
[iter 400] loss=-0.2308 val_loss=0.0000 scale=4.0000 norm=2.6837
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8630 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 200] loss=0.3981 val_loss=0.0000 scale=4.0000 norm=3.4013
[iter 300] loss=-0.1294 val_loss=0.0000 scale=4.0000 norm=3.3008
[iter 400] loss=-0.9508 val_loss=0.0000 scale=16.0000 norm=12.9102
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9641 val_loss=0.0000 scale=2.0000 norm=1.5390
[iter 200] loss=0.7023 val_loss=0.0000 scale=2.0000 norm=1.5229
[iter 300] loss=0.3617 val_loss=0.0000 scale=4.0000 norm=3.0030
[iter 400] loss=0.0916 val_loss=0.0000 scale=4.0000 norm=2.9479
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0161 val_loss=0.0000 scale=2.0000 norm=1.6035
[iter 200] loss=0.7566 val_loss=0.0000 scale=2.0000 norm=1.5935
[iter 300] loss=0.3761 val_loss=0.0000 scale=4.0000 norm=3.1385
[iter 400] loss=0.1156 val_loss=0.0000 scale=4.0000 norm=3.1070
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0497 val_loss=0.0000 scale=2.0000 norm=1.6596
[iter 200] loss=0.7332 val_loss=0.0000 scale=4.0000 norm=3.2501
[iter 300] loss=0.4585 val_loss=0.0000 scale=4.0000 norm=3.2303
[iter 400] loss=0.2540 val_loss=0.0000 scale=4.0000 norm=3.1700
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7938 val_loss=0.0000 scale=2.0000 norm=1.3564
[iter 200] loss=0.5713 val_loss=0.0000 scale=2.0000 norm=1.4208
[iter 300] loss=0.2653 val_loss=0.0000 scale=4.0000 norm=2.8674
[iter 400] loss=0.0071 val_loss=0.0000 scale=4.0000 norm=2.8506
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9896 val_loss=0.0000 scale=2.0000 norm=1.6080
[iter 200] loss=0.6121 val_loss=0.0000 scale=4.0000 norm=3.2121
[iter 300] loss=0.1242 val_loss=0.0000 scale=4.0000 norm=3.1665
[iter 400] loss=-0.2536 val_loss=0.0000 scale=4.0000 norm=3.0913
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0603 val_loss=0.0000 scale=2.0000 norm=1.6302
[iter 200] loss=0.8430 val_loss=0.0000 scale=4.0000 norm=3.2345
[iter 300] loss=0.5299 val_loss=0.0000 scale=4.0000 norm=3.1938
[iter 400] loss=0.3250 val_loss=0.0000 scale=4.0000 norm=3.1732
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8184 val_loss=0.0000 scale=2.0000 norm=1.4168
[iter 200] loss=0.6018 val_loss=0.0000 scale=2.0000 norm=1.4868
[iter 300] loss=0.2731 val_loss=0.0000 scale=4.0000 norm=2.9760
[iter 400] loss=0.0000 val_loss=0.0000 scale=4.0000 norm=2.9195
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9255 val_loss=0.0000 scale=2.0000 norm=1.5644
[iter 200] loss=0.5996 val_loss=0.0000 scale=4.0000 norm=3.0316
[iter 300] loss=0.2230 val_loss=0.0000 scale=4.0000 norm=2.9889
[iter 400] loss=-0.1011 val_loss=0.0000 scale=4.0000 norm=2.9455
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0140 val_loss=0.0000 scale=2.0000 norm=1.6164
[iter 200] loss=0.6701 val_loss=0.0000 scale=4.0000 norm=3.2362
[iter 300] loss=0.1588 val_loss=0.0000 scale=4.0000 norm=3.2360
[iter 400] loss=-0.3339 val_loss=0.0000 scale=8.0000 norm=6.3885
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9605 val_loss=0.0000 scale=2.0000 norm=1.5490
[iter 200] loss=0.6895 val_loss=0.0000 scale=2.0000 norm=1.5190
[iter 300] loss=0.3803 val_loss=0.0000 scale=4.0000 norm=3.0036
[iter 400] loss=0.1592 val_loss=0.0000 scale=4.0000 norm=2.9623
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0335 val_loss=0.0000 scale=2.0000 norm=1.6312
[iter 200] loss=0.8007 val_loss=0.0000 scale=2.0000 norm=1.6330
[iter 300] loss=0.4438 val_loss=0.0000 scale=4.0000 norm=3.2551
[iter 400] loss=0.1390 val_loss=0.0000 scale=4.0000 norm=3.2166
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9777 val_loss=0.0000 scale=2.0000 norm=1.6262
[iter 200] loss=0.5851 val_loss=0.0000 scale=4.0000 norm=3.2013
[iter 300] loss=0.0945 val_loss=0.0000 scale=4.0000 norm=3.1531
[iter 400] loss=-0.3010 val_loss=0.0000 scale=4.0000 norm=3.0749
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0426 val_loss=0.0000 scale=2.0000 norm=1.6077
[iter 200] loss=0.8184 val_loss=0.0000 scale=4.0000 norm=3.1961
[iter 300] loss=0.4877 val_loss=0.0000 scale=4.0000 norm=3.1260
[iter 400] loss=0.3124 val_loss=0.0000 scale=4.0000 norm=3.0840
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0230 val_loss=0.0000 scale=2.0000 norm=1.6189
[iter 200] loss=0.7439 val_loss=0.0000 scale=4.0000 norm=3.2123
[iter 300] loss=0.4306 val_loss=0.0000 scale=4.0000 norm=3.1470
[iter 400] loss=0.2565 val_loss=0.0000 scale=4.0000 norm=3.1095
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7903 val_loss=0.0000 scale=2.0000 norm=1.5039
[iter 200] loss=0.4903 val_loss=0.0000 scale=4.0000 norm=3.1665
[iter 300] loss=0.0575 val_loss=0.0000 scale=4.0000 norm=3.1419
[iter 400] loss=-0.6002 val_loss=0.0000 scale=8.0000 norm=6.2143
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0651 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 200] loss=0.8404 val_loss=0.0000 scale=2.0000 norm=1.6496
[iter 300] loss=0.4816 val_loss=0.0000 scale=4.0000 norm=3.2646
[iter 400] loss=0.2525 val_loss=0.0000 scale=4.0000 norm=3.2079
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8184 val_loss=0.0000 scale=2.0000 norm=1.4216
[iter 200] loss=0.5884 val_loss=0.0000 scale=2.0000 norm=1.4640
[iter 300] loss=0.2842 val_loss=0.0000 scale=4.0000 norm=2.9013
[iter 400] loss=0.0387 val_loss=0.0000 scale=4.0000 norm=2.8415
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9385 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 200] loss=0.7385 val_loss=0.0000 scale=4.0000 norm=3.1936
[iter 300] loss=0.4380 val_loss=0.0000 scale=4.0000 norm=3.2107
[iter 400] loss=-0.1290 val_loss=0.0000 scale=8.0000 norm=6.4220
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0348 val_loss=0.0000 scale=2.0000 norm=1.6094
[iter 200] loss=0.7933 val_loss=0.0000 scale=4.0000 norm=3.1928
[iter 300] loss=0.4823 val_loss=0.0000 scale=4.0000 norm=3.1576
[iter 400] loss=0.3216 val_loss=0.0000 scale=4.0000 norm=3.1039
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0150 val_loss=0.0000 scale=2.0000 norm=1.5801
[iter 200] loss=0.7910 val_loss=0.0000 scale=2.0000 norm=1.5637
[iter 300] loss=0.5236 val_loss=0.0000 scale=4.0000 norm=3.0980
[iter 400] loss=0.2806 val_loss=0.0000 scale=4.0000 norm=3.0366
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0004 val_loss=0.0000 scale=2.0000 norm=1.5765
[iter 200] loss=0.7791 val_loss=0.0000 scale=2.0000 norm=1.5404
[iter 300] loss=0.4829 val_loss=0.0000 scale=4.0000 norm=3.0444
[iter 400] loss=0.3417 val_loss=0.0000 scale=4.0000 norm=3.0022
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0640 val_loss=0.0000 scale=2.0000 norm=1.6309
[iter 200] loss=0.8540 val_loss=0.0000 scale=2.0000 norm=1.6081
[iter 300] loss=0.5514 val_loss=0.0000 scale=4.0000 norm=3.1646
[iter 400] loss=0.3034 val_loss=0.0000 scale=4.0000 norm=3.1591
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0113 val_loss=0.0000 scale=2.0000 norm=1.5885
[iter 200] loss=0.7809 val_loss=0.0000 scale=2.0000 norm=1.5794
[iter 300] loss=0.4632 val_loss=0.0000 scale=4.0000 norm=3.1189
[iter 400] loss=0.2787 val_loss=0.0000 scale=4.0000 norm=3.0939
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0310 val_loss=0.0000 scale=2.0000 norm=1.6053
[iter 200] loss=0.8092 val_loss=0.0000 scale=2.0000 norm=1.5912
[iter 300] loss=0.4975 val_loss=0.0000 scale=4.0000 norm=3.1532
[iter 400] loss=0.2989 val_loss=0.0000 scale=4.0000 norm=3.0852
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0457 val_loss=0.0000 scale=2.0000 norm=1.6195
[iter 200] loss=0.8366 val_loss=0.0000 scale=2.0000 norm=1.6221
[iter 300] loss=0.5576 val_loss=0.0000 scale=4.0000 norm=3.2545
[iter 400] loss=0.3674 val_loss=0.0000 scale=4.0000 norm=3.1723
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0156 val_loss=0.0000 scale=2.0000 norm=1.5940
[iter 200] loss=0.7805 val_loss=0.0000 scale=4.0000 norm=3.1799
[iter 300] loss=0.4442 val_loss=0.0000 scale=4.0000 norm=3.1248
[iter 400] loss=0.2773 val_loss=0.0000 scale=4.0000 norm=3.1219
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7884 val_loss=0.0000 scale=2.0000 norm=1.5476
[iter 200] loss=0.2849 val_loss=0.0000 scale=4.0000 norm=2.9901
[iter 300] loss=-0.2650 val_loss=0.0000 scale=4.0000 norm=2.8787
[iter 400] loss=-0.6724 val_loss=0.0000 scale=4.0000 norm=2.8606
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.5676 val_loss=0.0000 scale=2.0000 norm=1.3047
[iter 200] loss=-0.0708 val_loss=0.0000 scale=4.0000 norm=2.7344
[iter 300] loss=-0.8612 val_loss=0.0000 scale=4.0000 norm=2.7331
[iter 400] loss=-1.7962 val_loss=0.0000 scale=8.0000 norm=5.1165
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0320 val_loss=0.0000 scale=2.0000 norm=1.6447
[iter 200] loss=0.7568 val_loss=0.0000 scale=2.0000 norm=1.6450
[iter 300] loss=0.3482 val_loss=0.0000 scale=4.0000 norm=3.2632
[iter 400] loss=-0.0003 val_loss=0.0000 scale=4.0000 norm=3.1992
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0442 val_loss=0.0000 scale=2.0000 norm=1.6685
[iter 200] loss=0.6848 val_loss=0.0000 scale=4.0000 norm=3.2652
[iter 300] loss=0.3238 val_loss=0.0000 scale=4.0000 norm=3.2091
[iter 400] loss=0.0511 val_loss=0.0000 scale=4.0000 norm=3.1165
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6240 val_loss=0.0000 scale=2.0000 norm=1.1969
[iter 200] loss=0.2815 val_loss=0.0000 scale=2.0000 norm=1.2527
[iter 300] loss=-0.1276 val_loss=0.0000 scale=4.0000 norm=2.4826
[iter 400] loss=-0.4302 val_loss=0.0000 scale=4.0000 norm=2.4183
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0246 val_loss=0.0000 scale=2.0000 norm=1.6107
[iter 200] loss=0.7568 val_loss=0.0000 scale=4.0000 norm=3.1805
[iter 300] loss=0.4122 val_loss=0.0000 scale=4.0000 norm=3.1425
[iter 400] loss=0.1764 val_loss=0.0000 scale=4.0000 norm=3.1213
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0388 val_loss=0.0000 scale=2.0000 norm=1.6323
[iter 200] loss=0.6976 val_loss=0.0000 scale=4.0000 norm=3.2164
[iter 300] loss=0.3137 val_loss=0.0000 scale=4.0000 norm=3.1650
[iter 400] loss=0.0828 val_loss=0.0000 scale=4.0000 norm=3.1343
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9892 val_loss=0.0000 scale=2.0000 norm=1.5638
[iter 200] loss=0.7615 val_loss=0.0000 scale=2.0000 norm=1.5432
[iter 300] loss=0.4450 val_loss=0.0000 scale=4.0000 norm=3.0683
[iter 400] loss=0.2275 val_loss=0.0000 scale=4.0000 norm=3.0572
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9333 val_loss=0.0000 scale=2.0000 norm=1.5432
[iter 200] loss=0.6530 val_loss=0.0000 scale=4.0000 norm=3.0282
[iter 300] loss=0.2825 val_loss=0.0000 scale=4.0000 norm=2.9921
[iter 400] loss=0.1045 val_loss=0.0000 scale=4.0000 norm=2.9688
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9630 val_loss=0.0000 scale=2.0000 norm=1.6808
[iter 200] loss=0.5682 val_loss=0.0000 scale=4.0000 norm=3.3239
[iter 300] loss=-0.0770 val_loss=0.0000 scale=8.0000 norm=6.6579
[iter 400] loss=-1.3370 val_loss=0.0000 scale=16.0000 norm=13.3158
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9630 val_loss=0.0000 scale=2.0000 norm=1.5497
[iter 200] loss=0.6923 val_loss=0.0000 scale=2.0000 norm=1.5169
[iter 300] loss=0.3571 val_loss=0.0000 scale=4.0000 norm=3.0074
[iter 400] loss=0.1224 val_loss=0.0000 scale=4.0000 norm=2.9705
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0317 val_loss=0.0000 scale=2.0000 norm=1.6446
[iter 200] loss=0.7648 val_loss=0.0000 scale=4.0000 norm=3.3176
[iter 300] loss=0.2648 val_loss=0.0000 scale=4.0000 norm=3.3251
[iter 400] loss=-0.6502 val_loss=0.0000 scale=8.0000 norm=6.6505
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0073 val_loss=0.0000 scale=2.0000 norm=1.6294
[iter 200] loss=0.6890 val_loss=0.0000 scale=4.0000 norm=3.2139
[iter 300] loss=0.2598 val_loss=0.0000 scale=4.0000 norm=3.1736
[iter 400] loss=-0.0872 val_loss=0.0000 scale=4.0000 norm=3.1157
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0138 val_loss=0.0000 scale=2.0000 norm=1.5903
[iter 200] loss=0.7717 val_loss=0.0000 scale=4.0000 norm=3.1702
[iter 300] loss=0.4544 val_loss=0.0000 scale=4.0000 norm=3.1057
[iter 400] loss=0.3067 val_loss=0.0000 scale=4.0000 norm=3.1286
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8987 val_loss=0.0000 scale=2.0000 norm=1.6413
[iter 200] loss=0.2516 val_loss=0.0000 scale=4.0000 norm=3.1660
[iter 300] loss=-0.3210 val_loss=0.0000 scale=8.0000 norm=5.9007
[iter 400] loss=-1.3860 val_loss=0.0000 scale=16.0000 norm=11.6727
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8111 val_loss=0.0000 scale=2.0000 norm=1.5876
[iter 200] loss=0.3391 val_loss=0.0000 scale=4.0000 norm=3.2644
[iter 300] loss=-0.3037 val_loss=0.0000 scale=4.0000 norm=3.2572
[iter 400] loss=-0.9995 val_loss=0.0000 scale=8.0000 norm=6.2012
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0030 val_loss=0.0000 scale=2.0000 norm=1.5937
[iter 200] loss=0.7363 val_loss=0.0000 scale=2.0000 norm=1.5815
[iter 300] loss=0.3525 val_loss=0.0000 scale=4.0000 norm=3.1335
[iter 400] loss=0.0772 val_loss=0.0000 scale=4.0000 norm=3.0514
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7207 val_loss=0.0000 scale=2.0000 norm=1.2637
[iter 200] loss=0.4461 val_loss=0.0000 scale=2.0000 norm=1.2818
[iter 300] loss=0.1431 val_loss=0.0000 scale=4.0000 norm=2.5250
[iter 400] loss=-0.0545 val_loss=0.0000 scale=2.0000 norm=1.2183
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0516 val_loss=0.0000 scale=2.0000 norm=1.6213
[iter 200] loss=0.8169 val_loss=0.0000 scale=4.0000 norm=3.2426
[iter 300] loss=0.4924 val_loss=0.0000 scale=4.0000 norm=3.2210
[iter 400] loss=0.2910 val_loss=0.0000 scale=4.0000 norm=3.1613
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0374 val_loss=0.0000 scale=2.0000 norm=1.6132
[iter 200] loss=0.7735 val_loss=0.0000 scale=4.0000 norm=3.1872
[iter 300] loss=0.3969 val_loss=0.0000 scale=4.0000 norm=3.1414
[iter 400] loss=0.1542 val_loss=0.0000 scale=4.0000 norm=3.1273
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0228 val_loss=0.0000 scale=2.0000 norm=1.6105
[iter 200] loss=0.8024 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 300] loss=0.4812 val_loss=0.0000 scale=4.0000 norm=3.1514
[iter 400] loss=0.2878 val_loss=0.0000 scale=4.0000 norm=3.0833
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8712 val_loss=0.0000 scale=2.0000 norm=1.5461
[iter 200] loss=0.5675 val_loss=0.0000 scale=2.0000 norm=1.5036
[iter 300] loss=0.2072 val_loss=0.0000 scale=4.0000 norm=3.0074
[iter 400] loss=-0.0874 val_loss=0.0000 scale=4.0000 norm=3.0000
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9408 val_loss=0.0000 scale=2.0000 norm=1.5946
[iter 200] loss=0.5938 val_loss=0.0000 scale=4.0000 norm=3.1181
[iter 300] loss=0.1453 val_loss=0.0000 scale=4.0000 norm=3.0942
[iter 400] loss=-0.1883 val_loss=0.0000 scale=4.0000 norm=3.0495
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9877 val_loss=0.0000 scale=2.0000 norm=1.5871
[iter 200] loss=0.7313 val_loss=0.0000 scale=2.0000 norm=1.5662
[iter 300] loss=0.4182 val_loss=0.0000 scale=4.0000 norm=3.0999
[iter 400] loss=0.2091 val_loss=0.0000 scale=4.0000 norm=3.0565
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8950 val_loss=0.0000 scale=2.0000 norm=1.5244
[iter 200] loss=0.6442 val_loss=0.0000 scale=4.0000 norm=3.1411
[iter 300] loss=0.2245 val_loss=0.0000 scale=4.0000 norm=3.1681
[iter 400] loss=-0.4494 val_loss=0.0000 scale=8.0000 norm=6.3384
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0208 val_loss=0.0000 scale=2.0000 norm=1.6143
[iter 200] loss=0.7667 val_loss=0.0000 scale=2.0000 norm=1.6033
[iter 300] loss=0.4291 val_loss=0.0000 scale=4.0000 norm=3.1873
[iter 400] loss=0.2098 val_loss=0.0000 scale=4.0000 norm=3.0913
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0102 val_loss=0.0000 scale=2.0000 norm=1.5818
[iter 200] loss=0.7846 val_loss=0.0000 scale=2.0000 norm=1.5625
[iter 300] loss=0.4650 val_loss=0.0000 scale=4.0000 norm=3.0884
[iter 400] loss=0.2768 val_loss=0.0000 scale=4.0000 norm=3.0591
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9616 val_loss=0.0000 scale=2.0000 norm=1.5390
[iter 200] loss=0.7056 val_loss=0.0000 scale=2.0000 norm=1.5218
[iter 300] loss=0.3168 val_loss=0.0000 scale=4.0000 norm=2.9990
[iter 400] loss=0.0360 val_loss=0.0000 scale=4.0000 norm=2.9564
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0661 val_loss=0.0000 scale=2.0000 norm=1.6226
[iter 200] loss=0.8776 val_loss=0.0000 scale=2.0000 norm=1.6136
[iter 300] loss=0.6555 val_loss=0.0000 scale=4.0000 norm=3.1939
[iter 400] loss=0.4740 val_loss=0.0000 scale=4.0000 norm=3.1556
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9736 val_loss=0.0000 scale=2.0000 norm=1.5670
[iter 200] loss=0.7151 val_loss=0.0000 scale=4.0000 norm=3.1102
[iter 300] loss=0.3733 val_loss=0.0000 scale=4.0000 norm=3.0970
[iter 400] loss=0.1765 val_loss=0.0000 scale=4.0000 norm=3.0729
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5194 val_loss=0.0000 scale=2.0000 norm=1.1262
[iter 200] loss=0.0249 val_loss=0.0000 scale=2.0000 norm=1.1797
[iter 300] loss=-0.7792 val_loss=0.0000 scale=4.0000 norm=2.3746
[iter 400] loss=-1.4727 val_loss=0.0000 scale=4.0000 norm=2.1874
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9858 val_loss=0.0000 scale=2.0000 norm=1.5614
[iter 200] loss=0.7594 val_loss=0.0000 scale=2.0000 norm=1.5434
[iter 300] loss=0.4545 val_loss=0.0000 scale=4.0000 norm=3.0836
[iter 400] loss=0.2499 val_loss=0.0000 scale=4.0000 norm=3.0584
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4123 val_loss=0.0000 scale=2.0000 norm=0.9616
[iter 200] loss=-0.0952 val_loss=0.0000 scale=2.0000 norm=0.9070
[iter 300] loss=-0.6839 val_loss=0.0000 scale=4.0000 norm=1.7277
[iter 400] loss=-1.5674 val_loss=0.0000 scale=8.0000 norm=3.3759
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0593 val_loss=0.0000 scale=2.0000 norm=1.6401
[iter 200] loss=0.8539 val_loss=0.0000 scale=2.0000 norm=1.6470
[iter 300] loss=0.6666 val_loss=0.0000 scale=2.0000 norm=1.6328
[iter 400] loss=0.4583 val_loss=0.0000 scale=4.0000 norm=3.2381
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9108 val_loss=0.0000 scale=2.0000 norm=1.5898
[iter 200] loss=0.5610 val_loss=0.0000 scale=4.0000 norm=3.0810
[iter 300] loss=0.1139 val_loss=0.0000 scale=8.0000 norm=6.1629
[iter 400] loss=-0.3225 val_loss=0.0000 scale=4.0000 norm=3.0513
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0205 val_loss=0.0000 scale=2.0000 norm=1.5874
[iter 200] loss=0.8021 val_loss=0.0000 scale=2.0000 norm=1.5629
[iter 300] loss=0.4885 val_loss=0.0000 scale=4.0000 norm=3.0743
[iter 400] loss=0.3365 val_loss=0.0000 scale=2.0000 norm=1.5267
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9374 val_loss=0.0000 scale=2.0000 norm=1.5883
[iter 200] loss=0.5474 val_loss=0.0000 scale=4.0000 norm=3.1481
[iter 300] loss=0.0733 val_loss=0.0000 scale=4.0000 norm=3.1129
[iter 400] loss=-0.2483 val_loss=0.0000 scale=4.0000 norm=3.0839
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9057 val_loss=0.0000 scale=2.0000 norm=1.5192
[iter 200] loss=0.5827 val_loss=0.0000 scale=4.0000 norm=3.0383
[iter 300] loss=0.1081 val_loss=0.0000 scale=4.0000 norm=3.0134
[iter 400] loss=-0.2089 val_loss=0.0000 scale=4.0000 norm=2.9371
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9078 val_loss=0.0000 scale=2.0000 norm=1.5213
[iter 200] loss=0.5709 val_loss=0.0000 scale=2.0000 norm=1.5197
[iter 300] loss=0.0645 val_loss=0.0000 scale=4.0000 norm=2.9977
[iter 400] loss=-0.2637 val_loss=0.0000 scale=4.0000 norm=2.8538
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0019 val_loss=0.0000 scale=2.0000 norm=1.5813
[iter 200] loss=0.7077 val_loss=0.0000 scale=4.0000 norm=3.1189
[iter 300] loss=0.3453 val_loss=0.0000 scale=4.0000 norm=3.0099
[iter 400] loss=0.1028 val_loss=0.0000 scale=4.0000 norm=3.0031
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8184 val_loss=0.0000 scale=2.0000 norm=1.5822
[iter 200] loss=0.1601 val_loss=0.0000 scale=4.0000 norm=3.1798
[iter 300] loss=-0.5547 val_loss=0.0000 scale=4.0000 norm=3.0734
[iter 400] loss=-1.0505 val_loss=0.0000 scale=4.0000 norm=2.8467
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0051 val_loss=0.0000 scale=2.0000 norm=1.5783
[iter 200] loss=0.7628 val_loss=0.0000 scale=2.0000 norm=1.5674
[iter 300] loss=0.4017 val_loss=0.0000 scale=4.0000 norm=3.0898
[iter 400] loss=0.2022 val_loss=0.0000 scale=4.0000 norm=3.0467
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9749 val_loss=0.0000 scale=2.0000 norm=1.5568
[iter 200] loss=0.7100 val_loss=0.0000 scale=4.0000 norm=3.0756
[iter 300] loss=0.3790 val_loss=0.0000 scale=4.0000 norm=3.0470
[iter 400] loss=0.1831 val_loss=0.0000 scale=8.0000 norm=6.0300
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0434 val_loss=0.0000 scale=2.0000 norm=1.6210
[iter 200] loss=0.7925 val_loss=0.0000 scale=4.0000 norm=3.2223
[iter 300] loss=0.4451 val_loss=0.0000 scale=4.0000 norm=3.1735
[iter 400] loss=0.2330 val_loss=0.0000 scale=4.0000 norm=3.1447
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0660 val_loss=0.0000 scale=2.0000 norm=1.6345
[iter 200] loss=0.8544 val_loss=0.0000 scale=2.0000 norm=1.6176
[iter 300] loss=0.5748 val_loss=0.0000 scale=4.0000 norm=3.1876
[iter 400] loss=0.3529 val_loss=0.0000 scale=2.0000 norm=1.5599
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0055 val_loss=0.0000 scale=2.0000 norm=1.5762
[iter 200] loss=0.7604 val_loss=0.0000 scale=2.0000 norm=1.5588
[iter 300] loss=0.3992 val_loss=0.0000 scale=4.0000 norm=3.0981
[iter 400] loss=0.1645 val_loss=0.0000 scale=4.0000 norm=2.9892
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0347 val_loss=0.0000 scale=2.0000 norm=1.6008
[iter 200] loss=0.8171 val_loss=0.0000 scale=2.0000 norm=1.5897
[iter 300] loss=0.5207 val_loss=0.0000 scale=4.0000 norm=3.1443
[iter 400] loss=0.3551 val_loss=0.0000 scale=4.0000 norm=3.1401
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0016 val_loss=0.0000 scale=2.0000 norm=1.5750
[iter 200] loss=0.7576 val_loss=0.0000 scale=2.0000 norm=1.5656
[iter 300] loss=0.4783 val_loss=0.0000 scale=4.0000 norm=3.1233
[iter 400] loss=0.2243 val_loss=0.0000 scale=2.0000 norm=1.5405
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9877 val_loss=0.0000 scale=2.0000 norm=1.5818
[iter 200] loss=0.7525 val_loss=0.0000 scale=2.0000 norm=1.5482
[iter 300] loss=0.4345 val_loss=0.0000 scale=4.0000 norm=3.0526
[iter 400] loss=0.2884 val_loss=0.0000 scale=4.0000 norm=3.0567
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4415 val_loss=0.0000 scale=2.0000 norm=0.9961
[iter 200] loss=-0.0069 val_loss=0.0000 scale=2.0000 norm=1.0106
[iter 300] loss=-0.6008 val_loss=0.0000 scale=4.0000 norm=2.0622
[iter 400] loss=-1.6328 val_loss=0.0000 scale=8.0000 norm=4.1286
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0090 val_loss=0.0000 scale=2.0000 norm=1.6955
[iter 200] loss=0.6999 val_loss=0.0000 scale=4.0000 norm=3.3966
[iter 300] loss=0.3105 val_loss=0.0000 scale=8.0000 norm=6.6500
[iter 400] loss=-0.4275 val_loss=0.0000 scale=16.0000 norm=13.3003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0361 val_loss=0.0000 scale=2.0000 norm=1.5973
[iter 200] loss=0.8315 val_loss=0.0000 scale=2.0000 norm=1.5821
[iter 300] loss=0.5635 val_loss=0.0000 scale=4.0000 norm=3.1446
[iter 400] loss=0.3790 val_loss=0.0000 scale=4.0000 norm=3.0949
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0405 val_loss=0.0000 scale=2.0000 norm=1.6827
[iter 200] loss=0.6862 val_loss=0.0000 scale=4.0000 norm=3.2956
[iter 300] loss=0.2360 val_loss=0.0000 scale=8.0000 norm=6.4260
[iter 400] loss=-0.5650 val_loss=0.0000 scale=16.0000 norm=12.8330
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0142 val_loss=0.0000 scale=2.0000 norm=1.5987
[iter 200] loss=0.7624 val_loss=0.0000 scale=4.0000 norm=3.1844
[iter 300] loss=0.4054 val_loss=0.0000 scale=4.0000 norm=3.1008
[iter 400] loss=0.2516 val_loss=0.0000 scale=4.0000 norm=3.1167
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0220 val_loss=0.0000 scale=2.0000 norm=1.6019
[iter 200] loss=0.7893 val_loss=0.0000 scale=4.0000 norm=3.1772
[iter 300] loss=0.4717 val_loss=0.0000 scale=4.0000 norm=3.1254
[iter 400] loss=0.3012 val_loss=0.0000 scale=4.0000 norm=3.0736
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0429 val_loss=0.0000 scale=2.0000 norm=1.5934
[iter 200] loss=0.8479 val_loss=0.0000 scale=2.0000 norm=1.5862
[iter 300] loss=0.5994 val_loss=0.0000 scale=4.0000 norm=3.1448
[iter 400] loss=0.4564 val_loss=0.0000 scale=4.0000 norm=3.0563
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6195 val_loss=0.0000 scale=2.0000 norm=1.1033
[iter 200] loss=0.2836 val_loss=0.0000 scale=4.0000 norm=2.2032
[iter 300] loss=-0.1897 val_loss=0.0000 scale=4.0000 norm=2.0853
[iter 400] loss=-0.9023 val_loss=0.0000 scale=8.0000 norm=4.1393
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0972 val_loss=0.0000 scale=2.0000 norm=1.6482
[iter 200] loss=0.9285 val_loss=0.0000 scale=4.0000 norm=3.2825
[iter 300] loss=0.6714 val_loss=0.0000 scale=4.0000 norm=3.2631
[iter 400] loss=0.4766 val_loss=0.0000 scale=4.0000 norm=3.2391
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7437 val_loss=0.0000 scale=2.0000 norm=1.2914
[iter 200] loss=0.4769 val_loss=0.0000 scale=2.0000 norm=1.2995
[iter 300] loss=0.0998 val_loss=0.0000 scale=4.0000 norm=2.5627
[iter 400] loss=-0.1686 val_loss=0.0000 scale=4.0000 norm=2.4124
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9398 val_loss=0.0000 scale=2.0000 norm=1.6125
[iter 200] loss=0.5065 val_loss=0.0000 scale=4.0000 norm=3.2317
[iter 300] loss=-0.0078 val_loss=0.0000 scale=4.0000 norm=3.1416
[iter 400] loss=-0.4284 val_loss=0.0000 scale=4.0000 norm=3.0651
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0405 val_loss=0.0000 scale=2.0000 norm=1.6425
[iter 200] loss=0.8091 val_loss=0.0000 scale=4.0000 norm=3.1957
[iter 300] loss=0.5725 val_loss=0.0000 scale=4.0000 norm=3.1633
[iter 400] loss=0.3583 val_loss=0.0000 scale=8.0000 norm=6.2148
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0344 val_loss=0.0000 scale=2.0000 norm=1.5821
[iter 200] loss=0.7711 val_loss=0.0000 scale=4.0000 norm=3.1459
[iter 300] loss=0.5009 val_loss=0.0000 scale=4.0000 norm=3.1034
[iter 400] loss=0.3299 val_loss=0.0000 scale=8.0000 norm=6.0576
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0221 val_loss=0.0000 scale=2.0000 norm=1.6060
[iter 200] loss=0.7969 val_loss=0.0000 scale=4.0000 norm=3.2048
[iter 300] loss=0.4969 val_loss=0.0000 scale=4.0000 norm=3.1964
[iter 400] loss=0.3183 val_loss=0.0000 scale=4.0000 norm=3.1412
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0434 val_loss=0.0000 scale=2.0000 norm=1.6062
[iter 200] loss=0.7944 val_loss=0.0000 scale=4.0000 norm=3.2203
[iter 300] loss=0.4549 val_loss=0.0000 scale=4.0000 norm=3.1746
[iter 400] loss=0.2682 val_loss=0.0000 scale=4.0000 norm=3.1746
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0065 val_loss=0.0000 scale=2.0000 norm=1.5935
[iter 200] loss=0.7403 val_loss=0.0000 scale=4.0000 norm=3.1642
[iter 300] loss=0.3919 val_loss=0.0000 scale=2.0000 norm=1.5498
[iter 400] loss=0.2507 val_loss=0.0000 scale=4.0000 norm=3.1125
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0133 val_loss=0.0000 scale=2.0000 norm=1.5855
[iter 200] loss=0.7976 val_loss=0.0000 scale=2.0000 norm=1.5698
[iter 300] loss=0.5274 val_loss=0.0000 scale=4.0000 norm=3.1214
[iter 400] loss=0.3389 val_loss=0.0000 scale=4.0000 norm=3.0915
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0227 val_loss=0.0000 scale=2.0000 norm=1.5971
[iter 200] loss=0.8109 val_loss=0.0000 scale=4.0000 norm=3.1719
[iter 300] loss=0.5149 val_loss=0.0000 scale=4.0000 norm=3.1750
[iter 400] loss=0.3311 val_loss=0.0000 scale=4.0000 norm=3.1269
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9651 val_loss=0.0000 scale=2.0000 norm=1.5743
[iter 200] loss=0.6853 val_loss=0.0000 scale=4.0000 norm=3.1006
[iter 300] loss=0.3157 val_loss=0.0000 scale=4.0000 norm=3.0394
[iter 400] loss=0.0600 val_loss=0.0000 scale=4.0000 norm=2.9365
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0265 val_loss=0.0000 scale=2.0000 norm=1.6110
[iter 200] loss=0.7569 val_loss=0.0000 scale=4.0000 norm=3.1862
[iter 300] loss=0.4534 val_loss=0.0000 scale=4.0000 norm=3.1162
[iter 400] loss=0.2972 val_loss=0.0000 scale=4.0000 norm=3.0920
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0432 val_loss=0.0000 scale=2.0000 norm=1.5918
[iter 200] loss=0.8371 val_loss=0.0000 scale=2.0000 norm=1.5785
[iter 300] loss=0.5462 val_loss=0.0000 scale=4.0000 norm=3.1251
[iter 400] loss=0.3905 val_loss=0.0000 scale=4.0000 norm=3.1227
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0789 val_loss=0.0000 scale=2.0000 norm=1.6171
[iter 200] loss=0.9080 val_loss=0.0000 scale=2.0000 norm=1.6045
[iter 300] loss=0.6537 val_loss=0.0000 scale=4.0000 norm=3.1895
[iter 400] loss=0.4427 val_loss=0.0000 scale=4.0000 norm=3.1646
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4187 val_loss=0.0000 scale=2.0000 norm=1.1959
[iter 200] loss=-0.1390 val_loss=0.0000 scale=4.0000 norm=2.2466
[iter 300] loss=-0.8449 val_loss=0.0000 scale=4.0000 norm=2.2486
[iter 400] loss=-2.1819 val_loss=0.0000 scale=8.0000 norm=4.4986
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9604 val_loss=0.0000 scale=2.0000 norm=1.5517
[iter 200] loss=0.6764 val_loss=0.0000 scale=2.0000 norm=1.5273
[iter 300] loss=0.2579 val_loss=0.0000 scale=4.0000 norm=3.0121
[iter 400] loss=0.0098 val_loss=0.0000 scale=4.0000 norm=2.9230
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9893 val_loss=0.0000 scale=2.0000 norm=1.5663
[iter 200] loss=0.7371 val_loss=0.0000 scale=2.0000 norm=1.5482
[iter 300] loss=0.4110 val_loss=0.0000 scale=4.0000 norm=3.1143
[iter 400] loss=0.1542 val_loss=0.0000 scale=2.0000 norm=1.5213
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0299 val_loss=0.0000 scale=2.0000 norm=1.6002
[iter 200] loss=0.7717 val_loss=0.0000 scale=4.0000 norm=3.1822
[iter 300] loss=0.4071 val_loss=0.0000 scale=4.0000 norm=3.1447
[iter 400] loss=0.2001 val_loss=0.0000 scale=2.0000 norm=1.5460
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0265 val_loss=0.0000 scale=2.0000 norm=1.5884
[iter 200] loss=0.8084 val_loss=0.0000 scale=2.0000 norm=1.5782
[iter 300] loss=0.4883 val_loss=0.0000 scale=4.0000 norm=3.1226
[iter 400] loss=0.2207 val_loss=0.0000 scale=4.0000 norm=3.0732
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0062 val_loss=0.0000 scale=2.0000 norm=1.5946
[iter 200] loss=0.7369 val_loss=0.0000 scale=4.0000 norm=3.1473
[iter 300] loss=0.4359 val_loss=0.0000 scale=4.0000 norm=3.1121
[iter 400] loss=0.2158 val_loss=0.0000 scale=4.0000 norm=3.0680
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0362 val_loss=0.0000 scale=2.0000 norm=1.5877
[iter 200] loss=0.8123 val_loss=0.0000 scale=2.0000 norm=1.5840
[iter 300] loss=0.4583 val_loss=0.0000 scale=4.0000 norm=3.1223
[iter 400] loss=0.2380 val_loss=0.0000 scale=4.0000 norm=3.1391
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9890 val_loss=0.0000 scale=2.0000 norm=1.5520
[iter 200] loss=0.7520 val_loss=0.0000 scale=4.0000 norm=3.0704
[iter 300] loss=0.4238 val_loss=0.0000 scale=4.0000 norm=3.1078
[iter 400] loss=0.1186 val_loss=0.0000 scale=4.0000 norm=3.0727
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0596 val_loss=0.0000 scale=2.0000 norm=1.6372
[iter 200] loss=0.8330 val_loss=0.0000 scale=4.0000 norm=3.2502
[iter 300] loss=0.5194 val_loss=0.0000 scale=4.0000 norm=3.2105
[iter 400] loss=0.3731 val_loss=0.0000 scale=4.0000 norm=3.1847
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5968 val_loss=0.0000 scale=2.0000 norm=1.2872
[iter 200] loss=0.0884 val_loss=0.0000 scale=2.0000 norm=1.2449
[iter 300] loss=-0.3068 val_loss=0.0000 scale=4.0000 norm=2.6108
[iter 400] loss=-0.6288 val_loss=0.0000 scale=8.0000 norm=4.9224
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0046 val_loss=0.0000 scale=2.0000 norm=1.5889
[iter 200] loss=0.7490 val_loss=0.0000 scale=4.0000 norm=3.1218
[iter 300] loss=0.4075 val_loss=0.0000 scale=4.0000 norm=3.0727
[iter 400] loss=0.2206 val_loss=0.0000 scale=4.0000 norm=3.0083
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0282 val_loss=0.0000 scale=2.0000 norm=1.5996
[iter 200] loss=0.7947 val_loss=0.0000 scale=4.0000 norm=3.1577
[iter 300] loss=0.4689 val_loss=0.0000 scale=4.0000 norm=3.1010
[iter 400] loss=0.3172 val_loss=0.0000 scale=4.0000 norm=3.1582
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6957 val_loss=0.0000 scale=2.0000 norm=1.3640
[iter 200] loss=0.4210 val_loss=0.0000 scale=4.0000 norm=2.9320
[iter 300] loss=0.0760 val_loss=0.0000 scale=4.0000 norm=2.9360
[iter 400] loss=-0.2509 val_loss=0.0000 scale=8.0000 norm=5.7198
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8748 val_loss=0.0000 scale=2.0000 norm=1.6157
[iter 200] loss=0.4005 val_loss=0.0000 scale=4.0000 norm=3.2493
[iter 300] loss=-0.3972 val_loss=0.0000 scale=4.0000 norm=3.2484
[iter 400] loss=-1.1784 val_loss=0.0000 scale=8.0000 norm=6.2432
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9895 val_loss=0.0000 scale=2.0000 norm=1.5834
[iter 200] loss=0.7113 val_loss=0.0000 scale=4.0000 norm=3.1279
[iter 300] loss=0.2941 val_loss=0.0000 scale=4.0000 norm=3.0833
[iter 400] loss=0.0549 val_loss=0.0000 scale=4.0000 norm=3.0374
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9450 val_loss=0.0000 scale=2.0000 norm=1.5470
[iter 200] loss=0.6810 val_loss=0.0000 scale=2.0000 norm=1.5435
[iter 300] loss=0.2797 val_loss=0.0000 scale=4.0000 norm=3.0687
[iter 400] loss=-0.0428 val_loss=0.0000 scale=4.0000 norm=2.9982
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0438 val_loss=0.0000 scale=2.0000 norm=1.6093
[iter 200] loss=0.8353 val_loss=0.0000 scale=2.0000 norm=1.6009
[iter 300] loss=0.5318 val_loss=0.0000 scale=4.0000 norm=3.1827
[iter 400] loss=0.3476 val_loss=0.0000 scale=4.0000 norm=3.1149
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5518 val_loss=0.0000 scale=2.0000 norm=1.1036
[iter 200] loss=0.1678 val_loss=0.0000 scale=2.0000 norm=1.1139
[iter 300] loss=-0.3946 val_loss=0.0000 scale=4.0000 norm=2.2315
[iter 400] loss=-0.9038 val_loss=0.0000 scale=4.0000 norm=2.2124
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9935 val_loss=0.0000 scale=2.0000 norm=1.5668
[iter 200] loss=0.7289 val_loss=0.0000 scale=4.0000 norm=3.1173
[iter 300] loss=0.4070 val_loss=0.0000 scale=4.0000 norm=3.0645
[iter 400] loss=0.2062 val_loss=0.0000 scale=4.0000 norm=3.0194
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9652 val_loss=0.0000 scale=2.0000 norm=1.5741
[iter 200] loss=0.6329 val_loss=0.0000 scale=4.0000 norm=3.0560
[iter 300] loss=0.2610 val_loss=0.0000 scale=4.0000 norm=2.9945
[iter 400] loss=-0.0492 val_loss=0.0000 scale=4.0000 norm=2.9249
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0318 val_loss=0.0000 scale=2.0000 norm=1.6118
[iter 200] loss=0.7787 val_loss=0.0000 scale=4.0000 norm=3.2000
[iter 300] loss=0.4381 val_loss=0.0000 scale=4.0000 norm=3.1749
[iter 400] loss=0.2319 val_loss=0.0000 scale=4.0000 norm=3.1442
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0573 val_loss=0.0000 scale=2.0000 norm=1.5994
[iter 200] loss=0.8538 val_loss=0.0000 scale=2.0000 norm=1.5805
[iter 300] loss=0.5259 val_loss=0.0000 scale=4.0000 norm=3.1827
[iter 400] loss=0.1824 val_loss=0.0000 scale=8.0000 norm=6.2919
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9923 val_loss=0.0000 scale=2.0000 norm=1.5688
[iter 200] loss=0.7090 val_loss=0.0000 scale=4.0000 norm=3.1212
[iter 300] loss=0.3003 val_loss=0.0000 scale=4.0000 norm=3.0852
[iter 400] loss=0.0296 val_loss=0.0000 scale=4.0000 norm=3.0252
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5086 val_loss=0.0000 scale=2.0000 norm=1.0902
[iter 200] loss=0.0117 val_loss=0.0000 scale=2.0000 norm=1.1383
[iter 300] loss=-0.6654 val_loss=0.0000 scale=4.0000 norm=2.2548
[iter 400] loss=-1.7527 val_loss=0.0000 scale=8.0000 norm=4.5378
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7953 val_loss=0.0000 scale=2.0000 norm=1.6092
[iter 200] loss=0.2738 val_loss=0.0000 scale=4.0000 norm=3.1503
[iter 300] loss=-0.5520 val_loss=0.0000 scale=8.0000 norm=6.2905
[iter 400] loss=-2.1820 val_loss=0.0000 scale=16.0000 norm=12.5810
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6013 val_loss=0.0000 scale=2.0000 norm=1.2588
[iter 200] loss=0.0531 val_loss=0.0000 scale=4.0000 norm=2.5775
[iter 300] loss=-0.7469 val_loss=0.0000 scale=4.0000 norm=2.5878
[iter 400] loss=-2.3069 val_loss=0.0000 scale=8.0000 norm=5.1760
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9761 val_loss=0.0000 scale=2.0000 norm=1.5541
[iter 200] loss=0.7074 val_loss=0.0000 scale=4.0000 norm=3.0703
[iter 300] loss=0.3292 val_loss=0.0000 scale=4.0000 norm=3.0071
[iter 400] loss=0.1239 val_loss=0.0000 scale=4.0000 norm=2.9764
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7717 val_loss=0.0000 scale=2.0000 norm=1.4674
[iter 200] loss=0.4716 val_loss=0.0000 scale=4.0000 norm=3.0518
[iter 300] loss=0.0547 val_loss=0.0000 scale=4.0000 norm=3.0280
[iter 400] loss=-0.3420 val_loss=0.0000 scale=8.0000 norm=5.8510
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9328 val_loss=0.0000 scale=2.0000 norm=1.6484
[iter 200] loss=0.5626 val_loss=0.0000 scale=4.0000 norm=3.3265
[iter 300] loss=0.0065 val_loss=0.0000 scale=4.0000 norm=3.3264
[iter 400] loss=-0.4939 val_loss=0.0000 scale=4.0000 norm=3.2326
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6113 val_loss=0.0000 scale=2.0000 norm=1.1761
[iter 200] loss=0.2322 val_loss=0.0000 scale=2.0000 norm=1.1737
[iter 300] loss=-0.3027 val_loss=0.0000 scale=4.0000 norm=2.2147
[iter 400] loss=-0.7034 val_loss=0.0000 scale=4.0000 norm=2.0816
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7510 val_loss=0.0000 scale=2.0000 norm=1.2707
[iter 200] loss=0.3824 val_loss=0.0000 scale=4.0000 norm=2.5426
[iter 300] loss=-0.2179 val_loss=0.0000 scale=4.0000 norm=2.5505
[iter 400] loss=-1.3699 val_loss=0.0000 scale=8.0000 norm=5.1014
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0031 val_loss=0.0000 scale=2.0000 norm=1.5807
[iter 200] loss=0.7551 val_loss=0.0000 scale=2.0000 norm=1.5704
[iter 300] loss=0.3802 val_loss=0.0000 scale=4.0000 norm=3.1340
[iter 400] loss=0.1584 val_loss=0.0000 scale=4.0000 norm=3.1682
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8619 val_loss=0.0000 scale=2.0000 norm=1.4617
[iter 200] loss=0.6209 val_loss=0.0000 scale=2.0000 norm=1.4849
[iter 300] loss=0.4092 val_loss=0.0000 scale=4.0000 norm=2.9673
[iter 400] loss=0.1693 val_loss=0.0000 scale=4.0000 norm=2.9027
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8689 val_loss=0.0000 scale=2.0000 norm=1.4656
[iter 200] loss=0.6468 val_loss=0.0000 scale=4.0000 norm=3.0305
[iter 300] loss=0.3203 val_loss=0.0000 scale=4.0000 norm=3.0234
[iter 400] loss=0.0421 val_loss=0.0000 scale=4.0000 norm=2.9911
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9502 val_loss=0.0000 scale=2.0000 norm=1.5566
[iter 200] loss=0.6612 val_loss=0.0000 scale=4.0000 norm=3.0450
[iter 300] loss=0.3179 val_loss=0.0000 scale=4.0000 norm=3.0087
[iter 400] loss=0.0669 val_loss=0.0000 scale=4.0000 norm=2.9452
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0288 val_loss=0.0000 scale=2.0000 norm=1.5990
[iter 200] loss=0.7626 val_loss=0.0000 scale=4.0000 norm=3.1975
[iter 300] loss=0.4113 val_loss=0.0000 scale=4.0000 norm=3.1521
[iter 400] loss=0.1841 val_loss=0.0000 scale=4.0000 norm=3.1333
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0030 val_loss=0.0000 scale=2.0000 norm=1.5877
[iter 200] loss=0.7386 val_loss=0.0000 scale=4.0000 norm=3.1516
[iter 300] loss=0.3817 val_loss=0.0000 scale=4.0000 norm=3.1224
[iter 400] loss=0.1641 val_loss=0.0000 scale=4.0000 norm=3.0937
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9605 val_loss=0.0000 scale=2.0000 norm=1.5471
[iter 200] loss=0.7248 val_loss=0.0000 scale=2.0000 norm=1.5683
[iter 300] loss=0.3142 val_loss=0.0000 scale=4.0000 norm=3.1564
[iter 400] loss=-0.1102 val_loss=0.0000 scale=4.0000 norm=3.1121

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n06>
Subject: Job 853007: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:38 2024
Job was executed on host(s) <4*c202n06>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:40 2024
                            <4*c202n14>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:44:40 2024
Terminated at Tue Oct 22 17:50:06 2024
Results reported at Tue Oct 22 17:50:06 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "RRU Monomer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4278.00 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.38 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   337 sec.
    Turnaround time :                            328 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.

Dimer
Filename: (Mordred)_NGB_hypOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(Mordred)_NGB_hypOFF_generalizability_scores.json
Done Saving scores!
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0333 val_loss=0.0000 scale=2.0000 norm=1.7085
[iter 200] loss=0.5021 val_loss=0.0000 scale=4.0000 norm=3.3743
[iter 300] loss=-0.1130 val_loss=0.0000 scale=8.0000 norm=6.4116
[iter 400] loss=-1.2890 val_loss=0.0000 scale=16.0000 norm=12.8159
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0156 val_loss=0.0000 scale=2.0000 norm=1.5928
[iter 200] loss=0.7619 val_loss=0.0000 scale=4.0000 norm=3.1814
[iter 300] loss=0.4299 val_loss=0.0000 scale=4.0000 norm=3.1221
[iter 400] loss=0.2675 val_loss=0.0000 scale=4.0000 norm=3.1294
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6027 val_loss=0.0000 scale=2.0000 norm=1.2587
[iter 200] loss=0.1036 val_loss=0.0000 scale=4.0000 norm=2.5741
[iter 300] loss=-0.6962 val_loss=0.0000 scale=4.0000 norm=2.5877
[iter 400] loss=-2.0242 val_loss=0.0000 scale=8.0000 norm=5.1760
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0433 val_loss=0.0000 scale=2.0000 norm=1.5931
[iter 200] loss=0.8378 val_loss=0.0000 scale=2.0000 norm=1.5791
[iter 300] loss=0.5569 val_loss=0.0000 scale=4.0000 norm=3.1264
[iter 400] loss=0.3960 val_loss=0.0000 scale=4.0000 norm=3.1236
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9329 val_loss=0.0000 scale=2.0000 norm=1.6484
[iter 200] loss=0.5598 val_loss=0.0000 scale=4.0000 norm=3.3268
[iter 300] loss=0.0037 val_loss=0.0000 scale=4.0000 norm=3.3263
[iter 400] loss=-0.4960 val_loss=0.0000 scale=4.0000 norm=3.2316
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0649 val_loss=0.0000 scale=2.0000 norm=1.6384
[iter 200] loss=0.8544 val_loss=0.0000 scale=2.0000 norm=1.6285
[iter 300] loss=0.5830 val_loss=0.0000 scale=4.0000 norm=3.2246
[iter 400] loss=0.2954 val_loss=0.0000 scale=4.0000 norm=3.1612
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9873 val_loss=0.0000 scale=2.0000 norm=1.5596
[iter 200] loss=0.7494 val_loss=0.0000 scale=4.0000 norm=3.0879
[iter 300] loss=0.4096 val_loss=0.0000 scale=4.0000 norm=3.0698
[iter 400] loss=0.2351 val_loss=0.0000 scale=4.0000 norm=3.0576
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0345 val_loss=0.0000 scale=2.0000 norm=1.6002
[iter 200] loss=0.8171 val_loss=0.0000 scale=2.0000 norm=1.5919
[iter 300] loss=0.5220 val_loss=0.0000 scale=4.0000 norm=3.1265
[iter 400] loss=0.3528 val_loss=0.0000 scale=4.0000 norm=3.1270
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9818 val_loss=0.0000 scale=2.0000 norm=1.5686
[iter 200] loss=0.6934 val_loss=0.0000 scale=4.0000 norm=3.0653
[iter 300] loss=0.3576 val_loss=0.0000 scale=4.0000 norm=3.0074
[iter 400] loss=0.1079 val_loss=0.0000 scale=4.0000 norm=2.9498
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5863 val_loss=0.0000 scale=2.0000 norm=1.1503
[iter 200] loss=0.2166 val_loss=0.0000 scale=2.0000 norm=1.1021
[iter 300] loss=-0.1534 val_loss=0.0000 scale=4.0000 norm=2.2001
[iter 400] loss=-0.4617 val_loss=0.0000 scale=4.0000 norm=2.1871
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0205 val_loss=0.0000 scale=2.0000 norm=1.6140
[iter 200] loss=0.7674 val_loss=0.0000 scale=2.0000 norm=1.6028
[iter 300] loss=0.4250 val_loss=0.0000 scale=4.0000 norm=3.1883
[iter 400] loss=0.2085 val_loss=0.0000 scale=4.0000 norm=3.0880
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9885 val_loss=0.0000 scale=2.0000 norm=1.6062
[iter 200] loss=0.6685 val_loss=0.0000 scale=4.0000 norm=3.1849
[iter 300] loss=0.2836 val_loss=0.0000 scale=4.0000 norm=3.1722
[iter 400] loss=-0.0859 val_loss=0.0000 scale=4.0000 norm=3.0778
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9699 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 200] loss=0.6746 val_loss=0.0000 scale=2.0000 norm=1.5604
[iter 300] loss=0.2207 val_loss=0.0000 scale=4.0000 norm=3.0744
[iter 400] loss=-0.0404 val_loss=0.0000 scale=4.0000 norm=2.9798
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5194 val_loss=0.0000 scale=2.0000 norm=1.1262
[iter 200] loss=0.0249 val_loss=0.0000 scale=2.0000 norm=1.1796
[iter 300] loss=-0.7791 val_loss=0.0000 scale=4.0000 norm=2.3744
[iter 400] loss=-1.4726 val_loss=0.0000 scale=4.0000 norm=2.1876
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0428 val_loss=0.0000 scale=2.0000 norm=1.6090
[iter 200] loss=0.8350 val_loss=0.0000 scale=2.0000 norm=1.6009
[iter 300] loss=0.5337 val_loss=0.0000 scale=4.0000 norm=3.1749
[iter 400] loss=0.3467 val_loss=0.0000 scale=4.0000 norm=3.1120
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9452 val_loss=0.0000 scale=2.0000 norm=1.5669
[iter 200] loss=0.5604 val_loss=0.0000 scale=4.0000 norm=3.0335
[iter 300] loss=0.1939 val_loss=0.0000 scale=4.0000 norm=2.9502
[iter 400] loss=-0.0193 val_loss=0.0000 scale=2.0000 norm=1.4487
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0016 val_loss=0.0000 scale=2.0000 norm=1.5879
[iter 200] loss=0.6935 val_loss=0.0000 scale=4.0000 norm=3.1622
[iter 300] loss=0.3227 val_loss=0.0000 scale=4.0000 norm=3.1162
[iter 400] loss=0.1054 val_loss=0.0000 scale=4.0000 norm=3.1120
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9871 val_loss=0.0000 scale=2.0000 norm=1.5810
[iter 200] loss=0.7316 val_loss=0.0000 scale=4.0000 norm=3.0949
[iter 300] loss=0.4182 val_loss=0.0000 scale=4.0000 norm=3.0494
[iter 400] loss=0.2796 val_loss=0.0000 scale=4.0000 norm=3.0529
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5974 val_loss=0.0000 scale=2.0000 norm=1.1760
[iter 200] loss=0.2284 val_loss=0.0000 scale=2.0000 norm=1.1450
[iter 300] loss=-0.1461 val_loss=0.0000 scale=4.0000 norm=2.3376
[iter 400] loss=-0.4303 val_loss=0.0000 scale=4.0000 norm=2.2976
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0083 val_loss=0.0000 scale=2.0000 norm=1.6289
[iter 200] loss=0.7001 val_loss=0.0000 scale=4.0000 norm=3.2231
[iter 300] loss=0.2665 val_loss=0.0000 scale=4.0000 norm=3.1717
[iter 400] loss=-0.0820 val_loss=0.0000 scale=4.0000 norm=3.1176
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9627 val_loss=0.0000 scale=2.0000 norm=1.5392
[iter 200] loss=0.7002 val_loss=0.0000 scale=2.0000 norm=1.5224
[iter 300] loss=0.3123 val_loss=0.0000 scale=4.0000 norm=2.9820
[iter 400] loss=0.0641 val_loss=0.0000 scale=4.0000 norm=2.9507
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9765 val_loss=0.0000 scale=2.0000 norm=1.5554
[iter 200] loss=0.7327 val_loss=0.0000 scale=2.0000 norm=1.5361
[iter 300] loss=0.3987 val_loss=0.0000 scale=4.0000 norm=3.0513
[iter 400] loss=0.1992 val_loss=0.0000 scale=8.0000 norm=6.0729
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7213 val_loss=0.0000 scale=2.0000 norm=1.2630
[iter 200] loss=0.4522 val_loss=0.0000 scale=4.0000 norm=2.5674
[iter 300] loss=0.1329 val_loss=0.0000 scale=4.0000 norm=2.5201
[iter 400] loss=-0.0559 val_loss=0.0000 scale=2.0000 norm=1.2169
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0327 val_loss=0.0000 scale=2.0000 norm=1.5957
[iter 200] loss=0.8177 val_loss=0.0000 scale=4.0000 norm=3.1556
[iter 300] loss=0.5329 val_loss=0.0000 scale=4.0000 norm=3.1184
[iter 400] loss=0.3826 val_loss=0.0000 scale=4.0000 norm=3.0980
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9505 val_loss=0.0000 scale=2.0000 norm=1.5559
[iter 200] loss=0.6434 val_loss=0.0000 scale=4.0000 norm=3.0447
[iter 300] loss=0.3062 val_loss=0.0000 scale=4.0000 norm=2.9980
[iter 400] loss=0.0612 val_loss=0.0000 scale=4.0000 norm=2.9456
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0319 val_loss=0.0000 scale=2.0000 norm=1.6199
[iter 200] loss=0.8105 val_loss=0.0000 scale=4.0000 norm=3.2393
[iter 300] loss=0.4846 val_loss=0.0000 scale=4.0000 norm=3.1969
[iter 400] loss=0.3194 val_loss=0.0000 scale=4.0000 norm=3.1643
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0153 val_loss=0.0000 scale=2.0000 norm=1.6006
[iter 200] loss=0.7671 val_loss=0.0000 scale=4.0000 norm=3.1757
[iter 300] loss=0.4350 val_loss=0.0000 scale=4.0000 norm=3.1208
[iter 400] loss=0.2341 val_loss=0.0000 scale=4.0000 norm=3.0505
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9172 val_loss=0.0000 scale=2.0000 norm=1.6222
[iter 200] loss=0.6127 val_loss=0.0000 scale=4.0000 norm=3.3532
[iter 300] loss=0.1624 val_loss=0.0000 scale=4.0000 norm=3.3582
[iter 400] loss=-0.2062 val_loss=0.0000 scale=4.0000 norm=3.2360
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0517 val_loss=0.0000 scale=2.0000 norm=1.6023
[iter 200] loss=0.8611 val_loss=0.0000 scale=2.0000 norm=1.5936
[iter 300] loss=0.5997 val_loss=0.0000 scale=4.0000 norm=3.1605
[iter 400] loss=0.4674 val_loss=0.0000 scale=4.0000 norm=3.0737
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0368 val_loss=0.0000 scale=2.0000 norm=1.7344
[iter 200] loss=0.7840 val_loss=0.0000 scale=4.0000 norm=3.4524
[iter 300] loss=0.4129 val_loss=0.0000 scale=4.0000 norm=3.4190
[iter 400] loss=0.0825 val_loss=0.0000 scale=4.0000 norm=3.3615
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0045 val_loss=0.0000 scale=2.0000 norm=1.6028
[iter 200] loss=0.6973 val_loss=0.0000 scale=4.0000 norm=3.1866
[iter 300] loss=0.3505 val_loss=0.0000 scale=4.0000 norm=3.1386
[iter 400] loss=0.1214 val_loss=0.0000 scale=4.0000 norm=3.0931
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9898 val_loss=0.0000 scale=2.0000 norm=1.6079
[iter 200] loss=0.6126 val_loss=0.0000 scale=4.0000 norm=3.2128
[iter 300] loss=0.1245 val_loss=0.0000 scale=4.0000 norm=3.1666
[iter 400] loss=-0.2532 val_loss=0.0000 scale=4.0000 norm=3.0911
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9892 val_loss=0.0000 scale=2.0000 norm=1.5708
[iter 200] loss=0.7442 val_loss=0.0000 scale=4.0000 norm=3.1036
[iter 300] loss=0.4010 val_loss=0.0000 scale=4.0000 norm=3.1088
[iter 400] loss=0.1207 val_loss=0.0000 scale=4.0000 norm=3.0602
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8970 val_loss=0.0000 scale=2.0000 norm=1.5607
[iter 200] loss=0.5414 val_loss=0.0000 scale=4.0000 norm=3.0580
[iter 300] loss=-0.0644 val_loss=0.0000 scale=4.0000 norm=3.0232
[iter 400] loss=-1.1084 val_loss=0.0000 scale=8.0000 norm=6.0466
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9870 val_loss=0.0000 scale=2.0000 norm=1.5709
[iter 200] loss=0.7282 val_loss=0.0000 scale=4.0000 norm=3.1095
[iter 300] loss=0.3686 val_loss=0.0000 scale=4.0000 norm=3.0717
[iter 400] loss=0.1514 val_loss=0.0000 scale=4.0000 norm=3.0509
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0256 val_loss=0.0000 scale=2.0000 norm=1.5853
[iter 200] loss=0.8147 val_loss=0.0000 scale=2.0000 norm=1.5709
[iter 300] loss=0.5215 val_loss=0.0000 scale=2.0000 norm=1.5414
[iter 400] loss=0.4280 val_loss=0.0000 scale=4.0000 norm=3.0866
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9876 val_loss=0.0000 scale=2.0000 norm=1.5868
[iter 200] loss=0.7316 val_loss=0.0000 scale=2.0000 norm=1.5655
[iter 300] loss=0.4367 val_loss=0.0000 scale=4.0000 norm=3.1051
[iter 400] loss=0.2221 val_loss=0.0000 scale=4.0000 norm=3.0574
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0165 val_loss=0.0000 scale=2.0000 norm=1.5861
[iter 200] loss=0.7869 val_loss=0.0000 scale=4.0000 norm=3.1403
[iter 300] loss=0.4579 val_loss=0.0000 scale=4.0000 norm=3.1070
[iter 400] loss=0.2710 val_loss=0.0000 scale=4.0000 norm=3.0652
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0038 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 200] loss=0.7595 val_loss=0.0000 scale=4.0000 norm=3.1369
[iter 300] loss=0.3974 val_loss=0.0000 scale=4.0000 norm=3.0857
[iter 400] loss=0.1985 val_loss=0.0000 scale=4.0000 norm=3.0389
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0726 val_loss=0.0000 scale=2.0000 norm=1.6231
[iter 200] loss=0.8834 val_loss=0.0000 scale=2.0000 norm=1.6127
[iter 300] loss=0.5948 val_loss=0.0000 scale=4.0000 norm=3.1857
[iter 400] loss=0.3845 val_loss=0.0000 scale=4.0000 norm=3.1082
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0496 val_loss=0.0000 scale=2.0000 norm=1.6595
[iter 200] loss=0.7348 val_loss=0.0000 scale=4.0000 norm=3.2502
[iter 300] loss=0.4596 val_loss=0.0000 scale=4.0000 norm=3.2314
[iter 400] loss=0.2548 val_loss=0.0000 scale=4.0000 norm=3.1699
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9461 val_loss=0.0000 scale=2.0000 norm=1.5463
[iter 200] loss=0.6688 val_loss=0.0000 scale=4.0000 norm=3.0933
[iter 300] loss=0.2532 val_loss=0.0000 scale=4.0000 norm=3.0629
[iter 400] loss=-0.0628 val_loss=0.0000 scale=4.0000 norm=2.9687
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9395 val_loss=0.0000 scale=2.0000 norm=1.5559
[iter 200] loss=0.7674 val_loss=0.0000 scale=2.0000 norm=1.5488
[iter 300] loss=0.5611 val_loss=0.0000 scale=4.0000 norm=3.0599
[iter 400] loss=0.2355 val_loss=0.0000 scale=8.0000 norm=6.1201
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0156 val_loss=0.0000 scale=2.0000 norm=1.5797
[iter 200] loss=0.7899 val_loss=0.0000 scale=2.0000 norm=1.5640
[iter 300] loss=0.4734 val_loss=0.0000 scale=4.0000 norm=3.0854
[iter 400] loss=0.2682 val_loss=0.0000 scale=4.0000 norm=3.0286
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0648 val_loss=0.0000 scale=2.0000 norm=1.6312
[iter 200] loss=0.8524 val_loss=0.0000 scale=2.0000 norm=1.6125
[iter 300] loss=0.5353 val_loss=0.0000 scale=4.0000 norm=3.1616
[iter 400] loss=0.2905 val_loss=0.0000 scale=4.0000 norm=3.1575
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0295 val_loss=0.0000 scale=2.0000 norm=1.5983
[iter 200] loss=0.7793 val_loss=0.0000 scale=4.0000 norm=3.1969
[iter 300] loss=0.4226 val_loss=0.0000 scale=4.0000 norm=3.1516
[iter 400] loss=0.1924 val_loss=0.0000 scale=4.0000 norm=3.1387
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9044 val_loss=0.0000 scale=2.0000 norm=1.5198
[iter 200] loss=0.5439 val_loss=0.0000 scale=4.0000 norm=3.0431
[iter 300] loss=0.0759 val_loss=0.0000 scale=4.0000 norm=3.0043
[iter 400] loss=-0.2262 val_loss=0.0000 scale=4.0000 norm=2.9319
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0074 val_loss=0.0000 scale=2.0000 norm=1.5875
[iter 200] loss=0.7690 val_loss=0.0000 scale=2.0000 norm=1.5626
[iter 300] loss=0.4219 val_loss=0.0000 scale=4.0000 norm=3.0742
[iter 400] loss=0.2228 val_loss=0.0000 scale=4.0000 norm=3.0110
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8117 val_loss=0.0000 scale=2.0000 norm=1.5876
[iter 200] loss=0.3569 val_loss=0.0000 scale=4.0000 norm=3.2630
[iter 300] loss=-0.2867 val_loss=0.0000 scale=4.0000 norm=3.2587
[iter 400] loss=-0.9726 val_loss=0.0000 scale=8.0000 norm=6.2062
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6583 val_loss=0.0000 scale=2.0000 norm=1.2787
[iter 200] loss=0.4069 val_loss=0.0000 scale=2.0000 norm=1.3032
[iter 300] loss=0.1949 val_loss=0.0000 scale=4.0000 norm=2.6841
[iter 400] loss=-0.0507 val_loss=0.0000 scale=8.0000 norm=5.3463
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9878 val_loss=0.0000 scale=2.0000 norm=1.5683
[iter 200] loss=0.7470 val_loss=0.0000 scale=4.0000 norm=3.1018
[iter 300] loss=0.3974 val_loss=0.0000 scale=4.0000 norm=3.0653
[iter 400] loss=0.2082 val_loss=0.0000 scale=4.0000 norm=3.0533
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8653 val_loss=0.0000 scale=2.0000 norm=1.5258
[iter 200] loss=0.5307 val_loss=0.0000 scale=2.0000 norm=1.5157
[iter 300] loss=0.0323 val_loss=0.0000 scale=4.0000 norm=2.9525
[iter 400] loss=-0.3502 val_loss=0.0000 scale=4.0000 norm=2.8943
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0115 val_loss=0.0000 scale=2.0000 norm=1.5967
[iter 200] loss=0.7597 val_loss=0.0000 scale=4.0000 norm=3.1715
[iter 300] loss=0.4113 val_loss=0.0000 scale=4.0000 norm=3.1288
[iter 400] loss=0.2534 val_loss=0.0000 scale=4.0000 norm=3.1665
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0554 val_loss=0.0000 scale=2.0000 norm=1.6347
[iter 200] loss=0.7425 val_loss=0.0000 scale=4.0000 norm=3.2624
[iter 300] loss=0.4707 val_loss=0.0000 scale=4.0000 norm=3.2154
[iter 400] loss=0.3812 val_loss=0.0000 scale=2.0000 norm=1.5815
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9572 val_loss=0.0000 scale=2.0000 norm=1.6006
[iter 200] loss=0.6904 val_loss=0.0000 scale=4.0000 norm=3.1545
[iter 300] loss=0.3686 val_loss=0.0000 scale=4.0000 norm=3.1579
[iter 400] loss=0.1043 val_loss=0.0000 scale=4.0000 norm=3.1082
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0813 val_loss=0.0000 scale=2.0000 norm=1.6174
[iter 200] loss=0.9039 val_loss=0.0000 scale=2.0000 norm=1.5981
[iter 300] loss=0.6554 val_loss=0.0000 scale=4.0000 norm=3.1712
[iter 400] loss=0.4510 val_loss=0.0000 scale=4.0000 norm=3.0800
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0077 val_loss=0.0000 scale=2.0000 norm=1.5988
[iter 200] loss=0.7627 val_loss=0.0000 scale=2.0000 norm=1.5920
[iter 300] loss=0.4322 val_loss=0.0000 scale=4.0000 norm=3.1624
[iter 400] loss=0.1768 val_loss=0.0000 scale=4.0000 norm=3.1003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0141 val_loss=0.0000 scale=2.0000 norm=1.5899
[iter 200] loss=0.7542 val_loss=0.0000 scale=4.0000 norm=3.1736
[iter 300] loss=0.4443 val_loss=0.0000 scale=4.0000 norm=3.0973
[iter 400] loss=0.3045 val_loss=0.0000 scale=4.0000 norm=3.1275
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9890 val_loss=0.0000 scale=2.0000 norm=1.6445
[iter 200] loss=0.4643 val_loss=0.0000 scale=4.0000 norm=3.2642
[iter 300] loss=-0.1974 val_loss=0.0000 scale=4.0000 norm=3.2028
[iter 400] loss=-0.7550 val_loss=0.0000 scale=4.0000 norm=3.2003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9914 val_loss=0.0000 scale=2.0000 norm=1.5958
[iter 200] loss=0.7136 val_loss=0.0000 scale=4.0000 norm=3.1811
[iter 300] loss=0.3358 val_loss=0.0000 scale=4.0000 norm=3.1466
[iter 400] loss=0.1125 val_loss=0.0000 scale=4.0000 norm=3.1421
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6195 val_loss=0.0000 scale=2.0000 norm=1.1033
[iter 200] loss=0.2836 val_loss=0.0000 scale=4.0000 norm=2.2032
[iter 300] loss=-0.1897 val_loss=0.0000 scale=4.0000 norm=2.0853
[iter 400] loss=-0.9023 val_loss=0.0000 scale=8.0000 norm=4.1393
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0386 val_loss=0.0000 scale=2.0000 norm=1.6161
[iter 200] loss=0.8107 val_loss=0.0000 scale=4.0000 norm=3.2087
[iter 300] loss=0.4505 val_loss=0.0000 scale=4.0000 norm=3.1853
[iter 400] loss=0.2026 val_loss=0.0000 scale=4.0000 norm=3.1492
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9402 val_loss=0.0000 scale=2.0000 norm=1.5683
[iter 200] loss=0.6329 val_loss=0.0000 scale=4.0000 norm=3.1391
[iter 300] loss=0.2695 val_loss=0.0000 scale=4.0000 norm=3.1222
[iter 400] loss=-0.0074 val_loss=0.0000 scale=4.0000 norm=3.0501
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9348 val_loss=0.0000 scale=2.0000 norm=1.5530
[iter 200] loss=0.5696 val_loss=0.0000 scale=4.0000 norm=3.0707
[iter 300] loss=0.0313 val_loss=0.0000 scale=4.0000 norm=3.0081
[iter 400] loss=-0.4542 val_loss=0.0000 scale=4.0000 norm=2.9567
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5524 val_loss=0.0000 scale=2.0000 norm=1.1038
[iter 200] loss=0.1726 val_loss=0.0000 scale=4.0000 norm=2.2284
[iter 300] loss=-0.3849 val_loss=0.0000 scale=4.0000 norm=2.2323
[iter 400] loss=-0.8943 val_loss=0.0000 scale=4.0000 norm=2.2128
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9921 val_loss=0.0000 scale=2.0000 norm=1.5679
[iter 200] loss=0.6894 val_loss=0.0000 scale=4.0000 norm=3.1176
[iter 300] loss=0.3323 val_loss=0.0000 scale=4.0000 norm=3.0811
[iter 400] loss=0.1143 val_loss=0.0000 scale=4.0000 norm=3.0199
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6962 val_loss=0.0000 scale=2.0000 norm=1.5054
[iter 200] loss=0.1550 val_loss=0.0000 scale=4.0000 norm=2.8205
[iter 300] loss=-0.4380 val_loss=0.0000 scale=4.0000 norm=2.7354
[iter 400] loss=-0.8441 val_loss=0.0000 scale=4.0000 norm=2.7300
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0094 val_loss=0.0000 scale=2.0000 norm=1.6211
[iter 200] loss=0.6720 val_loss=0.0000 scale=4.0000 norm=3.2044
[iter 300] loss=0.2627 val_loss=0.0000 scale=4.0000 norm=3.1672
[iter 400] loss=-0.0188 val_loss=0.0000 scale=4.0000 norm=3.1480
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0168 val_loss=0.0000 scale=2.0000 norm=1.5805
[iter 200] loss=0.7967 val_loss=0.0000 scale=2.0000 norm=1.5677
[iter 300] loss=0.4815 val_loss=0.0000 scale=4.0000 norm=3.1217
[iter 400] loss=0.3266 val_loss=0.0000 scale=4.0000 norm=3.1311
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9585 val_loss=0.0000 scale=2.0000 norm=1.5388
[iter 200] loss=0.7023 val_loss=0.0000 scale=2.0000 norm=1.5212
[iter 300] loss=0.3034 val_loss=0.0000 scale=4.0000 norm=2.9959
[iter 400] loss=0.0264 val_loss=0.0000 scale=4.0000 norm=2.9552
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0888 val_loss=0.0000 scale=2.0000 norm=1.6448
[iter 200] loss=0.9169 val_loss=0.0000 scale=2.0000 norm=1.6386
[iter 300] loss=0.6735 val_loss=0.0000 scale=4.0000 norm=3.2608
[iter 400] loss=0.5169 val_loss=0.0000 scale=8.0000 norm=6.4234
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0962 val_loss=0.0000 scale=2.0000 norm=1.6489
[iter 200] loss=0.9092 val_loss=0.0000 scale=4.0000 norm=3.2869
[iter 300] loss=0.6555 val_loss=0.0000 scale=4.0000 norm=3.2634
[iter 400] loss=0.4650 val_loss=0.0000 scale=4.0000 norm=3.2414
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8226 val_loss=0.0000 scale=2.0000 norm=1.5020
[iter 200] loss=0.5026 val_loss=0.0000 scale=2.0000 norm=1.5239
[iter 300] loss=0.0555 val_loss=0.0000 scale=4.0000 norm=2.9517
[iter 400] loss=-0.3554 val_loss=0.0000 scale=4.0000 norm=2.9325
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0790 val_loss=0.0000 scale=2.0000 norm=1.6575
[iter 200] loss=0.8007 val_loss=0.0000 scale=4.0000 norm=3.2794
[iter 300] loss=0.4690 val_loss=0.0000 scale=4.0000 norm=3.1951
[iter 400] loss=0.2738 val_loss=0.0000 scale=8.0000 norm=6.3183
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9109 val_loss=0.0000 scale=2.0000 norm=1.5896
[iter 200] loss=0.5569 val_loss=0.0000 scale=4.0000 norm=3.0840
[iter 300] loss=0.1308 val_loss=0.0000 scale=8.0000 norm=6.1644
[iter 400] loss=-0.3114 val_loss=0.0000 scale=4.0000 norm=3.0542
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0313 val_loss=0.0000 scale=2.0000 norm=1.6093
[iter 200] loss=0.8095 val_loss=0.0000 scale=4.0000 norm=3.1988
[iter 300] loss=0.4814 val_loss=0.0000 scale=4.0000 norm=3.1577
[iter 400] loss=0.2849 val_loss=0.0000 scale=4.0000 norm=3.1402
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0569 val_loss=0.0000 scale=2.0000 norm=1.6140
[iter 200] loss=0.8487 val_loss=0.0000 scale=4.0000 norm=3.2027
[iter 300] loss=0.5393 val_loss=0.0000 scale=4.0000 norm=3.1411
[iter 400] loss=0.3534 val_loss=0.0000 scale=4.0000 norm=3.0862
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1198 val_loss=0.0000 scale=2.0000 norm=1.7053
[iter 200] loss=0.8633 val_loss=0.0000 scale=4.0000 norm=3.3776
[iter 300] loss=0.5294 val_loss=0.0000 scale=4.0000 norm=3.3040
[iter 400] loss=0.1481 val_loss=0.0000 scale=8.0000 norm=6.4526
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0019 val_loss=0.0000 scale=2.0000 norm=1.5943
[iter 200] loss=0.7314 val_loss=0.0000 scale=4.0000 norm=3.1646
[iter 300] loss=0.3762 val_loss=0.0000 scale=4.0000 norm=3.1259
[iter 400] loss=0.1736 val_loss=0.0000 scale=4.0000 norm=3.1403
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0403 val_loss=0.0000 scale=2.0000 norm=1.5894
[iter 200] loss=0.8320 val_loss=0.0000 scale=4.0000 norm=3.1529
[iter 300] loss=0.5361 val_loss=0.0000 scale=4.0000 norm=3.1234
[iter 400] loss=0.3939 val_loss=0.0000 scale=4.0000 norm=3.1024
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0328 val_loss=0.0000 scale=2.0000 norm=1.6196
[iter 200] loss=0.7836 val_loss=0.0000 scale=4.0000 norm=3.2352
[iter 300] loss=0.3832 val_loss=0.0000 scale=4.0000 norm=3.1880
[iter 400] loss=0.1169 val_loss=0.0000 scale=4.0000 norm=3.1461
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7657 val_loss=0.0000 scale=2.0000 norm=1.3380
[iter 200] loss=0.5161 val_loss=0.0000 scale=2.0000 norm=1.3827
[iter 300] loss=0.2018 val_loss=0.0000 scale=4.0000 norm=2.7565
[iter 400] loss=-0.1013 val_loss=0.0000 scale=4.0000 norm=2.7080
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0030 val_loss=0.0000 scale=2.0000 norm=1.6187
[iter 200] loss=0.7296 val_loss=0.0000 scale=4.0000 norm=3.1317
[iter 300] loss=0.4589 val_loss=0.0000 scale=4.0000 norm=3.0396
[iter 400] loss=0.2489 val_loss=0.0000 scale=8.0000 norm=5.9110
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0237 val_loss=0.0000 scale=2.0000 norm=1.6107
[iter 200] loss=0.8039 val_loss=0.0000 scale=2.0000 norm=1.6095
[iter 300] loss=0.5501 val_loss=0.0000 scale=4.0000 norm=3.1725
[iter 400] loss=0.3243 val_loss=0.0000 scale=4.0000 norm=3.1058
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1173 val_loss=0.0000 scale=2.0000 norm=1.6860
[iter 200] loss=0.9336 val_loss=0.0000 scale=2.0000 norm=1.6569
[iter 300] loss=0.6402 val_loss=0.0000 scale=4.0000 norm=3.3172
[iter 400] loss=0.3484 val_loss=0.0000 scale=4.0000 norm=3.2962
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0443 val_loss=0.0000 scale=2.0000 norm=1.6045
[iter 200] loss=0.7921 val_loss=0.0000 scale=4.0000 norm=3.2195
[iter 300] loss=0.4537 val_loss=0.0000 scale=4.0000 norm=3.1752
[iter 400] loss=0.2747 val_loss=0.0000 scale=4.0000 norm=3.1983
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0426 val_loss=0.0000 scale=2.0000 norm=1.6422
[iter 200] loss=0.8166 val_loss=0.0000 scale=4.0000 norm=3.2850
[iter 300] loss=0.4968 val_loss=0.0000 scale=4.0000 norm=3.2437
[iter 400] loss=0.3389 val_loss=0.0000 scale=4.0000 norm=3.1963
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8996 val_loss=0.0000 scale=2.0000 norm=1.5807
[iter 200] loss=0.6610 val_loss=0.0000 scale=2.0000 norm=1.6230
[iter 300] loss=0.3571 val_loss=0.0000 scale=4.0000 norm=3.1533
[iter 400] loss=0.0495 val_loss=0.0000 scale=8.0000 norm=6.3104
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8184 val_loss=0.0000 scale=2.0000 norm=1.5822
[iter 200] loss=0.1601 val_loss=0.0000 scale=4.0000 norm=3.1798
[iter 300] loss=-0.5547 val_loss=0.0000 scale=4.0000 norm=3.0734
[iter 400] loss=-1.0506 val_loss=0.0000 scale=4.0000 norm=2.8467
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8419 val_loss=0.0000 scale=2.0000 norm=1.6270
[iter 200] loss=0.3599 val_loss=0.0000 scale=4.0000 norm=3.3314
[iter 300] loss=-0.4441 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-1.9801 val_loss=0.0000 scale=16.0000 norm=13.3436
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4187 val_loss=0.0000 scale=2.0000 norm=1.1959
[iter 200] loss=-0.1390 val_loss=0.0000 scale=4.0000 norm=2.2464
[iter 300] loss=-0.8448 val_loss=0.0000 scale=4.0000 norm=2.2485
[iter 400] loss=-2.0978 val_loss=0.0000 scale=8.0000 norm=4.4986
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7137 val_loss=0.0000 scale=2.0000 norm=1.3012
[iter 200] loss=0.3388 val_loss=0.0000 scale=4.0000 norm=2.6952
[iter 300] loss=-0.1875 val_loss=0.0000 scale=4.0000 norm=2.6585
[iter 400] loss=-0.6500 val_loss=0.0000 scale=4.0000 norm=2.6756
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0064 val_loss=0.0000 scale=2.0000 norm=1.5755
[iter 200] loss=0.7634 val_loss=0.0000 scale=2.0000 norm=1.5606
[iter 300] loss=0.4219 val_loss=0.0000 scale=4.0000 norm=3.1034
[iter 400] loss=0.1826 val_loss=0.0000 scale=4.0000 norm=3.0005
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0624 val_loss=0.0000 scale=2.0000 norm=1.6413
[iter 200] loss=0.8432 val_loss=0.0000 scale=2.0000 norm=1.6452
[iter 300] loss=0.5139 val_loss=0.0000 scale=4.0000 norm=3.2355
[iter 400] loss=0.3100 val_loss=0.0000 scale=4.0000 norm=3.2437
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7350 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 200] loss=-0.1757 val_loss=0.0000 scale=4.0000 norm=3.0687
[iter 300] loss=-1.3577 val_loss=0.0000 scale=8.0000 norm=5.8057
[iter 400] loss=-3.5817 val_loss=0.0000 scale=16.0000 norm=11.6070
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0086 val_loss=0.0000 scale=2.0000 norm=1.5753
[iter 200] loss=0.7674 val_loss=0.0000 scale=4.0000 norm=3.1241
[iter 300] loss=0.4003 val_loss=0.0000 scale=4.0000 norm=3.0514
[iter 400] loss=0.1975 val_loss=0.0000 scale=4.0000 norm=3.0594
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9933 val_loss=0.0000 scale=2.0000 norm=1.5687
[iter 200] loss=0.6973 val_loss=0.0000 scale=4.0000 norm=3.1206
[iter 300] loss=0.2909 val_loss=0.0000 scale=4.0000 norm=3.0922
[iter 400] loss=0.0251 val_loss=0.0000 scale=4.0000 norm=3.0373
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0493 val_loss=0.0000 scale=2.0000 norm=1.6410
[iter 200] loss=0.8027 val_loss=0.0000 scale=4.0000 norm=3.2698
[iter 300] loss=0.4886 val_loss=0.0000 scale=4.0000 norm=3.2078
[iter 400] loss=0.3563 val_loss=0.0000 scale=4.0000 norm=3.2099
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.5678 val_loss=0.0000 scale=2.0000 norm=1.3048
[iter 200] loss=-0.0662 val_loss=0.0000 scale=4.0000 norm=2.7345
[iter 300] loss=-0.8568 val_loss=0.0000 scale=4.0000 norm=2.7336
[iter 400] loss=-1.7869 val_loss=0.0000 scale=8.0000 norm=5.1172
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0669 val_loss=0.0000 scale=2.0000 norm=1.6223
[iter 200] loss=0.8785 val_loss=0.0000 scale=2.0000 norm=1.6145
[iter 300] loss=0.6454 val_loss=0.0000 scale=4.0000 norm=3.1942
[iter 400] loss=0.4666 val_loss=0.0000 scale=4.0000 norm=3.1348
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0660 val_loss=0.0000 scale=2.0000 norm=1.6710
[iter 200] loss=0.7422 val_loss=0.0000 scale=4.0000 norm=3.3213
[iter 300] loss=0.3754 val_loss=0.0000 scale=4.0000 norm=3.2882
[iter 400] loss=0.0590 val_loss=0.0000 scale=4.0000 norm=3.2214
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9596 val_loss=0.0000 scale=2.0000 norm=1.5541
[iter 200] loss=0.6740 val_loss=0.0000 scale=2.0000 norm=1.5289
[iter 300] loss=0.3998 val_loss=0.0000 scale=2.0000 norm=1.5099
[iter 400] loss=0.0702 val_loss=0.0000 scale=4.0000 norm=2.9567
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0621 val_loss=0.0000 scale=2.0000 norm=1.6576
[iter 200] loss=0.7617 val_loss=0.0000 scale=4.0000 norm=3.2869
[iter 300] loss=0.3519 val_loss=0.0000 scale=4.0000 norm=3.2123
[iter 400] loss=0.0159 val_loss=0.0000 scale=8.0000 norm=6.3783
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9473 val_loss=0.0000 scale=2.0000 norm=1.6440
[iter 200] loss=0.5493 val_loss=0.0000 scale=4.0000 norm=3.2434
[iter 300] loss=0.0869 val_loss=0.0000 scale=4.0000 norm=3.1974
[iter 400] loss=-0.2494 val_loss=0.0000 scale=4.0000 norm=3.0742
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0409 val_loss=0.0000 scale=2.0000 norm=1.6155
[iter 200] loss=0.8216 val_loss=0.0000 scale=2.0000 norm=1.6066
[iter 300] loss=0.4743 val_loss=0.0000 scale=4.0000 norm=3.1827
[iter 400] loss=0.2512 val_loss=0.0000 scale=4.0000 norm=3.2004
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0008 val_loss=0.0000 scale=2.0000 norm=1.5828
[iter 200] loss=0.7362 val_loss=0.0000 scale=4.0000 norm=3.1411
[iter 300] loss=0.3914 val_loss=0.0000 scale=4.0000 norm=3.0664
[iter 400] loss=0.2174 val_loss=0.0000 scale=4.0000 norm=3.0934
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9995 val_loss=0.0000 scale=2.0000 norm=1.5969
[iter 200] loss=0.7354 val_loss=0.0000 scale=2.0000 norm=1.5920
[iter 300] loss=0.3965 val_loss=0.0000 scale=4.0000 norm=3.1689
[iter 400] loss=0.1556 val_loss=0.0000 scale=4.0000 norm=3.0457
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9659 val_loss=0.0000 scale=2.0000 norm=1.5741
[iter 200] loss=0.7013 val_loss=0.0000 scale=4.0000 norm=3.1061
[iter 300] loss=0.3299 val_loss=0.0000 scale=4.0000 norm=3.0466
[iter 400] loss=0.0697 val_loss=0.0000 scale=4.0000 norm=2.9392
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0099 val_loss=0.0000 scale=2.0000 norm=1.5805
[iter 200] loss=0.7761 val_loss=0.0000 scale=4.0000 norm=3.1280
[iter 300] loss=0.4392 val_loss=0.0000 scale=4.0000 norm=3.0873
[iter 400] loss=0.2754 val_loss=0.0000 scale=4.0000 norm=3.0601
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0221 val_loss=0.0000 scale=2.0000 norm=1.6006
[iter 200] loss=0.7537 val_loss=0.0000 scale=4.0000 norm=3.1804
[iter 300] loss=0.4467 val_loss=0.0000 scale=4.0000 norm=3.0990
[iter 400] loss=0.2909 val_loss=0.0000 scale=4.0000 norm=3.0733
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7419 val_loss=0.0000 scale=2.0000 norm=1.4644
[iter 200] loss=0.3760 val_loss=0.0000 scale=4.0000 norm=3.0492
[iter 300] loss=-0.1668 val_loss=0.0000 scale=4.0000 norm=3.0562
[iter 400] loss=-0.9446 val_loss=0.0000 scale=8.0000 norm=5.9929
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9631 val_loss=0.0000 scale=2.0000 norm=1.5457
[iter 200] loss=0.7018 val_loss=0.0000 scale=2.0000 norm=1.5299
[iter 300] loss=0.3348 val_loss=0.0000 scale=4.0000 norm=3.0117
[iter 400] loss=0.0926 val_loss=0.0000 scale=4.0000 norm=2.9875
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6626 val_loss=0.0000 scale=2.0000 norm=1.2794
[iter 200] loss=0.3100 val_loss=0.0000 scale=2.0000 norm=1.2811
[iter 300] loss=-0.1632 val_loss=0.0000 scale=4.0000 norm=2.5236
[iter 400] loss=-0.5593 val_loss=0.0000 scale=4.0000 norm=2.5002
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0260 val_loss=0.0000 scale=2.0000 norm=1.5884
[iter 200] loss=0.8086 val_loss=0.0000 scale=2.0000 norm=1.5806
[iter 300] loss=0.5173 val_loss=0.0000 scale=4.0000 norm=3.1261
[iter 400] loss=0.2365 val_loss=0.0000 scale=4.0000 norm=3.0804
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0140 val_loss=0.0000 scale=2.0000 norm=1.6164
[iter 200] loss=0.6703 val_loss=0.0000 scale=4.0000 norm=3.2366
[iter 300] loss=0.1590 val_loss=0.0000 scale=4.0000 norm=3.2360
[iter 400] loss=-0.3338 val_loss=0.0000 scale=8.0000 norm=6.3886
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0112 val_loss=0.0000 scale=2.0000 norm=1.5850
[iter 200] loss=0.7711 val_loss=0.0000 scale=2.0000 norm=1.5701
[iter 300] loss=0.4296 val_loss=0.0000 scale=4.0000 norm=3.0797
[iter 400] loss=0.2532 val_loss=0.0000 scale=4.0000 norm=3.0614
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9782 val_loss=0.0000 scale=2.0000 norm=1.6259
[iter 200] loss=0.5992 val_loss=0.0000 scale=4.0000 norm=3.2033
[iter 300] loss=0.1067 val_loss=0.0000 scale=4.0000 norm=3.1538
[iter 400] loss=-0.2921 val_loss=0.0000 scale=4.0000 norm=3.0773
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0426 val_loss=0.0000 scale=2.0000 norm=1.6188
[iter 200] loss=0.7742 val_loss=0.0000 scale=4.0000 norm=3.2005
[iter 300] loss=0.4799 val_loss=0.0000 scale=2.0000 norm=1.5555
[iter 400] loss=0.3435 val_loss=0.0000 scale=4.0000 norm=3.0970
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1220 val_loss=0.0000 scale=2.0000 norm=1.6697
[iter 200] loss=0.9522 val_loss=0.0000 scale=4.0000 norm=3.3100
[iter 300] loss=0.7143 val_loss=0.0000 scale=4.0000 norm=3.2602
[iter 400] loss=0.5602 val_loss=0.0000 scale=4.0000 norm=3.2190
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0443 val_loss=0.0000 scale=2.0000 norm=1.6685
[iter 200] loss=0.6850 val_loss=0.0000 scale=4.0000 norm=3.2649
[iter 300] loss=0.3240 val_loss=0.0000 scale=4.0000 norm=3.2090
[iter 400] loss=0.0514 val_loss=0.0000 scale=4.0000 norm=3.1159
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0469 val_loss=0.0000 scale=2.0000 norm=1.6126
[iter 200] loss=0.8206 val_loss=0.0000 scale=2.0000 norm=1.5889
[iter 300] loss=0.5360 val_loss=0.0000 scale=4.0000 norm=3.1190
[iter 400] loss=0.2731 val_loss=0.0000 scale=4.0000 norm=3.0855
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0407 val_loss=0.0000 scale=2.0000 norm=1.6424
[iter 200] loss=0.8080 val_loss=0.0000 scale=4.0000 norm=3.1980
[iter 300] loss=0.5717 val_loss=0.0000 scale=4.0000 norm=3.1624
[iter 400] loss=0.3578 val_loss=0.0000 scale=8.0000 norm=6.2141
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0011 val_loss=0.0000 scale=2.0000 norm=1.5750
[iter 200] loss=0.7563 val_loss=0.0000 scale=2.0000 norm=1.5675
[iter 300] loss=0.4377 val_loss=0.0000 scale=4.0000 norm=3.1094
[iter 400] loss=0.2098 val_loss=0.0000 scale=4.0000 norm=3.0878
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0288 val_loss=0.0000 scale=2.0000 norm=1.6088
[iter 200] loss=0.7914 val_loss=0.0000 scale=4.0000 norm=3.1569
[iter 300] loss=0.4474 val_loss=0.0000 scale=4.0000 norm=3.1002
[iter 400] loss=0.2460 val_loss=0.0000 scale=8.0000 norm=6.2270
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9615 val_loss=0.0000 scale=2.0000 norm=1.5483
[iter 200] loss=0.6908 val_loss=0.0000 scale=2.0000 norm=1.5203
[iter 300] loss=0.4077 val_loss=0.0000 scale=4.0000 norm=3.0119
[iter 400] loss=0.1802 val_loss=0.0000 scale=4.0000 norm=2.9887
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0787 val_loss=0.0000 scale=2.0000 norm=1.6319
[iter 200] loss=0.8333 val_loss=0.0000 scale=4.0000 norm=3.2201
[iter 300] loss=0.5582 val_loss=0.0000 scale=4.0000 norm=3.1236
[iter 400] loss=0.1662 val_loss=0.0000 scale=8.0000 norm=6.2975
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8987 val_loss=0.0000 scale=2.0000 norm=1.6413
[iter 200] loss=0.2516 val_loss=0.0000 scale=4.0000 norm=3.1660
[iter 300] loss=-0.3210 val_loss=0.0000 scale=8.0000 norm=5.9007
[iter 400] loss=-1.3860 val_loss=0.0000 scale=16.0000 norm=11.6727
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1126 val_loss=0.0000 scale=2.0000 norm=1.6664
[iter 200] loss=0.9602 val_loss=0.0000 scale=2.0000 norm=1.6687
[iter 300] loss=0.7646 val_loss=0.0000 scale=4.0000 norm=3.3268
[iter 400] loss=0.5327 val_loss=0.0000 scale=8.0000 norm=6.6673
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1180 val_loss=0.0000 scale=2.0000 norm=1.6808
[iter 200] loss=0.9461 val_loss=0.0000 scale=4.0000 norm=3.3417
[iter 300] loss=0.7048 val_loss=0.0000 scale=4.0000 norm=3.3052
[iter 400] loss=0.5362 val_loss=0.0000 scale=4.0000 norm=3.2684
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4758 val_loss=0.0000 scale=2.0000 norm=1.0500
[iter 200] loss=0.0729 val_loss=0.0000 scale=2.0000 norm=1.1270
[iter 300] loss=-0.3956 val_loss=0.0000 scale=4.0000 norm=2.2556
[iter 400] loss=-1.1922 val_loss=0.0000 scale=8.0000 norm=4.5003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0552 val_loss=0.0000 scale=2.0000 norm=1.6239
[iter 200] loss=0.8223 val_loss=0.0000 scale=4.0000 norm=3.2316
[iter 300] loss=0.4941 val_loss=0.0000 scale=4.0000 norm=3.2068
[iter 400] loss=0.3025 val_loss=0.0000 scale=4.0000 norm=3.1580
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0506 val_loss=0.0000 scale=2.0000 norm=1.6251
[iter 200] loss=0.8149 val_loss=0.0000 scale=4.0000 norm=3.2528
[iter 300] loss=0.5225 val_loss=0.0000 scale=4.0000 norm=3.1823
[iter 400] loss=0.3993 val_loss=0.0000 scale=4.0000 norm=3.1872
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0056 val_loss=0.0000 scale=2.0000 norm=1.5836
[iter 200] loss=0.7730 val_loss=0.0000 scale=4.0000 norm=3.1241
[iter 300] loss=0.4510 val_loss=0.0000 scale=4.0000 norm=3.1027
[iter 400] loss=0.2362 val_loss=0.0000 scale=4.0000 norm=3.0601
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0065 val_loss=0.0000 scale=2.0000 norm=1.5921
[iter 200] loss=0.7252 val_loss=0.0000 scale=4.0000 norm=3.1634
[iter 300] loss=0.3938 val_loss=0.0000 scale=2.0000 norm=1.5430
[iter 400] loss=0.2505 val_loss=0.0000 scale=4.0000 norm=3.1084
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0370 val_loss=0.0000 scale=2.0000 norm=1.6306
[iter 200] loss=0.8065 val_loss=0.0000 scale=2.0000 norm=1.6265
[iter 300] loss=0.4881 val_loss=0.0000 scale=2.0000 norm=1.5923
[iter 400] loss=0.3321 val_loss=0.0000 scale=4.0000 norm=3.2073
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0485 val_loss=0.0000 scale=2.0000 norm=1.6541
[iter 200] loss=0.7275 val_loss=0.0000 scale=4.0000 norm=3.3123
[iter 300] loss=0.2958 val_loss=0.0000 scale=4.0000 norm=3.2729
[iter 400] loss=-0.0130 val_loss=0.0000 scale=4.0000 norm=3.2220
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0164 val_loss=0.0000 scale=2.0000 norm=1.6033
[iter 200] loss=0.7570 val_loss=0.0000 scale=2.0000 norm=1.5932
[iter 300] loss=0.3619 val_loss=0.0000 scale=4.0000 norm=3.1343
[iter 400] loss=0.1125 val_loss=0.0000 scale=4.0000 norm=3.0969
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9893 val_loss=0.0000 scale=2.0000 norm=1.5688
[iter 200] loss=0.7437 val_loss=0.0000 scale=2.0000 norm=1.5572
[iter 300] loss=0.4173 val_loss=0.0000 scale=4.0000 norm=3.0745
[iter 400] loss=0.1994 val_loss=0.0000 scale=4.0000 norm=3.0806
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0377 val_loss=0.0000 scale=2.0000 norm=1.6135
[iter 200] loss=0.7728 val_loss=0.0000 scale=4.0000 norm=3.1863
[iter 300] loss=0.3960 val_loss=0.0000 scale=4.0000 norm=3.1409
[iter 400] loss=0.1543 val_loss=0.0000 scale=4.0000 norm=3.1278
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0120 val_loss=0.0000 scale=2.0000 norm=1.5910
[iter 200] loss=0.7731 val_loss=0.0000 scale=4.0000 norm=3.1643
[iter 300] loss=0.4534 val_loss=0.0000 scale=4.0000 norm=3.1325
[iter 400] loss=0.2937 val_loss=0.0000 scale=4.0000 norm=3.1042
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9236 val_loss=0.0000 scale=2.0000 norm=1.6351
[iter 200] loss=0.5878 val_loss=0.0000 scale=4.0000 norm=3.3115
[iter 300] loss=0.0686 val_loss=0.0000 scale=4.0000 norm=3.2918
[iter 400] loss=-0.4367 val_loss=0.0000 scale=4.0000 norm=3.2799
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0317 val_loss=0.0000 scale=2.0000 norm=1.6446
[iter 200] loss=0.7648 val_loss=0.0000 scale=4.0000 norm=3.3176
[iter 300] loss=0.2648 val_loss=0.0000 scale=4.0000 norm=3.3251
[iter 400] loss=-0.6502 val_loss=0.0000 scale=8.0000 norm=6.6505
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0222 val_loss=0.0000 scale=2.0000 norm=1.5965
[iter 200] loss=0.7940 val_loss=0.0000 scale=4.0000 norm=3.1730
[iter 300] loss=0.4885 val_loss=0.0000 scale=4.0000 norm=3.1519
[iter 400] loss=0.3194 val_loss=0.0000 scale=4.0000 norm=3.1238
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9588 val_loss=0.0000 scale=2.0000 norm=1.5258
[iter 200] loss=0.7128 val_loss=0.0000 scale=2.0000 norm=1.5073
[iter 300] loss=0.3550 val_loss=0.0000 scale=4.0000 norm=2.9719
[iter 400] loss=0.0763 val_loss=0.0000 scale=4.0000 norm=2.9505
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9758 val_loss=0.0000 scale=2.0000 norm=1.5654
[iter 200] loss=0.7054 val_loss=0.0000 scale=4.0000 norm=3.1108
[iter 300] loss=0.3626 val_loss=0.0000 scale=4.0000 norm=3.0813
[iter 400] loss=0.1727 val_loss=0.0000 scale=4.0000 norm=3.0681
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8255 val_loss=0.0000 scale=2.0000 norm=1.4210
[iter 200] loss=0.5937 val_loss=0.0000 scale=2.0000 norm=1.4635
[iter 300] loss=0.3137 val_loss=0.0000 scale=4.0000 norm=2.9051
[iter 400] loss=0.0518 val_loss=0.0000 scale=4.0000 norm=2.8366
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0001 val_loss=0.0000 scale=2.0000 norm=1.5809
[iter 200] loss=0.7562 val_loss=0.0000 scale=2.0000 norm=1.5702
[iter 300] loss=0.3904 val_loss=0.0000 scale=4.0000 norm=3.1280
[iter 400] loss=0.1626 val_loss=0.0000 scale=4.0000 norm=3.1661
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9943 val_loss=0.0000 scale=2.0000 norm=1.5652
[iter 200] loss=0.7166 val_loss=0.0000 scale=4.0000 norm=3.1139
[iter 300] loss=0.4008 val_loss=0.0000 scale=4.0000 norm=3.0626
[iter 400] loss=0.2059 val_loss=0.0000 scale=4.0000 norm=3.0114
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8792 val_loss=0.0000 scale=2.0000 norm=1.4642
[iter 200] loss=0.6641 val_loss=0.0000 scale=2.0000 norm=1.5133
[iter 300] loss=0.3798 val_loss=0.0000 scale=4.0000 norm=3.0335
[iter 400] loss=0.0835 val_loss=0.0000 scale=4.0000 norm=2.9996
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0202 val_loss=0.0000 scale=2.0000 norm=1.5867
[iter 200] loss=0.7936 val_loss=0.0000 scale=2.0000 norm=1.5645
[iter 300] loss=0.4859 val_loss=0.0000 scale=4.0000 norm=3.0749
[iter 400] loss=0.3371 val_loss=0.0000 scale=2.0000 norm=1.5281
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0603 val_loss=0.0000 scale=2.0000 norm=1.6359
[iter 200] loss=0.8422 val_loss=0.0000 scale=4.0000 norm=3.2571
[iter 300] loss=0.5249 val_loss=0.0000 scale=4.0000 norm=3.2079
[iter 400] loss=0.3753 val_loss=0.0000 scale=4.0000 norm=3.1883
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0196 val_loss=0.0000 scale=2.0000 norm=1.5927
[iter 200] loss=0.7182 val_loss=0.0000 scale=4.0000 norm=3.1546
[iter 300] loss=0.3575 val_loss=0.0000 scale=4.0000 norm=3.0449
[iter 400] loss=0.1470 val_loss=0.0000 scale=4.0000 norm=2.9971
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0166 val_loss=0.0000 scale=2.0000 norm=1.5954
[iter 200] loss=0.7829 val_loss=0.0000 scale=4.0000 norm=3.1669
[iter 300] loss=0.4421 val_loss=0.0000 scale=4.0000 norm=3.1302
[iter 400] loss=0.2608 val_loss=0.0000 scale=4.0000 norm=3.1770
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9687 val_loss=0.0000 scale=2.0000 norm=1.5344
[iter 200] loss=0.7289 val_loss=0.0000 scale=2.0000 norm=1.5243
[iter 300] loss=0.3852 val_loss=0.0000 scale=4.0000 norm=3.0133
[iter 400] loss=0.1934 val_loss=0.0000 scale=4.0000 norm=2.9670
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9580 val_loss=0.0000 scale=2.0000 norm=1.5522
[iter 200] loss=0.6850 val_loss=0.0000 scale=2.0000 norm=1.5297
[iter 300] loss=0.2760 val_loss=0.0000 scale=4.0000 norm=3.0026
[iter 400] loss=0.0161 val_loss=0.0000 scale=4.0000 norm=2.9321
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0299 val_loss=0.0000 scale=2.0000 norm=1.5997
[iter 200] loss=0.7655 val_loss=0.0000 scale=4.0000 norm=3.1804
[iter 300] loss=0.4021 val_loss=0.0000 scale=4.0000 norm=3.1436
[iter 400] loss=0.1991 val_loss=0.0000 scale=4.0000 norm=3.1052
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9888 val_loss=0.0000 scale=2.0000 norm=1.5635
[iter 200] loss=0.7592 val_loss=0.0000 scale=2.0000 norm=1.5439
[iter 300] loss=0.4314 val_loss=0.0000 scale=4.0000 norm=3.0638
[iter 400] loss=0.2206 val_loss=0.0000 scale=4.0000 norm=3.0548
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0247 val_loss=0.0000 scale=2.0000 norm=1.5849
[iter 200] loss=0.8239 val_loss=0.0000 scale=2.0000 norm=1.5595
[iter 300] loss=0.5498 val_loss=0.0000 scale=2.0000 norm=1.5437
[iter 400] loss=0.4116 val_loss=0.0000 scale=4.0000 norm=3.0909
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9409 val_loss=0.0000 scale=2.0000 norm=1.5946
[iter 200] loss=0.5938 val_loss=0.0000 scale=4.0000 norm=3.1182
[iter 300] loss=0.1453 val_loss=0.0000 scale=4.0000 norm=3.0942
[iter 400] loss=-0.1884 val_loss=0.0000 scale=4.0000 norm=3.0494
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0465 val_loss=0.0000 scale=2.0000 norm=1.6254
[iter 200] loss=0.8280 val_loss=0.0000 scale=4.0000 norm=3.2355
[iter 300] loss=0.4913 val_loss=0.0000 scale=4.0000 norm=3.1772
[iter 400] loss=0.3386 val_loss=0.0000 scale=2.0000 norm=1.5547
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7953 val_loss=0.0000 scale=2.0000 norm=1.6092
[iter 200] loss=0.2713 val_loss=0.0000 scale=4.0000 norm=3.1500
[iter 300] loss=-0.5495 val_loss=0.0000 scale=8.0000 norm=6.2905
[iter 400] loss=-2.1795 val_loss=0.0000 scale=16.0000 norm=12.5810
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0142 val_loss=0.0000 scale=2.0000 norm=1.5981
[iter 200] loss=0.7670 val_loss=0.0000 scale=4.0000 norm=3.1846
[iter 300] loss=0.4129 val_loss=0.0000 scale=2.0000 norm=1.5488
[iter 400] loss=0.2568 val_loss=0.0000 scale=4.0000 norm=3.1170
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9884 val_loss=0.0000 scale=2.0000 norm=1.5639
[iter 200] loss=0.7421 val_loss=0.0000 scale=4.0000 norm=3.0985
[iter 300] loss=0.4122 val_loss=0.0000 scale=4.0000 norm=3.0632
[iter 400] loss=0.2261 val_loss=0.0000 scale=4.0000 norm=3.0299
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0228 val_loss=0.0000 scale=2.0000 norm=1.6189
[iter 200] loss=0.7451 val_loss=0.0000 scale=4.0000 norm=3.2212
[iter 300] loss=0.4291 val_loss=0.0000 scale=4.0000 norm=3.1286
[iter 400] loss=0.2690 val_loss=0.0000 scale=4.0000 norm=3.1219
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9642 val_loss=0.0000 scale=2.0000 norm=1.5736
[iter 200] loss=0.6586 val_loss=0.0000 scale=2.0000 norm=1.5630
[iter 300] loss=0.1728 val_loss=0.0000 scale=4.0000 norm=3.0974
[iter 400] loss=-0.1096 val_loss=0.0000 scale=4.0000 norm=3.0380
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9996 val_loss=0.0000 scale=2.0000 norm=1.5863
[iter 200] loss=0.7471 val_loss=0.0000 scale=2.0000 norm=1.5590
[iter 300] loss=0.4617 val_loss=0.0000 scale=4.0000 norm=3.0766
[iter 400] loss=0.2012 val_loss=0.0000 scale=4.0000 norm=2.9763
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0090 val_loss=0.0000 scale=2.0000 norm=1.5898
[iter 200] loss=0.7350 val_loss=0.0000 scale=4.0000 norm=3.1317
[iter 300] loss=0.3636 val_loss=0.0000 scale=4.0000 norm=3.0769
[iter 400] loss=0.0950 val_loss=0.0000 scale=4.0000 norm=2.9715
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8658 val_loss=0.0000 scale=2.0000 norm=1.4609
[iter 200] loss=0.6250 val_loss=0.0000 scale=2.0000 norm=1.4840
[iter 300] loss=0.4154 val_loss=0.0000 scale=4.0000 norm=2.9679
[iter 400] loss=0.1734 val_loss=0.0000 scale=4.0000 norm=2.9001
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8547 val_loss=0.0000 scale=2.0000 norm=1.6121
[iter 200] loss=0.4188 val_loss=0.0000 scale=4.0000 norm=3.2086
[iter 300] loss=-0.0572 val_loss=0.0000 scale=4.0000 norm=3.1638
[iter 400] loss=-0.3913 val_loss=0.0000 scale=4.0000 norm=3.0563
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0302 val_loss=0.0000 scale=2.0000 norm=1.6077
[iter 200] loss=0.8036 val_loss=0.0000 scale=2.0000 norm=1.5905
[iter 300] loss=0.4672 val_loss=0.0000 scale=4.0000 norm=3.1443
[iter 400] loss=0.2640 val_loss=0.0000 scale=4.0000 norm=3.1587
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.5874
[iter 200] loss=0.7081 val_loss=0.0000 scale=4.0000 norm=3.1520
[iter 300] loss=0.3576 val_loss=0.0000 scale=4.0000 norm=3.1070
[iter 400] loss=0.1515 val_loss=0.0000 scale=4.0000 norm=3.0880
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5086 val_loss=0.0000 scale=2.0000 norm=1.0902
[iter 200] loss=0.0118 val_loss=0.0000 scale=2.0000 norm=1.1383
[iter 300] loss=-0.6654 val_loss=0.0000 scale=4.0000 norm=2.2547
[iter 400] loss=-1.7526 val_loss=0.0000 scale=8.0000 norm=4.5378
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8630 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 200] loss=0.3981 val_loss=0.0000 scale=4.0000 norm=3.4013
[iter 300] loss=-0.1294 val_loss=0.0000 scale=4.0000 norm=3.3008
[iter 400] loss=-0.9508 val_loss=0.0000 scale=16.0000 norm=12.9102
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.3175
[iter 200] loss=-0.2334 val_loss=0.0000 scale=2.0000 norm=1.2639
[iter 300] loss=-0.8684 val_loss=0.0000 scale=4.0000 norm=2.5376
[iter 400] loss=-1.7033 val_loss=0.0000 scale=8.0000 norm=4.9721
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0159 val_loss=0.0000 scale=2.0000 norm=1.5953
[iter 200] loss=0.7240 val_loss=0.0000 scale=4.0000 norm=3.1447
[iter 300] loss=0.3043 val_loss=0.0000 scale=4.0000 norm=3.1496
[iter 400] loss=-0.5078 val_loss=0.0000 scale=8.0000 norm=6.2995
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0358 val_loss=0.0000 scale=2.0000 norm=1.6342
[iter 200] loss=0.7520 val_loss=0.0000 scale=4.0000 norm=3.2207
[iter 300] loss=0.3576 val_loss=0.0000 scale=4.0000 norm=3.1986
[iter 400] loss=0.0450 val_loss=0.0000 scale=4.0000 norm=3.1240
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9907 val_loss=0.0000 scale=2.0000 norm=1.5889
[iter 200] loss=0.7308 val_loss=0.0000 scale=4.0000 norm=3.1569
[iter 300] loss=0.3976 val_loss=0.0000 scale=4.0000 norm=3.1396
[iter 400] loss=0.1608 val_loss=0.0000 scale=4.0000 norm=3.0913
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0269 val_loss=0.0000 scale=2.0000 norm=1.5978
[iter 200] loss=0.7698 val_loss=0.0000 scale=4.0000 norm=3.1835
[iter 300] loss=0.4181 val_loss=0.0000 scale=4.0000 norm=3.1327
[iter 400] loss=0.2225 val_loss=0.0000 scale=4.0000 norm=3.0747
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0068 val_loss=0.0000 scale=2.0000 norm=1.5943
[iter 200] loss=0.7462 val_loss=0.0000 scale=4.0000 norm=3.1441
[iter 300] loss=0.4438 val_loss=0.0000 scale=4.0000 norm=3.1073
[iter 400] loss=0.2222 val_loss=0.0000 scale=4.0000 norm=3.0616
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0147 val_loss=0.0000 scale=2.0000 norm=1.5859
[iter 200] loss=0.7843 val_loss=0.0000 scale=2.0000 norm=1.5805
[iter 300] loss=0.4742 val_loss=0.0000 scale=4.0000 norm=3.1195
[iter 400] loss=0.2811 val_loss=0.0000 scale=2.0000 norm=1.5503
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0124 val_loss=0.0000 scale=2.0000 norm=1.5753
[iter 200] loss=0.7904 val_loss=0.0000 scale=2.0000 norm=1.5601
[iter 300] loss=0.4914 val_loss=0.0000 scale=4.0000 norm=3.0944
[iter 400] loss=0.2910 val_loss=0.0000 scale=4.0000 norm=3.0943
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9242 val_loss=0.0000 scale=2.0000 norm=1.5181
[iter 200] loss=0.6486 val_loss=0.0000 scale=4.0000 norm=2.9839
[iter 300] loss=0.2723 val_loss=0.0000 scale=4.0000 norm=2.9542
[iter 400] loss=0.0097 val_loss=0.0000 scale=4.0000 norm=2.9469
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4375 val_loss=0.0000 scale=2.0000 norm=1.1901
[iter 200] loss=-0.1530 val_loss=0.0000 scale=4.0000 norm=2.4690
[iter 300] loss=-0.8569 val_loss=0.0000 scale=4.0000 norm=2.5415
[iter 400] loss=-2.2359 val_loss=0.0000 scale=8.0000 norm=5.0862
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0434 val_loss=0.0000 scale=2.0000 norm=1.6199
[iter 200] loss=0.7846 val_loss=0.0000 scale=4.0000 norm=3.2186
[iter 300] loss=0.4406 val_loss=0.0000 scale=4.0000 norm=3.1666
[iter 400] loss=0.2256 val_loss=0.0000 scale=4.0000 norm=3.1345
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9843 val_loss=0.0000 scale=2.0000 norm=1.5740
[iter 200] loss=0.7430 val_loss=0.0000 scale=2.0000 norm=1.5695
[iter 300] loss=0.4062 val_loss=0.0000 scale=4.0000 norm=3.1371
[iter 400] loss=0.1862 val_loss=0.0000 scale=4.0000 norm=3.0986
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0396 val_loss=0.0000 scale=2.0000 norm=1.6325
[iter 200] loss=0.7020 val_loss=0.0000 scale=4.0000 norm=3.2169
[iter 300] loss=0.3172 val_loss=0.0000 scale=4.0000 norm=3.1646
[iter 400] loss=0.0871 val_loss=0.0000 scale=4.0000 norm=3.1367
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0022 val_loss=0.0000 scale=2.0000 norm=1.6054
[iter 200] loss=0.7678 val_loss=0.0000 scale=4.0000 norm=3.1701
[iter 300] loss=0.4644 val_loss=0.0000 scale=4.0000 norm=3.1760
[iter 400] loss=0.2366 val_loss=0.0000 scale=4.0000 norm=3.1026
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4415 val_loss=0.0000 scale=2.0000 norm=0.9961
[iter 200] loss=-0.0068 val_loss=0.0000 scale=2.0000 norm=1.0106
[iter 300] loss=-0.6006 val_loss=0.0000 scale=4.0000 norm=2.0618
[iter 400] loss=-1.6326 val_loss=0.0000 scale=8.0000 norm=4.1286
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8944 val_loss=0.0000 scale=2.0000 norm=1.5239
[iter 200] loss=0.6415 val_loss=0.0000 scale=4.0000 norm=3.1417
[iter 300] loss=0.2218 val_loss=0.0000 scale=4.0000 norm=3.1681
[iter 400] loss=-0.5317 val_loss=0.0000 scale=8.0000 norm=6.3384
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6274 val_loss=0.0000 scale=2.0000 norm=1.1955
[iter 200] loss=0.2705 val_loss=0.0000 scale=2.0000 norm=1.2833
[iter 300] loss=-0.1173 val_loss=0.0000 scale=4.0000 norm=2.5903
[iter 400] loss=-0.4487 val_loss=0.0000 scale=4.0000 norm=2.4849
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0422 val_loss=0.0000 scale=2.0000 norm=1.6074
[iter 200] loss=0.8183 val_loss=0.0000 scale=2.0000 norm=1.5987
[iter 300] loss=0.4877 val_loss=0.0000 scale=4.0000 norm=3.1268
[iter 400] loss=0.3255 val_loss=0.0000 scale=2.0000 norm=1.5529
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9775 val_loss=0.0000 scale=2.0000 norm=1.5746
[iter 200] loss=0.7237 val_loss=0.0000 scale=4.0000 norm=3.0845
[iter 300] loss=0.3841 val_loss=0.0000 scale=4.0000 norm=3.0187
[iter 400] loss=0.2025 val_loss=0.0000 scale=4.0000 norm=2.9520
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0122 val_loss=0.0000 scale=2.0000 norm=1.5973
[iter 200] loss=0.7615 val_loss=0.0000 scale=4.0000 norm=3.1544
[iter 300] loss=0.4382 val_loss=0.0000 scale=4.0000 norm=3.1019
[iter 400] loss=0.2610 val_loss=0.0000 scale=4.0000 norm=3.1658
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0000 val_loss=0.0000 scale=2.0000 norm=1.5852
[iter 200] loss=0.7503 val_loss=0.0000 scale=4.0000 norm=3.1710
[iter 300] loss=0.4161 val_loss=0.0000 scale=4.0000 norm=3.1323
[iter 400] loss=0.2470 val_loss=0.0000 scale=4.0000 norm=3.1274
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0780 val_loss=0.0000 scale=2.0000 norm=1.6180
[iter 200] loss=0.9069 val_loss=0.0000 scale=2.0000 norm=1.6040
[iter 300] loss=0.6454 val_loss=0.0000 scale=4.0000 norm=3.1895
[iter 400] loss=0.4362 val_loss=0.0000 scale=4.0000 norm=3.1638
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0663 val_loss=0.0000 scale=2.0000 norm=1.6350
[iter 200] loss=0.8536 val_loss=0.0000 scale=2.0000 norm=1.6185
[iter 300] loss=0.5484 val_loss=0.0000 scale=4.0000 norm=3.1837
[iter 400] loss=0.3496 val_loss=0.0000 scale=2.0000 norm=1.5534
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0203 val_loss=0.0000 scale=2.0000 norm=1.5996
[iter 200] loss=0.7867 val_loss=0.0000 scale=2.0000 norm=1.5782
[iter 300] loss=0.4426 val_loss=0.0000 scale=4.0000 norm=3.1199
[iter 400] loss=0.2679 val_loss=0.0000 scale=4.0000 norm=3.0911
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9654 val_loss=0.0000 scale=2.0000 norm=1.5741
[iter 200] loss=0.6331 val_loss=0.0000 scale=4.0000 norm=3.0557
[iter 300] loss=0.2625 val_loss=0.0000 scale=4.0000 norm=2.9916
[iter 400] loss=-0.0477 val_loss=0.0000 scale=4.0000 norm=2.9244
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9856 val_loss=0.0000 scale=2.0000 norm=1.5620
[iter 200] loss=0.7479 val_loss=0.0000 scale=2.0000 norm=1.5510
[iter 300] loss=0.4214 val_loss=0.0000 scale=4.0000 norm=3.0534
[iter 400] loss=0.2277 val_loss=0.0000 scale=4.0000 norm=3.0071
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5968 val_loss=0.0000 scale=2.0000 norm=1.2872
[iter 200] loss=0.0884 val_loss=0.0000 scale=2.0000 norm=1.2449
[iter 300] loss=-0.3068 val_loss=0.0000 scale=4.0000 norm=2.6108
[iter 400] loss=-0.6288 val_loss=0.0000 scale=8.0000 norm=4.9224
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9102 val_loss=0.0000 scale=2.0000 norm=1.5312
[iter 200] loss=0.5707 val_loss=0.0000 scale=4.0000 norm=2.9653
[iter 300] loss=0.1090 val_loss=0.0000 scale=4.0000 norm=2.7986
[iter 400] loss=-0.2209 val_loss=0.0000 scale=4.0000 norm=2.6912
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9796 val_loss=0.0000 scale=2.0000 norm=1.5520
[iter 200] loss=0.7264 val_loss=0.0000 scale=2.0000 norm=1.5366
[iter 300] loss=0.3572 val_loss=0.0000 scale=4.0000 norm=3.0165
[iter 400] loss=0.1373 val_loss=0.0000 scale=4.0000 norm=2.9761
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9869 val_loss=0.0000 scale=2.0000 norm=1.5664
[iter 200] loss=0.7513 val_loss=0.0000 scale=2.0000 norm=1.5549
[iter 300] loss=0.4141 val_loss=0.0000 scale=4.0000 norm=3.0798
[iter 400] loss=0.2156 val_loss=0.0000 scale=4.0000 norm=3.0445
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7441 val_loss=0.0000 scale=2.0000 norm=1.2915
[iter 200] loss=0.4809 val_loss=0.0000 scale=2.0000 norm=1.3026
[iter 300] loss=0.0959 val_loss=0.0000 scale=4.0000 norm=2.5666
[iter 400] loss=-0.1697 val_loss=0.0000 scale=4.0000 norm=2.4108
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0342 val_loss=0.0000 scale=2.0000 norm=1.6112
[iter 200] loss=0.7891 val_loss=0.0000 scale=4.0000 norm=3.1841
[iter 300] loss=0.4737 val_loss=0.0000 scale=4.0000 norm=3.1168
[iter 400] loss=0.2672 val_loss=0.0000 scale=4.0000 norm=3.0829
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8261 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 200] loss=0.3049 val_loss=0.0000 scale=4.0000 norm=3.2241
[iter 300] loss=-0.3025 val_loss=0.0000 scale=4.0000 norm=3.1621
[iter 400] loss=-0.8569 val_loss=0.0000 scale=4.0000 norm=3.0815
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0338 val_loss=0.0000 scale=2.0000 norm=1.6096
[iter 200] loss=0.8013 val_loss=0.0000 scale=4.0000 norm=3.2005
[iter 300] loss=0.4548 val_loss=0.0000 scale=4.0000 norm=3.1720
[iter 400] loss=0.2401 val_loss=0.0000 scale=4.0000 norm=3.1572
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9890 val_loss=0.0000 scale=2.0000 norm=1.5520
[iter 200] loss=0.7521 val_loss=0.0000 scale=4.0000 norm=3.0704
[iter 300] loss=0.4239 val_loss=0.0000 scale=4.0000 norm=3.1078
[iter 400] loss=0.1187 val_loss=0.0000 scale=4.0000 norm=3.0727
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9624 val_loss=0.0000 scale=2.0000 norm=1.5460
[iter 200] loss=0.7267 val_loss=0.0000 scale=2.0000 norm=1.5668
[iter 300] loss=0.3569 val_loss=0.0000 scale=4.0000 norm=3.1565
[iter 400] loss=-0.0736 val_loss=0.0000 scale=4.0000 norm=3.1197
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0454 val_loss=0.0000 scale=2.0000 norm=1.6179
[iter 200] loss=0.8367 val_loss=0.0000 scale=2.0000 norm=1.6211
[iter 300] loss=0.5517 val_loss=0.0000 scale=4.0000 norm=3.2538
[iter 400] loss=0.3636 val_loss=0.0000 scale=4.0000 norm=3.1719
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1635 val_loss=0.0000 scale=2.0000 norm=1.8724
[iter 200] loss=0.8688 val_loss=0.0000 scale=4.0000 norm=3.7634
[iter 300] loss=0.3648 val_loss=0.0000 scale=8.0000 norm=7.5297
[iter 400] loss=-0.6272 val_loss=0.0000 scale=16.0000 norm=15.0595
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9717 val_loss=0.0000 scale=2.0000 norm=1.5520
[iter 200] loss=0.7312 val_loss=0.0000 scale=2.0000 norm=1.5267
[iter 300] loss=0.4134 val_loss=0.0000 scale=4.0000 norm=3.0124
[iter 400] loss=0.1930 val_loss=0.0000 scale=4.0000 norm=2.9480
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0164 val_loss=0.0000 scale=2.0000 norm=1.5827
[iter 200] loss=0.7761 val_loss=0.0000 scale=4.0000 norm=3.1327
[iter 300] loss=0.4447 val_loss=0.0000 scale=4.0000 norm=3.1067
[iter 400] loss=0.2534 val_loss=0.0000 scale=4.0000 norm=3.1609
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0285 val_loss=0.0000 scale=2.0000 norm=1.6461
[iter 200] loss=0.7166 val_loss=0.0000 scale=4.0000 norm=3.2961
[iter 300] loss=0.2429 val_loss=0.0000 scale=4.0000 norm=3.2437
[iter 400] loss=-0.0635 val_loss=0.0000 scale=4.0000 norm=3.1870
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6241 val_loss=0.0000 scale=2.0000 norm=1.1969
[iter 200] loss=0.2841 val_loss=0.0000 scale=4.0000 norm=2.5039
[iter 300] loss=-0.1259 val_loss=0.0000 scale=4.0000 norm=2.4831
[iter 400] loss=-0.4289 val_loss=0.0000 scale=4.0000 norm=2.4188
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9385 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 200] loss=0.7385 val_loss=0.0000 scale=4.0000 norm=3.1936
[iter 300] loss=0.4380 val_loss=0.0000 scale=4.0000 norm=3.2107
[iter 400] loss=-0.1290 val_loss=0.0000 scale=8.0000 norm=6.4220
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4128 val_loss=0.0000 scale=2.0000 norm=0.9614
[iter 200] loss=-0.0950 val_loss=0.0000 scale=2.0000 norm=0.9026
[iter 300] loss=-0.6772 val_loss=0.0000 scale=4.0000 norm=1.7291
[iter 400] loss=-1.5059 val_loss=0.0000 scale=8.0000 norm=3.3758
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0303 val_loss=0.0000 scale=2.0000 norm=1.6080
[iter 200] loss=0.8190 val_loss=0.0000 scale=2.0000 norm=1.6070
[iter 300] loss=0.5548 val_loss=0.0000 scale=4.0000 norm=3.1740
[iter 400] loss=0.3231 val_loss=0.0000 scale=2.0000 norm=1.5601
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0429 val_loss=0.0000 scale=2.0000 norm=1.6686
[iter 200] loss=0.6958 val_loss=0.0000 scale=4.0000 norm=3.3336
[iter 300] loss=0.1358 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-0.9442 val_loss=0.0000 scale=16.0000 norm=13.3438
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9499 val_loss=0.0000 scale=2.0000 norm=1.6676
[iter 200] loss=0.4851 val_loss=0.0000 scale=4.0000 norm=3.3495
[iter 300] loss=-0.3147 val_loss=0.0000 scale=4.0000 norm=3.3524
[iter 400] loss=-1.1804 val_loss=0.0000 scale=4.0000 norm=3.3246
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9769 val_loss=0.0000 scale=2.0000 norm=1.5631
[iter 200] loss=0.6859 val_loss=0.0000 scale=4.0000 norm=3.1167
[iter 300] loss=0.2748 val_loss=0.0000 scale=4.0000 norm=3.0518
[iter 400] loss=0.0088 val_loss=0.0000 scale=4.0000 norm=3.0198
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0996 val_loss=0.0000 scale=2.0000 norm=1.6764
[iter 200] loss=0.8764 val_loss=0.0000 scale=4.0000 norm=3.3008
[iter 300] loss=0.6019 val_loss=0.0000 scale=4.0000 norm=3.2696
[iter 400] loss=0.3078 val_loss=0.0000 scale=8.0000 norm=6.4094
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9956 val_loss=0.0000 scale=2.0000 norm=1.5922
[iter 200] loss=0.6896 val_loss=0.0000 scale=4.0000 norm=3.1709
[iter 300] loss=0.3511 val_loss=0.0000 scale=4.0000 norm=3.0826
[iter 400] loss=0.1493 val_loss=0.0000 scale=4.0000 norm=3.0023
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0435 val_loss=0.0000 scale=2.0000 norm=1.5924
[iter 200] loss=0.8426 val_loss=0.0000 scale=4.0000 norm=3.1746
[iter 300] loss=0.5676 val_loss=0.0000 scale=4.0000 norm=3.1364
[iter 400] loss=0.4431 val_loss=0.0000 scale=4.0000 norm=3.0385
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8748 val_loss=0.0000 scale=2.0000 norm=1.6157
[iter 200] loss=0.4005 val_loss=0.0000 scale=4.0000 norm=3.2493
[iter 300] loss=-0.3971 val_loss=0.0000 scale=4.0000 norm=3.2484
[iter 400] loss=-1.1784 val_loss=0.0000 scale=8.0000 norm=6.2435
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0284 val_loss=0.0000 scale=2.0000 norm=1.6085
[iter 200] loss=0.7802 val_loss=0.0000 scale=4.0000 norm=3.1985
[iter 300] loss=0.4423 val_loss=0.0000 scale=4.0000 norm=3.1488
[iter 400] loss=0.2575 val_loss=0.0000 scale=4.0000 norm=3.1045
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0260 val_loss=0.0000 scale=2.0000 norm=1.6103
[iter 200] loss=0.7664 val_loss=0.0000 scale=4.0000 norm=3.1836
[iter 300] loss=0.4189 val_loss=0.0000 scale=4.0000 norm=3.1402
[iter 400] loss=0.1836 val_loss=0.0000 scale=4.0000 norm=3.1240
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0475 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 200] loss=0.8308 val_loss=0.0000 scale=4.0000 norm=3.4606
[iter 300] loss=0.4707 val_loss=0.0000 scale=8.0000 norm=6.9339
[iter 400] loss=-0.2373 val_loss=0.0000 scale=16.0000 norm=13.8680
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7867 val_loss=0.0000 scale=2.0000 norm=1.4120
[iter 200] loss=0.5550 val_loss=0.0000 scale=4.0000 norm=2.9958
[iter 300] loss=0.2277 val_loss=0.0000 scale=4.0000 norm=3.0371
[iter 400] loss=-0.3747 val_loss=0.0000 scale=8.0000 norm=6.0770
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0079 val_loss=0.0000 scale=2.0000 norm=1.5884
[iter 200] loss=0.7541 val_loss=0.0000 scale=4.0000 norm=3.1426
[iter 300] loss=0.4110 val_loss=0.0000 scale=4.0000 norm=3.1105
[iter 400] loss=0.2426 val_loss=0.0000 scale=4.0000 norm=3.1119
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0373 val_loss=0.0000 scale=2.0000 norm=1.5869
[iter 200] loss=0.8156 val_loss=0.0000 scale=2.0000 norm=1.5837
[iter 300] loss=0.4878 val_loss=0.0000 scale=4.0000 norm=3.1282
[iter 400] loss=0.2582 val_loss=0.0000 scale=4.0000 norm=3.1534
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8726 val_loss=0.0000 scale=2.0000 norm=1.5368
[iter 200] loss=0.4637 val_loss=0.0000 scale=4.0000 norm=3.0415
[iter 300] loss=-0.0291 val_loss=0.0000 scale=4.0000 norm=3.0154
[iter 400] loss=-0.3672 val_loss=0.0000 scale=4.0000 norm=2.9544
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0329 val_loss=0.0000 scale=2.0000 norm=1.6310
[iter 200] loss=0.8002 val_loss=0.0000 scale=2.0000 norm=1.6309
[iter 300] loss=0.4920 val_loss=0.0000 scale=4.0000 norm=3.2477
[iter 400] loss=0.1722 val_loss=0.0000 scale=4.0000 norm=3.2158
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0408 val_loss=0.0000 scale=2.0000 norm=1.6822
[iter 200] loss=0.6884 val_loss=0.0000 scale=4.0000 norm=3.2922
[iter 300] loss=0.2608 val_loss=0.0000 scale=8.0000 norm=6.4257
[iter 400] loss=-0.4425 val_loss=0.0000 scale=16.0000 norm=12.8331
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0290 val_loss=0.0000 scale=2.0000 norm=1.5974
[iter 200] loss=0.7994 val_loss=0.0000 scale=4.0000 norm=3.1569
[iter 300] loss=0.4718 val_loss=0.0000 scale=4.0000 norm=3.1025
[iter 400] loss=0.3245 val_loss=0.0000 scale=4.0000 norm=3.1622
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0018 val_loss=0.0000 scale=2.0000 norm=1.5942
[iter 200] loss=0.7352 val_loss=0.0000 scale=2.0000 norm=1.5819
[iter 300] loss=0.3328 val_loss=0.0000 scale=4.0000 norm=3.1297
[iter 400] loss=0.0748 val_loss=0.0000 scale=2.0000 norm=1.5224
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6119 val_loss=0.0000 scale=2.0000 norm=1.1761
[iter 200] loss=0.2328 val_loss=0.0000 scale=2.0000 norm=1.1736
[iter 300] loss=-0.3097 val_loss=0.0000 scale=4.0000 norm=2.2127
[iter 400] loss=-0.7084 val_loss=0.0000 scale=4.0000 norm=2.0812
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6310 val_loss=0.0000 scale=2.0000 norm=1.3721
[iter 200] loss=0.2143 val_loss=0.0000 scale=4.0000 norm=2.6093
[iter 300] loss=-0.2981 val_loss=0.0000 scale=4.0000 norm=2.6618
[iter 400] loss=-0.7634 val_loss=0.0000 scale=8.0000 norm=4.9863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9606 val_loss=0.0000 scale=2.0000 norm=1.5382
[iter 200] loss=0.7498 val_loss=0.0000 scale=4.0000 norm=3.1874
[iter 300] loss=0.4137 val_loss=0.0000 scale=8.0000 norm=6.4023
[iter 400] loss=-0.2463 val_loss=0.0000 scale=16.0000 norm=12.8051
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8718 val_loss=0.0000 scale=2.0000 norm=1.5437
[iter 200] loss=0.5694 val_loss=0.0000 scale=2.0000 norm=1.5040
[iter 300] loss=0.2179 val_loss=0.0000 scale=4.0000 norm=3.0070
[iter 400] loss=-0.0782 val_loss=0.0000 scale=4.0000 norm=2.9983
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9711 val_loss=0.0000 scale=2.0000 norm=1.5806
[iter 200] loss=0.5988 val_loss=0.0000 scale=4.0000 norm=3.1600
[iter 300] loss=0.0847 val_loss=0.0000 scale=4.0000 norm=3.1626
[iter 400] loss=-0.3739 val_loss=0.0000 scale=4.0000 norm=3.0792
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9955 val_loss=0.0000 scale=2.0000 norm=1.5774
[iter 200] loss=0.7413 val_loss=0.0000 scale=2.0000 norm=1.5751
[iter 300] loss=0.3730 val_loss=0.0000 scale=4.0000 norm=3.1087
[iter 400] loss=0.1414 val_loss=0.0000 scale=4.0000 norm=3.0786
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0547 val_loss=0.0000 scale=2.0000 norm=1.6629
[iter 200] loss=0.8361 val_loss=0.0000 scale=2.0000 norm=1.6539
[iter 300] loss=0.5801 val_loss=0.0000 scale=4.0000 norm=3.2644
[iter 400] loss=0.3990 val_loss=0.0000 scale=4.0000 norm=3.2266
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9089 val_loss=0.0000 scale=2.0000 norm=1.5567
[iter 200] loss=0.5640 val_loss=0.0000 scale=4.0000 norm=3.0527
[iter 300] loss=0.1369 val_loss=0.0000 scale=4.0000 norm=2.9488
[iter 400] loss=-0.1461 val_loss=0.0000 scale=4.0000 norm=2.8183
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0007 val_loss=0.0000 scale=2.0000 norm=1.5658
[iter 200] loss=0.7754 val_loss=0.0000 scale=2.0000 norm=1.5494
[iter 300] loss=0.5153 val_loss=0.0000 scale=4.0000 norm=3.0755
[iter 400] loss=0.2998 val_loss=0.0000 scale=4.0000 norm=3.0155
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9775 val_loss=0.0000 scale=2.0000 norm=1.6017
[iter 200] loss=0.6391 val_loss=0.0000 scale=4.0000 norm=3.1951
[iter 300] loss=0.2913 val_loss=0.0000 scale=4.0000 norm=3.1093
[iter 400] loss=0.1031 val_loss=0.0000 scale=4.0000 norm=3.0872
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7951 val_loss=0.0000 scale=2.0000 norm=1.5042
[iter 200] loss=0.5445 val_loss=0.0000 scale=2.0000 norm=1.5762
[iter 300] loss=0.1058 val_loss=0.0000 scale=4.0000 norm=3.1521
[iter 400] loss=-0.5208 val_loss=0.0000 scale=8.0000 norm=6.2143
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0639 val_loss=0.0000 scale=2.0000 norm=1.6541
[iter 200] loss=0.8309 val_loss=0.0000 scale=2.0000 norm=1.6502
[iter 300] loss=0.4674 val_loss=0.0000 scale=4.0000 norm=3.2513
[iter 400] loss=0.2442 val_loss=0.0000 scale=4.0000 norm=3.2056
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8306 val_loss=0.0000 scale=2.0000 norm=1.4455
[iter 200] loss=0.4209 val_loss=0.0000 scale=4.0000 norm=2.9168
[iter 300] loss=-0.2122 val_loss=0.0000 scale=4.0000 norm=2.8186
[iter 400] loss=-1.1489 val_loss=0.0000 scale=8.0000 norm=5.4818
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0406 val_loss=0.0000 scale=2.0000 norm=1.6193
[iter 200] loss=0.7886 val_loss=0.0000 scale=4.0000 norm=3.2179
[iter 300] loss=0.4268 val_loss=0.0000 scale=4.0000 norm=3.1658
[iter 400] loss=0.2200 val_loss=0.0000 scale=4.0000 norm=3.1083
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8321 val_loss=0.0000 scale=2.0000 norm=1.4281
[iter 200] loss=0.5631 val_loss=0.0000 scale=2.0000 norm=1.4450
[iter 300] loss=0.3127 val_loss=0.0000 scale=2.0000 norm=1.4381
[iter 400] loss=0.0489 val_loss=0.0000 scale=4.0000 norm=2.7699
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9333 val_loss=0.0000 scale=2.0000 norm=1.5411
[iter 200] loss=0.6554 val_loss=0.0000 scale=4.0000 norm=3.0260
[iter 300] loss=0.2843 val_loss=0.0000 scale=4.0000 norm=2.9919
[iter 400] loss=0.0979 val_loss=0.0000 scale=4.0000 norm=2.9646
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0515 val_loss=0.0000 scale=2.0000 norm=1.6309
[iter 200] loss=0.7592 val_loss=0.0000 scale=4.0000 norm=3.2654
[iter 300] loss=0.3724 val_loss=0.0000 scale=4.0000 norm=3.2405
[iter 400] loss=0.0796 val_loss=0.0000 scale=4.0000 norm=3.2169
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0594 val_loss=0.0000 scale=2.0000 norm=1.5974
[iter 200] loss=0.8585 val_loss=0.0000 scale=2.0000 norm=1.5807
[iter 300] loss=0.5565 val_loss=0.0000 scale=4.0000 norm=3.1844
[iter 400] loss=0.2257 val_loss=0.0000 scale=8.0000 norm=6.2884
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0093 val_loss=0.0000 scale=2.0000 norm=1.5956
[iter 200] loss=0.7687 val_loss=0.0000 scale=2.0000 norm=1.5778
[iter 300] loss=0.4284 val_loss=0.0000 scale=4.0000 norm=3.1146
[iter 400] loss=0.1951 val_loss=0.0000 scale=4.0000 norm=3.0726
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0134 val_loss=0.0000 scale=2.0000 norm=1.5870
[iter 200] loss=0.7581 val_loss=0.0000 scale=4.0000 norm=3.1379
[iter 300] loss=0.4407 val_loss=0.0000 scale=4.0000 norm=3.0982
[iter 400] loss=0.2748 val_loss=0.0000 scale=4.0000 norm=3.0768
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0044 val_loss=0.0000 scale=2.0000 norm=1.6013
[iter 200] loss=0.7360 val_loss=0.0000 scale=4.0000 norm=3.1921
[iter 300] loss=0.3434 val_loss=0.0000 scale=4.0000 norm=3.1560
[iter 400] loss=0.0845 val_loss=0.0000 scale=4.0000 norm=3.1492
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7719 val_loss=0.0000 scale=2.0000 norm=1.4675
[iter 200] loss=0.4769 val_loss=0.0000 scale=4.0000 norm=3.0491
[iter 300] loss=0.0603 val_loss=0.0000 scale=4.0000 norm=3.0264
[iter 400] loss=-0.3135 val_loss=0.0000 scale=8.0000 norm=5.9073
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0292 val_loss=0.0000 scale=2.0000 norm=1.6203
[iter 200] loss=0.7190 val_loss=0.0000 scale=4.0000 norm=3.2185
[iter 300] loss=0.3729 val_loss=0.0000 scale=4.0000 norm=3.1510
[iter 400] loss=0.1454 val_loss=0.0000 scale=4.0000 norm=3.1110
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0396 val_loss=0.0000 scale=2.0000 norm=1.6202
[iter 200] loss=0.7559 val_loss=0.0000 scale=4.0000 norm=3.2156
[iter 300] loss=0.3866 val_loss=0.0000 scale=4.0000 norm=3.1620
[iter 400] loss=0.1185 val_loss=0.0000 scale=4.0000 norm=3.1001
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0341 val_loss=0.0000 scale=2.0000 norm=1.6120
[iter 200] loss=0.8213 val_loss=0.0000 scale=4.0000 norm=3.1770
[iter 300] loss=0.5351 val_loss=0.0000 scale=4.0000 norm=3.1233
[iter 400] loss=0.3176 val_loss=0.0000 scale=4.0000 norm=3.0628
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0226 val_loss=0.0000 scale=2.0000 norm=1.6048
[iter 200] loss=0.7963 val_loss=0.0000 scale=4.0000 norm=3.2072
[iter 300] loss=0.4948 val_loss=0.0000 scale=4.0000 norm=3.1910
[iter 400] loss=0.3165 val_loss=0.0000 scale=4.0000 norm=3.1409
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9635 val_loss=0.0000 scale=2.0000 norm=1.5492
[iter 200] loss=0.6923 val_loss=0.0000 scale=2.0000 norm=1.5191
[iter 300] loss=0.3892 val_loss=0.0000 scale=4.0000 norm=3.0092
[iter 400] loss=0.1491 val_loss=0.0000 scale=4.0000 norm=3.0015
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9098 val_loss=0.0000 scale=2.0000 norm=1.5097
[iter 200] loss=0.6751 val_loss=0.0000 scale=2.0000 norm=1.5381
[iter 300] loss=0.3709 val_loss=0.0000 scale=4.0000 norm=3.0846
[iter 400] loss=0.0203 val_loss=0.0000 scale=4.0000 norm=3.0262
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0629 val_loss=0.0000 scale=2.0000 norm=1.6664
[iter 200] loss=0.8678 val_loss=0.0000 scale=2.0000 norm=1.6739
[iter 300] loss=0.6668 val_loss=0.0000 scale=2.0000 norm=1.6513
[iter 400] loss=0.5320 val_loss=0.0000 scale=4.0000 norm=3.2787
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0265 val_loss=0.0000 scale=2.0000 norm=1.6107
[iter 200] loss=0.7426 val_loss=0.0000 scale=4.0000 norm=3.1894
[iter 300] loss=0.4486 val_loss=0.0000 scale=2.0000 norm=1.5467
[iter 400] loss=0.3108 val_loss=0.0000 scale=4.0000 norm=3.0969
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6989 val_loss=0.0000 scale=2.0000 norm=1.3643
[iter 200] loss=0.4304 val_loss=0.0000 scale=4.0000 norm=2.9286
[iter 300] loss=0.0832 val_loss=0.0000 scale=4.0000 norm=2.9374
[iter 400] loss=-0.2429 val_loss=0.0000 scale=8.0000 norm=5.7206
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9894 val_loss=0.0000 scale=2.0000 norm=1.5697
[iter 200] loss=0.7327 val_loss=0.0000 scale=4.0000 norm=3.1186
[iter 300] loss=0.3821 val_loss=0.0000 scale=4.0000 norm=3.0721
[iter 400] loss=0.2008 val_loss=0.0000 scale=4.0000 norm=3.0297
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7510 val_loss=0.0000 scale=2.0000 norm=1.2707
[iter 200] loss=0.3824 val_loss=0.0000 scale=4.0000 norm=2.5426
[iter 300] loss=-0.2179 val_loss=0.0000 scale=4.0000 norm=2.5505
[iter 400] loss=-1.3699 val_loss=0.0000 scale=8.0000 norm=5.1014
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0533 val_loss=0.0000 scale=2.0000 norm=1.6204
[iter 200] loss=0.8281 val_loss=0.0000 scale=4.0000 norm=3.2419
[iter 300] loss=0.4994 val_loss=0.0000 scale=4.0000 norm=3.2191
[iter 400] loss=0.2948 val_loss=0.0000 scale=4.0000 norm=3.1708
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0583 val_loss=0.0000 scale=2.0000 norm=1.6400
[iter 200] loss=0.8528 val_loss=0.0000 scale=2.0000 norm=1.6482
[iter 300] loss=0.6576 val_loss=0.0000 scale=2.0000 norm=1.6324
[iter 400] loss=0.4513 val_loss=0.0000 scale=4.0000 norm=3.2370
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0076 val_loss=0.0000 scale=2.0000 norm=1.6069
[iter 200] loss=0.7241 val_loss=0.0000 scale=4.0000 norm=3.1988
[iter 300] loss=0.4037 val_loss=0.0000 scale=4.0000 norm=3.1729
[iter 400] loss=0.2232 val_loss=0.0000 scale=4.0000 norm=3.1266
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9571 val_loss=0.0000 scale=2.0000 norm=1.5659
[iter 200] loss=0.6183 val_loss=0.0000 scale=4.0000 norm=3.1316
[iter 300] loss=0.1792 val_loss=0.0000 scale=4.0000 norm=3.1141
[iter 400] loss=-0.1602 val_loss=0.0000 scale=4.0000 norm=3.0530
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0090 val_loss=0.0000 scale=2.0000 norm=1.6955
[iter 200] loss=0.6999 val_loss=0.0000 scale=4.0000 norm=3.3966
[iter 300] loss=0.3105 val_loss=0.0000 scale=8.0000 norm=6.6500
[iter 400] loss=-0.4275 val_loss=0.0000 scale=16.0000 norm=13.3003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=2.0000 norm=1.5878
[iter 200] loss=0.7450 val_loss=0.0000 scale=4.0000 norm=3.1639
[iter 300] loss=0.3510 val_loss=0.0000 scale=4.0000 norm=3.1111
[iter 400] loss=0.1033 val_loss=0.0000 scale=4.0000 norm=3.0739
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0018 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 200] loss=0.7075 val_loss=0.0000 scale=4.0000 norm=3.1199
[iter 300] loss=0.3443 val_loss=0.0000 scale=4.0000 norm=3.0108
[iter 400] loss=0.1021 val_loss=0.0000 scale=4.0000 norm=3.0012
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9519 val_loss=0.0000 scale=2.0000 norm=1.5681
[iter 200] loss=0.5830 val_loss=0.0000 scale=4.0000 norm=3.1375
[iter 300] loss=0.1058 val_loss=0.0000 scale=4.0000 norm=3.0000
[iter 400] loss=-0.1579 val_loss=0.0000 scale=4.0000 norm=3.0061
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0127 val_loss=0.0000 scale=2.0000 norm=1.5936
[iter 200] loss=0.7902 val_loss=0.0000 scale=4.0000 norm=3.2073
[iter 300] loss=0.4388 val_loss=0.0000 scale=4.0000 norm=3.1622
[iter 400] loss=0.2294 val_loss=0.0000 scale=4.0000 norm=3.1632
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0729 val_loss=0.0000 scale=2.0000 norm=1.6353
[iter 200] loss=0.8747 val_loss=0.0000 scale=4.0000 norm=3.2497
[iter 300] loss=0.6094 val_loss=0.0000 scale=4.0000 norm=3.1818
[iter 400] loss=0.4837 val_loss=0.0000 scale=4.0000 norm=3.2012
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0518 val_loss=0.0000 scale=2.0000 norm=1.6267
[iter 200] loss=0.8222 val_loss=0.0000 scale=2.0000 norm=1.6067
[iter 300] loss=0.5263 val_loss=0.0000 scale=4.0000 norm=3.1631
[iter 400] loss=0.2545 val_loss=0.0000 scale=2.0000 norm=1.5440
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0595 val_loss=0.0000 scale=2.0000 norm=1.6300
[iter 200] loss=0.8462 val_loss=0.0000 scale=4.0000 norm=3.2363
[iter 300] loss=0.5310 val_loss=0.0000 scale=4.0000 norm=3.1904
[iter 400] loss=0.3206 val_loss=0.0000 scale=4.0000 norm=3.1624
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0356 val_loss=0.0000 scale=2.0000 norm=1.6171
[iter 200] loss=0.7913 val_loss=0.0000 scale=4.0000 norm=3.2289
[iter 300] loss=0.5102 val_loss=0.0000 scale=4.0000 norm=3.1656
[iter 400] loss=0.3714 val_loss=0.0000 scale=4.0000 norm=3.1482
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0119 val_loss=0.0000 scale=2.0000 norm=1.6398
[iter 200] loss=0.7960 val_loss=0.0000 scale=4.0000 norm=3.2199
[iter 300] loss=0.5087 val_loss=0.0000 scale=8.0000 norm=6.5049
[iter 400] loss=-0.0513 val_loss=0.0000 scale=16.0000 norm=13.0103
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9873 val_loss=0.0000 scale=2.0000 norm=1.5686
[iter 200] loss=0.7298 val_loss=0.0000 scale=4.0000 norm=3.1327
[iter 300] loss=0.3731 val_loss=0.0000 scale=4.0000 norm=3.0903
[iter 400] loss=0.1978 val_loss=0.0000 scale=4.0000 norm=3.1183
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0520 val_loss=0.0000 scale=2.0000 norm=1.6724
[iter 200] loss=0.8256 val_loss=0.0000 scale=2.0000 norm=1.6639
[iter 300] loss=0.5764 val_loss=0.0000 scale=4.0000 norm=3.2613
[iter 400] loss=0.4288 val_loss=0.0000 scale=4.0000 norm=3.2162
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7887 val_loss=0.0000 scale=2.0000 norm=1.5473
[iter 200] loss=0.2744 val_loss=0.0000 scale=4.0000 norm=2.9881
[iter 300] loss=-0.2730 val_loss=0.0000 scale=4.0000 norm=2.8770
[iter 400] loss=-0.6787 val_loss=0.0000 scale=4.0000 norm=2.8591
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0294 val_loss=0.0000 scale=2.0000 norm=1.6020
[iter 200] loss=0.8105 val_loss=0.0000 scale=4.0000 norm=3.2078
[iter 300] loss=0.4674 val_loss=0.0000 scale=4.0000 norm=3.1862
[iter 400] loss=0.2701 val_loss=0.0000 scale=4.0000 norm=3.1737
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1076 val_loss=0.0000 scale=2.0000 norm=1.6494
[iter 200] loss=0.9537 val_loss=0.0000 scale=2.0000 norm=1.6397
[iter 300] loss=0.7306 val_loss=0.0000 scale=4.0000 norm=3.2287
[iter 400] loss=0.5872 val_loss=0.0000 scale=4.0000 norm=3.2110
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7939 val_loss=0.0000 scale=2.0000 norm=1.3563
[iter 200] loss=0.5712 val_loss=0.0000 scale=2.0000 norm=1.4207
[iter 300] loss=0.2679 val_loss=0.0000 scale=4.0000 norm=2.8672
[iter 400] loss=0.0094 val_loss=0.0000 scale=4.0000 norm=2.8511
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0164 val_loss=0.0000 scale=2.0000 norm=1.5848
[iter 200] loss=0.7826 val_loss=0.0000 scale=2.0000 norm=1.5671
[iter 300] loss=0.4543 val_loss=0.0000 scale=4.0000 norm=3.1308
[iter 400] loss=0.1908 val_loss=0.0000 scale=4.0000 norm=3.0677
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0345 val_loss=0.0000 scale=2.0000 norm=1.5822
[iter 200] loss=0.7711 val_loss=0.0000 scale=4.0000 norm=3.1451
[iter 300] loss=0.5010 val_loss=0.0000 scale=4.0000 norm=3.1032
[iter 400] loss=0.3299 val_loss=0.0000 scale=8.0000 norm=6.0575
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8192 val_loss=0.0000 scale=2.0000 norm=1.4176
[iter 200] loss=0.6090 val_loss=0.0000 scale=2.0000 norm=1.4852
[iter 300] loss=0.2993 val_loss=0.0000 scale=4.0000 norm=2.9793
[iter 400] loss=0.0202 val_loss=0.0000 scale=4.0000 norm=2.9274
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0001 val_loss=0.0000 scale=2.0000 norm=1.5749
[iter 200] loss=0.7815 val_loss=0.0000 scale=4.0000 norm=3.0798
[iter 300] loss=0.4738 val_loss=0.0000 scale=4.0000 norm=3.0433
[iter 400] loss=0.3406 val_loss=0.0000 scale=4.0000 norm=3.0037
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0121 val_loss=0.0000 scale=2.0000 norm=1.5832
[iter 200] loss=0.7552 val_loss=0.0000 scale=2.0000 norm=1.5745
[iter 300] loss=0.3947 val_loss=0.0000 scale=4.0000 norm=3.0885
[iter 400] loss=0.1984 val_loss=0.0000 scale=4.0000 norm=3.0928
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8201 val_loss=0.0000 scale=2.0000 norm=1.5541
[iter 200] loss=0.3276 val_loss=0.0000 scale=4.0000 norm=3.1047
[iter 300] loss=-0.2564 val_loss=0.0000 scale=4.0000 norm=3.0742
[iter 400] loss=-0.7060 val_loss=0.0000 scale=4.0000 norm=2.9957
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0303 val_loss=0.0000 scale=2.0000 norm=1.6049
[iter 200] loss=0.7994 val_loss=0.0000 scale=4.0000 norm=3.1823
[iter 300] loss=0.4824 val_loss=0.0000 scale=4.0000 norm=3.1322
[iter 400] loss=0.2899 val_loss=0.0000 scale=4.0000 norm=3.0808
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9082 val_loss=0.0000 scale=2.0000 norm=1.5211
[iter 200] loss=0.5697 val_loss=0.0000 scale=4.0000 norm=3.0385
[iter 300] loss=0.0503 val_loss=0.0000 scale=4.0000 norm=2.9909
[iter 400] loss=-0.2717 val_loss=0.0000 scale=4.0000 norm=2.8433
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0022 val_loss=0.0000 scale=2.0000 norm=1.5807
[iter 200] loss=0.7556 val_loss=0.0000 scale=4.0000 norm=3.1286
[iter 300] loss=0.4202 val_loss=0.0000 scale=4.0000 norm=3.0745
[iter 400] loss=0.2721 val_loss=0.0000 scale=4.0000 norm=3.1334
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0220 val_loss=0.0000 scale=2.0000 norm=1.6018
[iter 200] loss=0.7762 val_loss=0.0000 scale=4.0000 norm=3.1721
[iter 300] loss=0.3979 val_loss=0.0000 scale=4.0000 norm=3.1052
[iter 400] loss=0.1767 val_loss=0.0000 scale=4.0000 norm=3.1026
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9875 val_loss=0.0000 scale=2.0000 norm=1.5842
[iter 200] loss=0.6794 val_loss=0.0000 scale=4.0000 norm=3.1287
[iter 300] loss=0.2704 val_loss=0.0000 scale=4.0000 norm=3.0810
[iter 400] loss=0.0405 val_loss=0.0000 scale=4.0000 norm=3.0390
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9398 val_loss=0.0000 scale=2.0000 norm=1.6125
[iter 200] loss=0.5064 val_loss=0.0000 scale=4.0000 norm=3.2318
[iter 300] loss=-0.0078 val_loss=0.0000 scale=4.0000 norm=3.1417
[iter 400] loss=-0.4284 val_loss=0.0000 scale=4.0000 norm=3.0658
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9880 val_loss=0.0000 scale=2.0000 norm=1.5662
[iter 200] loss=0.7390 val_loss=0.0000 scale=2.0000 norm=1.5555
[iter 300] loss=0.4096 val_loss=0.0000 scale=4.0000 norm=3.1150
[iter 400] loss=0.1401 val_loss=0.0000 scale=4.0000 norm=3.0262
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8910 val_loss=0.0000 scale=2.0000 norm=1.4817
[iter 200] loss=0.6574 val_loss=0.0000 scale=2.0000 norm=1.5095
[iter 300] loss=0.3438 val_loss=0.0000 scale=2.0000 norm=1.4840
[iter 400] loss=0.1555 val_loss=0.0000 scale=4.0000 norm=2.9720
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9254 val_loss=0.0000 scale=2.0000 norm=1.5642
[iter 200] loss=0.6022 val_loss=0.0000 scale=4.0000 norm=3.0308
[iter 300] loss=0.2251 val_loss=0.0000 scale=4.0000 norm=2.9885
[iter 400] loss=-0.0993 val_loss=0.0000 scale=4.0000 norm=2.9444
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9630 val_loss=0.0000 scale=2.0000 norm=1.6808
[iter 200] loss=0.5682 val_loss=0.0000 scale=4.0000 norm=3.3239
[iter 300] loss=-0.0770 val_loss=0.0000 scale=8.0000 norm=6.6579
[iter 400] loss=-1.3470 val_loss=0.0000 scale=16.0000 norm=13.3158
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9949 val_loss=0.0000 scale=2.0000 norm=1.5934
[iter 200] loss=0.6728 val_loss=0.0000 scale=4.0000 norm=3.1549
[iter 300] loss=0.2347 val_loss=0.0000 scale=4.0000 norm=3.1169
[iter 400] loss=-0.0496 val_loss=0.0000 scale=4.0000 norm=3.0442
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0366 val_loss=0.0000 scale=2.0000 norm=1.5959
[iter 200] loss=0.8312 val_loss=0.0000 scale=2.0000 norm=1.5822
[iter 300] loss=0.5584 val_loss=0.0000 scale=4.0000 norm=3.1273
[iter 400] loss=0.3750 val_loss=0.0000 scale=4.0000 norm=3.0884
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0623 val_loss=0.0000 scale=2.0000 norm=1.6500
[iter 200] loss=0.8252 val_loss=0.0000 scale=4.0000 norm=3.2927
[iter 300] loss=0.4854 val_loss=0.0000 scale=4.0000 norm=3.2034
[iter 400] loss=0.2765 val_loss=0.0000 scale=4.0000 norm=3.1473
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9153 val_loss=0.0000 scale=2.0000 norm=1.5393
[iter 200] loss=0.6019 val_loss=0.0000 scale=4.0000 norm=3.0003
[iter 300] loss=0.2364 val_loss=0.0000 scale=4.0000 norm=2.9679
[iter 400] loss=-0.0282 val_loss=0.0000 scale=4.0000 norm=2.9602
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0147 val_loss=0.0000 scale=2.0000 norm=1.5976
[iter 200] loss=0.7681 val_loss=0.0000 scale=2.0000 norm=1.5913
[iter 300] loss=0.4196 val_loss=0.0000 scale=2.0000 norm=1.5675
[iter 400] loss=0.2206 val_loss=0.0000 scale=4.0000 norm=3.1500
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0496 val_loss=0.0000 scale=2.0000 norm=1.6290
[iter 200] loss=0.8187 val_loss=0.0000 scale=4.0000 norm=3.2468
[iter 300] loss=0.5096 val_loss=0.0000 scale=4.0000 norm=3.1879
[iter 400] loss=0.3665 val_loss=0.0000 scale=4.0000 norm=3.1711
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0353 val_loss=0.0000 scale=2.0000 norm=1.6083
[iter 200] loss=0.8055 val_loss=0.0000 scale=4.0000 norm=3.1886
[iter 300] loss=0.4889 val_loss=0.0000 scale=4.0000 norm=3.1425
[iter 400] loss=0.3295 val_loss=0.0000 scale=4.0000 norm=3.1096
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8651 val_loss=0.0000 scale=2.0000 norm=1.5890
[iter 200] loss=0.4074 val_loss=0.0000 scale=4.0000 norm=3.1138
[iter 300] loss=-0.1945 val_loss=0.0000 scale=8.0000 norm=6.2682
[iter 400] loss=-0.8224 val_loss=0.0000 scale=4.0000 norm=3.0963
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9377 val_loss=0.0000 scale=2.0000 norm=1.5883
[iter 200] loss=0.5457 val_loss=0.0000 scale=4.0000 norm=3.1488
[iter 300] loss=0.0720 val_loss=0.0000 scale=4.0000 norm=3.1123
[iter 400] loss=-0.2479 val_loss=0.0000 scale=4.0000 norm=3.0943
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0131 val_loss=0.0000 scale=2.0000 norm=1.5842
[iter 200] loss=0.7977 val_loss=0.0000 scale=2.0000 norm=1.5710
[iter 300] loss=0.5191 val_loss=0.0000 scale=4.0000 norm=3.1135
[iter 400] loss=0.3361 val_loss=0.0000 scale=4.0000 norm=3.1000

------------------------------------------------------------
Sender: LSF System <lsfadmin@c027n04>
Subject: Job 853005: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:38 2024
Job was executed on host(s) <4*c027n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:40 2024
                            <4*c031n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:44:40 2024
Terminated at Tue Oct 22 17:50:16 2024
Results reported at Tue Oct 22 17:50:16 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "Dimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3114.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.23 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   354 sec.
    Turnaround time :                            338 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.

Trimer
Filename: (Mordred)_NGB_hypOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(Mordred)_NGB_hypOFF_generalizability_scores.json
Done Saving scores!
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9069 val_loss=0.0000 scale=2.0000 norm=1.5220
[iter 200] loss=0.5542 val_loss=0.0000 scale=2.0000 norm=1.5182
[iter 300] loss=0.0412 val_loss=0.0000 scale=4.0000 norm=2.9870
[iter 400] loss=-0.2776 val_loss=0.0000 scale=4.0000 norm=2.8409
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0787 val_loss=0.0000 scale=2.0000 norm=1.6319
[iter 200] loss=0.8367 val_loss=0.0000 scale=4.0000 norm=3.2215
[iter 300] loss=0.5605 val_loss=0.0000 scale=4.0000 norm=3.1276
[iter 400] loss=0.1641 val_loss=0.0000 scale=8.0000 norm=6.3015
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0160 val_loss=0.0000 scale=2.0000 norm=1.5828
[iter 200] loss=0.7734 val_loss=0.0000 scale=4.0000 norm=3.1317
[iter 300] loss=0.4431 val_loss=0.0000 scale=4.0000 norm=3.1065
[iter 400] loss=0.2558 val_loss=0.0000 scale=4.0000 norm=3.1607
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0295 val_loss=0.0000 scale=2.0000 norm=1.6021
[iter 200] loss=0.7879 val_loss=0.0000 scale=4.0000 norm=3.2137
[iter 300] loss=0.4494 val_loss=0.0000 scale=4.0000 norm=3.1687
[iter 400] loss=0.2590 val_loss=0.0000 scale=4.0000 norm=3.1565
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1179 val_loss=0.0000 scale=2.0000 norm=1.6810
[iter 200] loss=0.9457 val_loss=0.0000 scale=4.0000 norm=3.3418
[iter 300] loss=0.7045 val_loss=0.0000 scale=4.0000 norm=3.3030
[iter 400] loss=0.5364 val_loss=0.0000 scale=4.0000 norm=3.2655
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9328 val_loss=0.0000 scale=2.0000 norm=1.6486
[iter 200] loss=0.5627 val_loss=0.0000 scale=4.0000 norm=3.3267
[iter 300] loss=0.0066 val_loss=0.0000 scale=4.0000 norm=3.3264
[iter 400] loss=-0.4939 val_loss=0.0000 scale=4.0000 norm=3.2326
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0442 val_loss=0.0000 scale=2.0000 norm=1.6685
[iter 200] loss=0.6849 val_loss=0.0000 scale=4.0000 norm=3.2651
[iter 300] loss=0.3239 val_loss=0.0000 scale=4.0000 norm=3.2090
[iter 400] loss=0.0513 val_loss=0.0000 scale=4.0000 norm=3.1166
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8039 val_loss=0.0000 scale=2.0000 norm=1.3565
[iter 200] loss=0.5789 val_loss=0.0000 scale=2.0000 norm=1.4199
[iter 300] loss=0.2941 val_loss=0.0000 scale=4.0000 norm=2.8656
[iter 400] loss=0.0311 val_loss=0.0000 scale=4.0000 norm=2.8576
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9876 val_loss=0.0000 scale=2.0000 norm=1.5600
[iter 200] loss=0.7588 val_loss=0.0000 scale=4.0000 norm=3.0866
[iter 300] loss=0.4206 val_loss=0.0000 scale=4.0000 norm=3.0803
[iter 400] loss=0.2443 val_loss=0.0000 scale=4.0000 norm=3.0580
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0351 val_loss=0.0000 scale=2.0000 norm=1.6085
[iter 200] loss=0.8181 val_loss=0.0000 scale=2.0000 norm=1.5941
[iter 300] loss=0.4990 val_loss=0.0000 scale=4.0000 norm=3.1466
[iter 400] loss=0.3309 val_loss=0.0000 scale=4.0000 norm=3.1074
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0345 val_loss=0.0000 scale=2.0000 norm=1.5822
[iter 200] loss=0.7711 val_loss=0.0000 scale=4.0000 norm=3.1451
[iter 300] loss=0.5010 val_loss=0.0000 scale=4.0000 norm=3.1032
[iter 400] loss=0.3299 val_loss=0.0000 scale=8.0000 norm=6.0575
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7867 val_loss=0.0000 scale=2.0000 norm=1.4121
[iter 200] loss=0.5566 val_loss=0.0000 scale=4.0000 norm=2.9951
[iter 300] loss=0.2292 val_loss=0.0000 scale=4.0000 norm=3.0370
[iter 400] loss=-0.3666 val_loss=0.0000 scale=8.0000 norm=6.0770
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9125 val_loss=0.0000 scale=2.0000 norm=1.5880
[iter 200] loss=0.5992 val_loss=0.0000 scale=4.0000 norm=3.0850
[iter 300] loss=0.1747 val_loss=0.0000 scale=4.0000 norm=3.0827
[iter 400] loss=-0.2369 val_loss=0.0000 scale=4.0000 norm=3.0676
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0287 val_loss=0.0000 scale=2.0000 norm=1.6083
[iter 200] loss=0.7987 val_loss=0.0000 scale=4.0000 norm=3.1559
[iter 300] loss=0.4550 val_loss=0.0000 scale=4.0000 norm=3.1012
[iter 400] loss=0.2557 val_loss=0.0000 scale=4.0000 norm=3.1049
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9709 val_loss=0.0000 scale=2.0000 norm=1.5808
[iter 200] loss=0.5981 val_loss=0.0000 scale=4.0000 norm=3.1601
[iter 300] loss=0.0840 val_loss=0.0000 scale=4.0000 norm=3.1627
[iter 400] loss=-0.3745 val_loss=0.0000 scale=4.0000 norm=3.0791
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0427 val_loss=0.0000 scale=2.0000 norm=1.6420
[iter 200] loss=0.8197 val_loss=0.0000 scale=2.0000 norm=1.6444
[iter 300] loss=0.5063 val_loss=0.0000 scale=4.0000 norm=3.2533
[iter 400] loss=0.3491 val_loss=0.0000 scale=2.0000 norm=1.6093
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0081 val_loss=0.0000 scale=2.0000 norm=1.6286
[iter 200] loss=0.6970 val_loss=0.0000 scale=4.0000 norm=3.2211
[iter 300] loss=0.2648 val_loss=0.0000 scale=4.0000 norm=3.1716
[iter 400] loss=-0.0819 val_loss=0.0000 scale=4.0000 norm=3.1149
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9800 val_loss=0.0000 scale=2.0000 norm=1.5729
[iter 200] loss=0.7353 val_loss=0.0000 scale=2.0000 norm=1.5412
[iter 300] loss=0.4358 val_loss=0.0000 scale=4.0000 norm=3.0354
[iter 400] loss=0.2304 val_loss=0.0000 scale=4.0000 norm=2.9582
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0228 val_loss=0.0000 scale=2.0000 norm=1.6188
[iter 200] loss=0.7374 val_loss=0.0000 scale=4.0000 norm=3.2162
[iter 300] loss=0.4251 val_loss=0.0000 scale=4.0000 norm=3.1328
[iter 400] loss=0.2731 val_loss=0.0000 scale=4.0000 norm=3.1200
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0510 val_loss=0.0000 scale=2.0000 norm=1.6023
[iter 200] loss=0.8590 val_loss=0.0000 scale=4.0000 norm=3.1858
[iter 300] loss=0.5942 val_loss=0.0000 scale=4.0000 norm=3.1594
[iter 400] loss=0.4657 val_loss=0.0000 scale=4.0000 norm=3.0806
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4375 val_loss=0.0000 scale=2.0000 norm=1.1901
[iter 200] loss=-0.1530 val_loss=0.0000 scale=4.0000 norm=2.4690
[iter 300] loss=-0.8569 val_loss=0.0000 scale=4.0000 norm=2.5415
[iter 400] loss=-2.2359 val_loss=0.0000 scale=8.0000 norm=5.0862
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0962 val_loss=0.0000 scale=2.0000 norm=1.6485
[iter 200] loss=0.9134 val_loss=0.0000 scale=4.0000 norm=3.2864
[iter 300] loss=0.6589 val_loss=0.0000 scale=4.0000 norm=3.2620
[iter 400] loss=0.4674 val_loss=0.0000 scale=4.0000 norm=3.2394
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5194 val_loss=0.0000 scale=2.0000 norm=1.1262
[iter 200] loss=0.0248 val_loss=0.0000 scale=2.0000 norm=1.1798
[iter 300] loss=-0.7873 val_loss=0.0000 scale=4.0000 norm=2.3730
[iter 400] loss=-1.4787 val_loss=0.0000 scale=4.0000 norm=2.1855
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0399 val_loss=0.0000 scale=2.0000 norm=1.6199
[iter 200] loss=0.7565 val_loss=0.0000 scale=4.0000 norm=3.2153
[iter 300] loss=0.3870 val_loss=0.0000 scale=4.0000 norm=3.1572
[iter 400] loss=0.1188 val_loss=0.0000 scale=4.0000 norm=3.0984
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0395 val_loss=0.0000 scale=2.0000 norm=1.6325
[iter 200] loss=0.7065 val_loss=0.0000 scale=4.0000 norm=3.2163
[iter 300] loss=0.3203 val_loss=0.0000 scale=4.0000 norm=3.1637
[iter 400] loss=0.0865 val_loss=0.0000 scale=4.0000 norm=3.1325
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8543 val_loss=0.0000 scale=2.0000 norm=1.6122
[iter 200] loss=0.4152 val_loss=0.0000 scale=4.0000 norm=3.2085
[iter 300] loss=-0.0602 val_loss=0.0000 scale=4.0000 norm=3.1635
[iter 400] loss=-0.3928 val_loss=0.0000 scale=4.0000 norm=3.0547
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0305 val_loss=0.0000 scale=2.0000 norm=1.6075
[iter 200] loss=0.8034 val_loss=0.0000 scale=4.0000 norm=3.1809
[iter 300] loss=0.4703 val_loss=0.0000 scale=4.0000 norm=3.1356
[iter 400] loss=0.2682 val_loss=0.0000 scale=4.0000 norm=3.1439
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0030 val_loss=0.0000 scale=2.0000 norm=1.5877
[iter 200] loss=0.7196 val_loss=0.0000 scale=4.0000 norm=3.1509
[iter 300] loss=0.3661 val_loss=0.0000 scale=4.0000 norm=3.1082
[iter 400] loss=0.1566 val_loss=0.0000 scale=4.0000 norm=3.0865
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6275 val_loss=0.0000 scale=2.0000 norm=1.1954
[iter 200] loss=0.2696 val_loss=0.0000 scale=2.0000 norm=1.2846
[iter 300] loss=-0.1875 val_loss=0.0000 scale=4.0000 norm=2.5836
[iter 400] loss=-0.5440 val_loss=0.0000 scale=8.0000 norm=4.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0163 val_loss=0.0000 scale=2.0000 norm=1.5862
[iter 200] loss=0.7861 val_loss=0.0000 scale=2.0000 norm=1.5692
[iter 300] loss=0.4635 val_loss=0.0000 scale=4.0000 norm=3.1070
[iter 400] loss=0.2679 val_loss=0.0000 scale=4.0000 norm=3.0488
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8987 val_loss=0.0000 scale=2.0000 norm=1.6413
[iter 200] loss=0.2516 val_loss=0.0000 scale=4.0000 norm=3.1660
[iter 300] loss=-0.3210 val_loss=0.0000 scale=8.0000 norm=5.9007
[iter 400] loss=-1.3860 val_loss=0.0000 scale=16.0000 norm=11.6727
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0483 val_loss=0.0000 scale=2.0000 norm=1.6540
[iter 200] loss=0.7367 val_loss=0.0000 scale=4.0000 norm=3.3121
[iter 300] loss=0.3029 val_loss=0.0000 scale=4.0000 norm=3.2728
[iter 400] loss=-0.0091 val_loss=0.0000 scale=4.0000 norm=3.2184
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9685 val_loss=0.0000 scale=2.0000 norm=1.5344
[iter 200] loss=0.7289 val_loss=0.0000 scale=2.0000 norm=1.5233
[iter 300] loss=0.3812 val_loss=0.0000 scale=4.0000 norm=3.0135
[iter 400] loss=0.1872 val_loss=0.0000 scale=4.0000 norm=2.9497
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9874 val_loss=0.0000 scale=2.0000 norm=1.5840
[iter 200] loss=0.6792 val_loss=0.0000 scale=4.0000 norm=3.1291
[iter 300] loss=0.2700 val_loss=0.0000 scale=4.0000 norm=3.0813
[iter 400] loss=0.0501 val_loss=0.0000 scale=4.0000 norm=3.0358
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9484 val_loss=0.0000 scale=2.0000 norm=1.5456
[iter 200] loss=0.6877 val_loss=0.0000 scale=2.0000 norm=1.5451
[iter 300] loss=0.2828 val_loss=0.0000 scale=4.0000 norm=3.0710
[iter 400] loss=-0.0446 val_loss=0.0000 scale=4.0000 norm=2.9700
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0113 val_loss=0.0000 scale=2.0000 norm=1.5969
[iter 200] loss=0.7465 val_loss=0.0000 scale=4.0000 norm=3.1713
[iter 300] loss=0.4036 val_loss=0.0000 scale=2.0000 norm=1.5655
[iter 400] loss=0.2549 val_loss=0.0000 scale=4.0000 norm=3.1692
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0265 val_loss=0.0000 scale=2.0000 norm=1.5981
[iter 200] loss=0.7607 val_loss=0.0000 scale=4.0000 norm=3.1829
[iter 300] loss=0.4113 val_loss=0.0000 scale=4.0000 norm=3.1324
[iter 400] loss=0.2199 val_loss=0.0000 scale=4.0000 norm=3.0791
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0119 val_loss=0.0000 scale=2.0000 norm=1.5917
[iter 200] loss=0.7824 val_loss=0.0000 scale=2.0000 norm=1.5815
[iter 300] loss=0.4643 val_loss=0.0000 scale=4.0000 norm=3.1325
[iter 400] loss=0.2925 val_loss=0.0000 scale=4.0000 norm=3.0922
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9612 val_loss=0.0000 scale=2.0000 norm=1.5483
[iter 200] loss=0.6904 val_loss=0.0000 scale=2.0000 norm=1.5211
[iter 300] loss=0.4140 val_loss=0.0000 scale=4.0000 norm=3.0230
[iter 400] loss=0.1831 val_loss=0.0000 scale=4.0000 norm=2.9734
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0490 val_loss=0.0000 scale=2.0000 norm=1.6409
[iter 200] loss=0.8041 val_loss=0.0000 scale=4.0000 norm=3.2681
[iter 300] loss=0.5102 val_loss=0.0000 scale=4.0000 norm=3.2066
[iter 400] loss=0.3714 val_loss=0.0000 scale=4.0000 norm=3.2248
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9887 val_loss=0.0000 scale=2.0000 norm=1.6445
[iter 200] loss=0.4603 val_loss=0.0000 scale=4.0000 norm=3.2644
[iter 300] loss=-0.2008 val_loss=0.0000 scale=4.0000 norm=3.2018
[iter 400] loss=-0.7631 val_loss=0.0000 scale=4.0000 norm=3.1993
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9172 val_loss=0.0000 scale=2.0000 norm=1.6222
[iter 200] loss=0.6034 val_loss=0.0000 scale=4.0000 norm=3.3543
[iter 300] loss=0.1534 val_loss=0.0000 scale=4.0000 norm=3.3575
[iter 400] loss=-0.2120 val_loss=0.0000 scale=4.0000 norm=3.2328
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0221 val_loss=0.0000 scale=2.0000 norm=1.6016
[iter 200] loss=0.7679 val_loss=0.0000 scale=4.0000 norm=3.1703
[iter 300] loss=0.3936 val_loss=0.0000 scale=4.0000 norm=3.1038
[iter 400] loss=0.1701 val_loss=0.0000 scale=4.0000 norm=3.0894
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0355 val_loss=0.0000 scale=2.0000 norm=1.6342
[iter 200] loss=0.7471 val_loss=0.0000 scale=4.0000 norm=3.2201
[iter 300] loss=0.3537 val_loss=0.0000 scale=4.0000 norm=3.1981
[iter 400] loss=0.0448 val_loss=0.0000 scale=4.0000 norm=3.1428
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0663 val_loss=0.0000 scale=2.0000 norm=1.6354
[iter 200] loss=0.8534 val_loss=0.0000 scale=2.0000 norm=1.6186
[iter 300] loss=0.5532 val_loss=0.0000 scale=4.0000 norm=3.1842
[iter 400] loss=0.3463 val_loss=0.0000 scale=2.0000 norm=1.5522
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0408 val_loss=0.0000 scale=2.0000 norm=1.6422
[iter 200] loss=0.8082 val_loss=0.0000 scale=4.0000 norm=3.1982
[iter 300] loss=0.5718 val_loss=0.0000 scale=4.0000 norm=3.1624
[iter 400] loss=0.3579 val_loss=0.0000 scale=8.0000 norm=6.2142
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8195 val_loss=0.0000 scale=2.0000 norm=1.4175
[iter 200] loss=0.6072 val_loss=0.0000 scale=2.0000 norm=1.4857
[iter 300] loss=0.2961 val_loss=0.0000 scale=4.0000 norm=2.9790
[iter 400] loss=0.0171 val_loss=0.0000 scale=4.0000 norm=2.9279
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0054 val_loss=0.0000 scale=2.0000 norm=1.5840
[iter 200] loss=0.7772 val_loss=0.0000 scale=2.0000 norm=1.5634
[iter 300] loss=0.4605 val_loss=0.0000 scale=4.0000 norm=3.1286
[iter 400] loss=0.2397 val_loss=0.0000 scale=4.0000 norm=3.0639
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0140 val_loss=0.0000 scale=2.0000 norm=1.6164
[iter 200] loss=0.6700 val_loss=0.0000 scale=4.0000 norm=3.2315
[iter 300] loss=0.1589 val_loss=0.0000 scale=4.0000 norm=3.2355
[iter 400] loss=-0.3339 val_loss=0.0000 scale=8.0000 norm=6.3885
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9851 val_loss=0.0000 scale=2.0000 norm=1.5622
[iter 200] loss=0.7388 val_loss=0.0000 scale=2.0000 norm=1.5515
[iter 300] loss=0.4207 val_loss=0.0000 scale=4.0000 norm=3.0534
[iter 400] loss=0.2293 val_loss=0.0000 scale=4.0000 norm=3.0011
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0165 val_loss=0.0000 scale=2.0000 norm=1.5804
[iter 200] loss=0.8019 val_loss=0.0000 scale=2.0000 norm=1.5672
[iter 300] loss=0.4871 val_loss=0.0000 scale=4.0000 norm=3.1140
[iter 400] loss=0.3355 val_loss=0.0000 scale=4.0000 norm=3.1304
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0109 val_loss=0.0000 scale=2.0000 norm=1.5805
[iter 200] loss=0.7827 val_loss=0.0000 scale=2.0000 norm=1.5633
[iter 300] loss=0.4504 val_loss=0.0000 scale=4.0000 norm=3.0896
[iter 400] loss=0.2837 val_loss=0.0000 scale=4.0000 norm=3.0678
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0423 val_loss=0.0000 scale=2.0000 norm=1.6189
[iter 200] loss=0.7683 val_loss=0.0000 scale=4.0000 norm=3.2006
[iter 300] loss=0.4750 val_loss=0.0000 scale=4.0000 norm=3.1097
[iter 400] loss=0.3414 val_loss=0.0000 scale=4.0000 norm=3.0904
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8419 val_loss=0.0000 scale=2.0000 norm=1.6270
[iter 200] loss=0.3599 val_loss=0.0000 scale=4.0000 norm=3.3314
[iter 300] loss=-0.4441 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-1.9801 val_loss=0.0000 scale=16.0000 norm=13.3436
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0367 val_loss=0.0000 scale=2.0000 norm=1.7340
[iter 200] loss=0.8005 val_loss=0.0000 scale=4.0000 norm=3.4552
[iter 300] loss=0.4278 val_loss=0.0000 scale=4.0000 norm=3.4190
[iter 400] loss=0.0938 val_loss=0.0000 scale=4.0000 norm=3.3665
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0496 val_loss=0.0000 scale=2.0000 norm=1.6595
[iter 200] loss=0.7348 val_loss=0.0000 scale=4.0000 norm=3.2502
[iter 300] loss=0.4596 val_loss=0.0000 scale=4.0000 norm=3.2314
[iter 400] loss=0.2549 val_loss=0.0000 scale=4.0000 norm=3.1715
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0472 val_loss=0.0000 scale=2.0000 norm=1.6125
[iter 200] loss=0.8205 val_loss=0.0000 scale=2.0000 norm=1.5893
[iter 300] loss=0.5140 val_loss=0.0000 scale=4.0000 norm=3.1160
[iter 400] loss=0.2562 val_loss=0.0000 scale=4.0000 norm=3.0806
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9877 val_loss=0.0000 scale=2.0000 norm=1.5665
[iter 200] loss=0.7385 val_loss=0.0000 scale=2.0000 norm=1.5550
[iter 300] loss=0.3776 val_loss=0.0000 scale=4.0000 norm=3.1068
[iter 400] loss=0.1356 val_loss=0.0000 scale=4.0000 norm=3.0464
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5527 val_loss=0.0000 scale=2.0000 norm=1.1038
[iter 200] loss=0.1625 val_loss=0.0000 scale=2.0000 norm=1.1143
[iter 300] loss=-0.3908 val_loss=0.0000 scale=4.0000 norm=2.2318
[iter 400] loss=-0.9001 val_loss=0.0000 scale=4.0000 norm=2.2127
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0581 val_loss=0.0000 scale=2.0000 norm=1.6412
[iter 200] loss=0.8524 val_loss=0.0000 scale=2.0000 norm=1.6473
[iter 300] loss=0.6341 val_loss=0.0000 scale=2.0000 norm=1.6271
[iter 400] loss=0.4652 val_loss=0.0000 scale=4.0000 norm=3.2345
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0147 val_loss=0.0000 scale=2.0000 norm=1.5803
[iter 200] loss=0.7907 val_loss=0.0000 scale=2.0000 norm=1.5633
[iter 300] loss=0.4866 val_loss=0.0000 scale=4.0000 norm=3.0867
[iter 400] loss=0.2705 val_loss=0.0000 scale=4.0000 norm=3.0317
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9333 val_loss=0.0000 scale=2.0000 norm=1.5413
[iter 200] loss=0.6512 val_loss=0.0000 scale=4.0000 norm=3.0255
[iter 300] loss=0.2817 val_loss=0.0000 scale=4.0000 norm=2.9909
[iter 400] loss=0.0986 val_loss=0.0000 scale=4.0000 norm=2.9655
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0645 val_loss=0.0000 scale=2.0000 norm=1.6313
[iter 200] loss=0.8533 val_loss=0.0000 scale=2.0000 norm=1.6052
[iter 300] loss=0.5460 val_loss=0.0000 scale=4.0000 norm=3.1629
[iter 400] loss=0.2987 val_loss=0.0000 scale=4.0000 norm=3.1565
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0147 val_loss=0.0000 scale=2.0000 norm=1.5859
[iter 200] loss=0.7860 val_loss=0.0000 scale=2.0000 norm=1.5808
[iter 300] loss=0.4833 val_loss=0.0000 scale=4.0000 norm=3.1230
[iter 400] loss=0.2880 val_loss=0.0000 scale=4.0000 norm=3.0972
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9975 val_loss=0.0000 scale=2.0000 norm=1.5760
[iter 200] loss=0.7552 val_loss=0.0000 scale=2.0000 norm=1.5750
[iter 300] loss=0.3907 val_loss=0.0000 scale=4.0000 norm=3.1352
[iter 400] loss=0.1586 val_loss=0.0000 scale=4.0000 norm=3.0920
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0629 val_loss=0.0000 scale=2.0000 norm=1.6663
[iter 200] loss=0.8678 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 300] loss=0.6572 val_loss=0.0000 scale=2.0000 norm=1.6501
[iter 400] loss=0.5332 val_loss=0.0000 scale=4.0000 norm=3.2868
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9098 val_loss=0.0000 scale=2.0000 norm=1.5557
[iter 200] loss=0.5619 val_loss=0.0000 scale=4.0000 norm=3.0559
[iter 300] loss=0.1365 val_loss=0.0000 scale=4.0000 norm=2.9459
[iter 400] loss=-0.1462 val_loss=0.0000 scale=4.0000 norm=2.8179
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0035 val_loss=0.0000 scale=2.0000 norm=1.5808
[iter 200] loss=0.7669 val_loss=0.0000 scale=4.0000 norm=3.1298
[iter 300] loss=0.4356 val_loss=0.0000 scale=4.0000 norm=3.0809
[iter 400] loss=0.2783 val_loss=0.0000 scale=4.0000 norm=3.1285
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0399 val_loss=0.0000 scale=2.0000 norm=1.5895
[iter 200] loss=0.8366 val_loss=0.0000 scale=2.0000 norm=1.5758
[iter 300] loss=0.5473 val_loss=0.0000 scale=4.0000 norm=3.1261
[iter 400] loss=0.3991 val_loss=0.0000 scale=4.0000 norm=3.1049
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0781 val_loss=0.0000 scale=2.0000 norm=1.6180
[iter 200] loss=0.9070 val_loss=0.0000 scale=2.0000 norm=1.6055
[iter 300] loss=0.6477 val_loss=0.0000 scale=4.0000 norm=3.1892
[iter 400] loss=0.4378 val_loss=0.0000 scale=4.0000 norm=3.1617
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6115 val_loss=0.0000 scale=2.0000 norm=1.1760
[iter 200] loss=0.2324 val_loss=0.0000 scale=2.0000 norm=1.1732
[iter 300] loss=-0.3036 val_loss=0.0000 scale=4.0000 norm=2.2153
[iter 400] loss=-0.7041 val_loss=0.0000 scale=4.0000 norm=2.0830
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0404 val_loss=0.0000 scale=2.0000 norm=1.6195
[iter 200] loss=0.7887 val_loss=0.0000 scale=4.0000 norm=3.2227
[iter 300] loss=0.4256 val_loss=0.0000 scale=4.0000 norm=3.1704
[iter 400] loss=0.2176 val_loss=0.0000 scale=4.0000 norm=3.1097
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0376 val_loss=0.0000 scale=2.0000 norm=1.6136
[iter 200] loss=0.7646 val_loss=0.0000 scale=4.0000 norm=3.1850
[iter 300] loss=0.3899 val_loss=0.0000 scale=4.0000 norm=3.1400
[iter 400] loss=0.1505 val_loss=0.0000 scale=4.0000 norm=3.1245
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0222 val_loss=0.0000 scale=2.0000 norm=1.6053
[iter 200] loss=0.7926 val_loss=0.0000 scale=4.0000 norm=3.2089
[iter 300] loss=0.4926 val_loss=0.0000 scale=4.0000 norm=3.1865
[iter 400] loss=0.3163 val_loss=0.0000 scale=4.0000 norm=3.1404
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0247 val_loss=0.0000 scale=2.0000 norm=1.5865
[iter 200] loss=0.8134 val_loss=0.0000 scale=2.0000 norm=1.5711
[iter 300] loss=0.5268 val_loss=0.0000 scale=2.0000 norm=1.5427
[iter 400] loss=0.4318 val_loss=0.0000 scale=4.0000 norm=3.0868
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9500 val_loss=0.0000 scale=2.0000 norm=1.6676
[iter 200] loss=0.4892 val_loss=0.0000 scale=4.0000 norm=3.3495
[iter 300] loss=-0.3106 val_loss=0.0000 scale=4.0000 norm=3.3524
[iter 400] loss=-1.1764 val_loss=0.0000 scale=4.0000 norm=3.3247
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0320 val_loss=0.0000 scale=2.0000 norm=1.6197
[iter 200] loss=0.8065 val_loss=0.0000 scale=2.0000 norm=1.6186
[iter 300] loss=0.4934 val_loss=0.0000 scale=4.0000 norm=3.2075
[iter 400] loss=0.3364 val_loss=0.0000 scale=2.0000 norm=1.5917
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0333 val_loss=0.0000 scale=2.0000 norm=1.7085
[iter 200] loss=0.5021 val_loss=0.0000 scale=4.0000 norm=3.3743
[iter 300] loss=-0.1130 val_loss=0.0000 scale=8.0000 norm=6.4116
[iter 400] loss=-1.2970 val_loss=0.0000 scale=16.0000 norm=12.8159
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8998 val_loss=0.0000 scale=2.0000 norm=1.5809
[iter 200] loss=0.6612 val_loss=0.0000 scale=2.0000 norm=1.6230
[iter 300] loss=0.3588 val_loss=0.0000 scale=4.0000 norm=3.1543
[iter 400] loss=0.0324 val_loss=0.0000 scale=8.0000 norm=6.3107
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0119 val_loss=0.0000 scale=2.0000 norm=1.5973
[iter 200] loss=0.7569 val_loss=0.0000 scale=4.0000 norm=3.1530
[iter 300] loss=0.4356 val_loss=0.0000 scale=4.0000 norm=3.1003
[iter 400] loss=0.2595 val_loss=0.0000 scale=4.0000 norm=3.1651
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.3175
[iter 200] loss=-0.2334 val_loss=0.0000 scale=2.0000 norm=1.2639
[iter 300] loss=-0.8684 val_loss=0.0000 scale=4.0000 norm=2.5376
[iter 400] loss=-1.7033 val_loss=0.0000 scale=8.0000 norm=4.9721
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9763 val_loss=0.0000 scale=2.0000 norm=1.5554
[iter 200] loss=0.7288 val_loss=0.0000 scale=2.0000 norm=1.5362
[iter 300] loss=0.3925 val_loss=0.0000 scale=4.0000 norm=3.0483
[iter 400] loss=0.1975 val_loss=0.0000 scale=8.0000 norm=6.0490
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7441 val_loss=0.0000 scale=2.0000 norm=1.2917
[iter 200] loss=0.4805 val_loss=0.0000 scale=2.0000 norm=1.3002
[iter 300] loss=0.1004 val_loss=0.0000 scale=4.0000 norm=2.5631
[iter 400] loss=-0.1686 val_loss=0.0000 scale=4.0000 norm=2.4147
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0543 val_loss=0.0000 scale=2.0000 norm=1.6199
[iter 200] loss=0.8406 val_loss=0.0000 scale=2.0000 norm=1.6205
[iter 300] loss=0.5103 val_loss=0.0000 scale=4.0000 norm=3.2225
[iter 400] loss=0.3004 val_loss=0.0000 scale=4.0000 norm=3.1779
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0346 val_loss=0.0000 scale=2.0000 norm=1.5999
[iter 200] loss=0.8168 val_loss=0.0000 scale=2.0000 norm=1.5914
[iter 300] loss=0.5183 val_loss=0.0000 scale=4.0000 norm=3.1254
[iter 400] loss=0.3519 val_loss=0.0000 scale=4.0000 norm=3.1350
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8644 val_loss=0.0000 scale=2.0000 norm=1.5893
[iter 200] loss=0.4033 val_loss=0.0000 scale=4.0000 norm=3.1137
[iter 300] loss=-0.2404 val_loss=0.0000 scale=8.0000 norm=6.2680
[iter 400] loss=-0.8468 val_loss=0.0000 scale=4.0000 norm=3.0899
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0411 val_loss=0.0000 scale=2.0000 norm=1.6152
[iter 200] loss=0.8222 val_loss=0.0000 scale=2.0000 norm=1.6066
[iter 300] loss=0.4853 val_loss=0.0000 scale=4.0000 norm=3.1840
[iter 400] loss=0.2575 val_loss=0.0000 scale=4.0000 norm=3.1960
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0104 val_loss=0.0000 scale=2.0000 norm=1.6208
[iter 200] loss=0.6929 val_loss=0.0000 scale=4.0000 norm=3.2038
[iter 300] loss=0.2807 val_loss=0.0000 scale=4.0000 norm=3.1723
[iter 400] loss=-0.0070 val_loss=0.0000 scale=4.0000 norm=3.1499
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9063 val_loss=0.0000 scale=2.0000 norm=1.5179
[iter 200] loss=0.5989 val_loss=0.0000 scale=4.0000 norm=3.0367
[iter 300] loss=0.1221 val_loss=0.0000 scale=4.0000 norm=3.0168
[iter 400] loss=-0.2001 val_loss=0.0000 scale=4.0000 norm=2.9401
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5968 val_loss=0.0000 scale=2.0000 norm=1.2872
[iter 200] loss=0.0884 val_loss=0.0000 scale=2.0000 norm=1.2449
[iter 300] loss=-0.3068 val_loss=0.0000 scale=4.0000 norm=2.6108
[iter 400] loss=-0.6288 val_loss=0.0000 scale=8.0000 norm=4.9224
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0072 val_loss=0.0000 scale=2.0000 norm=1.5877
[iter 200] loss=0.7660 val_loss=0.0000 scale=2.0000 norm=1.5610
[iter 300] loss=0.4204 val_loss=0.0000 scale=4.0000 norm=3.0752
[iter 400] loss=0.2221 val_loss=0.0000 scale=4.0000 norm=3.0051
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0287 val_loss=0.0000 scale=2.0000 norm=1.5975
[iter 200] loss=0.7970 val_loss=0.0000 scale=4.0000 norm=3.1559
[iter 300] loss=0.4704 val_loss=0.0000 scale=4.0000 norm=3.1015
[iter 400] loss=0.3302 val_loss=0.0000 scale=4.0000 norm=3.1543
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1126 val_loss=0.0000 scale=2.0000 norm=1.6666
[iter 200] loss=0.9602 val_loss=0.0000 scale=2.0000 norm=1.6679
[iter 300] loss=0.7654 val_loss=0.0000 scale=4.0000 norm=3.3267
[iter 400] loss=0.5456 val_loss=0.0000 scale=8.0000 norm=6.6673
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0022 val_loss=0.0000 scale=2.0000 norm=1.5942
[iter 200] loss=0.7358 val_loss=0.0000 scale=2.0000 norm=1.5815
[iter 300] loss=0.3563 val_loss=0.0000 scale=4.0000 norm=3.1356
[iter 400] loss=0.0817 val_loss=0.0000 scale=2.0000 norm=1.5268
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9602 val_loss=0.0000 scale=2.0000 norm=1.5532
[iter 200] loss=0.6749 val_loss=0.0000 scale=2.0000 norm=1.5307
[iter 300] loss=0.3413 val_loss=0.0000 scale=4.0000 norm=3.0015
[iter 400] loss=0.0282 val_loss=0.0000 scale=4.0000 norm=2.9192
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0020 val_loss=0.0000 scale=2.0000 norm=1.5804
[iter 200] loss=0.7576 val_loss=0.0000 scale=4.0000 norm=3.1394
[iter 300] loss=0.3970 val_loss=0.0000 scale=4.0000 norm=3.1303
[iter 400] loss=0.1709 val_loss=0.0000 scale=4.0000 norm=3.1694
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8907 val_loss=0.0000 scale=2.0000 norm=1.4820
[iter 200] loss=0.6575 val_loss=0.0000 scale=2.0000 norm=1.5096
[iter 300] loss=0.3386 val_loss=0.0000 scale=2.0000 norm=1.4839
[iter 400] loss=0.1570 val_loss=0.0000 scale=4.0000 norm=2.9713
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0075 val_loss=0.0000 scale=2.0000 norm=1.6073
[iter 200] loss=0.7371 val_loss=0.0000 scale=4.0000 norm=3.1974
[iter 300] loss=0.4132 val_loss=0.0000 scale=4.0000 norm=3.1735
[iter 400] loss=0.2281 val_loss=0.0000 scale=4.0000 norm=3.1245
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0119 val_loss=0.0000 scale=2.0000 norm=1.5832
[iter 200] loss=0.7727 val_loss=0.0000 scale=4.0000 norm=3.1481
[iter 300] loss=0.4067 val_loss=0.0000 scale=4.0000 norm=3.0842
[iter 400] loss=0.2078 val_loss=0.0000 scale=4.0000 norm=3.0892
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0471 val_loss=0.0000 scale=2.0000 norm=1.6254
[iter 200] loss=0.8326 val_loss=0.0000 scale=2.0000 norm=1.6171
[iter 300] loss=0.5071 val_loss=0.0000 scale=4.0000 norm=3.1912
[iter 400] loss=0.3417 val_loss=0.0000 scale=2.0000 norm=1.5575
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8204 val_loss=0.0000 scale=2.0000 norm=1.5542
[iter 200] loss=0.3333 val_loss=0.0000 scale=4.0000 norm=3.1041
[iter 300] loss=-0.2511 val_loss=0.0000 scale=4.0000 norm=3.0758
[iter 400] loss=-0.7020 val_loss=0.0000 scale=4.0000 norm=2.9945
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0140 val_loss=0.0000 scale=2.0000 norm=1.5747
[iter 200] loss=0.7926 val_loss=0.0000 scale=2.0000 norm=1.5606
[iter 300] loss=0.5351 val_loss=0.0000 scale=4.0000 norm=3.1087
[iter 400] loss=0.3315 val_loss=0.0000 scale=4.0000 norm=3.0882
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6047 val_loss=0.0000 scale=2.0000 norm=1.2590
[iter 200] loss=0.1055 val_loss=0.0000 scale=4.0000 norm=2.5741
[iter 300] loss=-0.6943 val_loss=0.0000 scale=4.0000 norm=2.5877
[iter 400] loss=-2.0303 val_loss=0.0000 scale=8.0000 norm=5.1760
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9800 val_loss=0.0000 scale=2.0000 norm=1.5519
[iter 200] loss=0.7293 val_loss=0.0000 scale=2.0000 norm=1.5364
[iter 300] loss=0.3721 val_loss=0.0000 scale=4.0000 norm=3.0237
[iter 400] loss=0.1533 val_loss=0.0000 scale=4.0000 norm=2.9826
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9695 val_loss=0.0000 scale=2.0000 norm=1.5788
[iter 200] loss=0.6740 val_loss=0.0000 scale=2.0000 norm=1.5608
[iter 300] loss=0.2321 val_loss=0.0000 scale=4.0000 norm=3.0779
[iter 400] loss=-0.0371 val_loss=0.0000 scale=4.0000 norm=2.9783
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0656 val_loss=0.0000 scale=2.0000 norm=1.6702
[iter 200] loss=0.7358 val_loss=0.0000 scale=4.0000 norm=3.3172
[iter 300] loss=0.3697 val_loss=0.0000 scale=4.0000 norm=3.2889
[iter 400] loss=0.0549 val_loss=0.0000 scale=4.0000 norm=3.2206
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7186 val_loss=0.0000 scale=2.0000 norm=1.2647
[iter 200] loss=0.4458 val_loss=0.0000 scale=2.0000 norm=1.2840
[iter 300] loss=0.1104 val_loss=0.0000 scale=4.0000 norm=2.5161
[iter 400] loss=-0.0617 val_loss=0.0000 scale=2.0000 norm=1.2158
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9398 val_loss=0.0000 scale=2.0000 norm=1.6125
[iter 200] loss=0.5064 val_loss=0.0000 scale=4.0000 norm=3.2318
[iter 300] loss=-0.0078 val_loss=0.0000 scale=4.0000 norm=3.1417
[iter 400] loss=-0.4284 val_loss=0.0000 scale=4.0000 norm=3.0658
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0596 val_loss=0.0000 scale=2.0000 norm=1.6301
[iter 200] loss=0.8461 val_loss=0.0000 scale=4.0000 norm=3.2371
[iter 300] loss=0.5299 val_loss=0.0000 scale=4.0000 norm=3.1910
[iter 400] loss=0.3208 val_loss=0.0000 scale=4.0000 norm=3.1701
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0020 val_loss=0.0000 scale=2.0000 norm=1.6057
[iter 200] loss=0.7607 val_loss=0.0000 scale=4.0000 norm=3.1760
[iter 300] loss=0.4590 val_loss=0.0000 scale=4.0000 norm=3.1731
[iter 400] loss=0.2341 val_loss=0.0000 scale=4.0000 norm=3.1026
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0514 val_loss=0.0000 scale=2.0000 norm=1.6310
[iter 200] loss=0.7531 val_loss=0.0000 scale=4.0000 norm=3.2655
[iter 300] loss=0.3673 val_loss=0.0000 scale=4.0000 norm=3.2404
[iter 400] loss=0.0762 val_loss=0.0000 scale=4.0000 norm=3.2155
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9949 val_loss=0.0000 scale=2.0000 norm=1.5932
[iter 200] loss=0.6869 val_loss=0.0000 scale=4.0000 norm=3.1541
[iter 300] loss=0.2466 val_loss=0.0000 scale=4.0000 norm=3.1206
[iter 400] loss=-0.0466 val_loss=0.0000 scale=4.0000 norm=3.0441
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0302 val_loss=0.0000 scale=2.0000 norm=1.6049
[iter 200] loss=0.8098 val_loss=0.0000 scale=2.0000 norm=1.5915
[iter 300] loss=0.4980 val_loss=0.0000 scale=4.0000 norm=3.1352
[iter 400] loss=0.2979 val_loss=0.0000 scale=4.0000 norm=3.0795
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0219 val_loss=0.0000 scale=2.0000 norm=1.5966
[iter 200] loss=0.8067 val_loss=0.0000 scale=4.0000 norm=3.1723
[iter 300] loss=0.4994 val_loss=0.0000 scale=4.0000 norm=3.1606
[iter 400] loss=0.3291 val_loss=0.0000 scale=4.0000 norm=3.1293
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1633 val_loss=0.0000 scale=2.0000 norm=1.8733
[iter 200] loss=0.8688 val_loss=0.0000 scale=4.0000 norm=3.7637
[iter 300] loss=0.3648 val_loss=0.0000 scale=8.0000 norm=7.5297
[iter 400] loss=-0.6192 val_loss=0.0000 scale=16.0000 norm=15.0595
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0169 val_loss=0.0000 scale=2.0000 norm=1.5963
[iter 200] loss=0.7837 val_loss=0.0000 scale=2.0000 norm=1.5837
[iter 300] loss=0.4709 val_loss=0.0000 scale=4.0000 norm=3.1362
[iter 400] loss=0.2787 val_loss=0.0000 scale=4.0000 norm=3.1699
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6989 val_loss=0.0000 scale=2.0000 norm=1.3643
[iter 200] loss=0.4283 val_loss=0.0000 scale=4.0000 norm=2.9296
[iter 300] loss=0.0818 val_loss=0.0000 scale=4.0000 norm=2.9371
[iter 400] loss=-0.2482 val_loss=0.0000 scale=8.0000 norm=5.7201
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0348 val_loss=0.0000 scale=2.0000 norm=1.6198
[iter 200] loss=0.7897 val_loss=0.0000 scale=2.0000 norm=1.6217
[iter 300] loss=0.4110 val_loss=0.0000 scale=4.0000 norm=3.2030
[iter 400] loss=0.1344 val_loss=0.0000 scale=4.0000 norm=3.1553
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4758 val_loss=0.0000 scale=2.0000 norm=1.0500
[iter 200] loss=0.0729 val_loss=0.0000 scale=2.0000 norm=1.1270
[iter 300] loss=-0.3956 val_loss=0.0000 scale=4.0000 norm=2.2556
[iter 400] loss=-1.1922 val_loss=0.0000 scale=8.0000 norm=4.5003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9904 val_loss=0.0000 scale=2.0000 norm=1.5893
[iter 200] loss=0.7306 val_loss=0.0000 scale=4.0000 norm=3.1615
[iter 300] loss=0.3973 val_loss=0.0000 scale=4.0000 norm=3.1445
[iter 400] loss=0.1658 val_loss=0.0000 scale=4.0000 norm=3.0941
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0344 val_loss=0.0000 scale=2.0000 norm=1.6112
[iter 200] loss=0.7873 val_loss=0.0000 scale=4.0000 norm=3.1846
[iter 300] loss=0.4718 val_loss=0.0000 scale=4.0000 norm=3.1160
[iter 400] loss=0.2662 val_loss=0.0000 scale=4.0000 norm=3.0802
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0508 val_loss=0.0000 scale=2.0000 norm=1.6246
[iter 200] loss=0.8164 val_loss=0.0000 scale=4.0000 norm=3.2500
[iter 300] loss=0.5288 val_loss=0.0000 scale=4.0000 norm=3.1820
[iter 400] loss=0.4138 val_loss=0.0000 scale=4.0000 norm=3.1889
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0013 val_loss=0.0000 scale=2.0000 norm=1.5749
[iter 200] loss=0.7590 val_loss=0.0000 scale=2.0000 norm=1.5680
[iter 300] loss=0.5050 val_loss=0.0000 scale=4.0000 norm=3.1329
[iter 400] loss=0.2501 val_loss=0.0000 scale=4.0000 norm=3.0956
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9992 val_loss=0.0000 scale=2.0000 norm=1.5758
[iter 200] loss=0.7597 val_loss=0.0000 scale=4.0000 norm=3.0809
[iter 300] loss=0.4580 val_loss=0.0000 scale=4.0000 norm=3.0349
[iter 400] loss=0.3328 val_loss=0.0000 scale=4.0000 norm=3.0067
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9571 val_loss=0.0000 scale=2.0000 norm=1.5659
[iter 200] loss=0.6207 val_loss=0.0000 scale=4.0000 norm=3.1308
[iter 300] loss=0.1814 val_loss=0.0000 scale=4.0000 norm=3.1144
[iter 400] loss=-0.1587 val_loss=0.0000 scale=4.0000 norm=3.0536
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0063 val_loss=0.0000 scale=2.0000 norm=1.5922
[iter 200] loss=0.7182 val_loss=0.0000 scale=4.0000 norm=3.1639
[iter 300] loss=0.3822 val_loss=0.0000 scale=2.0000 norm=1.5431
[iter 400] loss=0.2377 val_loss=0.0000 scale=4.0000 norm=3.1086
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1198 val_loss=0.0000 scale=2.0000 norm=1.7053
[iter 200] loss=0.8632 val_loss=0.0000 scale=4.0000 norm=3.3777
[iter 300] loss=0.5293 val_loss=0.0000 scale=4.0000 norm=3.3038
[iter 400] loss=0.1459 val_loss=0.0000 scale=8.0000 norm=6.4525
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7953 val_loss=0.0000 scale=2.0000 norm=1.6092
[iter 200] loss=0.2713 val_loss=0.0000 scale=4.0000 norm=3.1500
[iter 300] loss=-0.5495 val_loss=0.0000 scale=8.0000 norm=6.2905
[iter 400] loss=-2.1795 val_loss=0.0000 scale=16.0000 norm=12.5810
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0020 val_loss=0.0000 scale=2.0000 norm=1.5944
[iter 200] loss=0.7435 val_loss=0.0000 scale=4.0000 norm=3.1627
[iter 300] loss=0.3860 val_loss=0.0000 scale=4.0000 norm=3.1296
[iter 400] loss=0.1827 val_loss=0.0000 scale=4.0000 norm=3.1425
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6583 val_loss=0.0000 scale=2.0000 norm=1.2787
[iter 200] loss=0.4069 val_loss=0.0000 scale=2.0000 norm=1.3032
[iter 300] loss=0.1949 val_loss=0.0000 scale=4.0000 norm=2.6841
[iter 400] loss=-0.0507 val_loss=0.0000 scale=8.0000 norm=5.3463
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9866 val_loss=0.0000 scale=2.0000 norm=1.5665
[iter 200] loss=0.7539 val_loss=0.0000 scale=2.0000 norm=1.5531
[iter 300] loss=0.4159 val_loss=0.0000 scale=4.0000 norm=3.0828
[iter 400] loss=0.2157 val_loss=0.0000 scale=4.0000 norm=3.0427
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9996 val_loss=0.0000 scale=2.0000 norm=1.5862
[iter 200] loss=0.7471 val_loss=0.0000 scale=2.0000 norm=1.5583
[iter 300] loss=0.4428 val_loss=0.0000 scale=4.0000 norm=3.0746
[iter 400] loss=0.2071 val_loss=0.0000 scale=2.0000 norm=1.4897
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0440 val_loss=0.0000 scale=2.0000 norm=1.6088
[iter 200] loss=0.8372 val_loss=0.0000 scale=2.0000 norm=1.6041
[iter 300] loss=0.5435 val_loss=0.0000 scale=4.0000 norm=3.1751
[iter 400] loss=0.3561 val_loss=0.0000 scale=4.0000 norm=3.1253
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6622 val_loss=0.0000 scale=2.0000 norm=1.2794
[iter 200] loss=0.3124 val_loss=0.0000 scale=2.0000 norm=1.2811
[iter 300] loss=-0.1575 val_loss=0.0000 scale=4.0000 norm=2.5239
[iter 400] loss=-0.5534 val_loss=0.0000 scale=4.0000 norm=2.5152
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8970 val_loss=0.0000 scale=2.0000 norm=1.5607
[iter 200] loss=0.5414 val_loss=0.0000 scale=4.0000 norm=3.0580
[iter 300] loss=-0.0644 val_loss=0.0000 scale=4.0000 norm=3.0232
[iter 400] loss=-1.1084 val_loss=0.0000 scale=8.0000 norm=6.0466
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0351 val_loss=0.0000 scale=2.0000 norm=1.6175
[iter 200] loss=0.7870 val_loss=0.0000 scale=4.0000 norm=3.2266
[iter 300] loss=0.5075 val_loss=0.0000 scale=4.0000 norm=3.1622
[iter 400] loss=0.3712 val_loss=0.0000 scale=4.0000 norm=3.1507
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5971 val_loss=0.0000 scale=2.0000 norm=1.1759
[iter 200] loss=0.2306 val_loss=0.0000 scale=2.0000 norm=1.1446
[iter 300] loss=-0.1403 val_loss=0.0000 scale=4.0000 norm=2.3390
[iter 400] loss=-0.4260 val_loss=0.0000 scale=4.0000 norm=2.2978
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0090 val_loss=0.0000 scale=2.0000 norm=1.5961
[iter 200] loss=0.7731 val_loss=0.0000 scale=2.0000 norm=1.5783
[iter 300] loss=0.4510 val_loss=0.0000 scale=4.0000 norm=3.1177
[iter 400] loss=0.2134 val_loss=0.0000 scale=4.0000 norm=3.0804
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0995 val_loss=0.0000 scale=2.0000 norm=1.6765
[iter 200] loss=0.8747 val_loss=0.0000 scale=4.0000 norm=3.3079
[iter 300] loss=0.6007 val_loss=0.0000 scale=4.0000 norm=3.2701
[iter 400] loss=0.2942 val_loss=0.0000 scale=8.0000 norm=6.4092
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0430 val_loss=0.0000 scale=2.0000 norm=1.6067
[iter 200] loss=0.8195 val_loss=0.0000 scale=4.0000 norm=3.1964
[iter 300] loss=0.4953 val_loss=0.0000 scale=4.0000 norm=3.1292
[iter 400] loss=0.3199 val_loss=0.0000 scale=4.0000 norm=3.1019
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0262 val_loss=0.0000 scale=2.0000 norm=1.6107
[iter 200] loss=0.7346 val_loss=0.0000 scale=4.0000 norm=3.1883
[iter 300] loss=0.4415 val_loss=0.0000 scale=4.0000 norm=3.0935
[iter 400] loss=0.2986 val_loss=0.0000 scale=4.0000 norm=3.0969
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7951 val_loss=0.0000 scale=2.0000 norm=1.5042
[iter 200] loss=0.5445 val_loss=0.0000 scale=2.0000 norm=1.5762
[iter 300] loss=0.1058 val_loss=0.0000 scale=4.0000 norm=3.1521
[iter 400] loss=-0.5208 val_loss=0.0000 scale=8.0000 norm=6.2143
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0436 val_loss=0.0000 scale=2.0000 norm=1.6203
[iter 200] loss=0.7872 val_loss=0.0000 scale=4.0000 norm=3.2210
[iter 300] loss=0.4440 val_loss=0.0000 scale=4.0000 norm=3.1692
[iter 400] loss=0.2286 val_loss=0.0000 scale=4.0000 norm=3.1344
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0493 val_loss=0.0000 scale=2.0000 norm=1.6295
[iter 200] loss=0.8243 val_loss=0.0000 scale=4.0000 norm=3.2468
[iter 300] loss=0.5150 val_loss=0.0000 scale=4.0000 norm=3.1837
[iter 400] loss=0.3720 val_loss=0.0000 scale=4.0000 norm=3.1722
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9385 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 200] loss=0.7385 val_loss=0.0000 scale=4.0000 norm=3.1936
[iter 300] loss=0.4380 val_loss=0.0000 scale=4.0000 norm=3.2107
[iter 400] loss=-0.1290 val_loss=0.0000 scale=8.0000 norm=6.4220
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0325 val_loss=0.0000 scale=2.0000 norm=1.5960
[iter 200] loss=0.8155 val_loss=0.0000 scale=4.0000 norm=3.1546
[iter 300] loss=0.5307 val_loss=0.0000 scale=4.0000 norm=3.1158
[iter 400] loss=0.3815 val_loss=0.0000 scale=4.0000 norm=3.0969
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0557 val_loss=0.0000 scale=2.0000 norm=1.6348
[iter 200] loss=0.7412 val_loss=0.0000 scale=4.0000 norm=3.2622
[iter 300] loss=0.4676 val_loss=0.0000 scale=4.0000 norm=3.2170
[iter 400] loss=0.3786 val_loss=0.0000 scale=2.0000 norm=1.5820
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0301 val_loss=0.0000 scale=2.0000 norm=1.6081
[iter 200] loss=0.8159 val_loss=0.0000 scale=2.0000 norm=1.6077
[iter 300] loss=0.5384 val_loss=0.0000 scale=4.0000 norm=3.1724
[iter 400] loss=0.3182 val_loss=0.0000 scale=2.0000 norm=1.5583
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9512 val_loss=0.0000 scale=2.0000 norm=1.5553
[iter 200] loss=0.6748 val_loss=0.0000 scale=4.0000 norm=3.0436
[iter 300] loss=0.3276 val_loss=0.0000 scale=4.0000 norm=3.0026
[iter 400] loss=0.0735 val_loss=0.0000 scale=4.0000 norm=2.9497
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0196 val_loss=0.0000 scale=2.0000 norm=1.5872
[iter 200] loss=0.8022 val_loss=0.0000 scale=2.0000 norm=1.5645
[iter 300] loss=0.4876 val_loss=0.0000 scale=4.0000 norm=3.0741
[iter 400] loss=0.3438 val_loss=0.0000 scale=2.0000 norm=1.5262
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0107 val_loss=0.0000 scale=2.0000 norm=1.5852
[iter 200] loss=0.7767 val_loss=0.0000 scale=2.0000 norm=1.5692
[iter 300] loss=0.4414 val_loss=0.0000 scale=4.0000 norm=3.0885
[iter 400] loss=0.2805 val_loss=0.0000 scale=4.0000 norm=3.0633
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9880 val_loss=0.0000 scale=2.0000 norm=1.5687
[iter 200] loss=0.7367 val_loss=0.0000 scale=4.0000 norm=3.1344
[iter 300] loss=0.3792 val_loss=0.0000 scale=4.0000 norm=3.1115
[iter 400] loss=0.2075 val_loss=0.0000 scale=4.0000 norm=3.1273
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0404 val_loss=0.0000 scale=2.0000 norm=1.6826
[iter 200] loss=0.6758 val_loss=0.0000 scale=4.0000 norm=3.2902
[iter 300] loss=0.2114 val_loss=0.0000 scale=8.0000 norm=6.4237
[iter 400] loss=-0.6156 val_loss=0.0000 scale=16.0000 norm=12.8330
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0134 val_loss=0.0000 scale=2.0000 norm=1.5871
[iter 200] loss=0.7735 val_loss=0.0000 scale=4.0000 norm=3.1379
[iter 300] loss=0.4506 val_loss=0.0000 scale=4.0000 norm=3.1004
[iter 400] loss=0.2717 val_loss=0.0000 scale=4.0000 norm=3.0701
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9777 val_loss=0.0000 scale=2.0000 norm=1.6017
[iter 200] loss=0.6366 val_loss=0.0000 scale=4.0000 norm=3.1903
[iter 300] loss=0.2900 val_loss=0.0000 scale=4.0000 norm=3.1071
[iter 400] loss=0.1005 val_loss=0.0000 scale=4.0000 norm=3.0755
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9999 val_loss=0.0000 scale=2.0000 norm=1.5852
[iter 200] loss=0.7395 val_loss=0.0000 scale=4.0000 norm=3.1716
[iter 300] loss=0.4077 val_loss=0.0000 scale=4.0000 norm=3.1214
[iter 400] loss=0.2438 val_loss=0.0000 scale=4.0000 norm=3.1205
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7719 val_loss=0.0000 scale=2.0000 norm=1.4675
[iter 200] loss=0.4770 val_loss=0.0000 scale=4.0000 norm=3.0492
[iter 300] loss=0.0611 val_loss=0.0000 scale=4.0000 norm=3.0244
[iter 400] loss=-0.2858 val_loss=0.0000 scale=8.0000 norm=5.8729
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4186 val_loss=0.0000 scale=2.0000 norm=1.1960
[iter 200] loss=-0.1232 val_loss=0.0000 scale=4.0000 norm=2.2501
[iter 300] loss=-0.8306 val_loss=0.0000 scale=4.0000 norm=2.2490
[iter 400] loss=-2.0976 val_loss=0.0000 scale=8.0000 norm=4.4986
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7657 val_loss=0.0000 scale=2.0000 norm=1.3380
[iter 200] loss=0.5162 val_loss=0.0000 scale=2.0000 norm=1.3829
[iter 300] loss=0.2018 val_loss=0.0000 scale=4.0000 norm=2.7565
[iter 400] loss=-0.1014 val_loss=0.0000 scale=4.0000 norm=2.7082
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9339 val_loss=0.0000 scale=2.0000 norm=1.5535
[iter 200] loss=0.5569 val_loss=0.0000 scale=4.0000 norm=3.0701
[iter 300] loss=0.0200 val_loss=0.0000 scale=4.0000 norm=3.0076
[iter 400] loss=-0.4635 val_loss=0.0000 scale=4.0000 norm=2.9532
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0296 val_loss=0.0000 scale=2.0000 norm=1.6000
[iter 200] loss=0.7671 val_loss=0.0000 scale=4.0000 norm=3.1798
[iter 300] loss=0.4033 val_loss=0.0000 scale=4.0000 norm=3.1439
[iter 400] loss=0.1998 val_loss=0.0000 scale=4.0000 norm=3.1004
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8261 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 200] loss=0.3049 val_loss=0.0000 scale=4.0000 norm=3.2241
[iter 300] loss=-0.3025 val_loss=0.0000 scale=4.0000 norm=3.1621
[iter 400] loss=-0.8569 val_loss=0.0000 scale=4.0000 norm=3.0815
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0246 val_loss=0.0000 scale=2.0000 norm=1.5851
[iter 200] loss=0.8283 val_loss=0.0000 scale=2.0000 norm=1.5594
[iter 300] loss=0.5512 val_loss=0.0000 scale=4.0000 norm=3.0886
[iter 400] loss=0.4137 val_loss=0.0000 scale=4.0000 norm=3.0972
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0440 val_loss=0.0000 scale=2.0000 norm=1.6048
[iter 200] loss=0.7977 val_loss=0.0000 scale=4.0000 norm=3.2200
[iter 300] loss=0.4572 val_loss=0.0000 scale=4.0000 norm=3.1752
[iter 400] loss=0.2707 val_loss=0.0000 scale=4.0000 norm=3.1873
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0006 val_loss=0.0000 scale=2.0000 norm=1.5831
[iter 200] loss=0.7439 val_loss=0.0000 scale=4.0000 norm=3.1410
[iter 300] loss=0.3972 val_loss=0.0000 scale=4.0000 norm=3.0665
[iter 400] loss=0.2183 val_loss=0.0000 scale=4.0000 norm=3.0985
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9044 val_loss=0.0000 scale=2.0000 norm=1.5119
[iter 200] loss=0.6633 val_loss=0.0000 scale=2.0000 norm=1.5395
[iter 300] loss=0.2745 val_loss=0.0000 scale=4.0000 norm=3.0666
[iter 400] loss=-0.0520 val_loss=0.0000 scale=4.0000 norm=3.0170
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0520 val_loss=0.0000 scale=2.0000 norm=1.6723
[iter 200] loss=0.8255 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 300] loss=0.5751 val_loss=0.0000 scale=4.0000 norm=3.2611
[iter 400] loss=0.4231 val_loss=0.0000 scale=4.0000 norm=3.2069
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0028 val_loss=0.0000 scale=2.0000 norm=1.5656
[iter 200] loss=0.7815 val_loss=0.0000 scale=2.0000 norm=1.5489
[iter 300] loss=0.5098 val_loss=0.0000 scale=4.0000 norm=3.0720
[iter 400] loss=0.3069 val_loss=0.0000 scale=4.0000 norm=3.0240
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9593 val_loss=0.0000 scale=2.0000 norm=1.5252
[iter 200] loss=0.7123 val_loss=0.0000 scale=2.0000 norm=1.5035
[iter 300] loss=0.3671 val_loss=0.0000 scale=4.0000 norm=2.9688
[iter 400] loss=0.0866 val_loss=0.0000 scale=4.0000 norm=2.9556
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0436 val_loss=0.0000 scale=2.0000 norm=1.5923
[iter 200] loss=0.8400 val_loss=0.0000 scale=2.0000 norm=1.5782
[iter 300] loss=0.5735 val_loss=0.0000 scale=2.0000 norm=1.5664
[iter 400] loss=0.4051 val_loss=0.0000 scale=4.0000 norm=3.1281
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9757 val_loss=0.0000 scale=2.0000 norm=1.5658
[iter 200] loss=0.7013 val_loss=0.0000 scale=4.0000 norm=3.1107
[iter 300] loss=0.3600 val_loss=0.0000 scale=4.0000 norm=3.0831
[iter 400] loss=0.1712 val_loss=0.0000 scale=4.0000 norm=3.0685
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9600 val_loss=0.0000 scale=2.0000 norm=1.5517
[iter 200] loss=0.6854 val_loss=0.0000 scale=2.0000 norm=1.5299
[iter 300] loss=0.2948 val_loss=0.0000 scale=4.0000 norm=3.0110
[iter 400] loss=0.0291 val_loss=0.0000 scale=2.0000 norm=1.4690
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0562 val_loss=0.0000 scale=2.0000 norm=1.6244
[iter 200] loss=0.8381 val_loss=0.0000 scale=4.0000 norm=3.2314
[iter 300] loss=0.5057 val_loss=0.0000 scale=4.0000 norm=3.2100
[iter 400] loss=0.3075 val_loss=0.0000 scale=4.0000 norm=3.1637
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8313 val_loss=0.0000 scale=2.0000 norm=1.4278
[iter 200] loss=0.5614 val_loss=0.0000 scale=2.0000 norm=1.4454
[iter 300] loss=0.2550 val_loss=0.0000 scale=4.0000 norm=2.8549
[iter 400] loss=-0.0023 val_loss=0.0000 scale=4.0000 norm=2.7859
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9472 val_loss=0.0000 scale=2.0000 norm=1.6441
[iter 200] loss=0.5491 val_loss=0.0000 scale=4.0000 norm=3.2425
[iter 300] loss=0.0869 val_loss=0.0000 scale=4.0000 norm=3.1973
[iter 400] loss=-0.2494 val_loss=0.0000 scale=4.0000 norm=3.0734
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9409 val_loss=0.0000 scale=2.0000 norm=1.5946
[iter 200] loss=0.5938 val_loss=0.0000 scale=4.0000 norm=3.1182
[iter 300] loss=0.1453 val_loss=0.0000 scale=4.0000 norm=3.0942
[iter 400] loss=-0.1883 val_loss=0.0000 scale=4.0000 norm=3.0494
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9236 val_loss=0.0000 scale=2.0000 norm=1.6351
[iter 200] loss=0.5905 val_loss=0.0000 scale=4.0000 norm=3.3119
[iter 300] loss=0.0711 val_loss=0.0000 scale=4.0000 norm=3.2918
[iter 400] loss=-0.4538 val_loss=0.0000 scale=4.0000 norm=3.2782
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0090 val_loss=0.0000 scale=2.0000 norm=1.6955
[iter 200] loss=0.6999 val_loss=0.0000 scale=4.0000 norm=3.3966
[iter 300] loss=0.3105 val_loss=0.0000 scale=8.0000 norm=6.6500
[iter 400] loss=-0.4275 val_loss=0.0000 scale=16.0000 norm=13.3003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0135 val_loss=0.0000 scale=2.0000 norm=1.5844
[iter 200] loss=0.7981 val_loss=0.0000 scale=2.0000 norm=1.5719
[iter 300] loss=0.5367 val_loss=0.0000 scale=4.0000 norm=3.1214
[iter 400] loss=0.3445 val_loss=0.0000 scale=4.0000 norm=3.1000
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9797 val_loss=0.0000 scale=2.0000 norm=1.6253
[iter 200] loss=0.6088 val_loss=0.0000 scale=4.0000 norm=3.2044
[iter 300] loss=0.1146 val_loss=0.0000 scale=4.0000 norm=3.1539
[iter 400] loss=-0.2857 val_loss=0.0000 scale=4.0000 norm=3.0789
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0156 val_loss=0.0000 scale=2.0000 norm=1.5928
[iter 200] loss=0.7539 val_loss=0.0000 scale=4.0000 norm=3.1811
[iter 300] loss=0.4246 val_loss=0.0000 scale=4.0000 norm=3.1215
[iter 400] loss=0.2694 val_loss=0.0000 scale=2.0000 norm=1.5615
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9248 val_loss=0.0000 scale=2.0000 norm=1.5176
[iter 200] loss=0.6523 val_loss=0.0000 scale=4.0000 norm=2.9842
[iter 300] loss=0.2746 val_loss=0.0000 scale=4.0000 norm=2.9553
[iter 400] loss=0.0109 val_loss=0.0000 scale=4.0000 norm=2.9465
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0669 val_loss=0.0000 scale=2.0000 norm=1.6221
[iter 200] loss=0.8788 val_loss=0.0000 scale=2.0000 norm=1.6141
[iter 300] loss=0.6733 val_loss=0.0000 scale=4.0000 norm=3.1975
[iter 400] loss=0.4815 val_loss=0.0000 scale=4.0000 norm=3.1463
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1220 val_loss=0.0000 scale=2.0000 norm=1.6697
[iter 200] loss=0.9510 val_loss=0.0000 scale=4.0000 norm=3.3097
[iter 300] loss=0.7134 val_loss=0.0000 scale=4.0000 norm=3.2598
[iter 400] loss=0.5596 val_loss=0.0000 scale=8.0000 norm=6.4375
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0647 val_loss=0.0000 scale=2.0000 norm=1.6386
[iter 200] loss=0.8544 val_loss=0.0000 scale=2.0000 norm=1.6289
[iter 300] loss=0.5799 val_loss=0.0000 scale=4.0000 norm=3.2241
[iter 400] loss=0.2927 val_loss=0.0000 scale=4.0000 norm=3.1657
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8646 val_loss=0.0000 scale=2.0000 norm=1.5263
[iter 200] loss=0.5181 val_loss=0.0000 scale=4.0000 norm=3.0313
[iter 300] loss=0.0176 val_loss=0.0000 scale=4.0000 norm=2.9502
[iter 400] loss=-0.3629 val_loss=0.0000 scale=4.0000 norm=2.8940
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0092 val_loss=0.0000 scale=2.0000 norm=1.5897
[iter 200] loss=0.7312 val_loss=0.0000 scale=4.0000 norm=3.1362
[iter 300] loss=0.3615 val_loss=0.0000 scale=4.0000 norm=3.0757
[iter 400] loss=0.0938 val_loss=0.0000 scale=4.0000 norm=2.9751
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0215 val_loss=0.0000 scale=2.0000 norm=1.5998
[iter 200] loss=0.7892 val_loss=0.0000 scale=4.0000 norm=3.1570
[iter 300] loss=0.4430 val_loss=0.0000 scale=4.0000 norm=3.1278
[iter 400] loss=0.2681 val_loss=0.0000 scale=4.0000 norm=3.0929
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8716 val_loss=0.0000 scale=2.0000 norm=1.4655
[iter 200] loss=0.6560 val_loss=0.0000 scale=2.0000 norm=1.5144
[iter 300] loss=0.3460 val_loss=0.0000 scale=4.0000 norm=3.0279
[iter 400] loss=0.0598 val_loss=0.0000 scale=4.0000 norm=2.9951
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7350 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 200] loss=-0.1758 val_loss=0.0000 scale=4.0000 norm=3.0687
[iter 300] loss=-1.3658 val_loss=0.0000 scale=8.0000 norm=5.8055
[iter 400] loss=-3.6058 val_loss=0.0000 scale=16.0000 norm=11.6070
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9630 val_loss=0.0000 scale=2.0000 norm=1.6808
[iter 200] loss=0.5682 val_loss=0.0000 scale=4.0000 norm=3.3239
[iter 300] loss=-0.0770 val_loss=0.0000 scale=8.0000 norm=6.6579
[iter 400] loss=-1.3370 val_loss=0.0000 scale=16.0000 norm=13.3158
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9375 val_loss=0.0000 scale=2.0000 norm=1.5883
[iter 200] loss=0.5453 val_loss=0.0000 scale=4.0000 norm=3.1488
[iter 300] loss=0.0716 val_loss=0.0000 scale=4.0000 norm=3.1123
[iter 400] loss=-0.2491 val_loss=0.0000 scale=4.0000 norm=3.0837
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0317 val_loss=0.0000 scale=2.0000 norm=1.6446
[iter 200] loss=0.7648 val_loss=0.0000 scale=4.0000 norm=3.3176
[iter 300] loss=0.2648 val_loss=0.0000 scale=4.0000 norm=3.3251
[iter 400] loss=-0.6502 val_loss=0.0000 scale=8.0000 norm=6.6505
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0153 val_loss=0.0000 scale=2.0000 norm=1.6005
[iter 200] loss=0.7790 val_loss=0.0000 scale=2.0000 norm=1.5868
[iter 300] loss=0.4447 val_loss=0.0000 scale=4.0000 norm=3.1232
[iter 400] loss=0.2416 val_loss=0.0000 scale=4.0000 norm=3.0630
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9896 val_loss=0.0000 scale=2.0000 norm=1.5636
[iter 200] loss=0.7590 val_loss=0.0000 scale=2.0000 norm=1.5482
[iter 300] loss=0.4273 val_loss=0.0000 scale=4.0000 norm=3.0662
[iter 400] loss=0.2484 val_loss=0.0000 scale=4.0000 norm=3.0377
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9607 val_loss=0.0000 scale=2.0000 norm=1.5382
[iter 200] loss=0.7046 val_loss=0.0000 scale=2.0000 norm=1.5216
[iter 300] loss=0.3196 val_loss=0.0000 scale=4.0000 norm=2.9999
[iter 400] loss=0.0378 val_loss=0.0000 scale=4.0000 norm=2.9565
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0427 val_loss=0.0000 scale=2.0000 norm=1.5928
[iter 200] loss=0.8457 val_loss=0.0000 scale=2.0000 norm=1.5874
[iter 300] loss=0.5694 val_loss=0.0000 scale=4.0000 norm=3.1372
[iter 400] loss=0.4444 val_loss=0.0000 scale=2.0000 norm=1.5244
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0148 val_loss=0.0000 scale=2.0000 norm=1.5948
[iter 200] loss=0.7127 val_loss=0.0000 scale=4.0000 norm=3.1432
[iter 300] loss=0.2932 val_loss=0.0000 scale=4.0000 norm=3.1496
[iter 400] loss=-0.5272 val_loss=0.0000 scale=8.0000 norm=6.2995
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1077 val_loss=0.0000 scale=2.0000 norm=1.6491
[iter 200] loss=0.9512 val_loss=0.0000 scale=4.0000 norm=3.2792
[iter 300] loss=0.7291 val_loss=0.0000 scale=4.0000 norm=3.2239
[iter 400] loss=0.5862 val_loss=0.0000 scale=4.0000 norm=3.2014
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0044 val_loss=0.0000 scale=2.0000 norm=1.6028
[iter 200] loss=0.6947 val_loss=0.0000 scale=4.0000 norm=3.1862
[iter 300] loss=0.3477 val_loss=0.0000 scale=4.0000 norm=3.1293
[iter 400] loss=0.1049 val_loss=0.0000 scale=4.0000 norm=3.0864
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8306 val_loss=0.0000 scale=2.0000 norm=1.4455
[iter 200] loss=0.4209 val_loss=0.0000 scale=4.0000 norm=2.9167
[iter 300] loss=-0.2119 val_loss=0.0000 scale=4.0000 norm=2.8182
[iter 400] loss=-1.1486 val_loss=0.0000 scale=8.0000 norm=5.4818
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9838 val_loss=0.0000 scale=2.0000 norm=1.5747
[iter 200] loss=0.7434 val_loss=0.0000 scale=4.0000 norm=3.1384
[iter 300] loss=0.4019 val_loss=0.0000 scale=4.0000 norm=3.1356
[iter 400] loss=0.1892 val_loss=0.0000 scale=4.0000 norm=3.1015
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4129 val_loss=0.0000 scale=2.0000 norm=0.9614
[iter 200] loss=-0.0937 val_loss=0.0000 scale=2.0000 norm=0.9073
[iter 300] loss=-0.6744 val_loss=0.0000 scale=4.0000 norm=1.7300
[iter 400] loss=-1.4982 val_loss=0.0000 scale=8.0000 norm=3.3759
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9919 val_loss=0.0000 scale=2.0000 norm=1.5674
[iter 200] loss=0.6892 val_loss=0.0000 scale=4.0000 norm=3.1175
[iter 300] loss=0.3292 val_loss=0.0000 scale=4.0000 norm=3.0819
[iter 400] loss=0.1116 val_loss=0.0000 scale=4.0000 norm=3.0191
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6962 val_loss=0.0000 scale=2.0000 norm=1.5054
[iter 200] loss=0.1625 val_loss=0.0000 scale=4.0000 norm=2.8205
[iter 300] loss=-0.4325 val_loss=0.0000 scale=4.0000 norm=2.7351
[iter 400] loss=-0.8402 val_loss=0.0000 scale=4.0000 norm=2.7310
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0336 val_loss=0.0000 scale=2.0000 norm=1.6098
[iter 200] loss=0.7988 val_loss=0.0000 scale=4.0000 norm=3.2026
[iter 300] loss=0.4525 val_loss=0.0000 scale=4.0000 norm=3.1796
[iter 400] loss=0.2414 val_loss=0.0000 scale=4.0000 norm=3.1578
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0119 val_loss=0.0000 scale=2.0000 norm=1.6398
[iter 200] loss=0.7960 val_loss=0.0000 scale=4.0000 norm=3.2199
[iter 300] loss=0.5087 val_loss=0.0000 scale=8.0000 norm=6.5049
[iter 400] loss=-0.0513 val_loss=0.0000 scale=16.0000 norm=13.0103
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0603 val_loss=0.0000 scale=2.0000 norm=1.6357
[iter 200] loss=0.8486 val_loss=0.0000 scale=2.0000 norm=1.6271
[iter 300] loss=0.5302 val_loss=0.0000 scale=4.0000 norm=3.2201
[iter 400] loss=0.3811 val_loss=0.0000 scale=2.0000 norm=1.6021
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0443 val_loss=0.0000 scale=2.0000 norm=1.6193
[iter 200] loss=0.8352 val_loss=0.0000 scale=2.0000 norm=1.6213
[iter 300] loss=0.5385 val_loss=0.0000 scale=4.0000 norm=3.2463
[iter 400] loss=0.3595 val_loss=0.0000 scale=4.0000 norm=3.1659
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9686 val_loss=0.0000 scale=2.0000 norm=1.5727
[iter 200] loss=0.7127 val_loss=0.0000 scale=2.0000 norm=1.5524
[iter 300] loss=0.3522 val_loss=0.0000 scale=4.0000 norm=3.0506
[iter 400] loss=0.0846 val_loss=0.0000 scale=4.0000 norm=2.9477
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7887 val_loss=0.0000 scale=2.0000 norm=1.5475
[iter 200] loss=0.2645 val_loss=0.0000 scale=4.0000 norm=2.9857
[iter 300] loss=-0.2799 val_loss=0.0000 scale=4.0000 norm=2.8752
[iter 400] loss=-0.6842 val_loss=0.0000 scale=4.0000 norm=2.8578
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0034 val_loss=0.0000 scale=2.0000 norm=1.5783
[iter 200] loss=0.7571 val_loss=0.0000 scale=2.0000 norm=1.5684
[iter 300] loss=0.4016 val_loss=0.0000 scale=4.0000 norm=3.0819
[iter 400] loss=0.2066 val_loss=0.0000 scale=4.0000 norm=3.0435
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0155 val_loss=0.0000 scale=2.0000 norm=1.5976
[iter 200] loss=0.7692 val_loss=0.0000 scale=2.0000 norm=1.5904
[iter 300] loss=0.4457 val_loss=0.0000 scale=4.0000 norm=3.1445
[iter 400] loss=0.2415 val_loss=0.0000 scale=4.0000 norm=3.1503
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9888 val_loss=0.0000 scale=2.0000 norm=1.5698
[iter 200] loss=0.7192 val_loss=0.0000 scale=4.0000 norm=3.1188
[iter 300] loss=0.3729 val_loss=0.0000 scale=4.0000 norm=3.0712
[iter 400] loss=0.2111 val_loss=0.0000 scale=4.0000 norm=3.0315
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7136 val_loss=0.0000 scale=2.0000 norm=1.3012
[iter 200] loss=0.3389 val_loss=0.0000 scale=4.0000 norm=2.6946
[iter 300] loss=-0.1862 val_loss=0.0000 scale=4.0000 norm=2.6593
[iter 400] loss=-0.6485 val_loss=0.0000 scale=4.0000 norm=2.6756
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9898 val_loss=0.0000 scale=2.0000 norm=1.6079
[iter 200] loss=0.6126 val_loss=0.0000 scale=4.0000 norm=3.2128
[iter 300] loss=0.1246 val_loss=0.0000 scale=4.0000 norm=3.1666
[iter 400] loss=-0.2533 val_loss=0.0000 scale=4.0000 norm=3.0912
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0790 val_loss=0.0000 scale=2.0000 norm=1.6574
[iter 200] loss=0.8047 val_loss=0.0000 scale=4.0000 norm=3.2805
[iter 300] loss=0.4722 val_loss=0.0000 scale=4.0000 norm=3.1950
[iter 400] loss=0.2682 val_loss=0.0000 scale=8.0000 norm=6.3098
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0234 val_loss=0.0000 scale=2.0000 norm=1.6104
[iter 200] loss=0.8056 val_loss=0.0000 scale=2.0000 norm=1.6091
[iter 300] loss=0.4917 val_loss=0.0000 scale=4.0000 norm=3.1493
[iter 400] loss=0.3167 val_loss=0.0000 scale=4.0000 norm=3.1007
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9868 val_loss=0.0000 scale=2.0000 norm=1.5813
[iter 200] loss=0.7403 val_loss=0.0000 scale=4.0000 norm=3.0952
[iter 300] loss=0.4229 val_loss=0.0000 scale=4.0000 norm=3.0497
[iter 400] loss=0.2823 val_loss=0.0000 scale=4.0000 norm=3.0573
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4416 val_loss=0.0000 scale=2.0000 norm=0.9961
[iter 200] loss=-0.0068 val_loss=0.0000 scale=2.0000 norm=1.0106
[iter 300] loss=-0.6006 val_loss=0.0000 scale=4.0000 norm=2.0618
[iter 400] loss=-1.6326 val_loss=0.0000 scale=8.0000 norm=4.1286
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8738 val_loss=0.0000 scale=2.0000 norm=1.5366
[iter 200] loss=0.4892 val_loss=0.0000 scale=4.0000 norm=3.0386
[iter 300] loss=-0.0086 val_loss=0.0000 scale=4.0000 norm=3.0190
[iter 400] loss=-0.3537 val_loss=0.0000 scale=4.0000 norm=2.9565
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5087 val_loss=0.0000 scale=2.0000 norm=1.0902
[iter 200] loss=0.0120 val_loss=0.0000 scale=2.0000 norm=1.1384
[iter 300] loss=-0.6681 val_loss=0.0000 scale=4.0000 norm=2.2551
[iter 400] loss=-1.7494 val_loss=0.0000 scale=8.0000 norm=4.5378
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0026 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 200] loss=0.7148 val_loss=0.0000 scale=4.0000 norm=3.1214
[iter 300] loss=0.3494 val_loss=0.0000 scale=4.0000 norm=3.0136
[iter 400] loss=0.1069 val_loss=0.0000 scale=4.0000 norm=3.0017
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8184 val_loss=0.0000 scale=2.0000 norm=1.5822
[iter 200] loss=0.1601 val_loss=0.0000 scale=4.0000 norm=3.1798
[iter 300] loss=-0.5547 val_loss=0.0000 scale=4.0000 norm=3.0734
[iter 400] loss=-1.0506 val_loss=0.0000 scale=4.0000 norm=2.8467
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0046 val_loss=0.0000 scale=2.0000 norm=1.6008
[iter 200] loss=0.7483 val_loss=0.0000 scale=4.0000 norm=3.1919
[iter 300] loss=0.3521 val_loss=0.0000 scale=4.0000 norm=3.1562
[iter 400] loss=0.0962 val_loss=0.0000 scale=4.0000 norm=3.1727
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9881 val_loss=0.0000 scale=2.0000 norm=1.5685
[iter 200] loss=0.7431 val_loss=0.0000 scale=2.0000 norm=1.5528
[iter 300] loss=0.3973 val_loss=0.0000 scale=4.0000 norm=3.0803
[iter 400] loss=0.2101 val_loss=0.0000 scale=4.0000 norm=3.0660
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9911 val_loss=0.0000 scale=2.0000 norm=1.5687
[iter 200] loss=0.7467 val_loss=0.0000 scale=2.0000 norm=1.5566
[iter 300] loss=0.4452 val_loss=0.0000 scale=4.0000 norm=3.0866
[iter 400] loss=0.2150 val_loss=0.0000 scale=4.0000 norm=3.0830
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6310 val_loss=0.0000 scale=2.0000 norm=1.3721
[iter 200] loss=0.2143 val_loss=0.0000 scale=4.0000 norm=2.6093
[iter 300] loss=-0.2981 val_loss=0.0000 scale=4.0000 norm=2.6618
[iter 400] loss=-0.7634 val_loss=0.0000 scale=8.0000 norm=4.9863
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9943 val_loss=0.0000 scale=2.0000 norm=1.5651
[iter 200] loss=0.7164 val_loss=0.0000 scale=4.0000 norm=3.1144
[iter 300] loss=0.3984 val_loss=0.0000 scale=4.0000 norm=3.0615
[iter 400] loss=0.2035 val_loss=0.0000 scale=4.0000 norm=3.0101
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0066 val_loss=0.0000 scale=2.0000 norm=1.5944
[iter 200] loss=0.7499 val_loss=0.0000 scale=4.0000 norm=3.1431
[iter 300] loss=0.4454 val_loss=0.0000 scale=4.0000 norm=3.1099
[iter 400] loss=0.2220 val_loss=0.0000 scale=4.0000 norm=3.0617
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9635 val_loss=0.0000 scale=2.0000 norm=1.5492
[iter 200] loss=0.6919 val_loss=0.0000 scale=2.0000 norm=1.5194
[iter 300] loss=0.3926 val_loss=0.0000 scale=4.0000 norm=3.0107
[iter 400] loss=0.1495 val_loss=0.0000 scale=4.0000 norm=2.9810
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0364 val_loss=0.0000 scale=2.0000 norm=1.5959
[iter 200] loss=0.8314 val_loss=0.0000 scale=2.0000 norm=1.5831
[iter 300] loss=0.5677 val_loss=0.0000 scale=4.0000 norm=3.1292
[iter 400] loss=0.3806 val_loss=0.0000 scale=4.0000 norm=3.0921
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9991 val_loss=0.0000 scale=2.0000 norm=1.5973
[iter 200] loss=0.7351 val_loss=0.0000 scale=2.0000 norm=1.5899
[iter 300] loss=0.4062 val_loss=0.0000 scale=4.0000 norm=3.1664
[iter 400] loss=0.1615 val_loss=0.0000 scale=4.0000 norm=3.0488
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0197 val_loss=0.0000 scale=2.0000 norm=1.5926
[iter 200] loss=0.7204 val_loss=0.0000 scale=4.0000 norm=3.1556
[iter 300] loss=0.3592 val_loss=0.0000 scale=4.0000 norm=3.0459
[iter 400] loss=0.1483 val_loss=0.0000 scale=4.0000 norm=3.0034
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8630 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 200] loss=0.3981 val_loss=0.0000 scale=4.0000 norm=3.4013
[iter 300] loss=-0.1294 val_loss=0.0000 scale=4.0000 norm=3.3008
[iter 400] loss=-0.9508 val_loss=0.0000 scale=16.0000 norm=12.9102
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9643 val_loss=0.0000 scale=2.0000 norm=1.5391
[iter 200] loss=0.7025 val_loss=0.0000 scale=2.0000 norm=1.5229
[iter 300] loss=0.3349 val_loss=0.0000 scale=4.0000 norm=2.9918
[iter 400] loss=0.0880 val_loss=0.0000 scale=2.0000 norm=1.4778
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9635 val_loss=0.0000 scale=2.0000 norm=1.5746
[iter 200] loss=0.6582 val_loss=0.0000 scale=2.0000 norm=1.5636
[iter 300] loss=0.1701 val_loss=0.0000 scale=4.0000 norm=3.0970
[iter 400] loss=-0.1100 val_loss=0.0000 scale=4.0000 norm=3.0398
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0292 val_loss=0.0000 scale=2.0000 norm=1.6202
[iter 200] loss=0.7188 val_loss=0.0000 scale=4.0000 norm=3.2185
[iter 300] loss=0.3726 val_loss=0.0000 scale=4.0000 norm=3.1510
[iter 400] loss=0.1456 val_loss=0.0000 scale=4.0000 norm=3.1110
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9407 val_loss=0.0000 scale=2.0000 norm=1.5683
[iter 200] loss=0.6587 val_loss=0.0000 scale=4.0000 norm=3.1317
[iter 300] loss=0.2906 val_loss=0.0000 scale=4.0000 norm=3.1255
[iter 400] loss=0.0057 val_loss=0.0000 scale=4.0000 norm=3.0479
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0062 val_loss=0.0000 scale=2.0000 norm=1.5757
[iter 200] loss=0.7639 val_loss=0.0000 scale=2.0000 norm=1.5626
[iter 300] loss=0.4332 val_loss=0.0000 scale=4.0000 norm=3.1053
[iter 400] loss=0.1830 val_loss=0.0000 scale=4.0000 norm=3.0116
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0475 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 200] loss=0.8308 val_loss=0.0000 scale=4.0000 norm=3.4606
[iter 300] loss=0.4707 val_loss=0.0000 scale=8.0000 norm=6.9339
[iter 400] loss=-0.2373 val_loss=0.0000 scale=16.0000 norm=13.8680
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9452 val_loss=0.0000 scale=2.0000 norm=1.5669
[iter 200] loss=0.5604 val_loss=0.0000 scale=4.0000 norm=3.0335
[iter 300] loss=0.1939 val_loss=0.0000 scale=4.0000 norm=2.9502
[iter 400] loss=-0.0193 val_loss=0.0000 scale=2.0000 norm=1.4487
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9881 val_loss=0.0000 scale=2.0000 norm=1.5638
[iter 200] loss=0.7578 val_loss=0.0000 scale=2.0000 norm=1.5440
[iter 300] loss=0.4299 val_loss=0.0000 scale=4.0000 norm=3.0609
[iter 400] loss=0.2316 val_loss=0.0000 scale=4.0000 norm=3.0476
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9818 val_loss=0.0000 scale=2.0000 norm=1.5686
[iter 200] loss=0.6915 val_loss=0.0000 scale=4.0000 norm=3.0674
[iter 300] loss=0.3548 val_loss=0.0000 scale=4.0000 norm=3.0073
[iter 400] loss=0.1062 val_loss=0.0000 scale=4.0000 norm=2.9512
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0429 val_loss=0.0000 scale=2.0000 norm=1.6686
[iter 200] loss=0.6958 val_loss=0.0000 scale=4.0000 norm=3.3336
[iter 300] loss=0.1358 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-0.9542 val_loss=0.0000 scale=16.0000 norm=13.3438
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0084 val_loss=0.0000 scale=2.0000 norm=1.5751
[iter 200] loss=0.7431 val_loss=0.0000 scale=4.0000 norm=3.1241
[iter 300] loss=0.3842 val_loss=0.0000 scale=4.0000 norm=3.0516
[iter 400] loss=0.1905 val_loss=0.0000 scale=4.0000 norm=3.0577
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0571 val_loss=0.0000 scale=2.0000 norm=1.6140
[iter 200] loss=0.8577 val_loss=0.0000 scale=2.0000 norm=1.6005
[iter 300] loss=0.5563 val_loss=0.0000 scale=4.0000 norm=3.1460
[iter 400] loss=0.3610 val_loss=0.0000 scale=4.0000 norm=3.0920
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9562 val_loss=0.0000 scale=2.0000 norm=1.5474
[iter 200] loss=0.7212 val_loss=0.0000 scale=2.0000 norm=1.5683
[iter 300] loss=0.3107 val_loss=0.0000 scale=4.0000 norm=3.1567
[iter 400] loss=-0.1150 val_loss=0.0000 scale=4.0000 norm=3.1133
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0371 val_loss=0.0000 scale=2.0000 norm=1.6304
[iter 200] loss=0.8069 val_loss=0.0000 scale=2.0000 norm=1.6263
[iter 300] loss=0.4956 val_loss=0.0000 scale=2.0000 norm=1.5967
[iter 400] loss=0.3418 val_loss=0.0000 scale=4.0000 norm=3.2127
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9518 val_loss=0.0000 scale=2.0000 norm=1.5679
[iter 200] loss=0.6051 val_loss=0.0000 scale=4.0000 norm=3.1372
[iter 300] loss=0.1197 val_loss=0.0000 scale=4.0000 norm=3.0089
[iter 400] loss=-0.1473 val_loss=0.0000 scale=4.0000 norm=3.0145
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.5678 val_loss=0.0000 scale=2.0000 norm=1.3048
[iter 200] loss=-0.0662 val_loss=0.0000 scale=4.0000 norm=2.7345
[iter 300] loss=-0.8568 val_loss=0.0000 scale=4.0000 norm=2.7335
[iter 400] loss=-1.7868 val_loss=0.0000 scale=8.0000 norm=5.1171
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0299 val_loss=0.0000 scale=2.0000 norm=1.6450
[iter 200] loss=0.7506 val_loss=0.0000 scale=2.0000 norm=1.6469
[iter 300] loss=0.2716 val_loss=0.0000 scale=4.0000 norm=3.2505
[iter 400] loss=-0.0457 val_loss=0.0000 scale=4.0000 norm=3.1910
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8748 val_loss=0.0000 scale=2.0000 norm=1.6157
[iter 200] loss=0.4005 val_loss=0.0000 scale=4.0000 norm=3.2493
[iter 300] loss=-0.3971 val_loss=0.0000 scale=4.0000 norm=3.2484
[iter 400] loss=-1.1784 val_loss=0.0000 scale=8.0000 norm=6.2435
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9648 val_loss=0.0000 scale=2.0000 norm=1.5458
[iter 200] loss=0.7037 val_loss=0.0000 scale=2.0000 norm=1.5310
[iter 300] loss=0.3412 val_loss=0.0000 scale=4.0000 norm=3.0195
[iter 400] loss=0.1038 val_loss=0.0000 scale=4.0000 norm=2.9904
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0029 val_loss=0.0000 scale=2.0000 norm=1.6188
[iter 200] loss=0.7294 val_loss=0.0000 scale=4.0000 norm=3.1324
[iter 300] loss=0.4588 val_loss=0.0000 scale=4.0000 norm=3.0396
[iter 400] loss=0.2488 val_loss=0.0000 scale=8.0000 norm=5.9111
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9606 val_loss=0.0000 scale=2.0000 norm=1.5382
[iter 200] loss=0.7498 val_loss=0.0000 scale=4.0000 norm=3.1874
[iter 300] loss=0.4137 val_loss=0.0000 scale=8.0000 norm=6.4023
[iter 400] loss=-0.2463 val_loss=0.0000 scale=16.0000 norm=12.8051
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9672 val_loss=0.0000 scale=2.0000 norm=1.5726
[iter 200] loss=0.6631 val_loss=0.0000 scale=4.0000 norm=3.0610
[iter 300] loss=0.2858 val_loss=0.0000 scale=4.0000 norm=2.9964
[iter 400] loss=-0.0298 val_loss=0.0000 scale=4.0000 norm=2.9341
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0363 val_loss=0.0000 scale=2.0000 norm=1.5870
[iter 200] loss=0.8101 val_loss=0.0000 scale=4.0000 norm=3.1787
[iter 300] loss=0.4433 val_loss=0.0000 scale=4.0000 norm=3.1265
[iter 400] loss=0.2302 val_loss=0.0000 scale=4.0000 norm=3.1496
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0814 val_loss=0.0000 scale=2.0000 norm=1.6175
[iter 200] loss=0.9035 val_loss=0.0000 scale=2.0000 norm=1.5992
[iter 300] loss=0.6420 val_loss=0.0000 scale=4.0000 norm=3.1615
[iter 400] loss=0.4431 val_loss=0.0000 scale=4.0000 norm=3.0758
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8939 val_loss=0.0000 scale=2.0000 norm=1.5226
[iter 200] loss=0.6387 val_loss=0.0000 scale=4.0000 norm=3.1436
[iter 300] loss=0.2196 val_loss=0.0000 scale=4.0000 norm=3.1685
[iter 400] loss=-0.5339 val_loss=0.0000 scale=8.0000 norm=6.3384
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0079 val_loss=0.0000 scale=2.0000 norm=1.5986
[iter 200] loss=0.7626 val_loss=0.0000 scale=2.0000 norm=1.5928
[iter 300] loss=0.4180 val_loss=0.0000 scale=4.0000 norm=3.1593
[iter 400] loss=0.1698 val_loss=0.0000 scale=4.0000 norm=3.0929
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0146 val_loss=0.0000 scale=2.0000 norm=1.5980
[iter 200] loss=0.7677 val_loss=0.0000 scale=2.0000 norm=1.5918
[iter 300] loss=0.4258 val_loss=0.0000 scale=2.0000 norm=1.5499
[iter 400] loss=0.2716 val_loss=0.0000 scale=2.0000 norm=1.5608
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9881 val_loss=0.0000 scale=2.0000 norm=1.6060
[iter 200] loss=0.6781 val_loss=0.0000 scale=4.0000 norm=3.1851
[iter 300] loss=0.2925 val_loss=0.0000 scale=4.0000 norm=3.1779
[iter 400] loss=-0.0810 val_loss=0.0000 scale=4.0000 norm=3.0764
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0129 val_loss=0.0000 scale=2.0000 norm=1.5934
[iter 200] loss=0.7905 val_loss=0.0000 scale=2.0000 norm=1.6034
[iter 300] loss=0.4516 val_loss=0.0000 scale=4.0000 norm=3.1577
[iter 400] loss=0.2352 val_loss=0.0000 scale=4.0000 norm=3.1460
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0713 val_loss=0.0000 scale=2.0000 norm=1.6243
[iter 200] loss=0.8817 val_loss=0.0000 scale=2.0000 norm=1.6132
[iter 300] loss=0.5903 val_loss=0.0000 scale=4.0000 norm=3.1849
[iter 400] loss=0.3808 val_loss=0.0000 scale=4.0000 norm=3.1077
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0733 val_loss=0.0000 scale=2.0000 norm=1.6352
[iter 200] loss=0.8817 val_loss=0.0000 scale=4.0000 norm=3.2495
[iter 300] loss=0.6136 val_loss=0.0000 scale=4.0000 norm=3.1833
[iter 400] loss=0.4887 val_loss=0.0000 scale=4.0000 norm=3.1989
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8179 val_loss=0.0000 scale=2.0000 norm=1.4216
[iter 200] loss=0.5877 val_loss=0.0000 scale=2.0000 norm=1.4644
[iter 300] loss=0.2833 val_loss=0.0000 scale=4.0000 norm=2.9095
[iter 400] loss=0.0321 val_loss=0.0000 scale=4.0000 norm=2.8335
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9913 val_loss=0.0000 scale=2.0000 norm=1.5700
[iter 200] loss=0.7459 val_loss=0.0000 scale=2.0000 norm=1.5523
[iter 300] loss=0.3997 val_loss=0.0000 scale=4.0000 norm=3.1133
[iter 400] loss=0.1252 val_loss=0.0000 scale=4.0000 norm=3.0725
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8634 val_loss=0.0000 scale=2.0000 norm=1.4612
[iter 200] loss=0.6210 val_loss=0.0000 scale=2.0000 norm=1.4858
[iter 300] loss=0.3527 val_loss=0.0000 scale=4.0000 norm=2.9469
[iter 400] loss=0.1306 val_loss=0.0000 scale=4.0000 norm=2.9095
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9571 val_loss=0.0000 scale=2.0000 norm=1.6007
[iter 200] loss=0.6880 val_loss=0.0000 scale=4.0000 norm=3.1542
[iter 300] loss=0.3663 val_loss=0.0000 scale=4.0000 norm=3.1551
[iter 400] loss=0.1037 val_loss=0.0000 scale=4.0000 norm=3.1052
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0295 val_loss=0.0000 scale=2.0000 norm=1.5983
[iter 200] loss=0.7791 val_loss=0.0000 scale=4.0000 norm=3.1977
[iter 300] loss=0.4237 val_loss=0.0000 scale=4.0000 norm=3.1526
[iter 400] loss=0.1982 val_loss=0.0000 scale=4.0000 norm=3.1469
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0580 val_loss=0.0000 scale=2.0000 norm=1.5986
[iter 200] loss=0.8568 val_loss=0.0000 scale=2.0000 norm=1.5795
[iter 300] loss=0.5480 val_loss=0.0000 scale=4.0000 norm=3.1842
[iter 400] loss=0.2115 val_loss=0.0000 scale=8.0000 norm=6.2893
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0319 val_loss=0.0000 scale=2.0000 norm=1.6319
[iter 200] loss=0.7996 val_loss=0.0000 scale=2.0000 norm=1.6326
[iter 300] loss=0.4679 val_loss=0.0000 scale=4.0000 norm=3.2433
[iter 400] loss=0.1553 val_loss=0.0000 scale=4.0000 norm=3.2074
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0624 val_loss=0.0000 scale=2.0000 norm=1.6497
[iter 200] loss=0.8116 val_loss=0.0000 scale=4.0000 norm=3.2921
[iter 300] loss=0.4771 val_loss=0.0000 scale=4.0000 norm=3.1975
[iter 400] loss=0.2713 val_loss=0.0000 scale=4.0000 norm=3.1443
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9100 val_loss=0.0000 scale=2.0000 norm=1.5314
[iter 200] loss=0.5544 val_loss=0.0000 scale=4.0000 norm=2.9584
[iter 300] loss=0.0981 val_loss=0.0000 scale=4.0000 norm=2.7946
[iter 400] loss=-0.2276 val_loss=0.0000 scale=4.0000 norm=2.6850
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9956 val_loss=0.0000 scale=2.0000 norm=1.5923
[iter 200] loss=0.6913 val_loss=0.0000 scale=4.0000 norm=3.1678
[iter 300] loss=0.3522 val_loss=0.0000 scale=4.0000 norm=3.0831
[iter 400] loss=0.1483 val_loss=0.0000 scale=4.0000 norm=2.9979
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9917 val_loss=0.0000 scale=2.0000 norm=1.5957
[iter 200] loss=0.7163 val_loss=0.0000 scale=4.0000 norm=3.1824
[iter 300] loss=0.3367 val_loss=0.0000 scale=4.0000 norm=3.1396
[iter 400] loss=0.1126 val_loss=0.0000 scale=4.0000 norm=3.1348
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6195 val_loss=0.0000 scale=2.0000 norm=1.1033
[iter 200] loss=0.2836 val_loss=0.0000 scale=4.0000 norm=2.2032
[iter 300] loss=-0.1897 val_loss=0.0000 scale=4.0000 norm=2.0853
[iter 400] loss=-0.9023 val_loss=0.0000 scale=8.0000 norm=4.1393
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0399 val_loss=0.0000 scale=2.0000 norm=1.6164
[iter 200] loss=0.8077 val_loss=0.0000 scale=4.0000 norm=3.2086
[iter 300] loss=0.4654 val_loss=0.0000 scale=4.0000 norm=3.1900
[iter 400] loss=0.2116 val_loss=0.0000 scale=4.0000 norm=3.1563
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8215 val_loss=0.0000 scale=2.0000 norm=1.5031
[iter 200] loss=0.5011 val_loss=0.0000 scale=2.0000 norm=1.5260
[iter 300] loss=0.0391 val_loss=0.0000 scale=4.0000 norm=2.9604
[iter 400] loss=-0.3704 val_loss=0.0000 scale=4.0000 norm=2.9385
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0259 val_loss=0.0000 scale=2.0000 norm=1.6104
[iter 200] loss=0.7601 val_loss=0.0000 scale=4.0000 norm=3.1849
[iter 300] loss=0.4145 val_loss=0.0000 scale=4.0000 norm=3.1412
[iter 400] loss=0.1805 val_loss=0.0000 scale=4.0000 norm=3.1238
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0616 val_loss=0.0000 scale=2.0000 norm=1.6574
[iter 200] loss=0.7707 val_loss=0.0000 scale=4.0000 norm=3.2873
[iter 300] loss=0.3591 val_loss=0.0000 scale=4.0000 norm=3.2140
[iter 400] loss=0.0303 val_loss=0.0000 scale=8.0000 norm=6.3812
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0255 val_loss=0.0000 scale=2.0000 norm=1.5886
[iter 200] loss=0.8080 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 300] loss=0.4914 val_loss=0.0000 scale=4.0000 norm=3.1236
[iter 400] loss=0.2213 val_loss=0.0000 scale=4.0000 norm=3.0755
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0077 val_loss=0.0000 scale=2.0000 norm=1.5887
[iter 200] loss=0.7652 val_loss=0.0000 scale=2.0000 norm=1.5732
[iter 300] loss=0.4269 val_loss=0.0000 scale=4.0000 norm=3.1195
[iter 400] loss=0.2559 val_loss=0.0000 scale=4.0000 norm=3.1162
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5863 val_loss=0.0000 scale=2.0000 norm=1.1503
[iter 200] loss=0.2166 val_loss=0.0000 scale=2.0000 norm=1.1021
[iter 300] loss=-0.1534 val_loss=0.0000 scale=4.0000 norm=2.2001
[iter 400] loss=-0.4617 val_loss=0.0000 scale=4.0000 norm=2.1871
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0011 val_loss=0.0000 scale=2.0000 norm=1.5875
[iter 200] loss=0.7430 val_loss=0.0000 scale=4.0000 norm=3.1675
[iter 300] loss=0.3493 val_loss=0.0000 scale=4.0000 norm=3.1270
[iter 400] loss=0.1002 val_loss=0.0000 scale=4.0000 norm=3.0778
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0194 val_loss=0.0000 scale=2.0000 norm=1.6145
[iter 200] loss=0.7650 val_loss=0.0000 scale=2.0000 norm=1.6016
[iter 300] loss=0.4254 val_loss=0.0000 scale=4.0000 norm=3.1876
[iter 400] loss=0.2110 val_loss=0.0000 scale=4.0000 norm=3.0856
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0141 val_loss=0.0000 scale=2.0000 norm=1.5898
[iter 200] loss=0.7482 val_loss=0.0000 scale=4.0000 norm=3.1808
[iter 300] loss=0.4410 val_loss=0.0000 scale=4.0000 norm=3.0989
[iter 400] loss=0.3034 val_loss=0.0000 scale=4.0000 norm=3.1299
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9153 val_loss=0.0000 scale=2.0000 norm=1.5393
[iter 200] loss=0.6022 val_loss=0.0000 scale=4.0000 norm=3.0000
[iter 300] loss=0.2365 val_loss=0.0000 scale=4.0000 norm=2.9681
[iter 400] loss=-0.0282 val_loss=0.0000 scale=4.0000 norm=2.9604
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8117 val_loss=0.0000 scale=2.0000 norm=1.5876
[iter 200] loss=0.3635 val_loss=0.0000 scale=4.0000 norm=3.2626
[iter 300] loss=-0.2803 val_loss=0.0000 scale=4.0000 norm=3.2593
[iter 400] loss=-0.9725 val_loss=0.0000 scale=8.0000 norm=6.2071
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7417 val_loss=0.0000 scale=2.0000 norm=1.4644
[iter 200] loss=0.3755 val_loss=0.0000 scale=4.0000 norm=3.0494
[iter 300] loss=-0.1672 val_loss=0.0000 scale=4.0000 norm=3.0559
[iter 400] loss=-0.9496 val_loss=0.0000 scale=8.0000 norm=5.9929
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0285 val_loss=0.0000 scale=2.0000 norm=1.6086
[iter 200] loss=0.7830 val_loss=0.0000 scale=4.0000 norm=3.2021
[iter 300] loss=0.4449 val_loss=0.0000 scale=4.0000 norm=3.1565
[iter 400] loss=0.2717 val_loss=0.0000 scale=4.0000 norm=3.1092
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0518 val_loss=0.0000 scale=2.0000 norm=1.6267
[iter 200] loss=0.8213 val_loss=0.0000 scale=2.0000 norm=1.6076
[iter 300] loss=0.4940 val_loss=0.0000 scale=4.0000 norm=3.1575
[iter 400] loss=0.2430 val_loss=0.0000 scale=2.0000 norm=1.5389
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0162 val_loss=0.0000 scale=2.0000 norm=1.5850
[iter 200] loss=0.7760 val_loss=0.0000 scale=4.0000 norm=3.1351
[iter 300] loss=0.4313 val_loss=0.0000 scale=4.0000 norm=3.1290
[iter 400] loss=0.1840 val_loss=0.0000 scale=4.0000 norm=3.0753
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9395 val_loss=0.0000 scale=2.0000 norm=1.5559
[iter 200] loss=0.7674 val_loss=0.0000 scale=2.0000 norm=1.5488
[iter 300] loss=0.5531 val_loss=0.0000 scale=4.0000 norm=3.0598
[iter 400] loss=0.2484 val_loss=0.0000 scale=8.0000 norm=6.1201
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0608 val_loss=0.0000 scale=2.0000 norm=1.6424
[iter 200] loss=0.8421 val_loss=0.0000 scale=2.0000 norm=1.6456
[iter 300] loss=0.5100 val_loss=0.0000 scale=4.0000 norm=3.2344
[iter 400] loss=0.3100 val_loss=0.0000 scale=4.0000 norm=3.2397
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9869 val_loss=0.0000 scale=2.0000 norm=1.5710
[iter 200] loss=0.7236 val_loss=0.0000 scale=4.0000 norm=3.1108
[iter 300] loss=0.3648 val_loss=0.0000 scale=4.0000 norm=3.0705
[iter 400] loss=0.1495 val_loss=0.0000 scale=4.0000 norm=3.0515
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9259 val_loss=0.0000 scale=2.0000 norm=1.5636
[iter 200] loss=0.6080 val_loss=0.0000 scale=4.0000 norm=3.0319
[iter 300] loss=0.2301 val_loss=0.0000 scale=4.0000 norm=2.9906
[iter 400] loss=-0.0950 val_loss=0.0000 scale=4.0000 norm=2.9472
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1163 val_loss=0.0000 scale=2.0000 norm=1.6864
[iter 200] loss=0.9328 val_loss=0.0000 scale=2.0000 norm=1.6524
[iter 300] loss=0.6285 val_loss=0.0000 scale=4.0000 norm=3.3157
[iter 400] loss=0.3378 val_loss=0.0000 scale=4.0000 norm=3.2940
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9876 val_loss=0.0000 scale=2.0000 norm=1.5871
[iter 200] loss=0.7312 val_loss=0.0000 scale=2.0000 norm=1.5669
[iter 300] loss=0.4431 val_loss=0.0000 scale=4.0000 norm=3.1169
[iter 400] loss=0.2282 val_loss=0.0000 scale=4.0000 norm=3.0527
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9964 val_loss=0.0000 scale=2.0000 norm=1.5674
[iter 200] loss=0.7347 val_loss=0.0000 scale=4.0000 norm=3.1191
[iter 300] loss=0.3223 val_loss=0.0000 scale=4.0000 norm=3.0912
[iter 400] loss=0.0427 val_loss=0.0000 scale=4.0000 norm=3.0296
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0547 val_loss=0.0000 scale=2.0000 norm=1.6624
[iter 200] loss=0.8360 val_loss=0.0000 scale=2.0000 norm=1.6538
[iter 300] loss=0.5672 val_loss=0.0000 scale=4.0000 norm=3.2662
[iter 400] loss=0.3884 val_loss=0.0000 scale=4.0000 norm=3.2196
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9743 val_loss=0.0000 scale=2.0000 norm=1.5501
[iter 200] loss=0.7339 val_loss=0.0000 scale=2.0000 norm=1.5257
[iter 300] loss=0.4510 val_loss=0.0000 scale=4.0000 norm=3.0213
[iter 400] loss=0.2184 val_loss=0.0000 scale=4.0000 norm=2.9661
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0218 val_loss=0.0000 scale=2.0000 norm=1.6009
[iter 200] loss=0.7472 val_loss=0.0000 scale=4.0000 norm=3.1803
[iter 300] loss=0.4420 val_loss=0.0000 scale=4.0000 norm=3.1001
[iter 400] loss=0.2996 val_loss=0.0000 scale=4.0000 norm=3.0709
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0889 val_loss=0.0000 scale=2.0000 norm=1.6450
[iter 200] loss=0.9199 val_loss=0.0000 scale=2.0000 norm=1.6379
[iter 300] loss=0.6735 val_loss=0.0000 scale=4.0000 norm=3.2608
[iter 400] loss=0.5135 val_loss=0.0000 scale=8.0000 norm=6.4188
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0156 val_loss=0.0000 scale=2.0000 norm=1.6041
[iter 200] loss=0.7563 val_loss=0.0000 scale=2.0000 norm=1.5937
[iter 300] loss=0.3681 val_loss=0.0000 scale=4.0000 norm=3.1364
[iter 400] loss=0.1162 val_loss=0.0000 scale=2.0000 norm=1.5513
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0655 val_loss=0.0000 scale=2.0000 norm=1.6536
[iter 200] loss=0.8411 val_loss=0.0000 scale=4.0000 norm=3.3003
[iter 300] loss=0.4828 val_loss=0.0000 scale=4.0000 norm=3.2617
[iter 400] loss=0.2511 val_loss=0.0000 scale=4.0000 norm=3.2123
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6241 val_loss=0.0000 scale=2.0000 norm=1.1969
[iter 200] loss=0.2841 val_loss=0.0000 scale=4.0000 norm=2.5039
[iter 300] loss=-0.1259 val_loss=0.0000 scale=4.0000 norm=2.4831
[iter 400] loss=-0.4289 val_loss=0.0000 scale=4.0000 norm=2.4186
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7510 val_loss=0.0000 scale=2.0000 norm=1.2707
[iter 200] loss=0.3824 val_loss=0.0000 scale=4.0000 norm=2.5426
[iter 300] loss=-0.2179 val_loss=0.0000 scale=4.0000 norm=2.5505
[iter 400] loss=-1.3699 val_loss=0.0000 scale=8.0000 norm=5.1014
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0338 val_loss=0.0000 scale=2.0000 norm=1.6122
[iter 200] loss=0.8184 val_loss=0.0000 scale=4.0000 norm=3.1751
[iter 300] loss=0.5326 val_loss=0.0000 scale=4.0000 norm=3.1224
[iter 400] loss=0.3157 val_loss=0.0000 scale=4.0000 norm=3.0620
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0019 val_loss=0.0000 scale=2.0000 norm=1.5877
[iter 200] loss=0.7053 val_loss=0.0000 scale=4.0000 norm=3.1699
[iter 300] loss=0.3301 val_loss=0.0000 scale=4.0000 norm=3.1407
[iter 400] loss=0.1082 val_loss=0.0000 scale=4.0000 norm=3.1234
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8718 val_loss=0.0000 scale=2.0000 norm=1.5436
[iter 200] loss=0.5696 val_loss=0.0000 scale=2.0000 norm=1.5032
[iter 300] loss=0.2218 val_loss=0.0000 scale=4.0000 norm=3.0068
[iter 400] loss=-0.0744 val_loss=0.0000 scale=4.0000 norm=2.9986
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0310 val_loss=0.0000 scale=2.0000 norm=1.6095
[iter 200] loss=0.7922 val_loss=0.0000 scale=4.0000 norm=3.2000
[iter 300] loss=0.4603 val_loss=0.0000 scale=4.0000 norm=3.1618
[iter 400] loss=0.2792 val_loss=0.0000 scale=4.0000 norm=3.1392
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9891 val_loss=0.0000 scale=2.0000 norm=1.5520
[iter 200] loss=0.7561 val_loss=0.0000 scale=4.0000 norm=3.0706
[iter 300] loss=0.4276 val_loss=0.0000 scale=4.0000 norm=3.1078
[iter 400] loss=0.1218 val_loss=0.0000 scale=4.0000 norm=3.0737
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9805 val_loss=0.0000 scale=2.0000 norm=1.5607
[iter 200] loss=0.7186 val_loss=0.0000 scale=2.0000 norm=1.5571
[iter 300] loss=0.3219 val_loss=0.0000 scale=4.0000 norm=3.0615
[iter 400] loss=0.0363 val_loss=0.0000 scale=4.0000 norm=3.0257

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n08>
Subject: Job 853006: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:38 2024
Job was executed on host(s) <4*c202n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:40 2024
                            <4*c202n09>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:44:40 2024
Terminated at Tue Oct 22 17:52:05 2024
Results reported at Tue Oct 22 17:52:05 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "Trimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3232.13 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.59 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   456 sec.
    Turnaround time :                            447 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.

Monomer
Filename: (Mordred)_NGB_hypOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(Mordred)_NGB_hypOFF_generalizability_scores.json
Done Saving scores!
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0368 val_loss=0.0000 scale=2.0000 norm=1.6323
[iter 200] loss=0.8028 val_loss=0.0000 scale=2.0000 norm=1.6274
[iter 300] loss=0.4798 val_loss=0.0000 scale=4.0000 norm=3.1936
[iter 400] loss=0.3260 val_loss=0.0000 scale=4.0000 norm=3.2119
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9991 val_loss=0.0000 scale=2.0000 norm=1.5841
[iter 200] loss=0.7541 val_loss=0.0000 scale=4.0000 norm=3.1315
[iter 300] loss=0.4214 val_loss=0.0000 scale=4.0000 norm=3.0774
[iter 400] loss=0.2707 val_loss=0.0000 scale=4.0000 norm=3.1293
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9733 val_loss=0.0000 scale=2.0000 norm=1.5672
[iter 200] loss=0.7217 val_loss=0.0000 scale=4.0000 norm=3.1108
[iter 300] loss=0.3761 val_loss=0.0000 scale=4.0000 norm=3.0822
[iter 400] loss=0.1798 val_loss=0.0000 scale=4.0000 norm=3.0631
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7138 val_loss=0.0000 scale=2.0000 norm=1.3011
[iter 200] loss=0.3361 val_loss=0.0000 scale=4.0000 norm=2.6950
[iter 300] loss=-0.1889 val_loss=0.0000 scale=4.0000 norm=2.6591
[iter 400] loss=-0.6512 val_loss=0.0000 scale=4.0000 norm=2.6756
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0541 val_loss=0.0000 scale=2.0000 norm=1.6263
[iter 200] loss=0.8309 val_loss=0.0000 scale=4.0000 norm=3.2339
[iter 300] loss=0.4998 val_loss=0.0000 scale=4.0000 norm=3.2091
[iter 400] loss=0.3066 val_loss=0.0000 scale=4.0000 norm=3.1650
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8917 val_loss=0.0000 scale=2.0000 norm=1.4818
[iter 200] loss=0.6588 val_loss=0.0000 scale=2.0000 norm=1.5093
[iter 300] loss=0.3474 val_loss=0.0000 scale=4.0000 norm=2.9687
[iter 400] loss=0.1522 val_loss=0.0000 scale=4.0000 norm=2.9752
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0351 val_loss=0.0000 scale=2.0000 norm=1.6179
[iter 200] loss=0.7898 val_loss=0.0000 scale=4.0000 norm=3.2256
[iter 300] loss=0.5099 val_loss=0.0000 scale=4.0000 norm=3.1628
[iter 400] loss=0.3705 val_loss=0.0000 scale=4.0000 norm=3.1452
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0435 val_loss=0.0000 scale=2.0000 norm=1.6061
[iter 200] loss=0.7920 val_loss=0.0000 scale=4.0000 norm=3.2190
[iter 300] loss=0.4532 val_loss=0.0000 scale=4.0000 norm=3.1725
[iter 400] loss=0.2668 val_loss=0.0000 scale=4.0000 norm=3.1642
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8716 val_loss=0.0000 scale=2.0000 norm=1.5375
[iter 200] loss=0.4611 val_loss=0.0000 scale=4.0000 norm=3.0407
[iter 300] loss=-0.0314 val_loss=0.0000 scale=4.0000 norm=3.0144
[iter 400] loss=-0.3695 val_loss=0.0000 scale=4.0000 norm=2.9462
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0784 val_loss=0.0000 scale=2.0000 norm=1.6319
[iter 200] loss=0.8313 val_loss=0.0000 scale=4.0000 norm=3.2195
[iter 300] loss=0.5566 val_loss=0.0000 scale=4.0000 norm=3.1225
[iter 400] loss=0.1477 val_loss=0.0000 scale=8.0000 norm=6.2974
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9900 val_loss=0.0000 scale=2.0000 norm=1.5639
[iter 200] loss=0.7616 val_loss=0.0000 scale=2.0000 norm=1.5472
[iter 300] loss=0.4816 val_loss=0.0000 scale=4.0000 norm=3.0772
[iter 400] loss=0.2616 val_loss=0.0000 scale=4.0000 norm=3.0341
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9617 val_loss=0.0000 scale=2.0000 norm=1.5391
[iter 200] loss=0.7061 val_loss=0.0000 scale=2.0000 norm=1.5225
[iter 300] loss=0.3461 val_loss=0.0000 scale=4.0000 norm=3.0047
[iter 400] loss=0.0562 val_loss=0.0000 scale=4.0000 norm=2.9637
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0740 val_loss=0.0000 scale=2.0000 norm=1.6220
[iter 200] loss=0.8843 val_loss=0.0000 scale=2.0000 norm=1.6090
[iter 300] loss=0.6045 val_loss=0.0000 scale=4.0000 norm=3.1889
[iter 400] loss=0.3910 val_loss=0.0000 scale=4.0000 norm=3.1093
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6114 val_loss=0.0000 scale=2.0000 norm=1.1761
[iter 200] loss=0.2323 val_loss=0.0000 scale=2.0000 norm=1.1737
[iter 300] loss=-0.3049 val_loss=0.0000 scale=4.0000 norm=2.2138
[iter 400] loss=-0.7039 val_loss=0.0000 scale=4.0000 norm=2.0821
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0096 val_loss=0.0000 scale=2.0000 norm=1.5892
[iter 200] loss=0.7552 val_loss=0.0000 scale=4.0000 norm=3.1343
[iter 300] loss=0.3804 val_loss=0.0000 scale=4.0000 norm=3.0811
[iter 400] loss=0.1077 val_loss=0.0000 scale=4.0000 norm=2.9760
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8643 val_loss=0.0000 scale=2.0000 norm=1.4615
[iter 200] loss=0.6245 val_loss=0.0000 scale=2.0000 norm=1.4843
[iter 300] loss=0.4181 val_loss=0.0000 scale=2.0000 norm=1.4846
[iter 400] loss=0.1789 val_loss=0.0000 scale=4.0000 norm=2.9163
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0063 val_loss=0.0000 scale=2.0000 norm=1.5837
[iter 200] loss=0.7812 val_loss=0.0000 scale=2.0000 norm=1.5620
[iter 300] loss=0.4847 val_loss=0.0000 scale=4.0000 norm=3.1222
[iter 400] loss=0.2558 val_loss=0.0000 scale=4.0000 norm=3.0797
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0092 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 200] loss=0.6681 val_loss=0.0000 scale=4.0000 norm=3.2050
[iter 300] loss=0.2595 val_loss=0.0000 scale=4.0000 norm=3.1661
[iter 400] loss=-0.0215 val_loss=0.0000 scale=4.0000 norm=3.1467
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0460 val_loss=0.0000 scale=2.0000 norm=1.6188
[iter 200] loss=0.8386 val_loss=0.0000 scale=2.0000 norm=1.6217
[iter 300] loss=0.5655 val_loss=0.0000 scale=4.0000 norm=3.2534
[iter 400] loss=0.3707 val_loss=0.0000 scale=4.0000 norm=3.1814
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0174 val_loss=0.0000 scale=2.0000 norm=1.5964
[iter 200] loss=0.7847 val_loss=0.0000 scale=2.0000 norm=1.5824
[iter 300] loss=0.5033 val_loss=0.0000 scale=4.0000 norm=3.1413
[iter 400] loss=0.2949 val_loss=0.0000 scale=4.0000 norm=3.1652
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9858 val_loss=0.0000 scale=2.0000 norm=1.5680
[iter 200] loss=0.7504 val_loss=0.0000 scale=2.0000 norm=1.5559
[iter 300] loss=0.4154 val_loss=0.0000 scale=4.0000 norm=3.0794
[iter 400] loss=0.2193 val_loss=0.0000 scale=4.0000 norm=3.0349
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0494 val_loss=0.0000 scale=2.0000 norm=1.6293
[iter 200] loss=0.8321 val_loss=0.0000 scale=2.0000 norm=1.6227
[iter 300] loss=0.5242 val_loss=0.0000 scale=4.0000 norm=3.1858
[iter 400] loss=0.3846 val_loss=0.0000 scale=2.0000 norm=1.5865
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0333 val_loss=0.0000 scale=2.0000 norm=1.6016
[iter 200] loss=0.8155 val_loss=0.0000 scale=2.0000 norm=1.5897
[iter 300] loss=0.5326 val_loss=0.0000 scale=4.0000 norm=3.1393
[iter 400] loss=0.3682 val_loss=0.0000 scale=4.0000 norm=3.1467
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9651 val_loss=0.0000 scale=2.0000 norm=1.5744
[iter 200] loss=0.6305 val_loss=0.0000 scale=4.0000 norm=3.0555
[iter 300] loss=0.2588 val_loss=0.0000 scale=4.0000 norm=2.9877
[iter 400] loss=-0.0506 val_loss=0.0000 scale=4.0000 norm=2.9278
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9875 val_loss=0.0000 scale=2.0000 norm=1.5868
[iter 200] loss=0.7329 val_loss=0.0000 scale=2.0000 norm=1.5658
[iter 300] loss=0.4187 val_loss=0.0000 scale=4.0000 norm=3.0974
[iter 400] loss=0.2117 val_loss=0.0000 scale=4.0000 norm=3.0534
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0625 val_loss=0.0000 scale=2.0000 norm=1.6498
[iter 200] loss=0.8234 val_loss=0.0000 scale=4.0000 norm=3.2887
[iter 300] loss=0.4842 val_loss=0.0000 scale=4.0000 norm=3.2012
[iter 400] loss=0.2761 val_loss=0.0000 scale=4.0000 norm=3.1460
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0039 val_loss=0.0000 scale=2.0000 norm=1.5653
[iter 200] loss=0.7815 val_loss=0.0000 scale=2.0000 norm=1.5467
[iter 300] loss=0.5198 val_loss=0.0000 scale=4.0000 norm=3.0727
[iter 400] loss=0.3052 val_loss=0.0000 scale=4.0000 norm=3.0177
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0061 val_loss=0.0000 scale=2.0000 norm=1.6006
[iter 200] loss=0.7566 val_loss=0.0000 scale=2.0000 norm=1.5947
[iter 300] loss=0.3638 val_loss=0.0000 scale=4.0000 norm=3.1566
[iter 400] loss=0.1005 val_loss=0.0000 scale=4.0000 norm=3.1520
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4375 val_loss=0.0000 scale=2.0000 norm=1.1901
[iter 200] loss=-0.1530 val_loss=0.0000 scale=4.0000 norm=2.4690
[iter 300] loss=-0.8569 val_loss=0.0000 scale=4.0000 norm=2.5415
[iter 400] loss=-2.2289 val_loss=0.0000 scale=8.0000 norm=5.0862
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4186 val_loss=0.0000 scale=2.0000 norm=1.1960
[iter 200] loss=-0.1391 val_loss=0.0000 scale=4.0000 norm=2.2464
[iter 300] loss=-0.8450 val_loss=0.0000 scale=4.0000 norm=2.2486
[iter 400] loss=-2.1890 val_loss=0.0000 scale=8.0000 norm=4.4986
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8256 val_loss=0.0000 scale=2.0000 norm=1.4212
[iter 200] loss=0.5943 val_loss=0.0000 scale=2.0000 norm=1.4631
[iter 300] loss=0.3475 val_loss=0.0000 scale=4.0000 norm=2.9065
[iter 400] loss=0.0737 val_loss=0.0000 scale=4.0000 norm=2.8544
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0196 val_loss=0.0000 scale=2.0000 norm=1.5998
[iter 200] loss=0.7931 val_loss=0.0000 scale=2.0000 norm=1.5780
[iter 300] loss=0.4735 val_loss=0.0000 scale=4.0000 norm=3.1261
[iter 400] loss=0.2806 val_loss=0.0000 scale=4.0000 norm=3.0870
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7349 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 200] loss=-0.1758 val_loss=0.0000 scale=4.0000 norm=3.0687
[iter 300] loss=-1.3578 val_loss=0.0000 scale=8.0000 norm=5.8057
[iter 400] loss=-3.5818 val_loss=0.0000 scale=16.0000 norm=11.6070
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0326 val_loss=0.0000 scale=2.0000 norm=1.6094
[iter 200] loss=0.8104 val_loss=0.0000 scale=2.0000 norm=1.5992
[iter 300] loss=0.4946 val_loss=0.0000 scale=4.0000 norm=3.1574
[iter 400] loss=0.2986 val_loss=0.0000 scale=4.0000 norm=3.1333
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9062 val_loss=0.0000 scale=2.0000 norm=1.5190
[iter 200] loss=0.5985 val_loss=0.0000 scale=4.0000 norm=3.0369
[iter 300] loss=0.1217 val_loss=0.0000 scale=4.0000 norm=3.0170
[iter 400] loss=-0.2007 val_loss=0.0000 scale=4.0000 norm=2.9428
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0513 val_loss=0.0000 scale=2.0000 norm=1.6722
[iter 200] loss=0.8258 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 300] loss=0.6017 val_loss=0.0000 scale=2.0000 norm=1.6376
[iter 400] loss=0.4455 val_loss=0.0000 scale=4.0000 norm=3.2202
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0279 val_loss=0.0000 scale=2.0000 norm=1.5989
[iter 200] loss=0.7965 val_loss=0.0000 scale=4.0000 norm=3.1583
[iter 300] loss=0.4817 val_loss=0.0000 scale=4.0000 norm=3.1025
[iter 400] loss=0.3278 val_loss=0.0000 scale=4.0000 norm=3.1530
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0310 val_loss=0.0000 scale=2.0000 norm=1.6439
[iter 200] loss=0.7550 val_loss=0.0000 scale=2.0000 norm=1.6454
[iter 300] loss=0.3273 val_loss=0.0000 scale=4.0000 norm=3.2604
[iter 400] loss=-0.0127 val_loss=0.0000 scale=4.0000 norm=3.1960
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9911 val_loss=0.0000 scale=2.0000 norm=1.5693
[iter 200] loss=0.7538 val_loss=0.0000 scale=4.0000 norm=3.1161
[iter 300] loss=0.4608 val_loss=0.0000 scale=4.0000 norm=3.0910
[iter 400] loss=0.2461 val_loss=0.0000 scale=4.0000 norm=3.0476
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0397 val_loss=0.0000 scale=2.0000 norm=1.6198
[iter 200] loss=0.7670 val_loss=0.0000 scale=4.0000 norm=3.2167
[iter 300] loss=0.3957 val_loss=0.0000 scale=4.0000 norm=3.1680
[iter 400] loss=0.1248 val_loss=0.0000 scale=4.0000 norm=3.1024
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0191 val_loss=0.0000 scale=2.0000 norm=1.5838
[iter 200] loss=0.7863 val_loss=0.0000 scale=2.0000 norm=1.5672
[iter 300] loss=0.4721 val_loss=0.0000 scale=4.0000 norm=3.1333
[iter 400] loss=0.2018 val_loss=0.0000 scale=4.0000 norm=3.0710
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9453 val_loss=0.0000 scale=2.0000 norm=1.5672
[iter 200] loss=0.5560 val_loss=0.0000 scale=4.0000 norm=3.0350
[iter 300] loss=0.1906 val_loss=0.0000 scale=4.0000 norm=2.9490
[iter 400] loss=-0.0203 val_loss=0.0000 scale=2.0000 norm=1.4484
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9606 val_loss=0.0000 scale=2.0000 norm=1.5382
[iter 200] loss=0.7498 val_loss=0.0000 scale=4.0000 norm=3.1874
[iter 300] loss=0.4137 val_loss=0.0000 scale=8.0000 norm=6.4023
[iter 400] loss=-0.2463 val_loss=0.0000 scale=16.0000 norm=12.8051
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6960 val_loss=0.0000 scale=2.0000 norm=1.5056
[iter 200] loss=0.1473 val_loss=0.0000 scale=4.0000 norm=2.8217
[iter 300] loss=-0.4449 val_loss=0.0000 scale=4.0000 norm=2.7360
[iter 400] loss=-0.8512 val_loss=0.0000 scale=4.0000 norm=2.7292
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0290 val_loss=0.0000 scale=2.0000 norm=1.5990
[iter 200] loss=0.7845 val_loss=0.0000 scale=4.0000 norm=3.1956
[iter 300] loss=0.4266 val_loss=0.0000 scale=4.0000 norm=3.1514
[iter 400] loss=0.1941 val_loss=0.0000 scale=4.0000 norm=3.1342
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0571 val_loss=0.0000 scale=2.0000 norm=1.6144
[iter 200] loss=0.8534 val_loss=0.0000 scale=4.0000 norm=3.1995
[iter 300] loss=0.5403 val_loss=0.0000 scale=4.0000 norm=3.1460
[iter 400] loss=0.3542 val_loss=0.0000 scale=4.0000 norm=3.0867
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0057 val_loss=0.0000 scale=2.0000 norm=1.5883
[iter 200] loss=0.7715 val_loss=0.0000 scale=2.0000 norm=1.5603
[iter 300] loss=0.4637 val_loss=0.0000 scale=4.0000 norm=3.0807
[iter 400] loss=0.2430 val_loss=0.0000 scale=4.0000 norm=3.0157
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9791 val_loss=0.0000 scale=2.0000 norm=1.6009
[iter 200] loss=0.6799 val_loss=0.0000 scale=4.0000 norm=3.1857
[iter 300] loss=0.3214 val_loss=0.0000 scale=4.0000 norm=3.1177
[iter 400] loss=0.1200 val_loss=0.0000 scale=4.0000 norm=3.0822
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0432 val_loss=0.0000 scale=2.0000 norm=1.5933
[iter 200] loss=0.8502 val_loss=0.0000 scale=2.0000 norm=1.5859
[iter 300] loss=0.5964 val_loss=0.0000 scale=4.0000 norm=3.1433
[iter 400] loss=0.4550 val_loss=0.0000 scale=4.0000 norm=3.0603
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1051 val_loss=0.0000 scale=2.0000 norm=1.6501
[iter 200] loss=0.9471 val_loss=0.0000 scale=4.0000 norm=3.2766
[iter 300] loss=0.7267 val_loss=0.0000 scale=4.0000 norm=3.2231
[iter 400] loss=0.5852 val_loss=0.0000 scale=4.0000 norm=3.2021
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0522 val_loss=0.0000 scale=2.0000 norm=1.6266
[iter 200] loss=0.8236 val_loss=0.0000 scale=2.0000 norm=1.6058
[iter 300] loss=0.5642 val_loss=0.0000 scale=4.0000 norm=3.1681
[iter 400] loss=0.2784 val_loss=0.0000 scale=4.0000 norm=3.1143
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0793 val_loss=0.0000 scale=2.0000 norm=1.6576
[iter 200] loss=0.7993 val_loss=0.0000 scale=4.0000 norm=3.2786
[iter 300] loss=0.4681 val_loss=0.0000 scale=4.0000 norm=3.1932
[iter 400] loss=0.2553 val_loss=0.0000 scale=8.0000 norm=6.2886
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0150 val_loss=0.0000 scale=2.0000 norm=1.5802
[iter 200] loss=0.7895 val_loss=0.0000 scale=2.0000 norm=1.5628
[iter 300] loss=0.5287 val_loss=0.0000 scale=4.0000 norm=3.0952
[iter 400] loss=0.2844 val_loss=0.0000 scale=4.0000 norm=3.0410
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0303 val_loss=0.0000 scale=2.0000 norm=1.6089
[iter 200] loss=0.7917 val_loss=0.0000 scale=4.0000 norm=3.1512
[iter 300] loss=0.4466 val_loss=0.0000 scale=4.0000 norm=3.0952
[iter 400] loss=0.2230 val_loss=0.0000 scale=8.0000 norm=6.2274
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9372 val_loss=0.0000 scale=2.0000 norm=1.5885
[iter 200] loss=0.5493 val_loss=0.0000 scale=4.0000 norm=3.1484
[iter 300] loss=0.0749 val_loss=0.0000 scale=4.0000 norm=3.1133
[iter 400] loss=-0.2463 val_loss=0.0000 scale=4.0000 norm=3.0942
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1198 val_loss=0.0000 scale=2.0000 norm=1.7053
[iter 200] loss=0.8633 val_loss=0.0000 scale=4.0000 norm=3.3776
[iter 300] loss=0.5294 val_loss=0.0000 scale=4.0000 norm=3.3040
[iter 400] loss=0.1481 val_loss=0.0000 scale=8.0000 norm=6.4526
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9675 val_loss=0.0000 scale=2.0000 norm=1.5730
[iter 200] loss=0.7085 val_loss=0.0000 scale=2.0000 norm=1.5500
[iter 300] loss=0.3537 val_loss=0.0000 scale=4.0000 norm=3.0542
[iter 400] loss=0.0867 val_loss=0.0000 scale=4.0000 norm=2.9443
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7908 val_loss=0.0000 scale=2.0000 norm=1.5464
[iter 200] loss=0.3525 val_loss=0.0000 scale=4.0000 norm=3.0050
[iter 300] loss=-0.2184 val_loss=0.0000 scale=4.0000 norm=2.8905
[iter 400] loss=-0.6363 val_loss=0.0000 scale=4.0000 norm=2.8688
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9936 val_loss=0.0000 scale=2.0000 norm=1.5955
[iter 200] loss=0.7452 val_loss=0.0000 scale=2.0000 norm=1.5895
[iter 300] loss=0.3689 val_loss=0.0000 scale=4.0000 norm=3.1417
[iter 400] loss=0.1304 val_loss=0.0000 scale=4.0000 norm=3.1465
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0357 val_loss=0.0000 scale=2.0000 norm=1.7346
[iter 200] loss=0.7720 val_loss=0.0000 scale=4.0000 norm=3.4508
[iter 300] loss=0.4025 val_loss=0.0000 scale=4.0000 norm=3.4183
[iter 400] loss=0.0752 val_loss=0.0000 scale=4.0000 norm=3.3609
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0296 val_loss=0.0000 scale=2.0000 norm=1.6095
[iter 200] loss=0.8016 val_loss=0.0000 scale=2.0000 norm=1.5984
[iter 300] loss=0.4649 val_loss=0.0000 scale=4.0000 norm=3.1495
[iter 400] loss=0.2683 val_loss=0.0000 scale=4.0000 norm=3.1175
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9398 val_loss=0.0000 scale=2.0000 norm=1.6124
[iter 200] loss=0.5065 val_loss=0.0000 scale=4.0000 norm=3.2317
[iter 300] loss=-0.0078 val_loss=0.0000 scale=4.0000 norm=3.1416
[iter 400] loss=-0.4284 val_loss=0.0000 scale=4.0000 norm=3.0649
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0069 val_loss=0.0000 scale=2.0000 norm=1.5748
[iter 200] loss=0.7643 val_loss=0.0000 scale=2.0000 norm=1.5608
[iter 300] loss=0.4462 val_loss=0.0000 scale=4.0000 norm=3.1103
[iter 400] loss=0.1994 val_loss=0.0000 scale=4.0000 norm=3.0186
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0497 val_loss=0.0000 scale=2.0000 norm=1.6269
[iter 200] loss=0.8330 val_loss=0.0000 scale=4.0000 norm=3.2494
[iter 300] loss=0.5343 val_loss=0.0000 scale=4.0000 norm=3.1800
[iter 400] loss=0.4072 val_loss=0.0000 scale=4.0000 norm=3.1847
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0093 val_loss=0.0000 scale=2.0000 norm=1.5881
[iter 200] loss=0.7758 val_loss=0.0000 scale=2.0000 norm=1.5726
[iter 300] loss=0.4341 val_loss=0.0000 scale=4.0000 norm=3.1194
[iter 400] loss=0.2680 val_loss=0.0000 scale=2.0000 norm=1.5575
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0119 val_loss=0.0000 scale=2.0000 norm=1.6398
[iter 200] loss=0.7960 val_loss=0.0000 scale=4.0000 norm=3.2199
[iter 300] loss=0.5087 val_loss=0.0000 scale=8.0000 norm=6.5049
[iter 400] loss=-0.0513 val_loss=0.0000 scale=16.0000 norm=13.0103
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9965 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 200] loss=0.7540 val_loss=0.0000 scale=4.0000 norm=3.1480
[iter 300] loss=0.4107 val_loss=0.0000 scale=4.0000 norm=3.1337
[iter 400] loss=0.1676 val_loss=0.0000 scale=4.0000 norm=3.1012
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0142 val_loss=0.0000 scale=2.0000 norm=1.5762
[iter 200] loss=0.7910 val_loss=0.0000 scale=2.0000 norm=1.5593
[iter 300] loss=0.5009 val_loss=0.0000 scale=4.0000 norm=3.0903
[iter 400] loss=0.3164 val_loss=0.0000 scale=4.0000 norm=3.0929
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0007 val_loss=0.0000 scale=2.0000 norm=1.5957
[iter 200] loss=0.7566 val_loss=0.0000 scale=2.0000 norm=1.5820
[iter 300] loss=0.3994 val_loss=0.0000 scale=4.0000 norm=3.1321
[iter 400] loss=0.1900 val_loss=0.0000 scale=4.0000 norm=3.1423
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0174 val_loss=0.0000 scale=2.0000 norm=1.6029
[iter 200] loss=0.7558 val_loss=0.0000 scale=2.0000 norm=1.5932
[iter 300] loss=0.3735 val_loss=0.0000 scale=4.0000 norm=3.1387
[iter 400] loss=0.1185 val_loss=0.0000 scale=4.0000 norm=3.1116
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0476 val_loss=0.0000 scale=2.0000 norm=1.6122
[iter 200] loss=0.8223 val_loss=0.0000 scale=2.0000 norm=1.5879
[iter 300] loss=0.5606 val_loss=0.0000 scale=4.0000 norm=3.1216
[iter 400] loss=0.2924 val_loss=0.0000 scale=4.0000 norm=3.0994
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0347 val_loss=0.0000 scale=2.0000 norm=1.6096
[iter 200] loss=0.7915 val_loss=0.0000 scale=4.0000 norm=3.1897
[iter 300] loss=0.4806 val_loss=0.0000 scale=4.0000 norm=3.1391
[iter 400] loss=0.3243 val_loss=0.0000 scale=4.0000 norm=3.1060
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0067 val_loss=0.0000 scale=2.0000 norm=1.6076
[iter 200] loss=0.7277 val_loss=0.0000 scale=4.0000 norm=3.1963
[iter 300] loss=0.4079 val_loss=0.0000 scale=4.0000 norm=3.1747
[iter 400] loss=0.2259 val_loss=0.0000 scale=4.0000 norm=3.1246
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9500 val_loss=0.0000 scale=2.0000 norm=1.6676
[iter 200] loss=0.4852 val_loss=0.0000 scale=4.0000 norm=3.3495
[iter 300] loss=-0.3146 val_loss=0.0000 scale=4.0000 norm=3.3524
[iter 400] loss=-1.1725 val_loss=0.0000 scale=4.0000 norm=3.3250
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9076 val_loss=0.0000 scale=2.0000 norm=1.5112
[iter 200] loss=0.6722 val_loss=0.0000 scale=2.0000 norm=1.5387
[iter 300] loss=0.3059 val_loss=0.0000 scale=4.0000 norm=3.0720
[iter 400] loss=-0.0266 val_loss=0.0000 scale=4.0000 norm=3.0149
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0040 val_loss=0.0000 scale=2.0000 norm=1.6032
[iter 200] loss=0.7601 val_loss=0.0000 scale=2.0000 norm=1.5912
[iter 300] loss=0.4527 val_loss=0.0000 scale=4.0000 norm=3.1665
[iter 400] loss=0.1883 val_loss=0.0000 scale=4.0000 norm=3.0973
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9891 val_loss=0.0000 scale=2.0000 norm=1.6445
[iter 200] loss=0.4610 val_loss=0.0000 scale=4.0000 norm=3.2640
[iter 300] loss=-0.2004 val_loss=0.0000 scale=4.0000 norm=3.2023
[iter 400] loss=-0.7577 val_loss=0.0000 scale=4.0000 norm=3.2001
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0294 val_loss=0.0000 scale=2.0000 norm=1.6022
[iter 200] loss=0.7856 val_loss=0.0000 scale=4.0000 norm=3.2128
[iter 300] loss=0.4480 val_loss=0.0000 scale=4.0000 norm=3.1749
[iter 400] loss=0.2586 val_loss=0.0000 scale=4.0000 norm=3.1531
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0737 val_loss=0.0000 scale=2.0000 norm=1.6349
[iter 200] loss=0.8886 val_loss=0.0000 scale=2.0000 norm=1.6235
[iter 300] loss=0.6225 val_loss=0.0000 scale=4.0000 norm=3.1812
[iter 400] loss=0.4905 val_loss=0.0000 scale=4.0000 norm=3.1960
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0665 val_loss=0.0000 scale=2.0000 norm=1.6353
[iter 200] loss=0.8542 val_loss=0.0000 scale=2.0000 norm=1.6183
[iter 300] loss=0.5685 val_loss=0.0000 scale=4.0000 norm=3.1853
[iter 400] loss=0.3503 val_loss=0.0000 scale=2.0000 norm=1.5589
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0034 val_loss=0.0000 scale=2.0000 norm=1.6187
[iter 200] loss=0.7341 val_loss=0.0000 scale=4.0000 norm=3.1309
[iter 300] loss=0.4624 val_loss=0.0000 scale=4.0000 norm=3.0401
[iter 400] loss=0.2540 val_loss=0.0000 scale=8.0000 norm=5.9152
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7865 val_loss=0.0000 scale=2.0000 norm=1.4127
[iter 200] loss=0.5549 val_loss=0.0000 scale=4.0000 norm=2.9963
[iter 300] loss=0.2276 val_loss=0.0000 scale=4.0000 norm=3.0371
[iter 400] loss=-0.3747 val_loss=0.0000 scale=8.0000 norm=6.0770
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8715 val_loss=0.0000 scale=2.0000 norm=1.5455
[iter 200] loss=0.5659 val_loss=0.0000 scale=2.0000 norm=1.5043
[iter 300] loss=0.2200 val_loss=0.0000 scale=4.0000 norm=3.0070
[iter 400] loss=-0.0765 val_loss=0.0000 scale=4.0000 norm=2.9985
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0518 val_loss=0.0000 scale=2.0000 norm=1.6316
[iter 200] loss=0.7864 val_loss=0.0000 scale=4.0000 norm=3.2614
[iter 300] loss=0.3963 val_loss=0.0000 scale=4.0000 norm=3.2431
[iter 400] loss=0.0962 val_loss=0.0000 scale=4.0000 norm=3.2228
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0323 val_loss=0.0000 scale=2.0000 norm=1.6203
[iter 200] loss=0.8116 val_loss=0.0000 scale=4.0000 norm=3.2383
[iter 300] loss=0.4891 val_loss=0.0000 scale=4.0000 norm=3.2074
[iter 400] loss=0.3329 val_loss=0.0000 scale=4.0000 norm=3.1808
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7953 val_loss=0.0000 scale=2.0000 norm=1.6092
[iter 200] loss=0.2738 val_loss=0.0000 scale=4.0000 norm=3.1503
[iter 300] loss=-0.5520 val_loss=0.0000 scale=8.0000 norm=6.2905
[iter 400] loss=-2.1820 val_loss=0.0000 scale=16.0000 norm=12.5810
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9097 val_loss=0.0000 scale=2.0000 norm=1.5564
[iter 200] loss=0.5988 val_loss=0.0000 scale=2.0000 norm=1.5249
[iter 300] loss=0.1649 val_loss=0.0000 scale=4.0000 norm=2.9508
[iter 400] loss=-0.1293 val_loss=0.0000 scale=4.0000 norm=2.8378
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9782 val_loss=0.0000 scale=2.0000 norm=1.5522
[iter 200] loss=0.7268 val_loss=0.0000 scale=2.0000 norm=1.5336
[iter 300] loss=0.3952 val_loss=0.0000 scale=4.0000 norm=3.0225
[iter 400] loss=0.1634 val_loss=0.0000 scale=4.0000 norm=2.9742
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0212 val_loss=0.0000 scale=2.0000 norm=1.6023
[iter 200] loss=0.7458 val_loss=0.0000 scale=4.0000 norm=3.1745
[iter 300] loss=0.3762 val_loss=0.0000 scale=4.0000 norm=3.0949
[iter 400] loss=0.1358 val_loss=0.0000 scale=8.0000 norm=6.1654
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7445 val_loss=0.0000 scale=2.0000 norm=1.2907
[iter 200] loss=0.4846 val_loss=0.0000 scale=2.0000 norm=1.2992
[iter 300] loss=0.1056 val_loss=0.0000 scale=4.0000 norm=2.5638
[iter 400] loss=-0.1660 val_loss=0.0000 scale=4.0000 norm=2.4164
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9920 val_loss=0.0000 scale=2.0000 norm=1.5709
[iter 200] loss=0.7419 val_loss=0.0000 scale=2.0000 norm=1.5523
[iter 300] loss=0.3809 val_loss=0.0000 scale=4.0000 norm=3.1062
[iter 400] loss=0.1081 val_loss=0.0000 scale=4.0000 norm=3.0664
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0343 val_loss=0.0000 scale=2.0000 norm=1.5821
[iter 200] loss=0.7740 val_loss=0.0000 scale=4.0000 norm=3.1453
[iter 300] loss=0.5033 val_loss=0.0000 scale=4.0000 norm=3.1038
[iter 400] loss=0.3317 val_loss=0.0000 scale=8.0000 norm=6.0544
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8783 val_loss=0.0000 scale=2.0000 norm=1.4646
[iter 200] loss=0.6622 val_loss=0.0000 scale=2.0000 norm=1.5144
[iter 300] loss=0.3473 val_loss=0.0000 scale=4.0000 norm=3.0268
[iter 400] loss=0.0619 val_loss=0.0000 scale=4.0000 norm=2.9962
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0247 val_loss=0.0000 scale=2.0000 norm=1.5859
[iter 200] loss=0.8280 val_loss=0.0000 scale=2.0000 norm=1.5591
[iter 300] loss=0.5769 val_loss=0.0000 scale=4.0000 norm=3.0977
[iter 400] loss=0.4225 val_loss=0.0000 scale=4.0000 norm=3.0977
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5977 val_loss=0.0000 scale=2.0000 norm=1.1755
[iter 200] loss=0.2303 val_loss=0.0000 scale=2.0000 norm=1.1448
[iter 300] loss=-0.1487 val_loss=0.0000 scale=4.0000 norm=2.3379
[iter 400] loss=-0.4322 val_loss=0.0000 scale=4.0000 norm=2.2962
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0308 val_loss=0.0000 scale=2.0000 norm=1.6049
[iter 200] loss=0.8137 val_loss=0.0000 scale=2.0000 norm=1.5931
[iter 300] loss=0.5123 val_loss=0.0000 scale=4.0000 norm=3.1374
[iter 400] loss=0.3104 val_loss=0.0000 scale=4.0000 norm=3.0927
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0332 val_loss=0.0000 scale=2.0000 norm=1.7085
[iter 200] loss=0.4992 val_loss=0.0000 scale=4.0000 norm=3.3739
[iter 300] loss=-0.1150 val_loss=0.0000 scale=8.0000 norm=6.4115
[iter 400] loss=-1.2991 val_loss=0.0000 scale=16.0000 norm=12.8159
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9099 val_loss=0.0000 scale=2.0000 norm=1.5312
[iter 200] loss=0.5901 val_loss=0.0000 scale=2.0000 norm=1.4798
[iter 300] loss=0.1354 val_loss=0.0000 scale=4.0000 norm=2.8055
[iter 400] loss=-0.2032 val_loss=0.0000 scale=4.0000 norm=2.7043
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9239 val_loss=0.0000 scale=2.0000 norm=1.5188
[iter 200] loss=0.6391 val_loss=0.0000 scale=4.0000 norm=2.9863
[iter 300] loss=0.2666 val_loss=0.0000 scale=4.0000 norm=2.9574
[iter 400] loss=0.0045 val_loss=0.0000 scale=4.0000 norm=2.9460
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0037 val_loss=0.0000 scale=2.0000 norm=1.5936
[iter 200] loss=0.7374 val_loss=0.0000 scale=2.0000 norm=1.5818
[iter 300] loss=0.3529 val_loss=0.0000 scale=4.0000 norm=3.1341
[iter 400] loss=0.0831 val_loss=0.0000 scale=4.0000 norm=3.0578
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9627 val_loss=0.0000 scale=2.0000 norm=1.5518
[iter 200] loss=0.6797 val_loss=0.0000 scale=2.0000 norm=1.5251
[iter 300] loss=0.3792 val_loss=0.0000 scale=4.0000 norm=3.0106
[iter 400] loss=0.0461 val_loss=0.0000 scale=4.0000 norm=2.9338
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0624 val_loss=0.0000 scale=2.0000 norm=1.6574
[iter 200] loss=0.7665 val_loss=0.0000 scale=4.0000 norm=3.2855
[iter 300] loss=0.3557 val_loss=0.0000 scale=4.0000 norm=3.2121
[iter 400] loss=0.0224 val_loss=0.0000 scale=8.0000 norm=6.3724
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0241 val_loss=0.0000 scale=2.0000 norm=1.6105
[iter 200] loss=0.8059 val_loss=0.0000 scale=2.0000 norm=1.6089
[iter 300] loss=0.4855 val_loss=0.0000 scale=4.0000 norm=3.1533
[iter 400] loss=0.2929 val_loss=0.0000 scale=4.0000 norm=3.0930
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1186 val_loss=0.0000 scale=2.0000 norm=1.6860
[iter 200] loss=0.9375 val_loss=0.0000 scale=2.0000 norm=1.6549
[iter 300] loss=0.6454 val_loss=0.0000 scale=4.0000 norm=3.3172
[iter 400] loss=0.3528 val_loss=0.0000 scale=4.0000 norm=3.2974
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0124 val_loss=0.0000 scale=2.0000 norm=1.5839
[iter 200] loss=0.7704 val_loss=0.0000 scale=2.0000 norm=1.5743
[iter 300] loss=0.4142 val_loss=0.0000 scale=4.0000 norm=3.0787
[iter 400] loss=0.2099 val_loss=0.0000 scale=4.0000 norm=3.0785
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9936 val_loss=0.0000 scale=2.0000 norm=1.5687
[iter 200] loss=0.7194 val_loss=0.0000 scale=4.0000 norm=3.1210
[iter 300] loss=0.3090 val_loss=0.0000 scale=4.0000 norm=3.0878
[iter 400] loss=0.0348 val_loss=0.0000 scale=4.0000 norm=3.0263
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0407 val_loss=0.0000 scale=2.0000 norm=1.6825
[iter 200] loss=0.6903 val_loss=0.0000 scale=4.0000 norm=3.2926
[iter 300] loss=0.2656 val_loss=0.0000 scale=8.0000 norm=6.4275
[iter 400] loss=-0.4312 val_loss=0.0000 scale=16.0000 norm=12.8331
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0155 val_loss=0.0000 scale=2.0000 norm=1.5942
[iter 200] loss=0.7638 val_loss=0.0000 scale=4.0000 norm=3.1808
[iter 300] loss=0.4313 val_loss=0.0000 scale=4.0000 norm=3.1178
[iter 400] loss=0.2732 val_loss=0.0000 scale=2.0000 norm=1.5622
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0484 val_loss=0.0000 scale=2.0000 norm=1.6539
[iter 200] loss=0.7322 val_loss=0.0000 scale=4.0000 norm=3.3124
[iter 300] loss=0.2996 val_loss=0.0000 scale=4.0000 norm=3.2726
[iter 400] loss=-0.0113 val_loss=0.0000 scale=4.0000 norm=3.2176
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0448 val_loss=0.0000 scale=2.0000 norm=1.5923
[iter 200] loss=0.8403 val_loss=0.0000 scale=2.0000 norm=1.5793
[iter 300] loss=0.5623 val_loss=0.0000 scale=2.0000 norm=1.5634
[iter 400] loss=0.4098 val_loss=0.0000 scale=4.0000 norm=3.1321
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0348 val_loss=0.0000 scale=2.0000 norm=1.6200
[iter 200] loss=0.7946 val_loss=0.0000 scale=2.0000 norm=1.6162
[iter 300] loss=0.4149 val_loss=0.0000 scale=4.0000 norm=3.1920
[iter 400] loss=0.1391 val_loss=0.0000 scale=4.0000 norm=3.1532
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7657 val_loss=0.0000 scale=2.0000 norm=1.3380
[iter 200] loss=0.5164 val_loss=0.0000 scale=2.0000 norm=1.3826
[iter 300] loss=0.2295 val_loss=0.0000 scale=4.0000 norm=2.7572
[iter 400] loss=-0.0808 val_loss=0.0000 scale=4.0000 norm=2.7173
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0406 val_loss=0.0000 scale=2.0000 norm=1.6423
[iter 200] loss=0.8124 val_loss=0.0000 scale=4.0000 norm=3.1964
[iter 300] loss=0.5755 val_loss=0.0000 scale=4.0000 norm=3.1642
[iter 400] loss=0.3612 val_loss=0.0000 scale=8.0000 norm=6.2151
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0305 val_loss=0.0000 scale=2.0000 norm=1.6080
[iter 200] loss=0.8193 val_loss=0.0000 scale=2.0000 norm=1.6069
[iter 300] loss=0.5153 val_loss=0.0000 scale=4.0000 norm=3.1626
[iter 400] loss=0.3032 val_loss=0.0000 scale=4.0000 norm=3.1056
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0203 val_loss=0.0000 scale=2.0000 norm=1.5879
[iter 200] loss=0.8005 val_loss=0.0000 scale=2.0000 norm=1.5643
[iter 300] loss=0.4989 val_loss=0.0000 scale=4.0000 norm=3.0791
[iter 400] loss=0.3339 val_loss=0.0000 scale=2.0000 norm=1.5249
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9709 val_loss=0.0000 scale=2.0000 norm=1.5808
[iter 200] loss=0.6001 val_loss=0.0000 scale=4.0000 norm=3.1596
[iter 300] loss=0.0859 val_loss=0.0000 scale=4.0000 norm=3.1627
[iter 400] loss=-0.3731 val_loss=0.0000 scale=4.0000 norm=3.0793
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0105 val_loss=0.0000 scale=2.0000 norm=1.5954
[iter 200] loss=0.7753 val_loss=0.0000 scale=2.0000 norm=1.5768
[iter 300] loss=0.4743 val_loss=0.0000 scale=4.0000 norm=3.1212
[iter 400] loss=0.2305 val_loss=0.0000 scale=4.0000 norm=3.0824
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0512 val_loss=0.0000 scale=2.0000 norm=1.6407
[iter 200] loss=0.8297 val_loss=0.0000 scale=2.0000 norm=1.6331
[iter 300] loss=0.6160 val_loss=0.0000 scale=2.0000 norm=1.6161
[iter 400] loss=0.4582 val_loss=0.0000 scale=4.0000 norm=3.2041
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0450 val_loss=0.0000 scale=2.0000 norm=1.6183
[iter 200] loss=0.8321 val_loss=0.0000 scale=4.0000 norm=3.2003
[iter 300] loss=0.5324 val_loss=0.0000 scale=4.0000 norm=3.1360
[iter 400] loss=0.3685 val_loss=0.0000 scale=4.0000 norm=3.0994
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7717 val_loss=0.0000 scale=2.0000 norm=1.4663
[iter 200] loss=0.5023 val_loss=0.0000 scale=2.0000 norm=1.5215
[iter 300] loss=0.0864 val_loss=0.0000 scale=4.0000 norm=3.0391
[iter 400] loss=-0.2252 val_loss=0.0000 scale=4.0000 norm=2.9401
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0356 val_loss=0.0000 scale=2.0000 norm=1.6341
[iter 200] loss=0.7519 val_loss=0.0000 scale=4.0000 norm=3.2201
[iter 300] loss=0.3576 val_loss=0.0000 scale=4.0000 norm=3.1985
[iter 400] loss=0.0440 val_loss=0.0000 scale=4.0000 norm=3.1177
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9856 val_loss=0.0000 scale=2.0000 norm=1.5743
[iter 200] loss=0.7571 val_loss=0.0000 scale=2.0000 norm=1.5671
[iter 300] loss=0.4740 val_loss=0.0000 scale=4.0000 norm=3.1334
[iter 400] loss=0.2307 val_loss=0.0000 scale=4.0000 norm=3.1056
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0272 val_loss=0.0000 scale=2.0000 norm=1.5983
[iter 200] loss=0.7994 val_loss=0.0000 scale=4.0000 norm=3.1756
[iter 300] loss=0.4414 val_loss=0.0000 scale=4.0000 norm=3.1402
[iter 400] loss=0.2436 val_loss=0.0000 scale=4.0000 norm=3.0853
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0075 val_loss=0.0000 scale=2.0000 norm=1.5941
[iter 200] loss=0.7798 val_loss=0.0000 scale=4.0000 norm=3.1351
[iter 300] loss=0.4768 val_loss=0.0000 scale=4.0000 norm=3.1146
[iter 400] loss=0.2445 val_loss=0.0000 scale=4.0000 norm=3.0780
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9961 val_loss=0.0000 scale=2.0000 norm=1.5929
[iter 200] loss=0.7075 val_loss=0.0000 scale=4.0000 norm=3.1534
[iter 300] loss=0.2644 val_loss=0.0000 scale=4.0000 norm=3.1257
[iter 400] loss=-0.0343 val_loss=0.0000 scale=4.0000 norm=3.0449
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0213 val_loss=0.0000 scale=2.0000 norm=1.6135
[iter 200] loss=0.7670 val_loss=0.0000 scale=2.0000 norm=1.6019
[iter 300] loss=0.4171 val_loss=0.0000 scale=4.0000 norm=3.1891
[iter 400] loss=0.2045 val_loss=0.0000 scale=4.0000 norm=3.0991
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8184 val_loss=0.0000 scale=2.0000 norm=1.5822
[iter 200] loss=0.1601 val_loss=0.0000 scale=4.0000 norm=3.1798
[iter 300] loss=-0.5546 val_loss=0.0000 scale=4.0000 norm=3.0733
[iter 400] loss=-1.0496 val_loss=0.0000 scale=4.0000 norm=2.8506
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0518 val_loss=0.0000 scale=2.0000 norm=1.6035
[iter 200] loss=0.8671 val_loss=0.0000 scale=2.0000 norm=1.5929
[iter 300] loss=0.6300 val_loss=0.0000 scale=4.0000 norm=3.1665
[iter 400] loss=0.4812 val_loss=0.0000 scale=4.0000 norm=3.0916
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0654 val_loss=0.0000 scale=2.0000 norm=1.6705
[iter 200] loss=0.7436 val_loss=0.0000 scale=4.0000 norm=3.3165
[iter 300] loss=0.3770 val_loss=0.0000 scale=4.0000 norm=3.2865
[iter 400] loss=0.0609 val_loss=0.0000 scale=4.0000 norm=3.2239
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9895 val_loss=0.0000 scale=2.0000 norm=1.5832
[iter 200] loss=0.7166 val_loss=0.0000 scale=2.0000 norm=1.5642
[iter 300] loss=0.3117 val_loss=0.0000 scale=4.0000 norm=3.0882
[iter 400] loss=0.0658 val_loss=0.0000 scale=2.0000 norm=1.5239
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0236 val_loss=0.0000 scale=2.0000 norm=1.6110
[iter 200] loss=0.7589 val_loss=0.0000 scale=4.0000 norm=3.1841
[iter 300] loss=0.4135 val_loss=0.0000 scale=4.0000 norm=3.1424
[iter 400] loss=0.1773 val_loss=0.0000 scale=4.0000 norm=3.1243
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0603 val_loss=0.0000 scale=2.0000 norm=1.6303
[iter 200] loss=0.8488 val_loss=0.0000 scale=4.0000 norm=3.2365
[iter 300] loss=0.5321 val_loss=0.0000 scale=4.0000 norm=3.1882
[iter 400] loss=0.3219 val_loss=0.0000 scale=4.0000 norm=3.1676
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0016 val_loss=0.0000 scale=2.0000 norm=1.6060
[iter 200] loss=0.7528 val_loss=0.0000 scale=4.0000 norm=3.1730
[iter 300] loss=0.4543 val_loss=0.0000 scale=4.0000 norm=3.1767
[iter 400] loss=0.2302 val_loss=0.0000 scale=4.0000 norm=3.1046
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9610 val_loss=0.0000 scale=2.0000 norm=1.5485
[iter 200] loss=0.6917 val_loss=0.0000 scale=2.0000 norm=1.5180
[iter 300] loss=0.4077 val_loss=0.0000 scale=4.0000 norm=3.0047
[iter 400] loss=0.1806 val_loss=0.0000 scale=4.0000 norm=2.9850
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8964 val_loss=0.0000 scale=2.0000 norm=1.5809
[iter 200] loss=0.6371 val_loss=0.0000 scale=2.0000 norm=1.6244
[iter 300] loss=0.2988 val_loss=0.0000 scale=4.0000 norm=3.1301
[iter 400] loss=-0.0589 val_loss=0.0000 scale=8.0000 norm=6.3114
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1635 val_loss=0.0000 scale=2.0000 norm=1.8724
[iter 200] loss=0.8688 val_loss=0.0000 scale=4.0000 norm=3.7634
[iter 300] loss=0.3648 val_loss=0.0000 scale=8.0000 norm=7.5297
[iter 400] loss=-0.6272 val_loss=0.0000 scale=16.0000 norm=15.0595
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9161 val_loss=0.0000 scale=2.0000 norm=1.5387
[iter 200] loss=0.6188 val_loss=0.0000 scale=4.0000 norm=2.9945
[iter 300] loss=0.2499 val_loss=0.0000 scale=4.0000 norm=2.9668
[iter 400] loss=-0.0179 val_loss=0.0000 scale=4.0000 norm=2.9625
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9706 val_loss=0.0000 scale=2.0000 norm=1.5778
[iter 200] loss=0.6753 val_loss=0.0000 scale=2.0000 norm=1.5613
[iter 300] loss=0.2280 val_loss=0.0000 scale=4.0000 norm=3.0763
[iter 400] loss=-0.0089 val_loss=0.0000 scale=4.0000 norm=2.9882
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9610 val_loss=0.0000 scale=2.0000 norm=1.5517
[iter 200] loss=0.6853 val_loss=0.0000 scale=2.0000 norm=1.5268
[iter 300] loss=0.2745 val_loss=0.0000 scale=4.0000 norm=3.0017
[iter 400] loss=0.0067 val_loss=0.0000 scale=4.0000 norm=2.9167
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0326 val_loss=0.0000 scale=2.0000 norm=1.5968
[iter 200] loss=0.8314 val_loss=0.0000 scale=2.0000 norm=1.5773
[iter 300] loss=0.5505 val_loss=0.0000 scale=4.0000 norm=3.1174
[iter 400] loss=0.3941 val_loss=0.0000 scale=4.0000 norm=3.0941
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0120 val_loss=0.0000 scale=2.0000 norm=1.5918
[iter 200] loss=0.7857 val_loss=0.0000 scale=2.0000 norm=1.5802
[iter 300] loss=0.5311 val_loss=0.0000 scale=4.0000 norm=3.1339
[iter 400] loss=0.3198 val_loss=0.0000 scale=4.0000 norm=3.1030
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0040 val_loss=0.0000 scale=2.0000 norm=1.5872
[iter 200] loss=0.7601 val_loss=0.0000 scale=4.0000 norm=3.1491
[iter 300] loss=0.3960 val_loss=0.0000 scale=4.0000 norm=3.1114
[iter 400] loss=0.1740 val_loss=0.0000 scale=4.0000 norm=3.0970
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9802 val_loss=0.0000 scale=2.0000 norm=1.6248
[iter 200] loss=0.6791 val_loss=0.0000 scale=2.0000 norm=1.6010
[iter 300] loss=0.1804 val_loss=0.0000 scale=4.0000 norm=3.1514
[iter 400] loss=-0.2352 val_loss=0.0000 scale=4.0000 norm=3.0960
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9513 val_loss=0.0000 scale=2.0000 norm=1.5692
[iter 200] loss=0.5798 val_loss=0.0000 scale=4.0000 norm=3.1382
[iter 300] loss=0.1038 val_loss=0.0000 scale=4.0000 norm=2.9997
[iter 400] loss=-0.1591 val_loss=0.0000 scale=4.0000 norm=3.0084
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6956 val_loss=0.0000 scale=2.0000 norm=1.3639
[iter 200] loss=0.4251 val_loss=0.0000 scale=4.0000 norm=2.9300
[iter 300] loss=0.0787 val_loss=0.0000 scale=4.0000 norm=2.9370
[iter 400] loss=-0.2450 val_loss=0.0000 scale=8.0000 norm=5.7206
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0294 val_loss=0.0000 scale=2.0000 norm=1.6206
[iter 200] loss=0.7330 val_loss=0.0000 scale=4.0000 norm=3.2177
[iter 300] loss=0.3819 val_loss=0.0000 scale=4.0000 norm=3.1551
[iter 400] loss=0.1510 val_loss=0.0000 scale=4.0000 norm=3.1101
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9896 val_loss=0.0000 scale=2.0000 norm=1.6080
[iter 200] loss=0.6121 val_loss=0.0000 scale=4.0000 norm=3.2121
[iter 300] loss=0.1242 val_loss=0.0000 scale=4.0000 norm=3.1665
[iter 400] loss=-0.2534 val_loss=0.0000 scale=4.0000 norm=3.0908
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0117 val_loss=0.0000 scale=2.0000 norm=1.5971
[iter 200] loss=0.7681 val_loss=0.0000 scale=4.0000 norm=3.1722
[iter 300] loss=0.4267 val_loss=0.0000 scale=2.0000 norm=1.5660
[iter 400] loss=0.2614 val_loss=0.0000 scale=4.0000 norm=3.1716
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9947 val_loss=0.0000 scale=2.0000 norm=1.5657
[iter 200] loss=0.7417 val_loss=0.0000 scale=4.0000 norm=3.1154
[iter 300] loss=0.4187 val_loss=0.0000 scale=4.0000 norm=3.0812
[iter 400] loss=0.2128 val_loss=0.0000 scale=4.0000 norm=3.0222
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8543 val_loss=0.0000 scale=2.0000 norm=1.6122
[iter 200] loss=0.4152 val_loss=0.0000 scale=4.0000 norm=3.2085
[iter 300] loss=-0.0602 val_loss=0.0000 scale=4.0000 norm=3.1636
[iter 400] loss=-0.3927 val_loss=0.0000 scale=4.0000 norm=3.0546
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0340 val_loss=0.0000 scale=2.0000 norm=1.6104
[iter 200] loss=0.8078 val_loss=0.0000 scale=2.0000 norm=1.6010
[iter 300] loss=0.4708 val_loss=0.0000 scale=4.0000 norm=3.1645
[iter 400] loss=0.2577 val_loss=0.0000 scale=4.0000 norm=3.1542
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0597 val_loss=0.0000 scale=2.0000 norm=1.6368
[iter 200] loss=0.8211 val_loss=0.0000 scale=4.0000 norm=3.2564
[iter 300] loss=0.5132 val_loss=0.0000 scale=4.0000 norm=3.2247
[iter 400] loss=0.3807 val_loss=0.0000 scale=2.0000 norm=1.5954
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5087 val_loss=0.0000 scale=2.0000 norm=1.0902
[iter 200] loss=0.0120 val_loss=0.0000 scale=2.0000 norm=1.1384
[iter 300] loss=-0.6682 val_loss=0.0000 scale=4.0000 norm=2.2551
[iter 400] loss=-1.7494 val_loss=0.0000 scale=8.0000 norm=4.5378
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0420 val_loss=0.0000 scale=2.0000 norm=1.6078
[iter 200] loss=0.8222 val_loss=0.0000 scale=2.0000 norm=1.5982
[iter 300] loss=0.4921 val_loss=0.0000 scale=4.0000 norm=3.1279
[iter 400] loss=0.3209 val_loss=0.0000 scale=2.0000 norm=1.5451
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9645 val_loss=0.0000 scale=2.0000 norm=1.5410
[iter 200] loss=0.7024 val_loss=0.0000 scale=2.0000 norm=1.5234
[iter 300] loss=0.3474 val_loss=0.0000 scale=4.0000 norm=2.9942
[iter 400] loss=0.0951 val_loss=0.0000 scale=4.0000 norm=2.9510
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7428 val_loss=0.0000 scale=2.0000 norm=1.4642
[iter 200] loss=0.4029 val_loss=0.0000 scale=4.0000 norm=3.0424
[iter 300] loss=-0.1411 val_loss=0.0000 scale=4.0000 norm=3.0578
[iter 400] loss=-0.8036 val_loss=0.0000 scale=8.0000 norm=5.9931
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0040 val_loss=0.0000 scale=2.0000 norm=1.6033
[iter 200] loss=0.6876 val_loss=0.0000 scale=4.0000 norm=3.1863
[iter 300] loss=0.3429 val_loss=0.0000 scale=4.0000 norm=3.1285
[iter 400] loss=0.1135 val_loss=0.0000 scale=4.0000 norm=3.0840
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7510 val_loss=0.0000 scale=2.0000 norm=1.2707
[iter 200] loss=0.3824 val_loss=0.0000 scale=4.0000 norm=2.5426
[iter 300] loss=-0.2179 val_loss=0.0000 scale=4.0000 norm=2.5505
[iter 400] loss=-1.3699 val_loss=0.0000 scale=8.0000 norm=5.1014
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0511 val_loss=0.0000 scale=2.0000 norm=1.6223
[iter 200] loss=0.8096 val_loss=0.0000 scale=4.0000 norm=3.2467
[iter 300] loss=0.4856 val_loss=0.0000 scale=4.0000 norm=3.2175
[iter 400] loss=0.2879 val_loss=0.0000 scale=4.0000 norm=3.1703
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6630 val_loss=0.0000 scale=2.0000 norm=1.2794
[iter 200] loss=0.3149 val_loss=0.0000 scale=2.0000 norm=1.2795
[iter 300] loss=-0.1375 val_loss=0.0000 scale=4.0000 norm=2.5264
[iter 400] loss=-0.5351 val_loss=0.0000 scale=4.0000 norm=2.5012
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0022 val_loss=0.0000 scale=2.0000 norm=1.5763
[iter 200] loss=0.7586 val_loss=0.0000 scale=2.0000 norm=1.5680
[iter 300] loss=0.4531 val_loss=0.0000 scale=4.0000 norm=3.1040
[iter 400] loss=0.2108 val_loss=0.0000 scale=4.0000 norm=3.0708
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0646 val_loss=0.0000 scale=2.0000 norm=1.6311
[iter 200] loss=0.8542 val_loss=0.0000 scale=2.0000 norm=1.6075
[iter 300] loss=0.5387 val_loss=0.0000 scale=4.0000 norm=3.1602
[iter 400] loss=0.2935 val_loss=0.0000 scale=4.0000 norm=3.1527
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9856 val_loss=0.0000 scale=2.0000 norm=1.5626
[iter 200] loss=0.7476 val_loss=0.0000 scale=2.0000 norm=1.5497
[iter 300] loss=0.4250 val_loss=0.0000 scale=4.0000 norm=3.0495
[iter 400] loss=0.2338 val_loss=0.0000 scale=4.0000 norm=3.0163
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9087 val_loss=0.0000 scale=2.0000 norm=1.5209
[iter 200] loss=0.5725 val_loss=0.0000 scale=2.0000 norm=1.5176
[iter 300] loss=0.0607 val_loss=0.0000 scale=4.0000 norm=2.9896
[iter 400] loss=-0.2667 val_loss=0.0000 scale=4.0000 norm=2.8532
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9952 val_loss=0.0000 scale=2.0000 norm=1.5925
[iter 200] loss=0.6846 val_loss=0.0000 scale=4.0000 norm=3.1636
[iter 300] loss=0.3478 val_loss=0.0000 scale=4.0000 norm=3.0873
[iter 400] loss=0.1449 val_loss=0.0000 scale=4.0000 norm=2.9998
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0897 val_loss=0.0000 scale=2.0000 norm=1.6443
[iter 200] loss=0.9217 val_loss=0.0000 scale=2.0000 norm=1.6384
[iter 300] loss=0.6813 val_loss=0.0000 scale=4.0000 norm=3.2610
[iter 400] loss=0.5246 val_loss=0.0000 scale=4.0000 norm=3.2177
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1178 val_loss=0.0000 scale=2.0000 norm=1.6818
[iter 200] loss=0.9593 val_loss=0.0000 scale=4.0000 norm=3.3392
[iter 300] loss=0.7151 val_loss=0.0000 scale=4.0000 norm=3.3083
[iter 400] loss=0.5430 val_loss=0.0000 scale=4.0000 norm=3.2670
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7223 val_loss=0.0000 scale=2.0000 norm=1.2630
[iter 200] loss=0.4503 val_loss=0.0000 scale=2.0000 norm=1.2824
[iter 300] loss=0.1603 val_loss=0.0000 scale=4.0000 norm=2.5291
[iter 400] loss=-0.0492 val_loss=0.0000 scale=2.0000 norm=1.2215
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6310 val_loss=0.0000 scale=2.0000 norm=1.3721
[iter 200] loss=0.2143 val_loss=0.0000 scale=4.0000 norm=2.6093
[iter 300] loss=-0.2981 val_loss=0.0000 scale=4.0000 norm=2.6618
[iter 400] loss=-0.7634 val_loss=0.0000 scale=8.0000 norm=4.9863
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9896 val_loss=0.0000 scale=2.0000 norm=1.5640
[iter 200] loss=0.7626 val_loss=0.0000 scale=2.0000 norm=1.5435
[iter 300] loss=0.4677 val_loss=0.0000 scale=4.0000 norm=3.0581
[iter 400] loss=0.2442 val_loss=0.0000 scale=4.0000 norm=3.0450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0171 val_loss=0.0000 scale=2.0000 norm=1.6161
[iter 200] loss=0.7273 val_loss=0.0000 scale=4.0000 norm=3.2332
[iter 300] loss=0.2156 val_loss=0.0000 scale=4.0000 norm=3.2365
[iter 400] loss=-0.2788 val_loss=0.0000 scale=4.0000 norm=3.2049
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9569 val_loss=0.0000 scale=2.0000 norm=1.5661
[iter 200] loss=0.6132 val_loss=0.0000 scale=4.0000 norm=3.1314
[iter 300] loss=0.1748 val_loss=0.0000 scale=4.0000 norm=3.1129
[iter 400] loss=-0.1632 val_loss=0.0000 scale=4.0000 norm=3.0517
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0476 val_loss=0.0000 scale=2.0000 norm=1.6259
[iter 200] loss=0.8285 val_loss=0.0000 scale=4.0000 norm=3.2346
[iter 300] loss=0.4934 val_loss=0.0000 scale=4.0000 norm=3.1850
[iter 400] loss=0.3421 val_loss=0.0000 scale=2.0000 norm=1.5575
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0227 val_loss=0.0000 scale=2.0000 norm=1.5970
[iter 200] loss=0.8082 val_loss=0.0000 scale=2.0000 norm=1.5857
[iter 300] loss=0.5055 val_loss=0.0000 scale=4.0000 norm=3.1567
[iter 400] loss=0.3310 val_loss=0.0000 scale=4.0000 norm=3.1255
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9883 val_loss=0.0000 scale=2.0000 norm=1.6061
[iter 200] loss=0.6667 val_loss=0.0000 scale=4.0000 norm=3.1892
[iter 300] loss=0.2816 val_loss=0.0000 scale=4.0000 norm=3.1721
[iter 400] loss=-0.0873 val_loss=0.0000 scale=4.0000 norm=3.0812
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0398 val_loss=0.0000 scale=2.0000 norm=1.5896
[iter 200] loss=0.8401 val_loss=0.0000 scale=2.0000 norm=1.5761
[iter 300] loss=0.5679 val_loss=0.0000 scale=4.0000 norm=3.1271
[iter 400] loss=0.4074 val_loss=0.0000 scale=4.0000 norm=3.1069
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8748 val_loss=0.0000 scale=2.0000 norm=1.6157
[iter 200] loss=0.4005 val_loss=0.0000 scale=4.0000 norm=3.2493
[iter 300] loss=-0.3972 val_loss=0.0000 scale=4.0000 norm=3.2484
[iter 400] loss=-1.1784 val_loss=0.0000 scale=8.0000 norm=6.2432
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0448 val_loss=0.0000 scale=2.0000 norm=1.6202
[iter 200] loss=0.8170 val_loss=0.0000 scale=4.0000 norm=3.2104
[iter 300] loss=0.4636 val_loss=0.0000 scale=4.0000 norm=3.1652
[iter 400] loss=0.2424 val_loss=0.0000 scale=4.0000 norm=3.1526
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9918 val_loss=0.0000 scale=2.0000 norm=1.5889
[iter 200] loss=0.7641 val_loss=0.0000 scale=2.0000 norm=1.5750
[iter 300] loss=0.4643 val_loss=0.0000 scale=4.0000 norm=3.1356
[iter 400] loss=0.2124 val_loss=0.0000 scale=4.0000 norm=3.0980
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0301 val_loss=0.0000 scale=2.0000 norm=1.5999
[iter 200] loss=0.7746 val_loss=0.0000 scale=4.0000 norm=3.1782
[iter 300] loss=0.4099 val_loss=0.0000 scale=4.0000 norm=3.1446
[iter 400] loss=0.2011 val_loss=0.0000 scale=4.0000 norm=3.0974
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9817 val_loss=0.0000 scale=2.0000 norm=1.5687
[iter 200] loss=0.6953 val_loss=0.0000 scale=4.0000 norm=3.0665
[iter 300] loss=0.3577 val_loss=0.0000 scale=4.0000 norm=3.0049
[iter 400] loss=0.1075 val_loss=0.0000 scale=4.0000 norm=2.9544
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9633 val_loss=0.0000 scale=2.0000 norm=1.5493
[iter 200] loss=0.6974 val_loss=0.0000 scale=2.0000 norm=1.5135
[iter 300] loss=0.4305 val_loss=0.0000 scale=4.0000 norm=3.0107
[iter 400] loss=0.1814 val_loss=0.0000 scale=4.0000 norm=2.9879
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0326 val_loss=0.0000 scale=2.0000 norm=1.6324
[iter 200] loss=0.7968 val_loss=0.0000 scale=2.0000 norm=1.6310
[iter 300] loss=0.4479 val_loss=0.0000 scale=4.0000 norm=3.2410
[iter 400] loss=0.1411 val_loss=0.0000 scale=4.0000 norm=3.2013
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0148 val_loss=0.0000 scale=2.0000 norm=1.5868
[iter 200] loss=0.7948 val_loss=0.0000 scale=2.0000 norm=1.5677
[iter 300] loss=0.5313 val_loss=0.0000 scale=4.0000 norm=3.1205
[iter 400] loss=0.3141 val_loss=0.0000 scale=4.0000 norm=3.0957
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8990 val_loss=0.0000 scale=2.0000 norm=1.6412
[iter 200] loss=0.2548 val_loss=0.0000 scale=4.0000 norm=3.1673
[iter 300] loss=-0.3109 val_loss=0.0000 scale=8.0000 norm=5.9080
[iter 400] loss=-1.3042 val_loss=0.0000 scale=16.0000 norm=11.6727
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0147 val_loss=0.0000 scale=2.0000 norm=1.5930
[iter 200] loss=0.7984 val_loss=0.0000 scale=2.0000 norm=1.6023
[iter 300] loss=0.4625 val_loss=0.0000 scale=4.0000 norm=3.1539
[iter 400] loss=0.2414 val_loss=0.0000 scale=4.0000 norm=3.1611
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0784 val_loss=0.0000 scale=2.0000 norm=1.6174
[iter 200] loss=0.9074 val_loss=0.0000 scale=2.0000 norm=1.6037
[iter 300] loss=0.6597 val_loss=0.0000 scale=4.0000 norm=3.1897
[iter 400] loss=0.4470 val_loss=0.0000 scale=4.0000 norm=3.1661
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9660 val_loss=0.0000 scale=2.0000 norm=1.5460
[iter 200] loss=0.7031 val_loss=0.0000 scale=2.0000 norm=1.5304
[iter 300] loss=0.3149 val_loss=0.0000 scale=4.0000 norm=3.0041
[iter 400] loss=0.0868 val_loss=0.0000 scale=2.0000 norm=1.4864
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9395 val_loss=0.0000 scale=2.0000 norm=1.5559
[iter 200] loss=0.7674 val_loss=0.0000 scale=2.0000 norm=1.5488
[iter 300] loss=0.5611 val_loss=0.0000 scale=4.0000 norm=3.0599
[iter 400] loss=0.2355 val_loss=0.0000 scale=8.0000 norm=6.1201
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9862 val_loss=0.0000 scale=2.0000 norm=1.5718
[iter 200] loss=0.7055 val_loss=0.0000 scale=4.0000 norm=3.1060
[iter 300] loss=0.3505 val_loss=0.0000 scale=4.0000 norm=3.0596
[iter 400] loss=0.1549 val_loss=0.0000 scale=4.0000 norm=3.0499
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0429 val_loss=0.0000 scale=2.0000 norm=1.6686
[iter 200] loss=0.6958 val_loss=0.0000 scale=4.0000 norm=3.3336
[iter 300] loss=0.1358 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-0.9442 val_loss=0.0000 scale=16.0000 norm=13.3438
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0431 val_loss=0.0000 scale=2.0000 norm=1.6148
[iter 200] loss=0.8250 val_loss=0.0000 scale=2.0000 norm=1.6041
[iter 300] loss=0.4933 val_loss=0.0000 scale=4.0000 norm=3.1863
[iter 400] loss=0.2590 val_loss=0.0000 scale=4.0000 norm=3.1955
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0431 val_loss=0.0000 scale=2.0000 norm=1.6432
[iter 200] loss=0.8269 val_loss=0.0000 scale=2.0000 norm=1.6423
[iter 300] loss=0.5046 val_loss=0.0000 scale=4.0000 norm=3.2567
[iter 400] loss=0.3525 val_loss=0.0000 scale=2.0000 norm=1.6035
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0996 val_loss=0.0000 scale=2.0000 norm=1.6764
[iter 200] loss=0.8732 val_loss=0.0000 scale=4.0000 norm=3.3009
[iter 300] loss=0.5996 val_loss=0.0000 scale=4.0000 norm=3.2687
[iter 400] loss=0.3024 val_loss=0.0000 scale=8.0000 norm=6.4098
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9786 val_loss=0.0000 scale=2.0000 norm=1.5729
[iter 200] loss=0.7325 val_loss=0.0000 scale=2.0000 norm=1.5393
[iter 300] loss=0.4210 val_loss=0.0000 scale=4.0000 norm=3.0263
[iter 400] loss=0.2319 val_loss=0.0000 scale=4.0000 norm=2.9624
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0070 val_loss=0.0000 scale=2.0000 norm=1.5770
[iter 200] loss=0.7717 val_loss=0.0000 scale=2.0000 norm=1.5650
[iter 300] loss=0.4221 val_loss=0.0000 scale=4.0000 norm=3.0829
[iter 400] loss=0.2239 val_loss=0.0000 scale=2.0000 norm=1.5215
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9750 val_loss=0.0000 scale=2.0000 norm=1.5568
[iter 200] loss=0.7096 val_loss=0.0000 scale=4.0000 norm=3.0734
[iter 300] loss=0.3788 val_loss=0.0000 scale=4.0000 norm=3.0432
[iter 400] loss=0.1977 val_loss=0.0000 scale=8.0000 norm=6.0395
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9922 val_loss=0.0000 scale=2.0000 norm=1.5692
[iter 200] loss=0.7475 val_loss=0.0000 scale=2.0000 norm=1.5575
[iter 300] loss=0.4503 val_loss=0.0000 scale=4.0000 norm=3.0791
[iter 400] loss=0.2384 val_loss=0.0000 scale=4.0000 norm=3.0746
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4127 val_loss=0.0000 scale=2.0000 norm=0.9614
[iter 200] loss=-0.0940 val_loss=0.0000 scale=2.0000 norm=0.9073
[iter 300] loss=-0.6774 val_loss=0.0000 scale=4.0000 norm=1.7293
[iter 400] loss=-1.5011 val_loss=0.0000 scale=8.0000 norm=3.3760
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8214 val_loss=0.0000 scale=2.0000 norm=1.4167
[iter 200] loss=0.6110 val_loss=0.0000 scale=2.0000 norm=1.4851
[iter 300] loss=0.3075 val_loss=0.0000 scale=4.0000 norm=2.9800
[iter 400] loss=0.0267 val_loss=0.0000 scale=4.0000 norm=2.9300
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9329 val_loss=0.0000 scale=2.0000 norm=1.5432
[iter 200] loss=0.6701 val_loss=0.0000 scale=2.0000 norm=1.5132
[iter 300] loss=0.2994 val_loss=0.0000 scale=4.0000 norm=2.9960
[iter 400] loss=0.1054 val_loss=0.0000 scale=4.0000 norm=2.9695
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0116 val_loss=0.0000 scale=2.0000 norm=1.5885
[iter 200] loss=0.7824 val_loss=0.0000 scale=2.0000 norm=1.5805
[iter 300] loss=0.4512 val_loss=0.0000 scale=4.0000 norm=3.1023
[iter 400] loss=0.2703 val_loss=0.0000 scale=4.0000 norm=3.0896
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0164 val_loss=0.0000 scale=2.0000 norm=1.5861
[iter 200] loss=0.7934 val_loss=0.0000 scale=2.0000 norm=1.5678
[iter 300] loss=0.5509 val_loss=0.0000 scale=4.0000 norm=3.1237
[iter 400] loss=0.3160 val_loss=0.0000 scale=4.0000 norm=3.0856
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0155 val_loss=0.0000 scale=2.0000 norm=1.5842
[iter 200] loss=0.7649 val_loss=0.0000 scale=4.0000 norm=3.1353
[iter 300] loss=0.4386 val_loss=0.0000 scale=4.0000 norm=3.1046
[iter 400] loss=0.2531 val_loss=0.0000 scale=4.0000 norm=3.1584
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0155 val_loss=0.0000 scale=2.0000 norm=1.5990
[iter 200] loss=0.7682 val_loss=0.0000 scale=2.0000 norm=1.5931
[iter 300] loss=0.4281 val_loss=0.0000 scale=4.0000 norm=3.1429
[iter 400] loss=0.2096 val_loss=0.0000 scale=4.0000 norm=3.1459
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7939 val_loss=0.0000 scale=2.0000 norm=1.3563
[iter 200] loss=0.5713 val_loss=0.0000 scale=2.0000 norm=1.4208
[iter 300] loss=0.2694 val_loss=0.0000 scale=4.0000 norm=2.8671
[iter 400] loss=0.0108 val_loss=0.0000 scale=4.0000 norm=2.8513
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9903 val_loss=0.0000 scale=2.0000 norm=1.5666
[iter 200] loss=0.7420 val_loss=0.0000 scale=2.0000 norm=1.5557
[iter 300] loss=0.4223 val_loss=0.0000 scale=4.0000 norm=3.1105
[iter 400] loss=0.1528 val_loss=0.0000 scale=2.0000 norm=1.5203
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0599 val_loss=0.0000 scale=2.0000 norm=1.6395
[iter 200] loss=0.8553 val_loss=0.0000 scale=2.0000 norm=1.6483
[iter 300] loss=0.6673 val_loss=0.0000 scale=2.0000 norm=1.6352
[iter 400] loss=0.4719 val_loss=0.0000 scale=4.0000 norm=3.2321
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9497 val_loss=0.0000 scale=2.0000 norm=1.5568
[iter 200] loss=0.6403 val_loss=0.0000 scale=4.0000 norm=3.0426
[iter 300] loss=0.3035 val_loss=0.0000 scale=4.0000 norm=3.0083
[iter 400] loss=0.0594 val_loss=0.0000 scale=4.0000 norm=2.9480
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0123 val_loss=0.0000 scale=2.0000 norm=1.5844
[iter 200] loss=0.7818 val_loss=0.0000 scale=2.0000 norm=1.5681
[iter 300] loss=0.4881 val_loss=0.0000 scale=4.0000 norm=3.1012
[iter 400] loss=0.2908 val_loss=0.0000 scale=2.0000 norm=1.5387
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5968 val_loss=0.0000 scale=2.0000 norm=1.2872
[iter 200] loss=0.0884 val_loss=0.0000 scale=2.0000 norm=1.2449
[iter 300] loss=-0.3068 val_loss=0.0000 scale=4.0000 norm=2.6108
[iter 400] loss=-0.6288 val_loss=0.0000 scale=8.0000 norm=4.9224
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0140 val_loss=0.0000 scale=2.0000 norm=1.5983
[iter 200] loss=0.7624 val_loss=0.0000 scale=4.0000 norm=3.1846
[iter 300] loss=0.4218 val_loss=0.0000 scale=2.0000 norm=1.5500
[iter 400] loss=0.2534 val_loss=0.0000 scale=4.0000 norm=3.1111
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.5676 val_loss=0.0000 scale=2.0000 norm=1.3047
[iter 200] loss=-0.0708 val_loss=0.0000 scale=4.0000 norm=2.7344
[iter 300] loss=-0.8612 val_loss=0.0000 scale=4.0000 norm=2.7332
[iter 400] loss=-1.7964 val_loss=0.0000 scale=8.0000 norm=5.1165
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9997 val_loss=0.0000 scale=2.0000 norm=1.5863
[iter 200] loss=0.7413 val_loss=0.0000 scale=4.0000 norm=3.1719
[iter 300] loss=0.4089 val_loss=0.0000 scale=4.0000 norm=3.1184
[iter 400] loss=0.2461 val_loss=0.0000 scale=4.0000 norm=3.1177
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0961 val_loss=0.0000 scale=2.0000 norm=1.6488
[iter 200] loss=0.9189 val_loss=0.0000 scale=4.0000 norm=3.2842
[iter 300] loss=0.6640 val_loss=0.0000 scale=4.0000 norm=3.2633
[iter 400] loss=0.4708 val_loss=0.0000 scale=4.0000 norm=3.2391
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4758 val_loss=0.0000 scale=2.0000 norm=1.0500
[iter 200] loss=0.0729 val_loss=0.0000 scale=2.0000 norm=1.1270
[iter 300] loss=-0.3956 val_loss=0.0000 scale=4.0000 norm=2.2556
[iter 400] loss=-1.1922 val_loss=0.0000 scale=8.0000 norm=4.5003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9385 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 200] loss=0.7385 val_loss=0.0000 scale=4.0000 norm=3.1936
[iter 300] loss=0.4380 val_loss=0.0000 scale=4.0000 norm=3.2107
[iter 400] loss=-0.1290 val_loss=0.0000 scale=8.0000 norm=6.4220
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0390 val_loss=0.0000 scale=2.0000 norm=1.6322
[iter 200] loss=0.7112 val_loss=0.0000 scale=4.0000 norm=3.2188
[iter 300] loss=0.3230 val_loss=0.0000 scale=4.0000 norm=3.1657
[iter 400] loss=0.0879 val_loss=0.0000 scale=4.0000 norm=3.1353
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0223 val_loss=0.0000 scale=2.0000 norm=1.6050
[iter 200] loss=0.7972 val_loss=0.0000 scale=4.0000 norm=3.2063
[iter 300] loss=0.4976 val_loss=0.0000 scale=4.0000 norm=3.1911
[iter 400] loss=0.3190 val_loss=0.0000 scale=4.0000 norm=3.1393
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0011 val_loss=0.0000 scale=2.0000 norm=1.5827
[iter 200] loss=0.7624 val_loss=0.0000 scale=2.0000 norm=1.5702
[iter 300] loss=0.4530 val_loss=0.0000 scale=4.0000 norm=3.0812
[iter 400] loss=0.2568 val_loss=0.0000 scale=4.0000 norm=3.0971
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0018 val_loss=0.0000 scale=2.0000 norm=1.5808
[iter 200] loss=0.7163 val_loss=0.0000 scale=4.0000 norm=3.1195
[iter 300] loss=0.3503 val_loss=0.0000 scale=4.0000 norm=3.0135
[iter 400] loss=0.1066 val_loss=0.0000 scale=4.0000 norm=3.0025
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0114 val_loss=0.0000 scale=2.0000 norm=1.5982
[iter 200] loss=0.7445 val_loss=0.0000 scale=4.0000 norm=3.1554
[iter 300] loss=0.4401 val_loss=0.0000 scale=4.0000 norm=3.0995
[iter 400] loss=0.2643 val_loss=0.0000 scale=4.0000 norm=3.1648
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7951 val_loss=0.0000 scale=2.0000 norm=1.5042
[iter 200] loss=0.5445 val_loss=0.0000 scale=2.0000 norm=1.5762
[iter 300] loss=0.1058 val_loss=0.0000 scale=4.0000 norm=3.1521
[iter 400] loss=-0.5208 val_loss=0.0000 scale=8.0000 norm=6.2143
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0648 val_loss=0.0000 scale=2.0000 norm=1.6386
[iter 200] loss=0.8545 val_loss=0.0000 scale=2.0000 norm=1.6291
[iter 300] loss=0.5766 val_loss=0.0000 scale=4.0000 norm=3.2236
[iter 400] loss=0.2890 val_loss=0.0000 scale=4.0000 norm=3.1647
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9878 val_loss=0.0000 scale=2.0000 norm=1.5601
[iter 200] loss=0.7620 val_loss=0.0000 scale=2.0000 norm=1.5420
[iter 300] loss=0.4787 val_loss=0.0000 scale=4.0000 norm=3.0803
[iter 400] loss=0.2632 val_loss=0.0000 scale=4.0000 norm=3.0594
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5525 val_loss=0.0000 scale=2.0000 norm=1.1032
[iter 200] loss=0.1739 val_loss=0.0000 scale=4.0000 norm=2.2266
[iter 300] loss=-0.3690 val_loss=0.0000 scale=4.0000 norm=2.2327
[iter 400] loss=-0.8790 val_loss=0.0000 scale=4.0000 norm=2.2136
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0258 val_loss=0.0000 scale=2.0000 norm=1.5882
[iter 200] loss=0.8044 val_loss=0.0000 scale=2.0000 norm=1.5856
[iter 300] loss=0.4968 val_loss=0.0000 scale=4.0000 norm=3.1301
[iter 400] loss=0.2242 val_loss=0.0000 scale=4.0000 norm=3.0746
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0252 val_loss=0.0000 scale=2.0000 norm=1.5858
[iter 200] loss=0.8138 val_loss=0.0000 scale=2.0000 norm=1.5705
[iter 300] loss=0.5466 val_loss=0.0000 scale=4.0000 norm=3.0937
[iter 400] loss=0.4480 val_loss=0.0000 scale=4.0000 norm=3.0824
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0081 val_loss=0.0000 scale=2.0000 norm=1.5922
[iter 200] loss=0.7659 val_loss=0.0000 scale=2.0000 norm=1.5816
[iter 300] loss=0.4284 val_loss=0.0000 scale=4.0000 norm=3.0943
[iter 400] loss=0.2706 val_loss=0.0000 scale=4.0000 norm=3.1114
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0084 val_loss=0.0000 scale=2.0000 norm=1.6291
[iter 200] loss=0.7072 val_loss=0.0000 scale=4.0000 norm=3.2175
[iter 300] loss=0.2737 val_loss=0.0000 scale=4.0000 norm=3.1721
[iter 400] loss=-0.0765 val_loss=0.0000 scale=4.0000 norm=3.1200
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0249 val_loss=0.0000 scale=2.0000 norm=1.6177
[iter 200] loss=0.7945 val_loss=0.0000 scale=2.0000 norm=1.6020
[iter 300] loss=0.5259 val_loss=0.0000 scale=4.0000 norm=3.1707
[iter 400] loss=0.3153 val_loss=0.0000 scale=4.0000 norm=3.1205
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9646 val_loss=0.0000 scale=2.0000 norm=1.5743
[iter 200] loss=0.6594 val_loss=0.0000 scale=2.0000 norm=1.5635
[iter 300] loss=0.1678 val_loss=0.0000 scale=4.0000 norm=3.0953
[iter 400] loss=-0.1068 val_loss=0.0000 scale=4.0000 norm=3.0391
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6240 val_loss=0.0000 scale=2.0000 norm=1.1969
[iter 200] loss=0.2815 val_loss=0.0000 scale=2.0000 norm=1.2527
[iter 300] loss=-0.1276 val_loss=0.0000 scale=4.0000 norm=2.4826
[iter 400] loss=-0.4302 val_loss=0.0000 scale=4.0000 norm=2.4181
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0438 val_loss=0.0000 scale=2.0000 norm=1.6094
[iter 200] loss=0.8371 val_loss=0.0000 scale=2.0000 norm=1.6006
[iter 300] loss=0.5425 val_loss=0.0000 scale=4.0000 norm=3.1750
[iter 400] loss=0.3553 val_loss=0.0000 scale=4.0000 norm=3.1213
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8970 val_loss=0.0000 scale=2.0000 norm=1.5607
[iter 200] loss=0.5414 val_loss=0.0000 scale=4.0000 norm=3.0580
[iter 300] loss=-0.0644 val_loss=0.0000 scale=4.0000 norm=3.0232
[iter 400] loss=-1.1264 val_loss=0.0000 scale=8.0000 norm=6.0466
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9909 val_loss=0.0000 scale=2.0000 norm=1.5696
[iter 200] loss=0.6989 val_loss=0.0000 scale=4.0000 norm=3.1194
[iter 300] loss=0.3389 val_loss=0.0000 scale=4.0000 norm=3.1025
[iter 400] loss=0.1165 val_loss=0.0000 scale=4.0000 norm=3.0269
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0004 val_loss=0.0000 scale=2.0000 norm=1.5767
[iter 200] loss=0.7808 val_loss=0.0000 scale=2.0000 norm=1.5399
[iter 300] loss=0.4828 val_loss=0.0000 scale=4.0000 norm=3.0449
[iter 400] loss=0.3413 val_loss=0.0000 scale=4.0000 norm=2.9992
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5863 val_loss=0.0000 scale=2.0000 norm=1.1503
[iter 200] loss=0.2166 val_loss=0.0000 scale=2.0000 norm=1.1021
[iter 300] loss=-0.1534 val_loss=0.0000 scale=4.0000 norm=2.2001
[iter 400] loss=-0.4617 val_loss=0.0000 scale=4.0000 norm=2.1871
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0317 val_loss=0.0000 scale=2.0000 norm=1.6446
[iter 200] loss=0.7648 val_loss=0.0000 scale=4.0000 norm=3.3176
[iter 300] loss=0.2648 val_loss=0.0000 scale=4.0000 norm=3.3251
[iter 400] loss=-0.6502 val_loss=0.0000 scale=8.0000 norm=6.6505
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0182 val_loss=0.0000 scale=2.0000 norm=1.5805
[iter 200] loss=0.8043 val_loss=0.0000 scale=2.0000 norm=1.5691
[iter 300] loss=0.5050 val_loss=0.0000 scale=4.0000 norm=3.1173
[iter 400] loss=0.3396 val_loss=0.0000 scale=4.0000 norm=3.1288
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0237 val_loss=0.0000 scale=2.0000 norm=1.6010
[iter 200] loss=0.7982 val_loss=0.0000 scale=4.0000 norm=3.1763
[iter 300] loss=0.4768 val_loss=0.0000 scale=4.0000 norm=3.1131
[iter 400] loss=0.3066 val_loss=0.0000 scale=4.0000 norm=3.0709
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9875 val_loss=0.0000 scale=2.0000 norm=1.5690
[iter 200] loss=0.7573 val_loss=0.0000 scale=2.0000 norm=1.5502
[iter 300] loss=0.4333 val_loss=0.0000 scale=4.0000 norm=3.0715
[iter 400] loss=0.2226 val_loss=0.0000 scale=4.0000 norm=3.0492
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8306 val_loss=0.0000 scale=2.0000 norm=1.4455
[iter 200] loss=0.4209 val_loss=0.0000 scale=4.0000 norm=2.9168
[iter 300] loss=-0.2122 val_loss=0.0000 scale=4.0000 norm=2.8186
[iter 400] loss=-1.1489 val_loss=0.0000 scale=8.0000 norm=5.4818
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9351 val_loss=0.0000 scale=2.0000 norm=1.5544
[iter 200] loss=0.5608 val_loss=0.0000 scale=4.0000 norm=3.0699
[iter 300] loss=0.0234 val_loss=0.0000 scale=4.0000 norm=3.0077
[iter 400] loss=-0.4605 val_loss=0.0000 scale=4.0000 norm=2.9541
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8329 val_loss=0.0000 scale=2.0000 norm=1.4284
[iter 200] loss=0.5640 val_loss=0.0000 scale=2.0000 norm=1.4453
[iter 300] loss=0.3257 val_loss=0.0000 scale=2.0000 norm=1.4401
[iter 400] loss=0.0997 val_loss=0.0000 scale=4.0000 norm=2.7975
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9477 val_loss=0.0000 scale=2.0000 norm=1.6432
[iter 200] loss=0.5545 val_loss=0.0000 scale=4.0000 norm=3.2427
[iter 300] loss=0.0907 val_loss=0.0000 scale=4.0000 norm=3.1942
[iter 400] loss=-0.2469 val_loss=0.0000 scale=4.0000 norm=3.0748
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0113 val_loss=0.0000 scale=2.0000 norm=1.5748
[iter 200] loss=0.7726 val_loss=0.0000 scale=2.0000 norm=1.5619
[iter 300] loss=0.4477 val_loss=0.0000 scale=2.0000 norm=1.5279
[iter 400] loss=0.2313 val_loss=0.0000 scale=4.0000 norm=3.0470
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8200 val_loss=0.0000 scale=2.0000 norm=1.5544
[iter 200] loss=0.3327 val_loss=0.0000 scale=4.0000 norm=3.1041
[iter 300] loss=-0.2519 val_loss=0.0000 scale=4.0000 norm=3.0759
[iter 400] loss=-0.7028 val_loss=0.0000 scale=4.0000 norm=2.9946
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9617 val_loss=0.0000 scale=2.0000 norm=1.5474
[iter 200] loss=0.7264 val_loss=0.0000 scale=2.0000 norm=1.5670
[iter 300] loss=0.3614 val_loss=0.0000 scale=4.0000 norm=3.1563
[iter 400] loss=-0.0683 val_loss=0.0000 scale=4.0000 norm=3.1188
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6272 val_loss=0.0000 scale=2.0000 norm=1.1954
[iter 200] loss=0.2703 val_loss=0.0000 scale=2.0000 norm=1.2833
[iter 300] loss=-0.1042 val_loss=0.0000 scale=4.0000 norm=2.5905
[iter 400] loss=-0.4407 val_loss=0.0000 scale=4.0000 norm=2.4898
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0114 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 200] loss=0.7856 val_loss=0.0000 scale=2.0000 norm=1.5621
[iter 300] loss=0.4917 val_loss=0.0000 scale=4.0000 norm=3.0965
[iter 400] loss=0.2863 val_loss=0.0000 scale=4.0000 norm=3.0658
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9597 val_loss=0.0000 scale=2.0000 norm=1.5270
[iter 200] loss=0.7126 val_loss=0.0000 scale=2.0000 norm=1.5048
[iter 300] loss=0.3549 val_loss=0.0000 scale=4.0000 norm=2.9652
[iter 400] loss=0.0784 val_loss=0.0000 scale=4.0000 norm=2.9497
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1127 val_loss=0.0000 scale=2.0000 norm=1.6676
[iter 200] loss=0.9591 val_loss=0.0000 scale=2.0000 norm=1.6675
[iter 300] loss=0.7648 val_loss=0.0000 scale=4.0000 norm=3.3267
[iter 400] loss=0.5390 val_loss=0.0000 scale=8.0000 norm=6.6679
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1225 val_loss=0.0000 scale=2.0000 norm=1.6698
[iter 200] loss=0.9566 val_loss=0.0000 scale=4.0000 norm=3.3073
[iter 300] loss=0.7177 val_loss=0.0000 scale=4.0000 norm=3.2602
[iter 400] loss=0.5627 val_loss=0.0000 scale=4.0000 norm=3.2174
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5194 val_loss=0.0000 scale=2.0000 norm=1.1262
[iter 200] loss=0.0249 val_loss=0.0000 scale=2.0000 norm=1.1797
[iter 300] loss=-0.7792 val_loss=0.0000 scale=4.0000 norm=2.3746
[iter 400] loss=-1.4727 val_loss=0.0000 scale=4.0000 norm=2.1874
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0380 val_loss=0.0000 scale=2.0000 norm=1.6206
[iter 200] loss=0.7701 val_loss=0.0000 scale=4.0000 norm=3.2244
[iter 300] loss=0.4138 val_loss=0.0000 scale=4.0000 norm=3.1751
[iter 400] loss=0.2124 val_loss=0.0000 scale=4.0000 norm=3.1040
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0629 val_loss=0.0000 scale=2.0000 norm=1.6418
[iter 200] loss=0.8452 val_loss=0.0000 scale=2.0000 norm=1.6456
[iter 300] loss=0.5348 val_loss=0.0000 scale=4.0000 norm=3.2395
[iter 400] loss=0.3212 val_loss=0.0000 scale=4.0000 norm=3.2389
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9248 val_loss=0.0000 scale=2.0000 norm=1.5647
[iter 200] loss=0.5925 val_loss=0.0000 scale=4.0000 norm=3.0308
[iter 300] loss=0.2169 val_loss=0.0000 scale=4.0000 norm=2.9874
[iter 400] loss=-0.1063 val_loss=0.0000 scale=4.0000 norm=2.9438
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0368 val_loss=0.0000 scale=2.0000 norm=1.5865
[iter 200] loss=0.8152 val_loss=0.0000 scale=2.0000 norm=1.5836
[iter 300] loss=0.4793 val_loss=0.0000 scale=4.0000 norm=3.1241
[iter 400] loss=0.2488 val_loss=0.0000 scale=4.0000 norm=3.1372
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0090 val_loss=0.0000 scale=2.0000 norm=1.6955
[iter 200] loss=0.6999 val_loss=0.0000 scale=4.0000 norm=3.3966
[iter 300] loss=0.3105 val_loss=0.0000 scale=8.0000 norm=6.6500
[iter 400] loss=-0.4275 val_loss=0.0000 scale=16.0000 norm=13.3003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8948 val_loss=0.0000 scale=2.0000 norm=1.5238
[iter 200] loss=0.6463 val_loss=0.0000 scale=4.0000 norm=3.1407
[iter 300] loss=0.2267 val_loss=0.0000 scale=4.0000 norm=3.1681
[iter 400] loss=-0.4808 val_loss=0.0000 scale=8.0000 norm=6.3384
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0552 val_loss=0.0000 scale=2.0000 norm=1.6623
[iter 200] loss=0.8350 val_loss=0.0000 scale=2.0000 norm=1.6527
[iter 300] loss=0.5659 val_loss=0.0000 scale=4.0000 norm=3.2606
[iter 400] loss=0.3882 val_loss=0.0000 scale=4.0000 norm=3.2287
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0282 val_loss=0.0000 scale=2.0000 norm=1.6105
[iter 200] loss=0.7931 val_loss=0.0000 scale=4.0000 norm=3.1906
[iter 300] loss=0.4819 val_loss=0.0000 scale=4.0000 norm=3.1135
[iter 400] loss=0.3243 val_loss=0.0000 scale=4.0000 norm=3.0962
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9684 val_loss=0.0000 scale=2.0000 norm=1.5356
[iter 200] loss=0.7225 val_loss=0.0000 scale=4.0000 norm=3.0516
[iter 300] loss=0.3744 val_loss=0.0000 scale=4.0000 norm=3.0093
[iter 400] loss=0.1983 val_loss=0.0000 scale=4.0000 norm=2.9585
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0019 val_loss=0.0000 scale=2.0000 norm=1.5853
[iter 200] loss=0.7514 val_loss=0.0000 scale=2.0000 norm=1.5543
[iter 300] loss=0.5019 val_loss=0.0000 scale=4.0000 norm=3.0819
[iter 400] loss=0.2251 val_loss=0.0000 scale=4.0000 norm=2.9930
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0027 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 200] loss=0.7550 val_loss=0.0000 scale=4.0000 norm=3.1418
[iter 300] loss=0.3803 val_loss=0.0000 scale=4.0000 norm=3.1278
[iter 400] loss=0.1613 val_loss=0.0000 scale=4.0000 norm=3.1831
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0547 val_loss=0.0000 scale=2.0000 norm=1.6349
[iter 200] loss=0.7421 val_loss=0.0000 scale=4.0000 norm=3.2624
[iter 300] loss=0.4679 val_loss=0.0000 scale=4.0000 norm=3.2171
[iter 400] loss=0.3783 val_loss=0.0000 scale=2.0000 norm=1.5822
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9570 val_loss=0.0000 scale=2.0000 norm=1.6008
[iter 200] loss=0.6796 val_loss=0.0000 scale=4.0000 norm=3.1544
[iter 300] loss=0.3601 val_loss=0.0000 scale=4.0000 norm=3.1578
[iter 400] loss=0.0986 val_loss=0.0000 scale=4.0000 norm=3.1027
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4414 val_loss=0.0000 scale=2.0000 norm=0.9961
[iter 200] loss=-0.0071 val_loss=0.0000 scale=2.0000 norm=1.0106
[iter 300] loss=-0.6010 val_loss=0.0000 scale=4.0000 norm=2.0622
[iter 400] loss=-1.6330 val_loss=0.0000 scale=8.0000 norm=4.1286
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9787 val_loss=0.0000 scale=2.0000 norm=1.5627
[iter 200] loss=0.7150 val_loss=0.0000 scale=2.0000 norm=1.5586
[iter 300] loss=0.3237 val_loss=0.0000 scale=4.0000 norm=3.0574
[iter 400] loss=0.0420 val_loss=0.0000 scale=4.0000 norm=3.0352
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0198 val_loss=0.0000 scale=2.0000 norm=1.5928
[iter 200] loss=0.7186 val_loss=0.0000 scale=4.0000 norm=3.1547
[iter 300] loss=0.3578 val_loss=0.0000 scale=4.0000 norm=3.0457
[iter 400] loss=0.1457 val_loss=0.0000 scale=4.0000 norm=2.9889
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6033 val_loss=0.0000 scale=2.0000 norm=1.2585
[iter 200] loss=0.1043 val_loss=0.0000 scale=4.0000 norm=2.5737
[iter 300] loss=-0.6953 val_loss=0.0000 scale=4.0000 norm=2.5876
[iter 400] loss=-1.9032 val_loss=0.0000 scale=8.0000 norm=5.1760
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0667 val_loss=0.0000 scale=2.0000 norm=1.6221
[iter 200] loss=0.8783 val_loss=0.0000 scale=2.0000 norm=1.6128
[iter 300] loss=0.6522 val_loss=0.0000 scale=4.0000 norm=3.1930
[iter 400] loss=0.4708 val_loss=0.0000 scale=4.0000 norm=3.1352
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9328 val_loss=0.0000 scale=2.0000 norm=1.6482
[iter 200] loss=0.5598 val_loss=0.0000 scale=4.0000 norm=3.3266
[iter 300] loss=0.0037 val_loss=0.0000 scale=4.0000 norm=3.3263
[iter 400] loss=-0.4959 val_loss=0.0000 scale=4.0000 norm=3.2315
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0442 val_loss=0.0000 scale=2.0000 norm=1.6685
[iter 200] loss=0.6848 val_loss=0.0000 scale=4.0000 norm=3.2651
[iter 300] loss=0.3238 val_loss=0.0000 scale=4.0000 norm=3.2085
[iter 400] loss=0.0513 val_loss=0.0000 scale=4.0000 norm=3.1156
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8231 val_loss=0.0000 scale=2.0000 norm=1.5022
[iter 200] loss=0.5034 val_loss=0.0000 scale=2.0000 norm=1.5250
[iter 300] loss=0.0686 val_loss=0.0000 scale=4.0000 norm=2.9556
[iter 400] loss=-0.3466 val_loss=0.0000 scale=4.0000 norm=2.9464
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0365 val_loss=0.0000 scale=2.0000 norm=1.6108
[iter 200] loss=0.8307 val_loss=0.0000 scale=2.0000 norm=1.5828
[iter 300] loss=0.5468 val_loss=0.0000 scale=4.0000 norm=3.1241
[iter 400] loss=0.3266 val_loss=0.0000 scale=4.0000 norm=3.0659
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8261 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 200] loss=0.3049 val_loss=0.0000 scale=4.0000 norm=3.2241
[iter 300] loss=-0.3026 val_loss=0.0000 scale=4.0000 norm=3.1618
[iter 400] loss=-0.8570 val_loss=0.0000 scale=4.0000 norm=3.0816
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9630 val_loss=0.0000 scale=2.0000 norm=1.6808
[iter 200] loss=0.5682 val_loss=0.0000 scale=4.0000 norm=3.3239
[iter 300] loss=-0.0770 val_loss=0.0000 scale=8.0000 norm=6.6579
[iter 400] loss=-1.3470 val_loss=0.0000 scale=16.0000 norm=13.3158
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9892 val_loss=0.0000 scale=2.0000 norm=1.5515
[iter 200] loss=0.7523 val_loss=0.0000 scale=4.0000 norm=3.0695
[iter 300] loss=0.4240 val_loss=0.0000 scale=4.0000 norm=3.1078
[iter 400] loss=0.1189 val_loss=0.0000 scale=4.0000 norm=3.0727
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9891 val_loss=0.0000 scale=2.0000 norm=1.5688
[iter 200] loss=0.7504 val_loss=0.0000 scale=2.0000 norm=1.5655
[iter 300] loss=0.4058 val_loss=0.0000 scale=4.0000 norm=3.1035
[iter 400] loss=0.2250 val_loss=0.0000 scale=4.0000 norm=3.1221
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0645 val_loss=0.0000 scale=2.0000 norm=1.6672
[iter 200] loss=0.8695 val_loss=0.0000 scale=2.0000 norm=1.6755
[iter 300] loss=0.6422 val_loss=0.0000 scale=2.0000 norm=1.6530
[iter 400] loss=0.5438 val_loss=0.0000 scale=4.0000 norm=3.2830
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9746 val_loss=0.0000 scale=2.0000 norm=1.5500
[iter 200] loss=0.7339 val_loss=0.0000 scale=2.0000 norm=1.5242
[iter 300] loss=0.4522 val_loss=0.0000 scale=4.0000 norm=3.0194
[iter 400] loss=0.2185 val_loss=0.0000 scale=4.0000 norm=2.9638
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9169 val_loss=0.0000 scale=2.0000 norm=1.6223
[iter 200] loss=0.6009 val_loss=0.0000 scale=4.0000 norm=3.3547
[iter 300] loss=0.1510 val_loss=0.0000 scale=4.0000 norm=3.3575
[iter 400] loss=-0.2138 val_loss=0.0000 scale=4.0000 norm=3.2323
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8419 val_loss=0.0000 scale=2.0000 norm=1.6270
[iter 200] loss=0.3599 val_loss=0.0000 scale=4.0000 norm=3.3314
[iter 300] loss=-0.4441 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-1.9801 val_loss=0.0000 scale=16.0000 norm=13.3436
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0404 val_loss=0.0000 scale=2.0000 norm=1.6163
[iter 200] loss=0.8123 val_loss=0.0000 scale=2.0000 norm=1.6039
[iter 300] loss=0.4608 val_loss=0.0000 scale=4.0000 norm=3.1804
[iter 400] loss=0.2093 val_loss=0.0000 scale=4.0000 norm=3.1428
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9406 val_loss=0.0000 scale=2.0000 norm=1.5695
[iter 200] loss=0.6877 val_loss=0.0000 scale=2.0000 norm=1.5610
[iter 300] loss=0.3523 val_loss=0.0000 scale=4.0000 norm=3.1183
[iter 400] loss=0.0627 val_loss=0.0000 scale=4.0000 norm=3.0881
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0381 val_loss=0.0000 scale=2.0000 norm=1.6124
[iter 200] loss=0.7783 val_loss=0.0000 scale=4.0000 norm=3.1850
[iter 300] loss=0.4009 val_loss=0.0000 scale=4.0000 norm=3.1403
[iter 400] loss=0.1592 val_loss=0.0000 scale=4.0000 norm=3.1300
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9109 val_loss=0.0000 scale=2.0000 norm=1.5896
[iter 200] loss=0.5569 val_loss=0.0000 scale=4.0000 norm=3.0837
[iter 300] loss=0.1225 val_loss=0.0000 scale=8.0000 norm=6.1637
[iter 400] loss=-0.3150 val_loss=0.0000 scale=4.0000 norm=3.0533
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9408 val_loss=0.0000 scale=2.0000 norm=1.5947
[iter 200] loss=0.5937 val_loss=0.0000 scale=4.0000 norm=3.1182
[iter 300] loss=0.1453 val_loss=0.0000 scale=4.0000 norm=3.0941
[iter 400] loss=-0.1882 val_loss=0.0000 scale=4.0000 norm=3.0494
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0811 val_loss=0.0000 scale=2.0000 norm=1.6175
[iter 200] loss=0.9032 val_loss=0.0000 scale=2.0000 norm=1.5954
[iter 300] loss=0.6389 val_loss=0.0000 scale=4.0000 norm=3.1633
[iter 400] loss=0.4416 val_loss=0.0000 scale=4.0000 norm=3.0738
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0134 val_loss=0.0000 scale=2.0000 norm=1.5848
[iter 200] loss=0.7981 val_loss=0.0000 scale=2.0000 norm=1.5700
[iter 300] loss=0.5810 val_loss=0.0000 scale=4.0000 norm=3.1204
[iter 400] loss=0.3718 val_loss=0.0000 scale=4.0000 norm=3.0941
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0022 val_loss=0.0000 scale=2.0000 norm=1.5948
[iter 200] loss=0.7385 val_loss=0.0000 scale=2.0000 norm=1.5891
[iter 300] loss=0.4068 val_loss=0.0000 scale=4.0000 norm=3.1717
[iter 400] loss=0.1628 val_loss=0.0000 scale=4.0000 norm=3.0600
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0141 val_loss=0.0000 scale=2.0000 norm=1.5901
[iter 200] loss=0.7699 val_loss=0.0000 scale=4.0000 norm=3.1727
[iter 300] loss=0.4528 val_loss=0.0000 scale=4.0000 norm=3.0976
[iter 400] loss=0.3067 val_loss=0.0000 scale=4.0000 norm=3.1251
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8114 val_loss=0.0000 scale=2.0000 norm=1.5883
[iter 200] loss=0.3727 val_loss=0.0000 scale=4.0000 norm=3.2620
[iter 300] loss=-0.2714 val_loss=0.0000 scale=4.0000 norm=3.2600
[iter 400] loss=-0.9470 val_loss=0.0000 scale=8.0000 norm=6.2033
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6583 val_loss=0.0000 scale=2.0000 norm=1.2787
[iter 200] loss=0.4069 val_loss=0.0000 scale=2.0000 norm=1.3032
[iter 300] loss=0.1949 val_loss=0.0000 scale=4.0000 norm=2.6841
[iter 400] loss=-0.0507 val_loss=0.0000 scale=8.0000 norm=5.3463
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6195 val_loss=0.0000 scale=2.0000 norm=1.1033
[iter 200] loss=0.2836 val_loss=0.0000 scale=4.0000 norm=2.2032
[iter 300] loss=-0.1897 val_loss=0.0000 scale=4.0000 norm=2.0853
[iter 400] loss=-0.9023 val_loss=0.0000 scale=8.0000 norm=4.1393
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0663 val_loss=0.0000 scale=2.0000 norm=1.6542
[iter 200] loss=0.8395 val_loss=0.0000 scale=2.0000 norm=1.6498
[iter 300] loss=0.4812 val_loss=0.0000 scale=4.0000 norm=3.2558
[iter 400] loss=0.2548 val_loss=0.0000 scale=2.0000 norm=1.6070
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9486 val_loss=0.0000 scale=2.0000 norm=1.5458
[iter 200] loss=0.6868 val_loss=0.0000 scale=2.0000 norm=1.5436
[iter 300] loss=0.2986 val_loss=0.0000 scale=4.0000 norm=3.0611
[iter 400] loss=-0.0191 val_loss=0.0000 scale=4.0000 norm=2.9909
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0475 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 200] loss=0.8308 val_loss=0.0000 scale=4.0000 norm=3.4606
[iter 300] loss=0.4707 val_loss=0.0000 scale=8.0000 norm=6.9339
[iter 400] loss=-0.2373 val_loss=0.0000 scale=16.0000 norm=13.8680
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9997 val_loss=0.0000 scale=2.0000 norm=1.5897
[iter 200] loss=0.7006 val_loss=0.0000 scale=4.0000 norm=3.1609
[iter 300] loss=0.3282 val_loss=0.0000 scale=4.0000 norm=3.1272
[iter 400] loss=0.1072 val_loss=0.0000 scale=4.0000 norm=3.1209
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9877 val_loss=0.0000 scale=2.0000 norm=1.5820
[iter 200] loss=0.7561 val_loss=0.0000 scale=2.0000 norm=1.5482
[iter 300] loss=0.4498 val_loss=0.0000 scale=4.0000 norm=3.0604
[iter 400] loss=0.2939 val_loss=0.0000 scale=4.0000 norm=3.0526
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9231 val_loss=0.0000 scale=2.0000 norm=1.6326
[iter 200] loss=0.5875 val_loss=0.0000 scale=4.0000 norm=3.3123
[iter 300] loss=0.0688 val_loss=0.0000 scale=4.0000 norm=3.2920
[iter 400] loss=-0.4365 val_loss=0.0000 scale=4.0000 norm=3.2797
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.5873
[iter 200] loss=0.7536 val_loss=0.0000 scale=2.0000 norm=1.5829
[iter 300] loss=0.3960 val_loss=0.0000 scale=4.0000 norm=3.1208
[iter 400] loss=0.1354 val_loss=0.0000 scale=4.0000 norm=3.0919
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0171 val_loss=0.0000 scale=2.0000 norm=1.6004
[iter 200] loss=0.7908 val_loss=0.0000 scale=2.0000 norm=1.5826
[iter 300] loss=0.4811 val_loss=0.0000 scale=4.0000 norm=3.1395
[iter 400] loss=0.2598 val_loss=0.0000 scale=4.0000 norm=3.0711
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8630 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 200] loss=0.3981 val_loss=0.0000 scale=4.0000 norm=3.4013
[iter 300] loss=-0.1294 val_loss=0.0000 scale=4.0000 norm=3.3008
[iter 400] loss=-0.9348 val_loss=0.0000 scale=16.0000 norm=12.9102
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.3175
[iter 200] loss=-0.2334 val_loss=0.0000 scale=2.0000 norm=1.2639
[iter 300] loss=-0.8684 val_loss=0.0000 scale=4.0000 norm=2.5376
[iter 400] loss=-1.7033 val_loss=0.0000 scale=8.0000 norm=4.9721
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0168 val_loss=0.0000 scale=2.0000 norm=1.5950
[iter 200] loss=0.7553 val_loss=0.0000 scale=4.0000 norm=3.1431
[iter 300] loss=0.3350 val_loss=0.0000 scale=4.0000 norm=3.1507
[iter 400] loss=-0.4143 val_loss=0.0000 scale=8.0000 norm=6.2995
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0496 val_loss=0.0000 scale=2.0000 norm=1.6596
[iter 200] loss=0.7332 val_loss=0.0000 scale=4.0000 norm=3.2501
[iter 300] loss=0.4585 val_loss=0.0000 scale=4.0000 norm=3.2304
[iter 400] loss=0.2540 val_loss=0.0000 scale=4.0000 norm=3.1703
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8682 val_loss=0.0000 scale=2.0000 norm=1.5243
[iter 200] loss=0.5385 val_loss=0.0000 scale=2.0000 norm=1.5137
[iter 300] loss=0.1389 val_loss=0.0000 scale=4.0000 norm=2.9787
[iter 400] loss=-0.2579 val_loss=0.0000 scale=4.0000 norm=2.9285
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0360 val_loss=0.0000 scale=2.0000 norm=1.6104
[iter 200] loss=0.8107 val_loss=0.0000 scale=4.0000 norm=3.1824
[iter 300] loss=0.4889 val_loss=0.0000 scale=4.0000 norm=3.1218
[iter 400] loss=0.2805 val_loss=0.0000 scale=4.0000 norm=3.1035
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8651 val_loss=0.0000 scale=2.0000 norm=1.5890
[iter 200] loss=0.4074 val_loss=0.0000 scale=4.0000 norm=3.1138
[iter 300] loss=-0.2064 val_loss=0.0000 scale=8.0000 norm=6.2682
[iter 400] loss=-0.8224 val_loss=0.0000 scale=4.0000 norm=3.0963
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0299 val_loss=0.0000 scale=2.0000 norm=1.6090
[iter 200] loss=0.7888 val_loss=0.0000 scale=4.0000 norm=3.1757
[iter 300] loss=0.4512 val_loss=0.0000 scale=4.0000 norm=3.1337
[iter 400] loss=0.2443 val_loss=0.0000 scale=4.0000 norm=3.1534
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0587 val_loss=0.0000 scale=2.0000 norm=1.5980
[iter 200] loss=0.8581 val_loss=0.0000 scale=2.0000 norm=1.5800
[iter 300] loss=0.5592 val_loss=0.0000 scale=4.0000 norm=3.1845
[iter 400] loss=0.2300 val_loss=0.0000 scale=8.0000 norm=6.2879
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0356 val_loss=0.0000 scale=2.0000 norm=1.5976
[iter 200] loss=0.8314 val_loss=0.0000 scale=2.0000 norm=1.5827
[iter 300] loss=0.5719 val_loss=0.0000 scale=4.0000 norm=3.1285
[iter 400] loss=0.3850 val_loss=0.0000 scale=4.0000 norm=3.0975

------------------------------------------------------------
Sender: LSF System <lsfadmin@c014n02>
Subject: Job 853004: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:38 2024
Job was executed on host(s) <4*c014n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:40 2024
                            <4*c016n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:44:40 2024
Terminated at Tue Oct 22 17:54:03 2024
Results reported at Tue Oct 22 17:54:03 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "Monomer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2594.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.43 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   583 sec.
    Turnaround time :                            565 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.

RRU Dimer
Filename: (Mordred)_NGB_hypOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(Mordred)_NGB_hypOFF_generalizability_scores.json
Done Saving scores!
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0440 val_loss=0.0000 scale=2.0000 norm=1.6195
[iter 200] loss=0.8340 val_loss=0.0000 scale=2.0000 norm=1.6188
[iter 300] loss=0.5445 val_loss=0.0000 scale=4.0000 norm=3.2469
[iter 400] loss=0.3601 val_loss=0.0000 scale=4.0000 norm=3.1678
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0028 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 200] loss=0.7369 val_loss=0.0000 scale=4.0000 norm=3.1200
[iter 300] loss=0.3652 val_loss=0.0000 scale=4.0000 norm=3.0196
[iter 400] loss=0.1183 val_loss=0.0000 scale=4.0000 norm=3.0040
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0154 val_loss=0.0000 scale=2.0000 norm=1.5844
[iter 200] loss=0.7625 val_loss=0.0000 scale=4.0000 norm=3.1291
[iter 300] loss=0.4380 val_loss=0.0000 scale=4.0000 norm=3.1041
[iter 400] loss=0.2503 val_loss=0.0000 scale=4.0000 norm=3.1596
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0141 val_loss=0.0000 scale=2.0000 norm=1.5935
[iter 200] loss=0.7975 val_loss=0.0000 scale=2.0000 norm=1.5918
[iter 300] loss=0.4524 val_loss=0.0000 scale=4.0000 norm=3.1435
[iter 400] loss=0.2384 val_loss=0.0000 scale=4.0000 norm=3.1633
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9748 val_loss=0.0000 scale=2.0000 norm=1.5568
[iter 200] loss=0.7171 val_loss=0.0000 scale=4.0000 norm=3.0733
[iter 300] loss=0.3835 val_loss=0.0000 scale=4.0000 norm=3.0435
[iter 400] loss=0.1790 val_loss=0.0000 scale=8.0000 norm=6.0693
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0347 val_loss=0.0000 scale=2.0000 norm=1.7342
[iter 200] loss=0.7825 val_loss=0.0000 scale=4.0000 norm=3.4531
[iter 300] loss=0.4117 val_loss=0.0000 scale=4.0000 norm=3.4187
[iter 400] loss=0.0816 val_loss=0.0000 scale=4.0000 norm=3.3607
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9895 val_loss=0.0000 scale=2.0000 norm=1.5689
[iter 200] loss=0.7426 val_loss=0.0000 scale=2.0000 norm=1.5539
[iter 300] loss=0.4054 val_loss=0.0000 scale=4.0000 norm=3.0597
[iter 400] loss=0.1912 val_loss=0.0000 scale=4.0000 norm=3.0625
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0592 val_loss=0.0000 scale=2.0000 norm=1.6575
[iter 200] loss=0.7249 val_loss=0.0000 scale=4.0000 norm=3.2824
[iter 300] loss=0.3230 val_loss=0.0000 scale=4.0000 norm=3.2077
[iter 400] loss=-0.0296 val_loss=0.0000 scale=8.0000 norm=6.3564
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7865 val_loss=0.0000 scale=2.0000 norm=1.4126
[iter 200] loss=0.5550 val_loss=0.0000 scale=4.0000 norm=2.9931
[iter 300] loss=0.2284 val_loss=0.0000 scale=4.0000 norm=3.0376
[iter 400] loss=-0.3512 val_loss=0.0000 scale=8.0000 norm=6.0770
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9819 val_loss=0.0000 scale=2.0000 norm=1.5689
[iter 200] loss=0.7010 val_loss=0.0000 scale=4.0000 norm=3.0625
[iter 300] loss=0.3616 val_loss=0.0000 scale=4.0000 norm=3.0039
[iter 400] loss=0.1101 val_loss=0.0000 scale=4.0000 norm=2.9518
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0288 val_loss=0.0000 scale=2.0000 norm=1.5991
[iter 200] loss=0.7682 val_loss=0.0000 scale=4.0000 norm=3.1975
[iter 300] loss=0.4151 val_loss=0.0000 scale=4.0000 norm=3.1519
[iter 400] loss=0.1949 val_loss=0.0000 scale=4.0000 norm=3.1519
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9992 val_loss=0.0000 scale=2.0000 norm=1.5835
[iter 200] loss=0.7295 val_loss=0.0000 scale=4.0000 norm=3.1373
[iter 300] loss=0.3969 val_loss=0.0000 scale=4.0000 norm=3.0634
[iter 400] loss=0.2196 val_loss=0.0000 scale=4.0000 norm=3.0917
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9814 val_loss=0.0000 scale=2.0000 norm=1.6240
[iter 200] loss=0.6828 val_loss=0.0000 scale=2.0000 norm=1.6013
[iter 300] loss=0.1828 val_loss=0.0000 scale=4.0000 norm=3.1516
[iter 400] loss=-0.2323 val_loss=0.0000 scale=4.0000 norm=3.0947
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9079 val_loss=0.0000 scale=2.0000 norm=1.5576
[iter 200] loss=0.5559 val_loss=0.0000 scale=4.0000 norm=3.0502
[iter 300] loss=0.1316 val_loss=0.0000 scale=4.0000 norm=2.9399
[iter 400] loss=-0.1486 val_loss=0.0000 scale=4.0000 norm=2.8143
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8987 val_loss=0.0000 scale=2.0000 norm=1.6413
[iter 200] loss=0.2516 val_loss=0.0000 scale=4.0000 norm=3.1660
[iter 300] loss=-0.3210 val_loss=0.0000 scale=8.0000 norm=5.9007
[iter 400] loss=-1.3860 val_loss=0.0000 scale=16.0000 norm=11.6727
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9174 val_loss=0.0000 scale=2.0000 norm=1.6220
[iter 200] loss=0.6245 val_loss=0.0000 scale=4.0000 norm=3.3517
[iter 300] loss=0.1738 val_loss=0.0000 scale=4.0000 norm=3.3590
[iter 400] loss=-0.1988 val_loss=0.0000 scale=4.0000 norm=3.2402
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0210 val_loss=0.0000 scale=2.0000 norm=1.6026
[iter 200] loss=0.7409 val_loss=0.0000 scale=4.0000 norm=3.1731
[iter 300] loss=0.3731 val_loss=0.0000 scale=4.0000 norm=3.0930
[iter 400] loss=0.0891 val_loss=0.0000 scale=8.0000 norm=6.2106
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0294 val_loss=0.0000 scale=2.0000 norm=1.6206
[iter 200] loss=0.7288 val_loss=0.0000 scale=4.0000 norm=3.2168
[iter 300] loss=0.3800 val_loss=0.0000 scale=4.0000 norm=3.1507
[iter 400] loss=0.1474 val_loss=0.0000 scale=4.0000 norm=3.0942
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9394 val_loss=0.0000 scale=2.0000 norm=1.5696
[iter 200] loss=0.6737 val_loss=0.0000 scale=4.0000 norm=3.1225
[iter 300] loss=0.3007 val_loss=0.0000 scale=4.0000 norm=3.1162
[iter 400] loss=0.0127 val_loss=0.0000 scale=4.0000 norm=3.0479
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0405 val_loss=0.0000 scale=2.0000 norm=1.6425
[iter 200] loss=0.8080 val_loss=0.0000 scale=4.0000 norm=3.1988
[iter 300] loss=0.5715 val_loss=0.0000 scale=4.0000 norm=3.1636
[iter 400] loss=0.3576 val_loss=0.0000 scale=8.0000 norm=6.2134
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9861 val_loss=0.0000 scale=2.0000 norm=1.5720
[iter 200] loss=0.7037 val_loss=0.0000 scale=4.0000 norm=3.1045
[iter 300] loss=0.3498 val_loss=0.0000 scale=4.0000 norm=3.0575
[iter 400] loss=0.1401 val_loss=0.0000 scale=4.0000 norm=3.0456
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0065 val_loss=0.0000 scale=2.0000 norm=1.5948
[iter 200] loss=0.7557 val_loss=0.0000 scale=4.0000 norm=3.1408
[iter 300] loss=0.4511 val_loss=0.0000 scale=4.0000 norm=3.1067
[iter 400] loss=0.2260 val_loss=0.0000 scale=4.0000 norm=3.0604
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0305 val_loss=0.0000 scale=2.0000 norm=1.6105
[iter 200] loss=0.7974 val_loss=0.0000 scale=4.0000 norm=3.1965
[iter 300] loss=0.4638 val_loss=0.0000 scale=4.0000 norm=3.1439
[iter 400] loss=0.2743 val_loss=0.0000 scale=4.0000 norm=3.1304
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0057 val_loss=0.0000 scale=2.0000 norm=1.5930
[iter 200] loss=0.7086 val_loss=0.0000 scale=4.0000 norm=3.1627
[iter 300] loss=0.3956 val_loss=0.0000 scale=4.0000 norm=3.0862
[iter 400] loss=0.2355 val_loss=0.0000 scale=4.0000 norm=3.1038
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0622 val_loss=0.0000 scale=2.0000 norm=1.6500
[iter 200] loss=0.8131 val_loss=0.0000 scale=4.0000 norm=3.2911
[iter 300] loss=0.4777 val_loss=0.0000 scale=4.0000 norm=3.1972
[iter 400] loss=0.2722 val_loss=0.0000 scale=4.0000 norm=3.1402
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8960 val_loss=0.0000 scale=2.0000 norm=1.5817
[iter 200] loss=0.6369 val_loss=0.0000 scale=2.0000 norm=1.6253
[iter 300] loss=0.2988 val_loss=0.0000 scale=4.0000 norm=3.1303
[iter 400] loss=-0.0845 val_loss=0.0000 scale=8.0000 norm=6.3115
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9877 val_loss=0.0000 scale=2.0000 norm=1.5648
[iter 200] loss=0.7513 val_loss=0.0000 scale=4.0000 norm=3.0906
[iter 300] loss=0.4195 val_loss=0.0000 scale=4.0000 norm=3.0608
[iter 400] loss=0.2379 val_loss=0.0000 scale=4.0000 norm=3.0279
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9782 val_loss=0.0000 scale=2.0000 norm=1.6013
[iter 200] loss=0.6509 val_loss=0.0000 scale=4.0000 norm=3.1869
[iter 300] loss=0.2999 val_loss=0.0000 scale=4.0000 norm=3.1076
[iter 400] loss=0.1087 val_loss=0.0000 scale=4.0000 norm=3.0863
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0296 val_loss=0.0000 scale=2.0000 norm=1.6023
[iter 200] loss=0.7783 val_loss=0.0000 scale=4.0000 norm=3.1986
[iter 300] loss=0.4437 val_loss=0.0000 scale=4.0000 norm=3.1622
[iter 400] loss=0.2555 val_loss=0.0000 scale=4.0000 norm=3.1502
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1225 val_loss=0.0000 scale=2.0000 norm=1.6698
[iter 200] loss=0.9593 val_loss=0.0000 scale=4.0000 norm=3.3062
[iter 300] loss=0.7196 val_loss=0.0000 scale=4.0000 norm=3.2617
[iter 400] loss=0.5638 val_loss=0.0000 scale=4.0000 norm=3.2196
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0442 val_loss=0.0000 scale=2.0000 norm=1.6685
[iter 200] loss=0.6848 val_loss=0.0000 scale=4.0000 norm=3.2652
[iter 300] loss=0.3238 val_loss=0.0000 scale=4.0000 norm=3.2091
[iter 400] loss=0.0511 val_loss=0.0000 scale=4.0000 norm=3.1165
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4758 val_loss=0.0000 scale=2.0000 norm=1.0500
[iter 200] loss=0.0729 val_loss=0.0000 scale=2.0000 norm=1.1270
[iter 300] loss=-0.3956 val_loss=0.0000 scale=4.0000 norm=2.2556
[iter 400] loss=-1.1922 val_loss=0.0000 scale=8.0000 norm=4.5003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7510 val_loss=0.0000 scale=2.0000 norm=1.2707
[iter 200] loss=0.3824 val_loss=0.0000 scale=4.0000 norm=2.5426
[iter 300] loss=-0.2179 val_loss=0.0000 scale=4.0000 norm=2.5505
[iter 400] loss=-1.3699 val_loss=0.0000 scale=8.0000 norm=5.1014
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.5712
[iter 200] loss=0.7398 val_loss=0.0000 scale=2.0000 norm=1.5491
[iter 300] loss=0.3991 val_loss=0.0000 scale=4.0000 norm=3.1094
[iter 400] loss=0.1205 val_loss=0.0000 scale=4.0000 norm=3.0587
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8311 val_loss=0.0000 scale=2.0000 norm=1.4287
[iter 200] loss=0.5613 val_loss=0.0000 scale=2.0000 norm=1.4461
[iter 300] loss=0.2564 val_loss=0.0000 scale=4.0000 norm=2.8557
[iter 400] loss=-0.0028 val_loss=0.0000 scale=4.0000 norm=2.7773
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0239 val_loss=0.0000 scale=2.0000 norm=1.6114
[iter 200] loss=0.8030 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 300] loss=0.4886 val_loss=0.0000 scale=4.0000 norm=3.1500
[iter 400] loss=0.3060 val_loss=0.0000 scale=4.0000 norm=3.0945
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9987 val_loss=0.0000 scale=2.0000 norm=1.5765
[iter 200] loss=0.7728 val_loss=0.0000 scale=4.0000 norm=3.0783
[iter 300] loss=0.4665 val_loss=0.0000 scale=4.0000 norm=3.0351
[iter 400] loss=0.3354 val_loss=0.0000 scale=4.0000 norm=2.9993
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0397 val_loss=0.0000 scale=2.0000 norm=1.6166
[iter 200] loss=0.8065 val_loss=0.0000 scale=4.0000 norm=3.2077
[iter 300] loss=0.4604 val_loss=0.0000 scale=4.0000 norm=3.1796
[iter 400] loss=0.2398 val_loss=0.0000 scale=4.0000 norm=3.1904
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9942 val_loss=0.0000 scale=2.0000 norm=1.5933
[iter 200] loss=0.6794 val_loss=0.0000 scale=4.0000 norm=3.1506
[iter 300] loss=0.2404 val_loss=0.0000 scale=4.0000 norm=3.1186
[iter 400] loss=-0.0510 val_loss=0.0000 scale=4.0000 norm=3.0401
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0317 val_loss=0.0000 scale=2.0000 norm=1.6446
[iter 200] loss=0.7648 val_loss=0.0000 scale=4.0000 norm=3.3176
[iter 300] loss=0.2648 val_loss=0.0000 scale=4.0000 norm=3.3251
[iter 400] loss=-0.6502 val_loss=0.0000 scale=8.0000 norm=6.6505
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0191 val_loss=0.0000 scale=2.0000 norm=1.6146
[iter 200] loss=0.7643 val_loss=0.0000 scale=2.0000 norm=1.6010
[iter 300] loss=0.4154 val_loss=0.0000 scale=4.0000 norm=3.1831
[iter 400] loss=0.2033 val_loss=0.0000 scale=4.0000 norm=3.0853
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9763 val_loss=0.0000 scale=2.0000 norm=1.5742
[iter 200] loss=0.7206 val_loss=0.0000 scale=2.0000 norm=1.5388
[iter 300] loss=0.3890 val_loss=0.0000 scale=4.0000 norm=3.0144
[iter 400] loss=0.2038 val_loss=0.0000 scale=4.0000 norm=2.9304
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7884 val_loss=0.0000 scale=2.0000 norm=1.5481
[iter 200] loss=0.2676 val_loss=0.0000 scale=4.0000 norm=2.9872
[iter 300] loss=-0.2780 val_loss=0.0000 scale=4.0000 norm=2.8767
[iter 400] loss=-0.6831 val_loss=0.0000 scale=4.0000 norm=2.8583
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.3175
[iter 200] loss=-0.2334 val_loss=0.0000 scale=2.0000 norm=1.2639
[iter 300] loss=-0.8684 val_loss=0.0000 scale=4.0000 norm=2.5376
[iter 400] loss=-1.7033 val_loss=0.0000 scale=8.0000 norm=4.9721
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7418 val_loss=0.0000 scale=2.0000 norm=1.4644
[iter 200] loss=0.3785 val_loss=0.0000 scale=4.0000 norm=3.0486
[iter 300] loss=-0.1644 val_loss=0.0000 scale=4.0000 norm=3.0564
[iter 400] loss=-0.9749 val_loss=0.0000 scale=8.0000 norm=5.9930
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0357 val_loss=0.0000 scale=2.0000 norm=1.6185
[iter 200] loss=0.7819 val_loss=0.0000 scale=4.0000 norm=3.2012
[iter 300] loss=0.4120 val_loss=0.0000 scale=4.0000 norm=3.1585
[iter 400] loss=0.1842 val_loss=0.0000 scale=4.0000 norm=3.1319
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8217 val_loss=0.0000 scale=2.0000 norm=1.5032
[iter 200] loss=0.4893 val_loss=0.0000 scale=2.0000 norm=1.5242
[iter 300] loss=0.0067 val_loss=0.0000 scale=4.0000 norm=2.9520
[iter 400] loss=-0.3995 val_loss=0.0000 scale=4.0000 norm=2.9362
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9872 val_loss=0.0000 scale=2.0000 norm=1.5598
[iter 200] loss=0.7589 val_loss=0.0000 scale=2.0000 norm=1.5404
[iter 300] loss=0.4352 val_loss=0.0000 scale=4.0000 norm=3.0617
[iter 400] loss=0.2390 val_loss=0.0000 scale=4.0000 norm=3.0467
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0787 val_loss=0.0000 scale=2.0000 norm=1.6566
[iter 200] loss=0.7911 val_loss=0.0000 scale=4.0000 norm=3.2771
[iter 300] loss=0.4630 val_loss=0.0000 scale=4.0000 norm=3.1887
[iter 400] loss=0.2639 val_loss=0.0000 scale=8.0000 norm=6.2965
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9451 val_loss=0.0000 scale=2.0000 norm=1.5670
[iter 200] loss=0.5559 val_loss=0.0000 scale=4.0000 norm=3.0326
[iter 300] loss=0.1905 val_loss=0.0000 scale=4.0000 norm=2.9489
[iter 400] loss=-0.0203 val_loss=0.0000 scale=2.0000 norm=1.4484
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0022 val_loss=0.0000 scale=2.0000 norm=1.6064
[iter 200] loss=0.7589 val_loss=0.0000 scale=4.0000 norm=3.1717
[iter 300] loss=0.4585 val_loss=0.0000 scale=4.0000 norm=3.1739
[iter 400] loss=0.2333 val_loss=0.0000 scale=4.0000 norm=3.1052
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0285 val_loss=0.0000 scale=2.0000 norm=1.6083
[iter 200] loss=0.7979 val_loss=0.0000 scale=2.0000 norm=1.5772
[iter 300] loss=0.4548 val_loss=0.0000 scale=4.0000 norm=3.0998
[iter 400] loss=0.2510 val_loss=0.0000 scale=8.0000 norm=6.2101
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0103 val_loss=0.0000 scale=2.0000 norm=1.5859
[iter 200] loss=0.7723 val_loss=0.0000 scale=2.0000 norm=1.5649
[iter 300] loss=0.4257 val_loss=0.0000 scale=4.0000 norm=3.0733
[iter 400] loss=0.2498 val_loss=0.0000 scale=4.0000 norm=3.0420
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0218 val_loss=0.0000 scale=2.0000 norm=1.5968
[iter 200] loss=0.8000 val_loss=0.0000 scale=4.0000 norm=3.1679
[iter 300] loss=0.4948 val_loss=0.0000 scale=4.0000 norm=3.1479
[iter 400] loss=0.3214 val_loss=0.0000 scale=4.0000 norm=3.1183
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0152 val_loss=0.0000 scale=2.0000 norm=1.5977
[iter 200] loss=0.7711 val_loss=0.0000 scale=2.0000 norm=1.5918
[iter 300] loss=0.4766 val_loss=0.0000 scale=4.0000 norm=3.1226
[iter 400] loss=0.2817 val_loss=0.0000 scale=4.0000 norm=3.1249
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9883 val_loss=0.0000 scale=2.0000 norm=1.6057
[iter 200] loss=0.6866 val_loss=0.0000 scale=4.0000 norm=3.1849
[iter 300] loss=0.2999 val_loss=0.0000 scale=4.0000 norm=3.1747
[iter 400] loss=-0.0715 val_loss=0.0000 scale=4.0000 norm=3.0833
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8114 val_loss=0.0000 scale=2.0000 norm=1.5877
[iter 200] loss=0.3460 val_loss=0.0000 scale=4.0000 norm=3.2640
[iter 300] loss=-0.2970 val_loss=0.0000 scale=4.0000 norm=3.2578
[iter 400] loss=-0.9899 val_loss=0.0000 scale=8.0000 norm=6.2032
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0507 val_loss=0.0000 scale=2.0000 norm=1.6028
[iter 200] loss=0.8612 val_loss=0.0000 scale=2.0000 norm=1.5896
[iter 300] loss=0.6044 val_loss=0.0000 scale=4.0000 norm=3.1557
[iter 400] loss=0.4711 val_loss=0.0000 scale=4.0000 norm=3.0795
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9729 val_loss=0.0000 scale=2.0000 norm=1.5675
[iter 200] loss=0.6875 val_loss=0.0000 scale=4.0000 norm=3.1083
[iter 300] loss=0.3504 val_loss=0.0000 scale=4.0000 norm=3.0795
[iter 400] loss=0.1672 val_loss=0.0000 scale=4.0000 norm=3.0664
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4187 val_loss=0.0000 scale=2.0000 norm=1.1959
[iter 200] loss=-0.1389 val_loss=0.0000 scale=4.0000 norm=2.2464
[iter 300] loss=-0.8447 val_loss=0.0000 scale=4.0000 norm=2.2487
[iter 400] loss=-2.1188 val_loss=0.0000 scale=8.0000 norm=4.4986
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7202 val_loss=0.0000 scale=2.0000 norm=1.2629
[iter 200] loss=0.4451 val_loss=0.0000 scale=2.0000 norm=1.2815
[iter 300] loss=0.1380 val_loss=0.0000 scale=4.0000 norm=2.5228
[iter 400] loss=-0.0567 val_loss=0.0000 scale=2.0000 norm=1.2181
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9896 val_loss=0.0000 scale=2.0000 norm=1.6080
[iter 200] loss=0.6119 val_loss=0.0000 scale=4.0000 norm=3.2085
[iter 300] loss=0.1242 val_loss=0.0000 scale=4.0000 norm=3.1663
[iter 400] loss=-0.2536 val_loss=0.0000 scale=4.0000 norm=3.0912
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0033 val_loss=0.0000 scale=2.0000 norm=1.6188
[iter 200] loss=0.7280 val_loss=0.0000 scale=4.0000 norm=3.1288
[iter 300] loss=0.4582 val_loss=0.0000 scale=4.0000 norm=3.0381
[iter 400] loss=0.2485 val_loss=0.0000 scale=8.0000 norm=5.9098
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0491 val_loss=0.0000 scale=2.0000 norm=1.6270
[iter 200] loss=0.8306 val_loss=0.0000 scale=4.0000 norm=3.2443
[iter 300] loss=0.5363 val_loss=0.0000 scale=4.0000 norm=3.1788
[iter 400] loss=0.4008 val_loss=0.0000 scale=4.0000 norm=3.1867
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0223 val_loss=0.0000 scale=2.0000 norm=1.6053
[iter 200] loss=0.7936 val_loss=0.0000 scale=4.0000 norm=3.2049
[iter 300] loss=0.4948 val_loss=0.0000 scale=4.0000 norm=3.1932
[iter 400] loss=0.3170 val_loss=0.0000 scale=4.0000 norm=3.1400
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0365 val_loss=0.0000 scale=2.0000 norm=1.5869
[iter 200] loss=0.8083 val_loss=0.0000 scale=2.0000 norm=1.5822
[iter 300] loss=0.4706 val_loss=0.0000 scale=4.0000 norm=3.1218
[iter 400] loss=0.2500 val_loss=0.0000 scale=4.0000 norm=3.1492
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0812 val_loss=0.0000 scale=2.0000 norm=1.6176
[iter 200] loss=0.9033 val_loss=0.0000 scale=2.0000 norm=1.5987
[iter 300] loss=0.6337 val_loss=0.0000 scale=4.0000 norm=3.1594
[iter 400] loss=0.4373 val_loss=0.0000 scale=4.0000 norm=3.0711
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9081 val_loss=0.0000 scale=2.0000 norm=1.5108
[iter 200] loss=0.6725 val_loss=0.0000 scale=2.0000 norm=1.5383
[iter 300] loss=0.3284 val_loss=0.0000 scale=4.0000 norm=3.0769
[iter 400] loss=-0.0100 val_loss=0.0000 scale=4.0000 norm=3.0184
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0374 val_loss=0.0000 scale=2.0000 norm=1.6296
[iter 200] loss=0.8072 val_loss=0.0000 scale=2.0000 norm=1.6232
[iter 300] loss=0.5423 val_loss=0.0000 scale=2.0000 norm=1.6011
[iter 400] loss=0.3634 val_loss=0.0000 scale=4.0000 norm=3.2045
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0160 val_loss=0.0000 scale=2.0000 norm=1.5961
[iter 200] loss=0.7811 val_loss=0.0000 scale=4.0000 norm=3.1635
[iter 300] loss=0.4436 val_loss=0.0000 scale=4.0000 norm=3.1244
[iter 400] loss=0.2607 val_loss=0.0000 scale=4.0000 norm=3.1704
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9691 val_loss=0.0000 scale=2.0000 norm=1.5783
[iter 200] loss=0.6709 val_loss=0.0000 scale=2.0000 norm=1.5603
[iter 300] loss=0.2123 val_loss=0.0000 scale=4.0000 norm=3.0714
[iter 400] loss=-0.0435 val_loss=0.0000 scale=4.0000 norm=2.9807
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6126 val_loss=0.0000 scale=2.0000 norm=1.1753
[iter 200] loss=0.2337 val_loss=0.0000 scale=2.0000 norm=1.1728
[iter 300] loss=-0.2951 val_loss=0.0000 scale=4.0000 norm=2.2166
[iter 400] loss=-0.6975 val_loss=0.0000 scale=4.0000 norm=2.0830
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0426 val_loss=0.0000 scale=2.0000 norm=1.6085
[iter 200] loss=0.8258 val_loss=0.0000 scale=4.0000 norm=3.1973
[iter 300] loss=0.5122 val_loss=0.0000 scale=4.0000 norm=3.1641
[iter 400] loss=0.3381 val_loss=0.0000 scale=4.0000 norm=3.1036
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5516 val_loss=0.0000 scale=2.0000 norm=1.1035
[iter 200] loss=0.1677 val_loss=0.0000 scale=2.0000 norm=1.1138
[iter 300] loss=-0.3944 val_loss=0.0000 scale=4.0000 norm=2.2314
[iter 400] loss=-0.9036 val_loss=0.0000 scale=4.0000 norm=2.2124
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9995 val_loss=0.0000 scale=2.0000 norm=1.5886
[iter 200] loss=0.7110 val_loss=0.0000 scale=4.0000 norm=3.1588
[iter 300] loss=0.3338 val_loss=0.0000 scale=4.0000 norm=3.1191
[iter 400] loss=0.1099 val_loss=0.0000 scale=4.0000 norm=3.1144
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8712 val_loss=0.0000 scale=2.0000 norm=1.5462
[iter 200] loss=0.5674 val_loss=0.0000 scale=2.0000 norm=1.5037
[iter 300] loss=0.2071 val_loss=0.0000 scale=4.0000 norm=3.0057
[iter 400] loss=-0.0875 val_loss=0.0000 scale=4.0000 norm=2.9960
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0521 val_loss=0.0000 scale=2.0000 norm=1.6318
[iter 200] loss=0.7805 val_loss=0.0000 scale=4.0000 norm=3.2622
[iter 300] loss=0.3909 val_loss=0.0000 scale=4.0000 norm=3.2422
[iter 400] loss=0.0925 val_loss=0.0000 scale=4.0000 norm=3.2213
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0119 val_loss=0.0000 scale=2.0000 norm=1.6398
[iter 200] loss=0.7960 val_loss=0.0000 scale=4.0000 norm=3.2199
[iter 300] loss=0.5087 val_loss=0.0000 scale=8.0000 norm=6.5049
[iter 400] loss=-0.0513 val_loss=0.0000 scale=16.0000 norm=13.0103
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0316 val_loss=0.0000 scale=2.0000 norm=1.6203
[iter 200] loss=0.8077 val_loss=0.0000 scale=4.0000 norm=3.2306
[iter 300] loss=0.4782 val_loss=0.0000 scale=4.0000 norm=3.1966
[iter 400] loss=0.3159 val_loss=0.0000 scale=4.0000 norm=3.1597
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9980 val_loss=0.0000 scale=2.0000 norm=1.5978
[iter 200] loss=0.7335 val_loss=0.0000 scale=2.0000 norm=1.5896
[iter 300] loss=0.3722 val_loss=0.0000 scale=4.0000 norm=3.1587
[iter 400] loss=0.1440 val_loss=0.0000 scale=4.0000 norm=3.0452
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9647 val_loss=0.0000 scale=2.0000 norm=1.5746
[iter 200] loss=0.6784 val_loss=0.0000 scale=4.0000 norm=3.0997
[iter 300] loss=0.3108 val_loss=0.0000 scale=4.0000 norm=3.0359
[iter 400] loss=0.0583 val_loss=0.0000 scale=8.0000 norm=5.8629
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0421 val_loss=0.0000 scale=2.0000 norm=1.6196
[iter 200] loss=0.7634 val_loss=0.0000 scale=4.0000 norm=3.1937
[iter 300] loss=0.4733 val_loss=0.0000 scale=4.0000 norm=3.1076
[iter 400] loss=0.3396 val_loss=0.0000 scale=4.0000 norm=3.0929
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0023 val_loss=0.0000 scale=2.0000 norm=1.5942
[iter 200] loss=0.7357 val_loss=0.0000 scale=2.0000 norm=1.5811
[iter 300] loss=0.3375 val_loss=0.0000 scale=4.0000 norm=3.1301
[iter 400] loss=0.0710 val_loss=0.0000 scale=4.0000 norm=3.0498
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9601 val_loss=0.0000 scale=2.0000 norm=1.5537
[iter 200] loss=0.6747 val_loss=0.0000 scale=2.0000 norm=1.5269
[iter 300] loss=0.3097 val_loss=0.0000 scale=4.0000 norm=2.9877
[iter 400] loss=0.0103 val_loss=0.0000 scale=4.0000 norm=2.9094
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0162 val_loss=0.0000 scale=2.0000 norm=1.5857
[iter 200] loss=0.7718 val_loss=0.0000 scale=2.0000 norm=1.5664
[iter 300] loss=0.4235 val_loss=0.0000 scale=4.0000 norm=3.1286
[iter 400] loss=0.1734 val_loss=0.0000 scale=4.0000 norm=3.0601
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8642 val_loss=0.0000 scale=2.0000 norm=1.4623
[iter 200] loss=0.6219 val_loss=0.0000 scale=2.0000 norm=1.4852
[iter 300] loss=0.3456 val_loss=0.0000 scale=4.0000 norm=2.9462
[iter 400] loss=0.1216 val_loss=0.0000 scale=4.0000 norm=2.9032
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8543 val_loss=0.0000 scale=2.0000 norm=1.6122
[iter 200] loss=0.4152 val_loss=0.0000 scale=4.0000 norm=3.2085
[iter 300] loss=-0.0602 val_loss=0.0000 scale=4.0000 norm=3.1636
[iter 400] loss=-0.3929 val_loss=0.0000 scale=4.0000 norm=3.0549
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0650 val_loss=0.0000 scale=2.0000 norm=1.6304
[iter 200] loss=0.8550 val_loss=0.0000 scale=2.0000 norm=1.6065
[iter 300] loss=0.5697 val_loss=0.0000 scale=4.0000 norm=3.1665
[iter 400] loss=0.3203 val_loss=0.0000 scale=4.0000 norm=3.1534
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0024 val_loss=0.0000 scale=2.0000 norm=1.5879
[iter 200] loss=0.7229 val_loss=0.0000 scale=4.0000 norm=3.1466
[iter 300] loss=0.3686 val_loss=0.0000 scale=4.0000 norm=3.1050
[iter 400] loss=0.1593 val_loss=0.0000 scale=4.0000 norm=3.0882
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0558 val_loss=0.0000 scale=2.0000 norm=1.6625
[iter 200] loss=0.8370 val_loss=0.0000 scale=2.0000 norm=1.6534
[iter 300] loss=0.5459 val_loss=0.0000 scale=4.0000 norm=3.2523
[iter 400] loss=0.3752 val_loss=0.0000 scale=4.0000 norm=3.2148
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9719 val_loss=0.0000 scale=2.0000 norm=1.5507
[iter 200] loss=0.7300 val_loss=0.0000 scale=2.0000 norm=1.5232
[iter 300] loss=0.4154 val_loss=0.0000 scale=4.0000 norm=3.0092
[iter 400] loss=0.1966 val_loss=0.0000 scale=4.0000 norm=2.9327
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9232 val_loss=0.0000 scale=2.0000 norm=1.5194
[iter 200] loss=0.6316 val_loss=0.0000 scale=4.0000 norm=2.9817
[iter 300] loss=0.2617 val_loss=0.0000 scale=4.0000 norm=2.9534
[iter 400] loss=0.0024 val_loss=0.0000 scale=4.0000 norm=2.9438
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0649 val_loss=0.0000 scale=2.0000 norm=1.6231
[iter 200] loss=0.8758 val_loss=0.0000 scale=2.0000 norm=1.6111
[iter 300] loss=0.6056 val_loss=0.0000 scale=4.0000 norm=3.1825
[iter 400] loss=0.4478 val_loss=0.0000 scale=4.0000 norm=3.1182
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0712 val_loss=0.0000 scale=2.0000 norm=1.6243
[iter 200] loss=0.8813 val_loss=0.0000 scale=2.0000 norm=1.6096
[iter 300] loss=0.5902 val_loss=0.0000 scale=4.0000 norm=3.1842
[iter 400] loss=0.3808 val_loss=0.0000 scale=4.0000 norm=3.1062
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0268 val_loss=0.0000 scale=2.0000 norm=1.6108
[iter 200] loss=0.7592 val_loss=0.0000 scale=4.0000 norm=3.1955
[iter 300] loss=0.4283 val_loss=0.0000 scale=4.0000 norm=3.1366
[iter 400] loss=0.2496 val_loss=0.0000 scale=4.0000 norm=3.0854
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0661 val_loss=0.0000 scale=2.0000 norm=1.6342
[iter 200] loss=0.8542 val_loss=0.0000 scale=2.0000 norm=1.6169
[iter 300] loss=0.5728 val_loss=0.0000 scale=4.0000 norm=3.1850
[iter 400] loss=0.3528 val_loss=0.0000 scale=2.0000 norm=1.5541
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9385 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 200] loss=0.7385 val_loss=0.0000 scale=4.0000 norm=3.1936
[iter 300] loss=0.4380 val_loss=0.0000 scale=4.0000 norm=3.2107
[iter 400] loss=-0.1290 val_loss=0.0000 scale=8.0000 norm=6.4220
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9998 val_loss=0.0000 scale=2.0000 norm=1.5811
[iter 200] loss=0.7379 val_loss=0.0000 scale=4.0000 norm=3.1355
[iter 300] loss=0.3567 val_loss=0.0000 scale=4.0000 norm=3.1223
[iter 400] loss=0.1511 val_loss=0.0000 scale=4.0000 norm=3.1692
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0264 val_loss=0.0000 scale=2.0000 norm=1.5985
[iter 200] loss=0.7557 val_loss=0.0000 scale=4.0000 norm=3.1726
[iter 300] loss=0.4094 val_loss=0.0000 scale=4.0000 norm=3.1262
[iter 400] loss=0.2303 val_loss=0.0000 scale=4.0000 norm=3.0823
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.5684
[iter 200] loss=0.6944 val_loss=0.0000 scale=4.0000 norm=3.1177
[iter 300] loss=0.3333 val_loss=0.0000 scale=4.0000 norm=3.0798
[iter 400] loss=0.1120 val_loss=0.0000 scale=4.0000 norm=3.0125
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9667 val_loss=0.0000 scale=2.0000 norm=1.5730
[iter 200] loss=0.6516 val_loss=0.0000 scale=4.0000 norm=3.0555
[iter 300] loss=0.2762 val_loss=0.0000 scale=4.0000 norm=2.9918
[iter 400] loss=-0.0373 val_loss=0.0000 scale=4.0000 norm=2.9362
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0294 val_loss=0.0000 scale=2.0000 norm=1.6088
[iter 200] loss=0.7785 val_loss=0.0000 scale=4.0000 norm=3.1787
[iter 300] loss=0.4456 val_loss=0.0000 scale=4.0000 norm=3.1320
[iter 400] loss=0.2171 val_loss=0.0000 scale=4.0000 norm=3.1469
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9374 val_loss=0.0000 scale=2.0000 norm=1.5883
[iter 200] loss=0.5494 val_loss=0.0000 scale=4.0000 norm=3.1437
[iter 300] loss=0.0752 val_loss=0.0000 scale=4.0000 norm=3.1131
[iter 400] loss=-0.2463 val_loss=0.0000 scale=4.0000 norm=3.0940
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8716 val_loss=0.0000 scale=2.0000 norm=1.5376
[iter 200] loss=0.4645 val_loss=0.0000 scale=4.0000 norm=3.0386
[iter 300] loss=-0.0283 val_loss=0.0000 scale=4.0000 norm=3.0131
[iter 400] loss=-0.3669 val_loss=0.0000 scale=4.0000 norm=2.9449
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0482 val_loss=0.0000 scale=2.0000 norm=1.6421
[iter 200] loss=0.8087 val_loss=0.0000 scale=4.0000 norm=3.2662
[iter 300] loss=0.4977 val_loss=0.0000 scale=4.0000 norm=3.2041
[iter 400] loss=0.3601 val_loss=0.0000 scale=4.0000 norm=3.2092
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9082 val_loss=0.0000 scale=2.0000 norm=1.5325
[iter 200] loss=0.5492 val_loss=0.0000 scale=4.0000 norm=2.9503
[iter 300] loss=0.0939 val_loss=0.0000 scale=4.0000 norm=2.7897
[iter 400] loss=-0.2307 val_loss=0.0000 scale=4.0000 norm=2.6816
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8630 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 200] loss=0.3981 val_loss=0.0000 scale=4.0000 norm=3.4013
[iter 300] loss=-0.1294 val_loss=0.0000 scale=4.0000 norm=3.3008
[iter 400] loss=-0.9508 val_loss=0.0000 scale=16.0000 norm=12.9102
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9626 val_loss=0.0000 scale=2.0000 norm=1.5394
[iter 200] loss=0.7000 val_loss=0.0000 scale=2.0000 norm=1.5192
[iter 300] loss=0.3404 val_loss=0.0000 scale=4.0000 norm=2.9870
[iter 400] loss=0.0783 val_loss=0.0000 scale=4.0000 norm=2.9484
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4375 val_loss=0.0000 scale=2.0000 norm=1.1901
[iter 200] loss=-0.1530 val_loss=0.0000 scale=4.0000 norm=2.4690
[iter 300] loss=-0.8569 val_loss=0.0000 scale=4.0000 norm=2.5415
[iter 400] loss=-2.2289 val_loss=0.0000 scale=8.0000 norm=5.0862
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8748 val_loss=0.0000 scale=2.0000 norm=1.6157
[iter 200] loss=0.4005 val_loss=0.0000 scale=4.0000 norm=3.2493
[iter 300] loss=-0.3972 val_loss=0.0000 scale=4.0000 norm=3.2484
[iter 400] loss=-1.1784 val_loss=0.0000 scale=8.0000 norm=6.2432
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9586 val_loss=0.0000 scale=2.0000 norm=1.5517
[iter 200] loss=0.6847 val_loss=0.0000 scale=4.0000 norm=3.0513
[iter 300] loss=0.2948 val_loss=0.0000 scale=4.0000 norm=3.0007
[iter 400] loss=0.0101 val_loss=0.0000 scale=4.0000 norm=2.9166
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0336 val_loss=0.0000 scale=2.0000 norm=1.6123
[iter 200] loss=0.8224 val_loss=0.0000 scale=4.0000 norm=3.1714
[iter 300] loss=0.5354 val_loss=0.0000 scale=4.0000 norm=3.1223
[iter 400] loss=0.3178 val_loss=0.0000 scale=4.0000 norm=3.0628
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8190 val_loss=0.0000 scale=2.0000 norm=1.4185
[iter 200] loss=0.6066 val_loss=0.0000 scale=2.0000 norm=1.4861
[iter 300] loss=0.2878 val_loss=0.0000 scale=4.0000 norm=2.9786
[iter 400] loss=0.0106 val_loss=0.0000 scale=4.0000 norm=2.9254
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0048 val_loss=0.0000 scale=2.0000 norm=1.5846
[iter 200] loss=0.7631 val_loss=0.0000 scale=4.0000 norm=3.1229
[iter 300] loss=0.4442 val_loss=0.0000 scale=4.0000 norm=3.1017
[iter 400] loss=0.2298 val_loss=0.0000 scale=4.0000 norm=3.0433
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5863 val_loss=0.0000 scale=2.0000 norm=1.1503
[iter 200] loss=0.2166 val_loss=0.0000 scale=2.0000 norm=1.1021
[iter 300] loss=-0.1534 val_loss=0.0000 scale=4.0000 norm=2.2001
[iter 400] loss=-0.4617 val_loss=0.0000 scale=4.0000 norm=2.1871
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0590 val_loss=0.0000 scale=2.0000 norm=1.6368
[iter 200] loss=0.8223 val_loss=0.0000 scale=4.0000 norm=3.2501
[iter 300] loss=0.5136 val_loss=0.0000 scale=4.0000 norm=3.2052
[iter 400] loss=0.3703 val_loss=0.0000 scale=4.0000 norm=3.1844
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5086 val_loss=0.0000 scale=2.0000 norm=1.0902
[iter 200] loss=0.0117 val_loss=0.0000 scale=2.0000 norm=1.1383
[iter 300] loss=-0.6715 val_loss=0.0000 scale=4.0000 norm=2.2552
[iter 400] loss=-1.7647 val_loss=0.0000 scale=8.0000 norm=4.5378
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0065 val_loss=0.0000 scale=2.0000 norm=1.5878
[iter 200] loss=0.7705 val_loss=0.0000 scale=2.0000 norm=1.5580
[iter 300] loss=0.4283 val_loss=0.0000 scale=4.0000 norm=3.0700
[iter 400] loss=0.2257 val_loss=0.0000 scale=4.0000 norm=2.9996
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9996 val_loss=0.0000 scale=2.0000 norm=1.5961
[iter 200] loss=0.7444 val_loss=0.0000 scale=4.0000 norm=3.1637
[iter 300] loss=0.3860 val_loss=0.0000 scale=4.0000 norm=3.1289
[iter 400] loss=0.1672 val_loss=0.0000 scale=8.0000 norm=6.3094
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0484 val_loss=0.0000 scale=2.0000 norm=1.6541
[iter 200] loss=0.7226 val_loss=0.0000 scale=4.0000 norm=3.3127
[iter 300] loss=0.2922 val_loss=0.0000 scale=4.0000 norm=3.2730
[iter 400] loss=-0.0116 val_loss=0.0000 scale=4.0000 norm=3.2417
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9675 val_loss=0.0000 scale=2.0000 norm=1.5362
[iter 200] loss=0.7072 val_loss=0.0000 scale=4.0000 norm=3.0458
[iter 300] loss=0.3635 val_loss=0.0000 scale=4.0000 norm=3.0059
[iter 400] loss=0.1833 val_loss=0.0000 scale=4.0000 norm=2.9626
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5194 val_loss=0.0000 scale=2.0000 norm=1.1262
[iter 200] loss=0.0248 val_loss=0.0000 scale=2.0000 norm=1.1798
[iter 300] loss=-0.7873 val_loss=0.0000 scale=4.0000 norm=2.3730
[iter 400] loss=-1.4787 val_loss=0.0000 scale=4.0000 norm=2.1855
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9398 val_loss=0.0000 scale=2.0000 norm=1.6125
[iter 200] loss=0.5065 val_loss=0.0000 scale=4.0000 norm=3.2317
[iter 300] loss=-0.0078 val_loss=0.0000 scale=4.0000 norm=3.1416
[iter 400] loss=-0.4283 val_loss=0.0000 scale=4.0000 norm=3.0657
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0321 val_loss=0.0000 scale=2.0000 norm=1.5967
[iter 200] loss=0.8121 val_loss=0.0000 scale=4.0000 norm=3.1534
[iter 300] loss=0.5294 val_loss=0.0000 scale=4.0000 norm=3.1127
[iter 400] loss=0.3797 val_loss=0.0000 scale=4.0000 norm=3.0881
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9478 val_loss=0.0000 scale=2.0000 norm=1.6438
[iter 200] loss=0.5549 val_loss=0.0000 scale=4.0000 norm=3.2423
[iter 300] loss=0.0914 val_loss=0.0000 scale=4.0000 norm=3.1957
[iter 400] loss=-0.2458 val_loss=0.0000 scale=4.0000 norm=3.0739
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0319 val_loss=0.0000 scale=2.0000 norm=1.6115
[iter 200] loss=0.7861 val_loss=0.0000 scale=4.0000 norm=3.1968
[iter 300] loss=0.4431 val_loss=0.0000 scale=4.0000 norm=3.1556
[iter 400] loss=0.2348 val_loss=0.0000 scale=4.0000 norm=3.1463
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0573 val_loss=0.0000 scale=2.0000 norm=1.5994
[iter 200] loss=0.8555 val_loss=0.0000 scale=4.0000 norm=3.1595
[iter 300] loss=0.5273 val_loss=0.0000 scale=4.0000 norm=3.1828
[iter 400] loss=0.1786 val_loss=0.0000 scale=8.0000 norm=6.2924
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9034 val_loss=0.0000 scale=2.0000 norm=1.5201
[iter 200] loss=0.5595 val_loss=0.0000 scale=4.0000 norm=3.0399
[iter 300] loss=0.0894 val_loss=0.0000 scale=4.0000 norm=3.0054
[iter 400] loss=-0.2183 val_loss=0.0000 scale=4.0000 norm=2.9328
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0129 val_loss=0.0000 scale=2.0000 norm=1.5753
[iter 200] loss=0.7886 val_loss=0.0000 scale=2.0000 norm=1.5556
[iter 300] loss=0.4746 val_loss=0.0000 scale=4.0000 norm=3.0751
[iter 400] loss=0.2834 val_loss=0.0000 scale=4.0000 norm=3.0754
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6013 val_loss=0.0000 scale=2.0000 norm=1.2585
[iter 200] loss=0.0533 val_loss=0.0000 scale=4.0000 norm=2.5770
[iter 300] loss=-0.7466 val_loss=0.0000 scale=4.0000 norm=2.5878
[iter 400] loss=-2.2026 val_loss=0.0000 scale=8.0000 norm=5.1760
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9807 val_loss=0.0000 scale=2.0000 norm=1.5506
[iter 200] loss=0.7294 val_loss=0.0000 scale=2.0000 norm=1.5314
[iter 300] loss=0.4254 val_loss=0.0000 scale=4.0000 norm=3.0227
[iter 400] loss=0.1694 val_loss=0.0000 scale=4.0000 norm=2.9698
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9658 val_loss=0.0000 scale=2.0000 norm=1.5743
[iter 200] loss=0.6609 val_loss=0.0000 scale=2.0000 norm=1.5625
[iter 300] loss=0.1959 val_loss=0.0000 scale=4.0000 norm=3.1023
[iter 400] loss=-0.0990 val_loss=0.0000 scale=4.0000 norm=3.0413
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0425 val_loss=0.0000 scale=2.0000 norm=1.6206
[iter 200] loss=0.7863 val_loss=0.0000 scale=4.0000 norm=3.2059
[iter 300] loss=0.4409 val_loss=0.0000 scale=4.0000 norm=3.1557
[iter 400] loss=0.2241 val_loss=0.0000 scale=4.0000 norm=3.1193
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8035 val_loss=0.0000 scale=2.0000 norm=1.3563
[iter 200] loss=0.5784 val_loss=0.0000 scale=2.0000 norm=1.4198
[iter 300] loss=0.2867 val_loss=0.0000 scale=4.0000 norm=2.8662
[iter 400] loss=0.0245 val_loss=0.0000 scale=4.0000 norm=2.8568
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0379 val_loss=0.0000 scale=2.0000 norm=1.6206
[iter 200] loss=0.7668 val_loss=0.0000 scale=4.0000 norm=3.2159
[iter 300] loss=0.4118 val_loss=0.0000 scale=4.0000 norm=3.1624
[iter 400] loss=0.2115 val_loss=0.0000 scale=4.0000 norm=3.0870
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0293 val_loss=0.0000 scale=2.0000 norm=1.6002
[iter 200] loss=0.7600 val_loss=0.0000 scale=4.0000 norm=3.1693
[iter 300] loss=0.3993 val_loss=0.0000 scale=4.0000 norm=3.1400
[iter 400] loss=0.2006 val_loss=0.0000 scale=4.0000 norm=3.1022
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0005 val_loss=0.0000 scale=2.0000 norm=1.5748
[iter 200] loss=0.7561 val_loss=0.0000 scale=2.0000 norm=1.5636
[iter 300] loss=0.4737 val_loss=0.0000 scale=4.0000 norm=3.1051
[iter 400] loss=0.2161 val_loss=0.0000 scale=4.0000 norm=3.0726
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9862 val_loss=0.0000 scale=2.0000 norm=1.5818
[iter 200] loss=0.7414 val_loss=0.0000 scale=4.0000 norm=3.0972
[iter 300] loss=0.4411 val_loss=0.0000 scale=2.0000 norm=1.5269
[iter 400] loss=0.2949 val_loss=0.0000 scale=4.0000 norm=3.0618
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4415 val_loss=0.0000 scale=2.0000 norm=0.9961
[iter 200] loss=-0.0069 val_loss=0.0000 scale=2.0000 norm=1.0106
[iter 300] loss=-0.6008 val_loss=0.0000 scale=4.0000 norm=2.0622
[iter 400] loss=-1.6328 val_loss=0.0000 scale=8.0000 norm=4.1286
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8199 val_loss=0.0000 scale=2.0000 norm=1.5547
[iter 200] loss=0.3321 val_loss=0.0000 scale=4.0000 norm=3.1046
[iter 300] loss=-0.2526 val_loss=0.0000 scale=4.0000 norm=3.0761
[iter 400] loss=-0.7043 val_loss=0.0000 scale=4.0000 norm=2.9973
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5968 val_loss=0.0000 scale=2.0000 norm=1.2872
[iter 200] loss=0.0884 val_loss=0.0000 scale=2.0000 norm=1.2449
[iter 300] loss=-0.3068 val_loss=0.0000 scale=4.0000 norm=2.6108
[iter 400] loss=-0.6288 val_loss=0.0000 scale=8.0000 norm=4.9224
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0164 val_loss=0.0000 scale=2.0000 norm=1.5857
[iter 200] loss=0.7924 val_loss=0.0000 scale=2.0000 norm=1.5645
[iter 300] loss=0.4886 val_loss=0.0000 scale=4.0000 norm=3.1080
[iter 400] loss=0.2810 val_loss=0.0000 scale=4.0000 norm=3.0502
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0212 val_loss=0.0000 scale=2.0000 norm=1.6019
[iter 200] loss=0.7585 val_loss=0.0000 scale=4.0000 norm=3.1746
[iter 300] loss=0.4501 val_loss=0.0000 scale=4.0000 norm=3.0991
[iter 400] loss=0.2925 val_loss=0.0000 scale=4.0000 norm=3.0746
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0154 val_loss=0.0000 scale=2.0000 norm=1.6038
[iter 200] loss=0.7560 val_loss=0.0000 scale=2.0000 norm=1.5915
[iter 300] loss=0.3774 val_loss=0.0000 scale=4.0000 norm=3.1388
[iter 400] loss=0.1162 val_loss=0.0000 scale=4.0000 norm=3.1047
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9642 val_loss=0.0000 scale=2.0000 norm=1.5459
[iter 200] loss=0.7014 val_loss=0.0000 scale=2.0000 norm=1.5265
[iter 300] loss=0.3301 val_loss=0.0000 scale=4.0000 norm=3.0047
[iter 400] loss=0.0834 val_loss=0.0000 scale=4.0000 norm=2.9691
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0192 val_loss=0.0000 scale=2.0000 norm=1.5999
[iter 200] loss=0.7766 val_loss=0.0000 scale=4.0000 norm=3.1531
[iter 300] loss=0.4345 val_loss=0.0000 scale=4.0000 norm=3.1140
[iter 400] loss=0.2612 val_loss=0.0000 scale=4.0000 norm=3.0850
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0118 val_loss=0.0000 scale=2.0000 norm=1.5911
[iter 200] loss=0.7831 val_loss=0.0000 scale=2.0000 norm=1.5764
[iter 300] loss=0.4763 val_loss=0.0000 scale=4.0000 norm=3.1157
[iter 400] loss=0.2870 val_loss=0.0000 scale=4.0000 norm=3.0892
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9708 val_loss=0.0000 scale=2.0000 norm=1.5808
[iter 200] loss=0.5994 val_loss=0.0000 scale=4.0000 norm=3.1542
[iter 300] loss=0.0855 val_loss=0.0000 scale=4.0000 norm=3.1619
[iter 400] loss=-0.3736 val_loss=0.0000 scale=4.0000 norm=3.0794
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8952 val_loss=0.0000 scale=2.0000 norm=1.5244
[iter 200] loss=0.6400 val_loss=0.0000 scale=4.0000 norm=3.1419
[iter 300] loss=0.2203 val_loss=0.0000 scale=4.0000 norm=3.1681
[iter 400] loss=-0.4830 val_loss=0.0000 scale=8.0000 norm=6.3384
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0077 val_loss=0.0000 scale=2.0000 norm=1.6290
[iter 200] loss=0.6940 val_loss=0.0000 scale=4.0000 norm=3.2173
[iter 300] loss=0.2602 val_loss=0.0000 scale=4.0000 norm=3.1677
[iter 400] loss=-0.0839 val_loss=0.0000 scale=4.0000 norm=3.1094
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0010 val_loss=0.0000 scale=2.0000 norm=1.5658
[iter 200] loss=0.7769 val_loss=0.0000 scale=2.0000 norm=1.5459
[iter 300] loss=0.4749 val_loss=0.0000 scale=4.0000 norm=3.0618
[iter 400] loss=0.2798 val_loss=0.0000 scale=4.0000 norm=3.0108
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9561 val_loss=0.0000 scale=2.0000 norm=1.5276
[iter 200] loss=0.7080 val_loss=0.0000 scale=2.0000 norm=1.5010
[iter 300] loss=0.3431 val_loss=0.0000 scale=4.0000 norm=2.9610
[iter 400] loss=0.0680 val_loss=0.0000 scale=4.0000 norm=2.9451
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0878 val_loss=0.0000 scale=2.0000 norm=1.6453
[iter 200] loss=0.9171 val_loss=0.0000 scale=2.0000 norm=1.6353
[iter 300] loss=0.6686 val_loss=0.0000 scale=4.0000 norm=3.2586
[iter 400] loss=0.5106 val_loss=0.0000 scale=8.0000 norm=6.4203
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0148 val_loss=0.0000 scale=2.0000 norm=1.5948
[iter 200] loss=0.7126 val_loss=0.0000 scale=4.0000 norm=3.1433
[iter 300] loss=0.2932 val_loss=0.0000 scale=4.0000 norm=3.1496
[iter 400] loss=-0.5398 val_loss=0.0000 scale=8.0000 norm=6.2995
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0629 val_loss=0.0000 scale=2.0000 norm=1.6557
[iter 200] loss=0.8169 val_loss=0.0000 scale=4.0000 norm=3.2928
[iter 300] loss=0.4511 val_loss=0.0000 scale=4.0000 norm=3.2463
[iter 400] loss=0.2363 val_loss=0.0000 scale=4.0000 norm=3.2031
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8306 val_loss=0.0000 scale=2.0000 norm=1.4455
[iter 200] loss=0.4209 val_loss=0.0000 scale=4.0000 norm=2.9168
[iter 300] loss=-0.2122 val_loss=0.0000 scale=4.0000 norm=2.8186
[iter 400] loss=-1.1489 val_loss=0.0000 scale=8.0000 norm=5.4818
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9902 val_loss=0.0000 scale=2.0000 norm=1.5898
[iter 200] loss=0.7481 val_loss=0.0000 scale=4.0000 norm=3.1505
[iter 300] loss=0.4094 val_loss=0.0000 scale=4.0000 norm=3.1273
[iter 400] loss=0.1680 val_loss=0.0000 scale=4.0000 norm=3.0837
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0589 val_loss=0.0000 scale=2.0000 norm=1.6306
[iter 200] loss=0.8288 val_loss=0.0000 scale=4.0000 norm=3.2369
[iter 300] loss=0.5173 val_loss=0.0000 scale=4.0000 norm=3.1885
[iter 400] loss=0.3126 val_loss=0.0000 scale=4.0000 norm=3.1772
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9930 val_loss=0.0000 scale=2.0000 norm=1.5656
[iter 200] loss=0.7181 val_loss=0.0000 scale=4.0000 norm=3.1090
[iter 300] loss=0.3996 val_loss=0.0000 scale=4.0000 norm=3.0624
[iter 400] loss=0.2024 val_loss=0.0000 scale=4.0000 norm=3.0177
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8644 val_loss=0.0000 scale=2.0000 norm=1.5893
[iter 200] loss=0.4033 val_loss=0.0000 scale=4.0000 norm=3.1137
[iter 300] loss=-0.2404 val_loss=0.0000 scale=8.0000 norm=6.2680
[iter 400] loss=-0.8468 val_loss=0.0000 scale=4.0000 norm=3.0899
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0181 val_loss=0.0000 scale=2.0000 norm=1.5880
[iter 200] loss=0.7858 val_loss=0.0000 scale=2.0000 norm=1.5615
[iter 300] loss=0.4629 val_loss=0.0000 scale=2.0000 norm=1.5328
[iter 400] loss=0.3370 val_loss=0.0000 scale=2.0000 norm=1.5257
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9882 val_loss=0.0000 scale=2.0000 norm=1.5872
[iter 200] loss=0.7320 val_loss=0.0000 scale=2.0000 norm=1.5665
[iter 300] loss=0.4087 val_loss=0.0000 scale=4.0000 norm=3.0952
[iter 400] loss=0.2046 val_loss=0.0000 scale=4.0000 norm=3.0485
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9625 val_loss=0.0000 scale=2.0000 norm=1.5465
[iter 200] loss=0.7268 val_loss=0.0000 scale=2.0000 norm=1.5675
[iter 300] loss=0.3525 val_loss=0.0000 scale=4.0000 norm=3.1568
[iter 400] loss=-0.0759 val_loss=0.0000 scale=4.0000 norm=3.1171
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0405 val_loss=0.0000 scale=2.0000 norm=1.6827
[iter 200] loss=0.6802 val_loss=0.0000 scale=4.0000 norm=3.2950
[iter 300] loss=0.2409 val_loss=0.0000 scale=8.0000 norm=6.4263
[iter 400] loss=-0.5471 val_loss=0.0000 scale=16.0000 norm=12.8330
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0199 val_loss=0.0000 scale=2.0000 norm=1.5931
[iter 200] loss=0.7269 val_loss=0.0000 scale=4.0000 norm=3.1547
[iter 300] loss=0.3637 val_loss=0.0000 scale=4.0000 norm=3.0481
[iter 400] loss=0.1491 val_loss=0.0000 scale=4.0000 norm=2.9881
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0225 val_loss=0.0000 scale=2.0000 norm=1.6187
[iter 200] loss=0.7566 val_loss=0.0000 scale=4.0000 norm=3.2085
[iter 300] loss=0.4464 val_loss=0.0000 scale=4.0000 norm=3.1252
[iter 400] loss=0.2675 val_loss=0.0000 scale=4.0000 norm=3.1112
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8419 val_loss=0.0000 scale=2.0000 norm=1.6270
[iter 200] loss=0.3599 val_loss=0.0000 scale=4.0000 norm=3.3314
[iter 300] loss=-0.4441 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-1.9801 val_loss=0.0000 scale=16.0000 norm=13.3436
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0734 val_loss=0.0000 scale=2.0000 norm=1.6350
[iter 200] loss=0.8878 val_loss=0.0000 scale=4.0000 norm=3.2379
[iter 300] loss=0.6183 val_loss=0.0000 scale=4.0000 norm=3.1778
[iter 400] loss=0.4902 val_loss=0.0000 scale=4.0000 norm=3.1959
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7657 val_loss=0.0000 scale=2.0000 norm=1.3380
[iter 200] loss=0.5159 val_loss=0.0000 scale=4.0000 norm=2.7661
[iter 300] loss=0.1998 val_loss=0.0000 scale=4.0000 norm=2.7565
[iter 400] loss=-0.1029 val_loss=0.0000 scale=4.0000 norm=2.7073
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0238 val_loss=0.0000 scale=2.0000 norm=1.6111
[iter 200] loss=0.7584 val_loss=0.0000 scale=4.0000 norm=3.1815
[iter 300] loss=0.4133 val_loss=0.0000 scale=4.0000 norm=3.1388
[iter 400] loss=0.1775 val_loss=0.0000 scale=4.0000 norm=3.1214
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0475 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 200] loss=0.8308 val_loss=0.0000 scale=4.0000 norm=3.4606
[iter 300] loss=0.4707 val_loss=0.0000 scale=8.0000 norm=6.9339
[iter 400] loss=-0.2373 val_loss=0.0000 scale=16.0000 norm=13.8680
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0614 val_loss=0.0000 scale=2.0000 norm=1.6421
[iter 200] loss=0.8434 val_loss=0.0000 scale=2.0000 norm=1.6445
[iter 300] loss=0.5421 val_loss=0.0000 scale=4.0000 norm=3.2442
[iter 400] loss=0.3218 val_loss=0.0000 scale=4.0000 norm=3.2333
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8261 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 200] loss=0.3049 val_loss=0.0000 scale=4.0000 norm=3.2241
[iter 300] loss=-0.3025 val_loss=0.0000 scale=4.0000 norm=3.1619
[iter 400] loss=-0.8570 val_loss=0.0000 scale=4.0000 norm=3.0816
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0232 val_loss=0.0000 scale=2.0000 norm=1.5858
[iter 200] loss=0.8227 val_loss=0.0000 scale=4.0000 norm=3.1176
[iter 300] loss=0.5553 val_loss=0.0000 scale=4.0000 norm=3.0869
[iter 400] loss=0.4142 val_loss=0.0000 scale=4.0000 norm=3.0921
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9610 val_loss=0.0000 scale=2.0000 norm=1.5484
[iter 200] loss=0.6903 val_loss=0.0000 scale=2.0000 norm=1.5198
[iter 300] loss=0.3856 val_loss=0.0000 scale=4.0000 norm=3.0030
[iter 400] loss=0.1634 val_loss=0.0000 scale=4.0000 norm=2.9579
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0042 val_loss=0.0000 scale=2.0000 norm=1.5873
[iter 200] loss=0.7542 val_loss=0.0000 scale=2.0000 norm=1.5809
[iter 300] loss=0.4085 val_loss=0.0000 scale=4.0000 norm=3.1197
[iter 400] loss=0.1349 val_loss=0.0000 scale=4.0000 norm=3.0889
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1198 val_loss=0.0000 scale=2.0000 norm=1.7053
[iter 200] loss=0.8633 val_loss=0.0000 scale=4.0000 norm=3.3776
[iter 300] loss=0.5294 val_loss=0.0000 scale=4.0000 norm=3.3040
[iter 400] loss=0.1458 val_loss=0.0000 scale=8.0000 norm=6.4525
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0784 val_loss=0.0000 scale=2.0000 norm=1.6319
[iter 200] loss=0.8329 val_loss=0.0000 scale=4.0000 norm=3.2134
[iter 300] loss=0.5578 val_loss=0.0000 scale=4.0000 norm=3.1219
[iter 400] loss=0.1510 val_loss=0.0000 scale=8.0000 norm=6.2974
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9955 val_loss=0.0000 scale=2.0000 norm=1.5918
[iter 200] loss=0.6965 val_loss=0.0000 scale=4.0000 norm=3.1614
[iter 300] loss=0.3560 val_loss=0.0000 scale=4.0000 norm=3.0893
[iter 400] loss=0.1497 val_loss=0.0000 scale=4.0000 norm=2.9661
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0060 val_loss=0.0000 scale=2.0000 norm=1.6010
[iter 200] loss=0.7347 val_loss=0.0000 scale=4.0000 norm=3.1898
[iter 300] loss=0.3434 val_loss=0.0000 scale=4.0000 norm=3.1563
[iter 400] loss=0.0859 val_loss=0.0000 scale=4.0000 norm=3.1491
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0312 val_loss=0.0000 scale=2.0000 norm=1.6454
[iter 200] loss=0.7518 val_loss=0.0000 scale=2.0000 norm=1.6458
[iter 300] loss=0.2834 val_loss=0.0000 scale=4.0000 norm=3.2524
[iter 400] loss=-0.0385 val_loss=0.0000 scale=4.0000 norm=3.1861
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9329 val_loss=0.0000 scale=2.0000 norm=1.6486
[iter 200] loss=0.5628 val_loss=0.0000 scale=4.0000 norm=3.3266
[iter 300] loss=0.0066 val_loss=0.0000 scale=4.0000 norm=3.3264
[iter 400] loss=-0.4939 val_loss=0.0000 scale=4.0000 norm=3.2328
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9890 val_loss=0.0000 scale=2.0000 norm=1.5830
[iter 200] loss=0.7208 val_loss=0.0000 scale=2.0000 norm=1.5634
[iter 300] loss=0.3152 val_loss=0.0000 scale=4.0000 norm=3.0830
[iter 400] loss=0.0595 val_loss=0.0000 scale=4.0000 norm=3.0375
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0475 val_loss=0.0000 scale=2.0000 norm=1.6124
[iter 200] loss=0.8216 val_loss=0.0000 scale=2.0000 norm=1.5877
[iter 300] loss=0.5416 val_loss=0.0000 scale=4.0000 norm=3.1159
[iter 400] loss=0.2775 val_loss=0.0000 scale=4.0000 norm=3.0746
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0530 val_loss=0.0000 scale=2.0000 norm=1.6211
[iter 200] loss=0.8296 val_loss=0.0000 scale=4.0000 norm=3.2386
[iter 300] loss=0.5017 val_loss=0.0000 scale=4.0000 norm=3.2170
[iter 400] loss=0.2961 val_loss=0.0000 scale=4.0000 norm=3.1640
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0388 val_loss=0.0000 scale=2.0000 norm=1.6323
[iter 200] loss=0.7000 val_loss=0.0000 scale=4.0000 norm=3.2170
[iter 300] loss=0.3150 val_loss=0.0000 scale=4.0000 norm=3.1637
[iter 400] loss=0.0839 val_loss=0.0000 scale=4.0000 norm=3.1315
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9605 val_loss=0.0000 scale=2.0000 norm=1.5387
[iter 200] loss=0.7498 val_loss=0.0000 scale=4.0000 norm=3.1875
[iter 300] loss=0.4137 val_loss=0.0000 scale=8.0000 norm=6.4023
[iter 400] loss=-0.2463 val_loss=0.0000 scale=16.0000 norm=12.8051
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0350 val_loss=0.0000 scale=2.0000 norm=1.6178
[iter 200] loss=0.7874 val_loss=0.0000 scale=4.0000 norm=3.2191
[iter 300] loss=0.5083 val_loss=0.0000 scale=4.0000 norm=3.1579
[iter 400] loss=0.3707 val_loss=0.0000 scale=4.0000 norm=3.1432
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5965 val_loss=0.0000 scale=2.0000 norm=1.1762
[iter 200] loss=0.2270 val_loss=0.0000 scale=2.0000 norm=1.1462
[iter 300] loss=-0.1538 val_loss=0.0000 scale=4.0000 norm=2.3402
[iter 400] loss=-0.4363 val_loss=0.0000 scale=4.0000 norm=2.2947
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9978 val_loss=0.0000 scale=2.0000 norm=1.5760
[iter 200] loss=0.7542 val_loss=0.0000 scale=2.0000 norm=1.5713
[iter 300] loss=0.3919 val_loss=0.0000 scale=4.0000 norm=3.1118
[iter 400] loss=0.1507 val_loss=0.0000 scale=4.0000 norm=3.0819
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0174 val_loss=0.0000 scale=2.0000 norm=1.5806
[iter 200] loss=0.8023 val_loss=0.0000 scale=2.0000 norm=1.5627
[iter 300] loss=0.5053 val_loss=0.0000 scale=4.0000 norm=3.1090
[iter 400] loss=0.3373 val_loss=0.0000 scale=4.0000 norm=3.1220
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0277 val_loss=0.0000 scale=2.0000 norm=1.5990
[iter 200] loss=0.7860 val_loss=0.0000 scale=4.0000 norm=3.1519
[iter 300] loss=0.4707 val_loss=0.0000 scale=4.0000 norm=3.0966
[iter 400] loss=0.3220 val_loss=0.0000 scale=4.0000 norm=3.1517
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0415 val_loss=0.0000 scale=2.0000 norm=1.5931
[iter 200] loss=0.8347 val_loss=0.0000 scale=4.0000 norm=3.1531
[iter 300] loss=0.5256 val_loss=0.0000 scale=4.0000 norm=3.1161
[iter 400] loss=0.3893 val_loss=0.0000 scale=4.0000 norm=3.1204
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0778 val_loss=0.0000 scale=2.0000 norm=1.6178
[iter 200] loss=0.8934 val_loss=0.0000 scale=4.0000 norm=3.2012
[iter 300] loss=0.6228 val_loss=0.0000 scale=4.0000 norm=3.1860
[iter 400] loss=0.4199 val_loss=0.0000 scale=4.0000 norm=3.1619
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0651 val_loss=0.0000 scale=2.0000 norm=1.6710
[iter 200] loss=0.7351 val_loss=0.0000 scale=4.0000 norm=3.3113
[iter 300] loss=0.3693 val_loss=0.0000 scale=4.0000 norm=3.2829
[iter 400] loss=0.0563 val_loss=0.0000 scale=4.0000 norm=3.2271
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7440 val_loss=0.0000 scale=2.0000 norm=1.2909
[iter 200] loss=0.4834 val_loss=0.0000 scale=2.0000 norm=1.2987
[iter 300] loss=0.1105 val_loss=0.0000 scale=4.0000 norm=2.5643
[iter 400] loss=-0.1627 val_loss=0.0000 scale=4.0000 norm=2.4241
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0396 val_loss=0.0000 scale=2.0000 norm=1.6201
[iter 200] loss=0.7559 val_loss=0.0000 scale=4.0000 norm=3.2093
[iter 300] loss=0.3869 val_loss=0.0000 scale=4.0000 norm=3.1558
[iter 400] loss=0.1186 val_loss=0.0000 scale=4.0000 norm=3.0978
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0339 val_loss=0.0000 scale=2.0000 norm=1.6113
[iter 200] loss=0.7920 val_loss=0.0000 scale=4.0000 norm=3.1820
[iter 300] loss=0.4756 val_loss=0.0000 scale=4.0000 norm=3.1163
[iter 400] loss=0.2707 val_loss=0.0000 scale=4.0000 norm=3.1003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0541 val_loss=0.0000 scale=2.0000 norm=1.6346
[iter 200] loss=0.7396 val_loss=0.0000 scale=4.0000 norm=3.2611
[iter 300] loss=0.4664 val_loss=0.0000 scale=4.0000 norm=3.2158
[iter 400] loss=0.3773 val_loss=0.0000 scale=2.0000 norm=1.5816
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9110 val_loss=0.0000 scale=2.0000 norm=1.5906
[iter 200] loss=0.5587 val_loss=0.0000 scale=4.0000 norm=3.0809
[iter 300] loss=0.1367 val_loss=0.0000 scale=4.0000 norm=3.0807
[iter 400] loss=-0.2873 val_loss=0.0000 scale=4.0000 norm=3.0593
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0234 val_loss=0.0000 scale=2.0000 norm=1.5866
[iter 200] loss=0.8090 val_loss=0.0000 scale=4.0000 norm=3.1347
[iter 300] loss=0.5189 val_loss=0.0000 scale=2.0000 norm=1.5399
[iter 400] loss=0.4252 val_loss=0.0000 scale=4.0000 norm=3.0926
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9500 val_loss=0.0000 scale=2.0000 norm=1.6676
[iter 200] loss=0.4892 val_loss=0.0000 scale=4.0000 norm=3.3495
[iter 300] loss=-0.3106 val_loss=0.0000 scale=4.0000 norm=3.3524
[iter 400] loss=-1.1764 val_loss=0.0000 scale=4.0000 norm=3.3247
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0419 val_loss=0.0000 scale=2.0000 norm=1.6425
[iter 200] loss=0.8278 val_loss=0.0000 scale=2.0000 norm=1.6401
[iter 300] loss=0.5041 val_loss=0.0000 scale=4.0000 norm=3.2436
[iter 400] loss=0.3407 val_loss=0.0000 scale=4.0000 norm=3.1983
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0995 val_loss=0.0000 scale=2.0000 norm=1.6764
[iter 200] loss=0.8763 val_loss=0.0000 scale=4.0000 norm=3.3019
[iter 300] loss=0.6017 val_loss=0.0000 scale=4.0000 norm=3.2700
[iter 400] loss=0.2967 val_loss=0.0000 scale=8.0000 norm=6.4068
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0122 val_loss=0.0000 scale=2.0000 norm=1.5881
[iter 200] loss=0.7635 val_loss=0.0000 scale=4.0000 norm=3.1345
[iter 300] loss=0.4450 val_loss=0.0000 scale=4.0000 norm=3.0944
[iter 400] loss=0.2695 val_loss=0.0000 scale=4.0000 norm=3.0704
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0009 val_loss=0.0000 scale=2.0000 norm=1.5821
[iter 200] loss=0.7344 val_loss=0.0000 scale=4.0000 norm=3.1243
[iter 300] loss=0.4150 val_loss=0.0000 scale=4.0000 norm=3.0700
[iter 400] loss=0.2685 val_loss=0.0000 scale=4.0000 norm=3.1271
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6583 val_loss=0.0000 scale=2.0000 norm=1.2787
[iter 200] loss=0.4069 val_loss=0.0000 scale=2.0000 norm=1.3032
[iter 300] loss=0.1949 val_loss=0.0000 scale=4.0000 norm=2.6841
[iter 400] loss=-0.0507 val_loss=0.0000 scale=8.0000 norm=5.3463
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6957 val_loss=0.0000 scale=2.0000 norm=1.3640
[iter 200] loss=0.4211 val_loss=0.0000 scale=4.0000 norm=2.9320
[iter 300] loss=0.0761 val_loss=0.0000 scale=4.0000 norm=2.9363
[iter 400] loss=-0.2471 val_loss=0.0000 scale=8.0000 norm=5.7188
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0496 val_loss=0.0000 scale=2.0000 norm=1.6596
[iter 200] loss=0.7332 val_loss=0.0000 scale=4.0000 norm=3.2500
[iter 300] loss=0.4585 val_loss=0.0000 scale=4.0000 norm=3.2312
[iter 400] loss=0.2540 val_loss=0.0000 scale=4.0000 norm=3.1703
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8192 val_loss=0.0000 scale=2.0000 norm=1.4215
[iter 200] loss=0.5894 val_loss=0.0000 scale=2.0000 norm=1.4642
[iter 300] loss=0.3001 val_loss=0.0000 scale=4.0000 norm=2.9028
[iter 400] loss=0.0563 val_loss=0.0000 scale=4.0000 norm=2.8390
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9875 val_loss=0.0000 scale=2.0000 norm=1.5674
[iter 200] loss=0.7342 val_loss=0.0000 scale=2.0000 norm=1.5477
[iter 300] loss=0.3769 val_loss=0.0000 scale=4.0000 norm=3.1053
[iter 400] loss=0.1194 val_loss=0.0000 scale=4.0000 norm=3.0167
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6629 val_loss=0.0000 scale=2.0000 norm=1.2795
[iter 200] loss=0.3129 val_loss=0.0000 scale=2.0000 norm=1.2816
[iter 300] loss=-0.1512 val_loss=0.0000 scale=4.0000 norm=2.5229
[iter 400] loss=-0.5478 val_loss=0.0000 scale=4.0000 norm=2.5081
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0267 val_loss=0.0000 scale=2.0000 norm=1.5889
[iter 200] loss=0.8087 val_loss=0.0000 scale=2.0000 norm=1.5791
[iter 300] loss=0.5004 val_loss=0.0000 scale=4.0000 norm=3.1232
[iter 400] loss=0.2281 val_loss=0.0000 scale=4.0000 norm=3.0750
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6962 val_loss=0.0000 scale=2.0000 norm=1.5055
[iter 200] loss=0.1511 val_loss=0.0000 scale=4.0000 norm=2.8233
[iter 300] loss=-0.4435 val_loss=0.0000 scale=4.0000 norm=2.7352
[iter 400] loss=-0.8497 val_loss=0.0000 scale=4.0000 norm=2.7297
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0429 val_loss=0.0000 scale=2.0000 norm=1.6686
[iter 200] loss=0.6958 val_loss=0.0000 scale=4.0000 norm=3.3336
[iter 300] loss=0.1358 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-0.9442 val_loss=0.0000 scale=16.0000 norm=13.3438
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0093 val_loss=0.0000 scale=2.0000 norm=1.5745
[iter 200] loss=0.7652 val_loss=0.0000 scale=2.0000 norm=1.5597
[iter 300] loss=0.4002 val_loss=0.0000 scale=4.0000 norm=3.0458
[iter 400] loss=0.1957 val_loss=0.0000 scale=4.0000 norm=3.0375
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9890 val_loss=0.0000 scale=2.0000 norm=1.5520
[iter 200] loss=0.7520 val_loss=0.0000 scale=4.0000 norm=3.0704
[iter 300] loss=0.4238 val_loss=0.0000 scale=4.0000 norm=3.1078
[iter 400] loss=0.1186 val_loss=0.0000 scale=4.0000 norm=3.0727
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0090 val_loss=0.0000 scale=2.0000 norm=1.6955
[iter 200] loss=0.6999 val_loss=0.0000 scale=4.0000 norm=3.3966
[iter 300] loss=0.3105 val_loss=0.0000 scale=8.0000 norm=6.6500
[iter 400] loss=-0.4275 val_loss=0.0000 scale=16.0000 norm=13.3003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0081 val_loss=0.0000 scale=2.0000 norm=1.5965
[iter 200] loss=0.7594 val_loss=0.0000 scale=4.0000 norm=3.1520
[iter 300] loss=0.4194 val_loss=0.0000 scale=4.0000 norm=3.1094
[iter 400] loss=0.1890 val_loss=0.0000 scale=4.0000 norm=3.0699
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0332 val_loss=0.0000 scale=2.0000 norm=1.7085
[iter 200] loss=0.4992 val_loss=0.0000 scale=4.0000 norm=3.3739
[iter 300] loss=-0.1150 val_loss=0.0000 scale=8.0000 norm=6.4115
[iter 400] loss=-1.2991 val_loss=0.0000 scale=16.0000 norm=12.8159
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7953 val_loss=0.0000 scale=2.0000 norm=1.6092
[iter 200] loss=0.2738 val_loss=0.0000 scale=4.0000 norm=3.1503
[iter 300] loss=-0.5520 val_loss=0.0000 scale=8.0000 norm=6.2905
[iter 400] loss=-2.1820 val_loss=0.0000 scale=16.0000 norm=12.5810
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0113 val_loss=0.0000 scale=2.0000 norm=1.5982
[iter 200] loss=0.7422 val_loss=0.0000 scale=4.0000 norm=3.1519
[iter 300] loss=0.4280 val_loss=0.0000 scale=4.0000 norm=3.0995
[iter 400] loss=0.2553 val_loss=0.0000 scale=4.0000 norm=3.1663
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0418 val_loss=0.0000 scale=2.0000 norm=1.5935
[iter 200] loss=0.8374 val_loss=0.0000 scale=2.0000 norm=1.5838
[iter 300] loss=0.5650 val_loss=0.0000 scale=4.0000 norm=3.1279
[iter 400] loss=0.4443 val_loss=0.0000 scale=4.0000 norm=3.0453
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9875 val_loss=0.0000 scale=2.0000 norm=1.5688
[iter 200] loss=0.7556 val_loss=0.0000 scale=2.0000 norm=1.5475
[iter 300] loss=0.4214 val_loss=0.0000 scale=4.0000 norm=3.0673
[iter 400] loss=0.2199 val_loss=0.0000 scale=4.0000 norm=3.0483
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0040 val_loss=0.0000 scale=2.0000 norm=1.6032
[iter 200] loss=0.6978 val_loss=0.0000 scale=4.0000 norm=3.1855
[iter 300] loss=0.3501 val_loss=0.0000 scale=4.0000 norm=3.1288
[iter 400] loss=0.1052 val_loss=0.0000 scale=2.0000 norm=1.5394
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9461 val_loss=0.0000 scale=2.0000 norm=1.5456
[iter 200] loss=0.6742 val_loss=0.0000 scale=2.0000 norm=1.5422
[iter 300] loss=0.2660 val_loss=0.0000 scale=4.0000 norm=3.0578
[iter 400] loss=-0.0550 val_loss=0.0000 scale=4.0000 norm=2.9610
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6310 val_loss=0.0000 scale=2.0000 norm=1.3721
[iter 200] loss=0.2143 val_loss=0.0000 scale=4.0000 norm=2.6093
[iter 300] loss=-0.2981 val_loss=0.0000 scale=4.0000 norm=2.6618
[iter 400] loss=-0.7634 val_loss=0.0000 scale=8.0000 norm=4.9863
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8970 val_loss=0.0000 scale=2.0000 norm=1.5607
[iter 200] loss=0.5414 val_loss=0.0000 scale=4.0000 norm=3.0579
[iter 300] loss=-0.0644 val_loss=0.0000 scale=4.0000 norm=3.0232
[iter 400] loss=-1.1264 val_loss=0.0000 scale=8.0000 norm=6.0466
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0143 val_loss=0.0000 scale=2.0000 norm=1.5806
[iter 200] loss=0.7894 val_loss=0.0000 scale=2.0000 norm=1.5615
[iter 300] loss=0.4922 val_loss=0.0000 scale=4.0000 norm=3.0882
[iter 400] loss=0.2713 val_loss=0.0000 scale=4.0000 norm=3.0314
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0076 val_loss=0.0000 scale=2.0000 norm=1.5893
[iter 200] loss=0.7547 val_loss=0.0000 scale=4.0000 norm=3.1421
[iter 300] loss=0.4096 val_loss=0.0000 scale=4.0000 norm=3.1074
[iter 400] loss=0.2404 val_loss=0.0000 scale=4.0000 norm=3.0936
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0124 val_loss=0.0000 scale=2.0000 norm=1.5816
[iter 200] loss=0.7729 val_loss=0.0000 scale=2.0000 norm=1.5705
[iter 300] loss=0.4282 val_loss=0.0000 scale=4.0000 norm=3.0773
[iter 400] loss=0.2128 val_loss=0.0000 scale=4.0000 norm=3.0715
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0563 val_loss=0.0000 scale=2.0000 norm=1.6139
[iter 200] loss=0.8547 val_loss=0.0000 scale=2.0000 norm=1.5970
[iter 300] loss=0.5432 val_loss=0.0000 scale=4.0000 norm=3.1381
[iter 400] loss=0.3549 val_loss=0.0000 scale=4.0000 norm=3.0866
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9812 val_loss=0.0000 scale=2.0000 norm=1.5610
[iter 200] loss=0.7195 val_loss=0.0000 scale=2.0000 norm=1.5582
[iter 300] loss=0.3486 val_loss=0.0000 scale=4.0000 norm=3.0605
[iter 400] loss=0.0488 val_loss=0.0000 scale=4.0000 norm=3.0347
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0628 val_loss=0.0000 scale=2.0000 norm=1.6668
[iter 200] loss=0.8670 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 300] loss=0.6577 val_loss=0.0000 scale=2.0000 norm=1.6518
[iter 400] loss=0.5411 val_loss=0.0000 scale=4.0000 norm=3.2780
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9507 val_loss=0.0000 scale=2.0000 norm=1.5699
[iter 200] loss=0.5647 val_loss=0.0000 scale=4.0000 norm=3.1348
[iter 300] loss=0.0947 val_loss=0.0000 scale=4.0000 norm=2.9958
[iter 400] loss=-0.1667 val_loss=0.0000 scale=4.0000 norm=3.0034
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9998 val_loss=0.0000 scale=2.0000 norm=1.5863
[iter 200] loss=0.7518 val_loss=0.0000 scale=4.0000 norm=3.1655
[iter 300] loss=0.4154 val_loss=0.0000 scale=4.0000 norm=3.1130
[iter 400] loss=0.2415 val_loss=0.0000 scale=4.0000 norm=3.1060
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1179 val_loss=0.0000 scale=2.0000 norm=1.6818
[iter 200] loss=0.9481 val_loss=0.0000 scale=4.0000 norm=3.3372
[iter 300] loss=0.7063 val_loss=0.0000 scale=4.0000 norm=3.3044
[iter 400] loss=0.5374 val_loss=0.0000 scale=4.0000 norm=3.2674
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0648 val_loss=0.0000 scale=2.0000 norm=1.6387
[iter 200] loss=0.8547 val_loss=0.0000 scale=2.0000 norm=1.6290
[iter 300] loss=0.5697 val_loss=0.0000 scale=4.0000 norm=3.2233
[iter 400] loss=0.2832 val_loss=0.0000 scale=4.0000 norm=3.1750
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8645 val_loss=0.0000 scale=2.0000 norm=1.5263
[iter 200] loss=0.5271 val_loss=0.0000 scale=4.0000 norm=3.0291
[iter 300] loss=0.0238 val_loss=0.0000 scale=4.0000 norm=2.9543
[iter 400] loss=-0.3579 val_loss=0.0000 scale=4.0000 norm=2.8961
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0057 val_loss=0.0000 scale=2.0000 norm=1.5760
[iter 200] loss=0.7606 val_loss=0.0000 scale=2.0000 norm=1.5585
[iter 300] loss=0.4099 val_loss=0.0000 scale=4.0000 norm=3.1022
[iter 400] loss=0.1690 val_loss=0.0000 scale=4.0000 norm=3.0001
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4124 val_loss=0.0000 scale=2.0000 norm=0.9616
[iter 200] loss=-0.0950 val_loss=0.0000 scale=2.0000 norm=0.9070
[iter 300] loss=-0.6838 val_loss=0.0000 scale=4.0000 norm=1.7278
[iter 400] loss=-1.5673 val_loss=0.0000 scale=8.0000 norm=3.3759
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0343 val_loss=0.0000 scale=2.0000 norm=1.5823
[iter 200] loss=0.7724 val_loss=0.0000 scale=4.0000 norm=3.1443
[iter 300] loss=0.5020 val_loss=0.0000 scale=4.0000 norm=3.1031
[iter 400] loss=0.3302 val_loss=0.0000 scale=8.0000 norm=6.0566
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9239 val_loss=0.0000 scale=2.0000 norm=1.5647
[iter 200] loss=0.5926 val_loss=0.0000 scale=4.0000 norm=3.0299
[iter 300] loss=0.2172 val_loss=0.0000 scale=4.0000 norm=2.9853
[iter 400] loss=-0.1059 val_loss=0.0000 scale=4.0000 norm=2.9422
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9408 val_loss=0.0000 scale=2.0000 norm=1.5946
[iter 200] loss=0.5938 val_loss=0.0000 scale=4.0000 norm=3.1181
[iter 300] loss=0.1453 val_loss=0.0000 scale=4.0000 norm=3.0942
[iter 400] loss=-0.1883 val_loss=0.0000 scale=4.0000 norm=3.0495
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0102 val_loss=0.0000 scale=2.0000 norm=1.6209
[iter 200] loss=0.6947 val_loss=0.0000 scale=4.0000 norm=3.2027
[iter 300] loss=0.2823 val_loss=0.0000 scale=4.0000 norm=3.1729
[iter 400] loss=-0.0073 val_loss=0.0000 scale=4.0000 norm=3.1469
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0342 val_loss=0.0000 scale=2.0000 norm=1.6311
[iter 200] loss=0.8010 val_loss=0.0000 scale=2.0000 norm=1.6271
[iter 300] loss=0.4696 val_loss=0.0000 scale=4.0000 norm=3.2404
[iter 400] loss=0.1589 val_loss=0.0000 scale=4.0000 norm=3.2024
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0506 val_loss=0.0000 scale=2.0000 norm=1.6723
[iter 200] loss=0.8200 val_loss=0.0000 scale=4.0000 norm=3.3250
[iter 300] loss=0.5428 val_loss=0.0000 scale=4.0000 norm=3.2464
[iter 400] loss=0.4097 val_loss=0.0000 scale=4.0000 norm=3.2012
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1633 val_loss=0.0000 scale=2.0000 norm=1.8733
[iter 200] loss=0.8688 val_loss=0.0000 scale=4.0000 norm=3.7637
[iter 300] loss=0.3648 val_loss=0.0000 scale=8.0000 norm=7.5297
[iter 400] loss=-0.6112 val_loss=0.0000 scale=16.0000 norm=15.0595
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9886 val_loss=0.0000 scale=2.0000 norm=1.6445
[iter 200] loss=0.4567 val_loss=0.0000 scale=4.0000 norm=3.2644
[iter 300] loss=-0.2040 val_loss=0.0000 scale=4.0000 norm=3.2013
[iter 400] loss=-0.7816 val_loss=0.0000 scale=4.0000 norm=3.1970
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0041 val_loss=0.0000 scale=2.0000 norm=1.5780
[iter 200] loss=0.7678 val_loss=0.0000 scale=2.0000 norm=1.5623
[iter 300] loss=0.4096 val_loss=0.0000 scale=4.0000 norm=3.0678
[iter 400] loss=0.1991 val_loss=0.0000 scale=4.0000 norm=3.0268
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9854 val_loss=0.0000 scale=2.0000 norm=1.5682
[iter 200] loss=0.7404 val_loss=0.0000 scale=4.0000 norm=3.1051
[iter 300] loss=0.4053 val_loss=0.0000 scale=4.0000 norm=3.0759
[iter 400] loss=0.2129 val_loss=0.0000 scale=4.0000 norm=3.0469
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0013 val_loss=0.0000 scale=2.0000 norm=1.5855
[iter 200] loss=0.7493 val_loss=0.0000 scale=2.0000 norm=1.5575
[iter 300] loss=0.4526 val_loss=0.0000 scale=4.0000 norm=3.0686
[iter 400] loss=0.1978 val_loss=0.0000 scale=4.0000 norm=2.9694
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0088 val_loss=0.0000 scale=2.0000 norm=1.5899
[iter 200] loss=0.7281 val_loss=0.0000 scale=4.0000 norm=3.1263
[iter 300] loss=0.3585 val_loss=0.0000 scale=4.0000 norm=3.0748
[iter 400] loss=0.0927 val_loss=0.0000 scale=4.0000 norm=2.9743
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0372 val_loss=0.0000 scale=2.0000 norm=1.6136
[iter 200] loss=0.7515 val_loss=0.0000 scale=4.0000 norm=3.1825
[iter 300] loss=0.3810 val_loss=0.0000 scale=4.0000 norm=3.1343
[iter 400] loss=0.1533 val_loss=0.0000 scale=4.0000 norm=3.1280
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0273 val_loss=0.0000 scale=2.0000 norm=1.6087
[iter 200] loss=0.8126 val_loss=0.0000 scale=2.0000 norm=1.6059
[iter 300] loss=0.4844 val_loss=0.0000 scale=4.0000 norm=3.1513
[iter 400] loss=0.2890 val_loss=0.0000 scale=4.0000 norm=3.0915
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9495 val_loss=0.0000 scale=2.0000 norm=1.5567
[iter 200] loss=0.6587 val_loss=0.0000 scale=4.0000 norm=3.0427
[iter 300] loss=0.3156 val_loss=0.0000 scale=4.0000 norm=3.0005
[iter 400] loss=0.0659 val_loss=0.0000 scale=4.0000 norm=2.9458
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9574 val_loss=0.0000 scale=2.0000 norm=1.5669
[iter 200] loss=0.6260 val_loss=0.0000 scale=4.0000 norm=3.1315
[iter 300] loss=0.1863 val_loss=0.0000 scale=4.0000 norm=3.1153
[iter 400] loss=-0.1552 val_loss=0.0000 scale=4.0000 norm=3.0548
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0170 val_loss=0.0000 scale=2.0000 norm=1.5854
[iter 200] loss=0.7866 val_loss=0.0000 scale=2.0000 norm=1.5754
[iter 300] loss=0.5044 val_loss=0.0000 scale=4.0000 norm=3.1167
[iter 400] loss=0.2927 val_loss=0.0000 scale=4.0000 norm=3.0954
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0290 val_loss=0.0000 scale=2.0000 norm=1.6060
[iter 200] loss=0.7881 val_loss=0.0000 scale=4.0000 norm=3.1762
[iter 300] loss=0.4697 val_loss=0.0000 scale=4.0000 norm=3.1224
[iter 400] loss=0.2833 val_loss=0.0000 scale=4.0000 norm=3.0765
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6274 val_loss=0.0000 scale=2.0000 norm=1.1954
[iter 200] loss=0.2693 val_loss=0.0000 scale=2.0000 norm=1.2846
[iter 300] loss=-0.1878 val_loss=0.0000 scale=4.0000 norm=2.5838
[iter 400] loss=-0.5361 val_loss=0.0000 scale=8.0000 norm=4.9597
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0137 val_loss=0.0000 scale=2.0000 norm=1.5904
[iter 200] loss=0.7575 val_loss=0.0000 scale=4.0000 norm=3.1682
[iter 300] loss=0.4604 val_loss=0.0000 scale=4.0000 norm=3.0944
[iter 400] loss=0.3115 val_loss=0.0000 scale=4.0000 norm=3.1237
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9150 val_loss=0.0000 scale=2.0000 norm=1.5396
[iter 200] loss=0.5946 val_loss=0.0000 scale=4.0000 norm=2.9950
[iter 300] loss=0.2309 val_loss=0.0000 scale=4.0000 norm=2.9680
[iter 400] loss=-0.0315 val_loss=0.0000 scale=8.0000 norm=5.9154
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9939 val_loss=0.0000 scale=2.0000 norm=1.5959
[iter 200] loss=0.7300 val_loss=0.0000 scale=4.0000 norm=3.1821
[iter 300] loss=0.3454 val_loss=0.0000 scale=4.0000 norm=3.1392
[iter 400] loss=0.1173 val_loss=0.0000 scale=4.0000 norm=3.1390
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7718 val_loss=0.0000 scale=2.0000 norm=1.4665
[iter 200] loss=0.4834 val_loss=0.0000 scale=4.0000 norm=3.0491
[iter 300] loss=0.0651 val_loss=0.0000 scale=4.0000 norm=3.0297
[iter 400] loss=-0.3484 val_loss=0.0000 scale=8.0000 norm=5.9204
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1069 val_loss=0.0000 scale=2.0000 norm=1.6496
[iter 200] loss=0.9497 val_loss=0.0000 scale=4.0000 norm=3.2680
[iter 300] loss=0.7246 val_loss=0.0000 scale=4.0000 norm=3.2225
[iter 400] loss=0.5835 val_loss=0.0000 scale=4.0000 norm=3.2069
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0522 val_loss=0.0000 scale=2.0000 norm=1.6267
[iter 200] loss=0.8226 val_loss=0.0000 scale=2.0000 norm=1.6064
[iter 300] loss=0.5187 val_loss=0.0000 scale=4.0000 norm=3.1593
[iter 400] loss=0.2546 val_loss=0.0000 scale=2.0000 norm=1.5429
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0114 val_loss=0.0000 scale=2.0000 norm=1.5969
[iter 200] loss=0.7393 val_loss=0.0000 scale=4.0000 norm=3.1677
[iter 300] loss=0.4084 val_loss=0.0000 scale=4.0000 norm=3.1260
[iter 400] loss=0.2432 val_loss=0.0000 scale=4.0000 norm=3.1619
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8919 val_loss=0.0000 scale=2.0000 norm=1.4818
[iter 200] loss=0.6587 val_loss=0.0000 scale=2.0000 norm=1.5096
[iter 300] loss=0.3260 val_loss=0.0000 scale=4.0000 norm=2.9658
[iter 400] loss=0.1399 val_loss=0.0000 scale=4.0000 norm=2.9707
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8676 val_loss=0.0000 scale=2.0000 norm=1.4664
[iter 200] loss=0.6490 val_loss=0.0000 scale=2.0000 norm=1.5150
[iter 300] loss=0.3051 val_loss=0.0000 scale=4.0000 norm=3.0201
[iter 400] loss=0.0323 val_loss=0.0000 scale=4.0000 norm=2.9884
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7349 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 200] loss=-0.1759 val_loss=0.0000 scale=4.0000 norm=3.0688
[iter 300] loss=-1.3658 val_loss=0.0000 scale=8.0000 norm=5.8055
[iter 400] loss=-3.5098 val_loss=0.0000 scale=16.0000 norm=11.6070
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0169 val_loss=0.0000 scale=2.0000 norm=1.6159
[iter 200] loss=0.7167 val_loss=0.0000 scale=4.0000 norm=3.2277
[iter 300] loss=0.2053 val_loss=0.0000 scale=4.0000 norm=3.2358
[iter 400] loss=-0.2880 val_loss=0.0000 scale=4.0000 norm=3.2030
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9232 val_loss=0.0000 scale=2.0000 norm=1.6326
[iter 200] loss=0.5877 val_loss=0.0000 scale=4.0000 norm=3.3123
[iter 300] loss=0.0687 val_loss=0.0000 scale=4.0000 norm=3.2919
[iter 400] loss=-0.4367 val_loss=0.0000 scale=4.0000 norm=3.2798
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9923 val_loss=0.0000 scale=2.0000 norm=1.5693
[iter 200] loss=0.6980 val_loss=0.0000 scale=4.0000 norm=3.1202
[iter 300] loss=0.2915 val_loss=0.0000 scale=4.0000 norm=3.0816
[iter 400] loss=0.0245 val_loss=0.0000 scale=4.0000 norm=3.0264
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9060 val_loss=0.0000 scale=2.0000 norm=1.5224
[iter 200] loss=0.5685 val_loss=0.0000 scale=4.0000 norm=3.0353
[iter 300] loss=0.0508 val_loss=0.0000 scale=4.0000 norm=2.9930
[iter 400] loss=-0.2715 val_loss=0.0000 scale=4.0000 norm=2.8450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0155 val_loss=0.0000 scale=2.0000 norm=1.5940
[iter 200] loss=0.7700 val_loss=0.0000 scale=4.0000 norm=3.1744
[iter 300] loss=0.4372 val_loss=0.0000 scale=4.0000 norm=3.1130
[iter 400] loss=0.2678 val_loss=0.0000 scale=4.0000 norm=3.1114
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0262 val_loss=0.0000 scale=2.0000 norm=1.6109
[iter 200] loss=0.7578 val_loss=0.0000 scale=4.0000 norm=3.1829
[iter 300] loss=0.4710 val_loss=0.0000 scale=4.0000 norm=3.0984
[iter 400] loss=0.3128 val_loss=0.0000 scale=4.0000 norm=3.0955
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7949 val_loss=0.0000 scale=2.0000 norm=1.5048
[iter 200] loss=0.5444 val_loss=0.0000 scale=2.0000 norm=1.5770
[iter 300] loss=0.1058 val_loss=0.0000 scale=4.0000 norm=3.1522
[iter 400] loss=-0.5208 val_loss=0.0000 scale=8.0000 norm=6.2143
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0301 val_loss=0.0000 scale=2.0000 norm=1.6205
[iter 200] loss=0.7538 val_loss=0.0000 scale=4.0000 norm=3.2290
[iter 300] loss=0.3609 val_loss=0.0000 scale=4.0000 norm=3.1858
[iter 400] loss=0.1041 val_loss=0.0000 scale=4.0000 norm=3.1382
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7133 val_loss=0.0000 scale=2.0000 norm=1.3002
[iter 200] loss=0.3414 val_loss=0.0000 scale=4.0000 norm=2.6920
[iter 300] loss=-0.1852 val_loss=0.0000 scale=4.0000 norm=2.6577
[iter 400] loss=-0.7149 val_loss=0.0000 scale=4.0000 norm=2.6712
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0546 val_loss=0.0000 scale=2.0000 norm=1.6247
[iter 200] loss=0.8189 val_loss=0.0000 scale=4.0000 norm=3.2300
[iter 300] loss=0.4921 val_loss=0.0000 scale=4.0000 norm=3.2044
[iter 400] loss=0.3016 val_loss=0.0000 scale=4.0000 norm=3.1565
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0335 val_loss=0.0000 scale=2.0000 norm=1.6000
[iter 200] loss=0.8152 val_loss=0.0000 scale=2.0000 norm=1.5864
[iter 300] loss=0.5193 val_loss=0.0000 scale=2.0000 norm=1.5618
[iter 400] loss=0.3584 val_loss=0.0000 scale=4.0000 norm=3.1272
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0081 val_loss=0.0000 scale=2.0000 norm=1.6075
[iter 200] loss=0.7637 val_loss=0.0000 scale=4.0000 norm=3.1910
[iter 300] loss=0.4344 val_loss=0.0000 scale=4.0000 norm=3.1838
[iter 400] loss=0.2367 val_loss=0.0000 scale=4.0000 norm=3.1243
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9630 val_loss=0.0000 scale=2.0000 norm=1.6808
[iter 200] loss=0.5682 val_loss=0.0000 scale=4.0000 norm=3.3239
[iter 300] loss=-0.0770 val_loss=0.0000 scale=8.0000 norm=6.6579
[iter 400] loss=-1.3370 val_loss=0.0000 scale=16.0000 norm=13.3158
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9849 val_loss=0.0000 scale=2.0000 norm=1.5630
[iter 200] loss=0.7442 val_loss=0.0000 scale=2.0000 norm=1.5465
[iter 300] loss=0.4059 val_loss=0.0000 scale=4.0000 norm=3.0348
[iter 400] loss=0.2218 val_loss=0.0000 scale=4.0000 norm=3.0013
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0348 val_loss=0.0000 scale=2.0000 norm=1.5974
[iter 200] loss=0.8275 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 300] loss=0.5549 val_loss=0.0000 scale=4.0000 norm=3.1209
[iter 400] loss=0.3731 val_loss=0.0000 scale=4.0000 norm=3.0848
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0012 val_loss=0.0000 scale=2.0000 norm=1.6051
[iter 200] loss=0.7072 val_loss=0.0000 scale=4.0000 norm=3.1905
[iter 300] loss=0.3580 val_loss=0.0000 scale=4.0000 norm=3.1490
[iter 400] loss=0.1319 val_loss=0.0000 scale=4.0000 norm=3.0744
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0086 val_loss=0.0000 scale=2.0000 norm=1.5813
[iter 200] loss=0.7818 val_loss=0.0000 scale=2.0000 norm=1.5589
[iter 300] loss=0.4587 val_loss=0.0000 scale=4.0000 norm=3.0845
[iter 400] loss=0.2721 val_loss=0.0000 scale=4.0000 norm=3.0627
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9578 val_loss=0.0000 scale=2.0000 norm=1.5392
[iter 200] loss=0.6611 val_loss=0.0000 scale=4.0000 norm=3.0385
[iter 300] loss=0.2650 val_loss=0.0000 scale=4.0000 norm=2.9922
[iter 400] loss=0.0049 val_loss=0.0000 scale=4.0000 norm=2.9525
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0392 val_loss=0.0000 scale=2.0000 norm=1.5899
[iter 200] loss=0.8242 val_loss=0.0000 scale=4.0000 norm=3.1478
[iter 300] loss=0.5301 val_loss=0.0000 scale=4.0000 norm=3.1203
[iter 400] loss=0.3922 val_loss=0.0000 scale=4.0000 norm=3.1024
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6195 val_loss=0.0000 scale=2.0000 norm=1.1033
[iter 200] loss=0.2836 val_loss=0.0000 scale=4.0000 norm=2.2032
[iter 300] loss=-0.1897 val_loss=0.0000 scale=4.0000 norm=2.0853
[iter 400] loss=-0.9023 val_loss=0.0000 scale=8.0000 norm=4.1393
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0959 val_loss=0.0000 scale=2.0000 norm=1.6489
[iter 200] loss=0.9140 val_loss=0.0000 scale=4.0000 norm=3.2755
[iter 300] loss=0.6595 val_loss=0.0000 scale=4.0000 norm=3.2614
[iter 400] loss=0.4677 val_loss=0.0000 scale=4.0000 norm=3.2392
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.5697
[iter 200] loss=0.7489 val_loss=0.0000 scale=2.0000 norm=1.5572
[iter 300] loss=0.4027 val_loss=0.0000 scale=4.0000 norm=3.0748
[iter 400] loss=0.2073 val_loss=0.0000 scale=4.0000 norm=3.0272
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0491 val_loss=0.0000 scale=2.0000 norm=1.6294
[iter 200] loss=0.8257 val_loss=0.0000 scale=2.0000 norm=1.6212
[iter 300] loss=0.5146 val_loss=0.0000 scale=4.0000 norm=3.1842
[iter 400] loss=0.3556 val_loss=0.0000 scale=4.0000 norm=3.1670
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9339 val_loss=0.0000 scale=2.0000 norm=1.5535
[iter 200] loss=0.5625 val_loss=0.0000 scale=4.0000 norm=3.0655
[iter 300] loss=0.0249 val_loss=0.0000 scale=4.0000 norm=3.0075
[iter 400] loss=-0.4594 val_loss=0.0000 scale=4.0000 norm=2.9547
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9395 val_loss=0.0000 scale=2.0000 norm=1.5559
[iter 200] loss=0.7674 val_loss=0.0000 scale=2.0000 norm=1.5488
[iter 300] loss=0.5611 val_loss=0.0000 scale=4.0000 norm=3.0599
[iter 400] loss=0.2355 val_loss=0.0000 scale=8.0000 norm=6.1201
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0574 val_loss=0.0000 scale=2.0000 norm=1.6404
[iter 200] loss=0.8513 val_loss=0.0000 scale=2.0000 norm=1.6459
[iter 300] loss=0.6595 val_loss=0.0000 scale=2.0000 norm=1.6316
[iter 400] loss=0.4533 val_loss=0.0000 scale=4.0000 norm=3.2380
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9581 val_loss=0.0000 scale=2.0000 norm=1.6012
[iter 200] loss=0.6943 val_loss=0.0000 scale=4.0000 norm=3.1566
[iter 300] loss=0.3704 val_loss=0.0000 scale=4.0000 norm=3.1559
[iter 400] loss=0.1059 val_loss=0.0000 scale=4.0000 norm=3.1085
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1168 val_loss=0.0000 scale=2.0000 norm=1.6862
[iter 200] loss=0.9342 val_loss=0.0000 scale=4.0000 norm=3.3066
[iter 300] loss=0.6290 val_loss=0.0000 scale=4.0000 norm=3.3164
[iter 400] loss=0.3383 val_loss=0.0000 scale=4.0000 norm=3.2942
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9632 val_loss=0.0000 scale=2.0000 norm=1.5497
[iter 200] loss=0.6916 val_loss=0.0000 scale=2.0000 norm=1.5203
[iter 300] loss=0.3597 val_loss=0.0000 scale=4.0000 norm=3.0047
[iter 400] loss=0.1263 val_loss=0.0000 scale=4.0000 norm=2.9819
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9875 val_loss=0.0000 scale=2.0000 norm=1.5689
[iter 200] loss=0.7416 val_loss=0.0000 scale=4.0000 norm=3.1241
[iter 300] loss=0.3806 val_loss=0.0000 scale=4.0000 norm=3.0905
[iter 400] loss=0.1968 val_loss=0.0000 scale=4.0000 norm=3.1176
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0137 val_loss=0.0000 scale=2.0000 norm=1.6018
[iter 200] loss=0.7490 val_loss=0.0000 scale=4.0000 norm=3.1682
[iter 300] loss=0.4222 val_loss=0.0000 scale=4.0000 norm=3.1222
[iter 400] loss=0.2268 val_loss=0.0000 scale=4.0000 norm=3.0555
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0425 val_loss=0.0000 scale=2.0000 norm=1.6068
[iter 200] loss=0.8226 val_loss=0.0000 scale=2.0000 norm=1.5970
[iter 300] loss=0.4995 val_loss=0.0000 scale=4.0000 norm=3.1303
[iter 400] loss=0.3273 val_loss=0.0000 scale=2.0000 norm=1.5481
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8184 val_loss=0.0000 scale=2.0000 norm=1.5822
[iter 200] loss=0.1601 val_loss=0.0000 scale=4.0000 norm=3.1798
[iter 300] loss=-0.5545 val_loss=0.0000 scale=4.0000 norm=3.0729
[iter 400] loss=-1.0489 val_loss=0.0000 scale=4.0000 norm=2.8461
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.5676 val_loss=0.0000 scale=2.0000 norm=1.3047
[iter 200] loss=-0.0708 val_loss=0.0000 scale=4.0000 norm=2.7344
[iter 300] loss=-0.8612 val_loss=0.0000 scale=4.0000 norm=2.7331
[iter 400] loss=-1.7962 val_loss=0.0000 scale=8.0000 norm=5.1165
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1119 val_loss=0.0000 scale=2.0000 norm=1.6675
[iter 200] loss=0.9594 val_loss=0.0000 scale=4.0000 norm=3.3340
[iter 300] loss=0.7591 val_loss=0.0000 scale=4.0000 norm=3.3262
[iter 400] loss=0.5214 val_loss=0.0000 scale=8.0000 norm=6.6673
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0151 val_loss=0.0000 scale=2.0000 norm=1.5973
[iter 200] loss=0.7667 val_loss=0.0000 scale=2.0000 norm=1.5882
[iter 300] loss=0.4399 val_loss=0.0000 scale=4.0000 norm=3.1432
[iter 400] loss=0.2132 val_loss=0.0000 scale=4.0000 norm=3.1525
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0356 val_loss=0.0000 scale=2.0000 norm=1.6341
[iter 200] loss=0.7496 val_loss=0.0000 scale=4.0000 norm=3.2198
[iter 300] loss=0.3557 val_loss=0.0000 scale=4.0000 norm=3.1987
[iter 400] loss=0.0431 val_loss=0.0000 scale=4.0000 norm=3.1464
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6240 val_loss=0.0000 scale=2.0000 norm=1.1969
[iter 200] loss=0.2815 val_loss=0.0000 scale=2.0000 norm=1.2527
[iter 300] loss=-0.1276 val_loss=0.0000 scale=4.0000 norm=2.4826
[iter 400] loss=-0.4302 val_loss=0.0000 scale=4.0000 norm=2.4182
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9841 val_loss=0.0000 scale=2.0000 norm=1.5749
[iter 200] loss=0.7507 val_loss=0.0000 scale=2.0000 norm=1.5655
[iter 300] loss=0.4218 val_loss=0.0000 scale=4.0000 norm=3.1240
[iter 400] loss=0.1937 val_loss=0.0000 scale=4.0000 norm=3.0957
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0342 val_loss=0.0000 scale=2.0000 norm=1.6093
[iter 200] loss=0.7911 val_loss=0.0000 scale=4.0000 norm=3.1874
[iter 300] loss=0.4797 val_loss=0.0000 scale=4.0000 norm=3.1366
[iter 400] loss=0.3228 val_loss=0.0000 scale=4.0000 norm=3.1016
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9884 val_loss=0.0000 scale=2.0000 norm=1.5638
[iter 200] loss=0.7565 val_loss=0.0000 scale=2.0000 norm=1.5413
[iter 300] loss=0.4260 val_loss=0.0000 scale=4.0000 norm=3.0513
[iter 400] loss=0.2140 val_loss=0.0000 scale=4.0000 norm=3.0406
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9322 val_loss=0.0000 scale=2.0000 norm=1.5429
[iter 200] loss=0.6375 val_loss=0.0000 scale=4.0000 norm=3.0255
[iter 300] loss=0.2730 val_loss=0.0000 scale=4.0000 norm=2.9849
[iter 400] loss=0.1001 val_loss=0.0000 scale=4.0000 norm=2.9692
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0430 val_loss=0.0000 scale=2.0000 norm=1.6062
[iter 200] loss=0.7874 val_loss=0.0000 scale=4.0000 norm=3.2196
[iter 300] loss=0.4512 val_loss=0.0000 scale=4.0000 norm=3.1723
[iter 400] loss=0.2714 val_loss=0.0000 scale=4.0000 norm=3.1900
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0475 val_loss=0.0000 scale=2.0000 norm=1.6255
[iter 200] loss=0.8209 val_loss=0.0000 scale=4.0000 norm=3.2258
[iter 300] loss=0.4875 val_loss=0.0000 scale=4.0000 norm=3.1725
[iter 400] loss=0.3378 val_loss=0.0000 scale=2.0000 norm=1.5524
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0121 val_loss=0.0000 scale=2.0000 norm=1.5850
[iter 200] loss=0.7947 val_loss=0.0000 scale=2.0000 norm=1.5668
[iter 300] loss=0.5073 val_loss=0.0000 scale=4.0000 norm=3.0911
[iter 400] loss=0.3283 val_loss=0.0000 scale=4.0000 norm=3.0865

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n04>
Subject: Job 853008: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:38 2024
Job was executed on host(s) <4*c205n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:40 2024
                            <4*c205n07>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:44:40 2024
Terminated at Tue Oct 22 18:02:03 2024
Results reported at Tue Oct 22 18:02:03 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "RRU Dimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4583.40 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.84 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   1045 sec.
    Turnaround time :                            1045 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.

RRU Trimer
Filename: (Mordred)_NGB_hypOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(Mordred)_NGB_hypOFF_generalizability_scores.json
Done Saving scores!
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1198 val_loss=0.0000 scale=2.0000 norm=1.7053
[iter 200] loss=0.8633 val_loss=0.0000 scale=4.0000 norm=3.3776
[iter 300] loss=0.5294 val_loss=0.0000 scale=4.0000 norm=3.3040
[iter 400] loss=0.1481 val_loss=0.0000 scale=8.0000 norm=6.4526
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0065 val_loss=0.0000 scale=2.0000 norm=1.5877
[iter 200] loss=0.7541 val_loss=0.0000 scale=4.0000 norm=3.1156
[iter 300] loss=0.4112 val_loss=0.0000 scale=4.0000 norm=3.0658
[iter 400] loss=0.2200 val_loss=0.0000 scale=4.0000 norm=3.0042
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6052 val_loss=0.0000 scale=2.0000 norm=1.2586
[iter 200] loss=0.1469 val_loss=0.0000 scale=4.0000 norm=2.5701
[iter 300] loss=-0.6528 val_loss=0.0000 scale=4.0000 norm=2.5876
[iter 400] loss=-1.9488 val_loss=0.0000 scale=8.0000 norm=5.1760
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9232 val_loss=0.0000 scale=2.0000 norm=1.5193
[iter 200] loss=0.6274 val_loss=0.0000 scale=4.0000 norm=2.9811
[iter 300] loss=0.2577 val_loss=0.0000 scale=4.0000 norm=2.9538
[iter 400] loss=-0.0013 val_loss=0.0000 scale=4.0000 norm=2.9433
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0882 val_loss=0.0000 scale=2.0000 norm=1.6449
[iter 200] loss=0.9193 val_loss=0.0000 scale=2.0000 norm=1.6354
[iter 300] loss=0.6805 val_loss=0.0000 scale=4.0000 norm=3.2587
[iter 400] loss=0.5251 val_loss=0.0000 scale=8.0000 norm=6.4283
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4375 val_loss=0.0000 scale=2.0000 norm=1.1901
[iter 200] loss=-0.1530 val_loss=0.0000 scale=4.0000 norm=2.4690
[iter 300] loss=-0.8569 val_loss=0.0000 scale=4.0000 norm=2.5415
[iter 400] loss=-2.2289 val_loss=0.0000 scale=8.0000 norm=5.0862
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1180 val_loss=0.0000 scale=2.0000 norm=1.6819
[iter 200] loss=0.9455 val_loss=0.0000 scale=4.0000 norm=3.3370
[iter 300] loss=0.7041 val_loss=0.0000 scale=4.0000 norm=3.3032
[iter 400] loss=0.5357 val_loss=0.0000 scale=4.0000 norm=3.2673
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9868 val_loss=0.0000 scale=2.0000 norm=1.5837
[iter 200] loss=0.6873 val_loss=0.0000 scale=4.0000 norm=3.1291
[iter 300] loss=0.2754 val_loss=0.0000 scale=4.0000 norm=3.0816
[iter 400] loss=0.0406 val_loss=0.0000 scale=4.0000 norm=3.0322
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8306 val_loss=0.0000 scale=2.0000 norm=1.4455
[iter 200] loss=0.4209 val_loss=0.0000 scale=4.0000 norm=2.9168
[iter 300] loss=-0.2122 val_loss=0.0000 scale=4.0000 norm=2.8186
[iter 400] loss=-1.1489 val_loss=0.0000 scale=8.0000 norm=5.4818
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9447 val_loss=0.0000 scale=2.0000 norm=1.5460
[iter 200] loss=0.6701 val_loss=0.0000 scale=2.0000 norm=1.5433
[iter 300] loss=0.2563 val_loss=0.0000 scale=4.0000 norm=3.0568
[iter 400] loss=-0.0620 val_loss=0.0000 scale=4.0000 norm=2.9617
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9989 val_loss=0.0000 scale=2.0000 norm=1.5815
[iter 200] loss=0.7155 val_loss=0.0000 scale=4.0000 norm=3.1379
[iter 300] loss=0.3412 val_loss=0.0000 scale=4.0000 norm=3.1237
[iter 400] loss=0.1408 val_loss=0.0000 scale=4.0000 norm=3.1668
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4123 val_loss=0.0000 scale=2.0000 norm=0.9616
[iter 200] loss=-0.0951 val_loss=0.0000 scale=2.0000 norm=0.9070
[iter 300] loss=-0.6840 val_loss=0.0000 scale=4.0000 norm=1.7278
[iter 400] loss=-1.5675 val_loss=0.0000 scale=8.0000 norm=3.3759
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8640 val_loss=0.0000 scale=2.0000 norm=1.4618
[iter 200] loss=0.6221 val_loss=0.0000 scale=2.0000 norm=1.4848
[iter 300] loss=0.3547 val_loss=0.0000 scale=4.0000 norm=2.9479
[iter 400] loss=0.1315 val_loss=0.0000 scale=4.0000 norm=2.9092
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9996 val_loss=0.0000 scale=2.0000 norm=1.5765
[iter 200] loss=0.7545 val_loss=0.0000 scale=2.0000 norm=1.5650
[iter 300] loss=0.4219 val_loss=0.0000 scale=4.0000 norm=3.0925
[iter 400] loss=0.2110 val_loss=0.0000 scale=2.0000 norm=1.5358
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6962 val_loss=0.0000 scale=2.0000 norm=1.5055
[iter 200] loss=0.1593 val_loss=0.0000 scale=4.0000 norm=2.8247
[iter 300] loss=-0.4382 val_loss=0.0000 scale=4.0000 norm=2.7345
[iter 400] loss=-0.8453 val_loss=0.0000 scale=4.0000 norm=2.7308
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0139 val_loss=0.0000 scale=2.0000 norm=1.6165
[iter 200] loss=0.6701 val_loss=0.0000 scale=4.0000 norm=3.2357
[iter 300] loss=0.1588 val_loss=0.0000 scale=4.0000 norm=3.2360
[iter 400] loss=-0.3340 val_loss=0.0000 scale=8.0000 norm=6.3887
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0120 val_loss=0.0000 scale=2.0000 norm=1.5833
[iter 200] loss=0.7697 val_loss=0.0000 scale=2.0000 norm=1.5710
[iter 300] loss=0.4034 val_loss=0.0000 scale=4.0000 norm=3.0788
[iter 400] loss=0.2158 val_loss=0.0000 scale=4.0000 norm=3.0844
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9890 val_loss=0.0000 scale=2.0000 norm=1.5520
[iter 200] loss=0.7468 val_loss=0.0000 scale=4.0000 norm=3.0718
[iter 300] loss=0.4189 val_loss=0.0000 scale=4.0000 norm=3.1078
[iter 400] loss=0.1146 val_loss=0.0000 scale=4.0000 norm=3.0713
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0472 val_loss=0.0000 scale=2.0000 norm=1.6255
[iter 200] loss=0.8188 val_loss=0.0000 scale=4.0000 norm=3.2254
[iter 300] loss=0.4859 val_loss=0.0000 scale=4.0000 norm=3.1737
[iter 400] loss=0.3379 val_loss=0.0000 scale=2.0000 norm=1.5531
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5968 val_loss=0.0000 scale=2.0000 norm=1.2872
[iter 200] loss=0.0884 val_loss=0.0000 scale=2.0000 norm=1.2449
[iter 300] loss=-0.3068 val_loss=0.0000 scale=4.0000 norm=2.6108
[iter 400] loss=-0.6288 val_loss=0.0000 scale=8.0000 norm=4.9224
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9805 val_loss=0.0000 scale=2.0000 norm=1.6257
[iter 200] loss=0.6571 val_loss=0.0000 scale=4.0000 norm=3.2051
[iter 300] loss=0.1544 val_loss=0.0000 scale=4.0000 norm=3.1521
[iter 400] loss=-0.2541 val_loss=0.0000 scale=4.0000 norm=3.0867
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9646 val_loss=0.0000 scale=2.0000 norm=1.5745
[iter 200] loss=0.6763 val_loss=0.0000 scale=4.0000 norm=3.1004
[iter 300] loss=0.3091 val_loss=0.0000 scale=4.0000 norm=3.0357
[iter 400] loss=0.0557 val_loss=0.0000 scale=4.0000 norm=2.9315
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9956 val_loss=0.0000 scale=2.0000 norm=1.5920
[iter 200] loss=0.6846 val_loss=0.0000 scale=4.0000 norm=3.1618
[iter 300] loss=0.3478 val_loss=0.0000 scale=4.0000 norm=3.0812
[iter 400] loss=0.1435 val_loss=0.0000 scale=4.0000 norm=2.9641
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8120 val_loss=0.0000 scale=2.0000 norm=1.5878
[iter 200] loss=0.3660 val_loss=0.0000 scale=4.0000 norm=3.2630
[iter 300] loss=-0.2780 val_loss=0.0000 scale=4.0000 norm=3.2596
[iter 400] loss=-0.9614 val_loss=0.0000 scale=8.0000 norm=6.2079
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0394 val_loss=0.0000 scale=2.0000 norm=1.5897
[iter 200] loss=0.8335 val_loss=0.0000 scale=4.0000 norm=3.1488
[iter 300] loss=0.5360 val_loss=0.0000 scale=4.0000 norm=3.1215
[iter 400] loss=0.3952 val_loss=0.0000 scale=4.0000 norm=3.1046
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9862 val_loss=0.0000 scale=2.0000 norm=1.5691
[iter 200] loss=0.7341 val_loss=0.0000 scale=4.0000 norm=3.0968
[iter 300] loss=0.3887 val_loss=0.0000 scale=4.0000 norm=3.0693
[iter 400] loss=0.2049 val_loss=0.0000 scale=4.0000 norm=3.0522
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0647 val_loss=0.0000 scale=2.0000 norm=1.6390
[iter 200] loss=0.8538 val_loss=0.0000 scale=2.0000 norm=1.6298
[iter 300] loss=0.5351 val_loss=0.0000 scale=4.0000 norm=3.2175
[iter 400] loss=0.2577 val_loss=0.0000 scale=4.0000 norm=3.1527
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0524 val_loss=0.0000 scale=2.0000 norm=1.6267
[iter 200] loss=0.8228 val_loss=0.0000 scale=2.0000 norm=1.6070
[iter 300] loss=0.5138 val_loss=0.0000 scale=4.0000 norm=3.1606
[iter 400] loss=0.2476 val_loss=0.0000 scale=2.0000 norm=1.5405
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0094 val_loss=0.0000 scale=2.0000 norm=1.5896
[iter 200] loss=0.7521 val_loss=0.0000 scale=4.0000 norm=3.1265
[iter 300] loss=0.3775 val_loss=0.0000 scale=4.0000 norm=3.0799
[iter 400] loss=0.1035 val_loss=0.0000 scale=4.0000 norm=2.9799
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0195 val_loss=0.0000 scale=2.0000 norm=1.5998
[iter 200] loss=0.7735 val_loss=0.0000 scale=4.0000 norm=3.1541
[iter 300] loss=0.4322 val_loss=0.0000 scale=4.0000 norm=3.1170
[iter 400] loss=0.2631 val_loss=0.0000 scale=4.0000 norm=3.0911
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8673 val_loss=0.0000 scale=2.0000 norm=1.4667
[iter 200] loss=0.6482 val_loss=0.0000 scale=2.0000 norm=1.5153
[iter 300] loss=0.2877 val_loss=0.0000 scale=4.0000 norm=3.0158
[iter 400] loss=0.0200 val_loss=0.0000 scale=4.0000 norm=2.9852
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9670 val_loss=0.0000 scale=2.0000 norm=1.5728
[iter 200] loss=0.6714 val_loss=0.0000 scale=4.0000 norm=3.0584
[iter 300] loss=0.2916 val_loss=0.0000 scale=4.0000 norm=2.9913
[iter 400] loss=-0.0255 val_loss=0.0000 scale=4.0000 norm=2.9341
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0429 val_loss=0.0000 scale=2.0000 norm=1.6686
[iter 200] loss=0.6958 val_loss=0.0000 scale=4.0000 norm=3.3336
[iter 300] loss=0.1358 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-0.9442 val_loss=0.0000 scale=16.0000 norm=13.3438
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0519 val_loss=0.0000 scale=2.0000 norm=1.6320
[iter 200] loss=0.7842 val_loss=0.0000 scale=4.0000 norm=3.2625
[iter 300] loss=0.3941 val_loss=0.0000 scale=4.0000 norm=3.2430
[iter 400] loss=0.0954 val_loss=0.0000 scale=4.0000 norm=3.2207
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9500 val_loss=0.0000 scale=2.0000 norm=1.6676
[iter 200] loss=0.4892 val_loss=0.0000 scale=4.0000 norm=3.3495
[iter 300] loss=-0.3106 val_loss=0.0000 scale=4.0000 norm=3.3524
[iter 400] loss=-1.1764 val_loss=0.0000 scale=4.0000 norm=3.3247
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0564 val_loss=0.0000 scale=2.0000 norm=1.6139
[iter 200] loss=0.8546 val_loss=0.0000 scale=4.0000 norm=3.1936
[iter 300] loss=0.5422 val_loss=0.0000 scale=4.0000 norm=3.1352
[iter 400] loss=0.3558 val_loss=0.0000 scale=4.0000 norm=3.0817
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0350 val_loss=0.0000 scale=2.0000 norm=1.5974
[iter 200] loss=0.8295 val_loss=0.0000 scale=2.0000 norm=1.5786
[iter 300] loss=0.5639 val_loss=0.0000 scale=4.0000 norm=3.1245
[iter 400] loss=0.3783 val_loss=0.0000 scale=4.0000 norm=3.0904
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0551 val_loss=0.0000 scale=2.0000 norm=1.6624
[iter 200] loss=0.8326 val_loss=0.0000 scale=2.0000 norm=1.6535
[iter 300] loss=0.5386 val_loss=0.0000 scale=4.0000 norm=3.2503
[iter 400] loss=0.3697 val_loss=0.0000 scale=4.0000 norm=3.2113
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0027 val_loss=0.0000 scale=2.0000 norm=1.5807
[iter 200] loss=0.7280 val_loss=0.0000 scale=4.0000 norm=3.1201
[iter 300] loss=0.3591 val_loss=0.0000 scale=4.0000 norm=3.0168
[iter 400] loss=0.1138 val_loss=0.0000 scale=4.0000 norm=3.0034
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0154 val_loss=0.0000 scale=2.0000 norm=1.5844
[iter 200] loss=0.7625 val_loss=0.0000 scale=4.0000 norm=3.1293
[iter 300] loss=0.4370 val_loss=0.0000 scale=4.0000 norm=3.1035
[iter 400] loss=0.2482 val_loss=0.0000 scale=4.0000 norm=3.1644
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0128 val_loss=0.0000 scale=2.0000 norm=1.5935
[iter 200] loss=0.7926 val_loss=0.0000 scale=2.0000 norm=1.5918
[iter 300] loss=0.4549 val_loss=0.0000 scale=4.0000 norm=3.1495
[iter 400] loss=0.2363 val_loss=0.0000 scale=4.0000 norm=3.1411
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7709 val_loss=0.0000 scale=2.0000 norm=1.4675
[iter 200] loss=0.4777 val_loss=0.0000 scale=4.0000 norm=3.0496
[iter 300] loss=0.0594 val_loss=0.0000 scale=4.0000 norm=3.0292
[iter 400] loss=-0.3397 val_loss=0.0000 scale=8.0000 norm=5.8483
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0348 val_loss=0.0000 scale=2.0000 norm=1.7342
[iter 200] loss=0.7850 val_loss=0.0000 scale=4.0000 norm=3.4529
[iter 300] loss=0.4144 val_loss=0.0000 scale=4.0000 norm=3.4186
[iter 400] loss=0.0838 val_loss=0.0000 scale=4.0000 norm=3.3616
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7441 val_loss=0.0000 scale=2.0000 norm=1.2916
[iter 200] loss=0.4815 val_loss=0.0000 scale=2.0000 norm=1.2996
[iter 300] loss=0.1048 val_loss=0.0000 scale=4.0000 norm=2.5632
[iter 400] loss=-0.1658 val_loss=0.0000 scale=4.0000 norm=2.4160
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7510 val_loss=0.0000 scale=2.0000 norm=1.2707
[iter 200] loss=0.3824 val_loss=0.0000 scale=4.0000 norm=2.5426
[iter 300] loss=-0.2179 val_loss=0.0000 scale=4.0000 norm=2.5505
[iter 400] loss=-1.3699 val_loss=0.0000 scale=8.0000 norm=5.1014
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9865 val_loss=0.0000 scale=2.0000 norm=1.5672
[iter 200] loss=0.7219 val_loss=0.0000 scale=4.0000 norm=3.0977
[iter 300] loss=0.3512 val_loss=0.0000 scale=4.0000 norm=3.1033
[iter 400] loss=0.1050 val_loss=0.0000 scale=4.0000 norm=3.0106
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0293 val_loss=0.0000 scale=2.0000 norm=1.6002
[iter 200] loss=0.7678 val_loss=0.0000 scale=4.0000 norm=3.1685
[iter 300] loss=0.4052 val_loss=0.0000 scale=4.0000 norm=3.1400
[iter 400] loss=0.1943 val_loss=0.0000 scale=4.0000 norm=3.0878
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9930 val_loss=0.0000 scale=2.0000 norm=1.5658
[iter 200] loss=0.7122 val_loss=0.0000 scale=4.0000 norm=3.1153
[iter 300] loss=0.3942 val_loss=0.0000 scale=4.0000 norm=3.0617
[iter 400] loss=0.2002 val_loss=0.0000 scale=4.0000 norm=3.0124
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0222 val_loss=0.0000 scale=2.0000 norm=1.6054
[iter 200] loss=0.7984 val_loss=0.0000 scale=4.0000 norm=3.2038
[iter 300] loss=0.4972 val_loss=0.0000 scale=4.0000 norm=3.1878
[iter 400] loss=0.3182 val_loss=0.0000 scale=4.0000 norm=3.1396
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9572 val_loss=0.0000 scale=2.0000 norm=1.5669
[iter 200] loss=0.6231 val_loss=0.0000 scale=4.0000 norm=3.1313
[iter 300] loss=0.1835 val_loss=0.0000 scale=4.0000 norm=3.1149
[iter 400] loss=-0.1564 val_loss=0.0000 scale=4.0000 norm=3.0524
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9232 val_loss=0.0000 scale=2.0000 norm=1.6326
[iter 200] loss=0.5877 val_loss=0.0000 scale=4.0000 norm=3.3123
[iter 300] loss=0.0687 val_loss=0.0000 scale=4.0000 norm=3.2919
[iter 400] loss=-0.4464 val_loss=0.0000 scale=4.0000 norm=3.2789
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0310 val_loss=0.0000 scale=2.0000 norm=1.6204
[iter 200] loss=0.8011 val_loss=0.0000 scale=2.0000 norm=1.6148
[iter 300] loss=0.4773 val_loss=0.0000 scale=4.0000 norm=3.1947
[iter 400] loss=0.3192 val_loss=0.0000 scale=4.0000 norm=3.1619
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9084 val_loss=0.0000 scale=2.0000 norm=1.5208
[iter 200] loss=0.5710 val_loss=0.0000 scale=2.0000 norm=1.5166
[iter 300] loss=0.0603 val_loss=0.0000 scale=4.0000 norm=2.9961
[iter 400] loss=-0.2665 val_loss=0.0000 scale=4.0000 norm=2.8483
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0784 val_loss=0.0000 scale=2.0000 norm=1.6320
[iter 200] loss=0.8329 val_loss=0.0000 scale=4.0000 norm=3.2135
[iter 300] loss=0.5579 val_loss=0.0000 scale=4.0000 norm=3.1218
[iter 400] loss=0.1532 val_loss=0.0000 scale=8.0000 norm=6.2974
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9889 val_loss=0.0000 scale=2.0000 norm=1.6445
[iter 200] loss=0.4573 val_loss=0.0000 scale=4.0000 norm=3.2641
[iter 300] loss=-0.2036 val_loss=0.0000 scale=4.0000 norm=3.2015
[iter 400] loss=-0.7606 val_loss=0.0000 scale=4.0000 norm=3.1998
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0030 val_loss=0.0000 scale=2.0000 norm=1.5783
[iter 200] loss=0.7583 val_loss=0.0000 scale=4.0000 norm=3.1255
[iter 300] loss=0.3945 val_loss=0.0000 scale=4.0000 norm=3.0668
[iter 400] loss=0.1952 val_loss=0.0000 scale=4.0000 norm=3.0339
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7418 val_loss=0.0000 scale=2.0000 norm=1.4644
[iter 200] loss=0.3840 val_loss=0.0000 scale=4.0000 norm=3.0472
[iter 300] loss=-0.1595 val_loss=0.0000 scale=4.0000 norm=3.0572
[iter 400] loss=-0.9751 val_loss=0.0000 scale=8.0000 norm=5.9930
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0775 val_loss=0.0000 scale=2.0000 norm=1.6180
[iter 200] loss=0.9038 val_loss=0.0000 scale=2.0000 norm=1.6003
[iter 300] loss=0.6345 val_loss=0.0000 scale=4.0000 norm=3.1866
[iter 400] loss=0.4280 val_loss=0.0000 scale=4.0000 norm=3.1601
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5194 val_loss=0.0000 scale=2.0000 norm=1.1262
[iter 200] loss=0.0247 val_loss=0.0000 scale=2.0000 norm=1.1800
[iter 300] loss=-0.7951 val_loss=0.0000 scale=4.0000 norm=2.3711
[iter 400] loss=-1.4845 val_loss=0.0000 scale=4.0000 norm=2.1837
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8647 val_loss=0.0000 scale=2.0000 norm=1.5260
[iter 200] loss=0.5004 val_loss=0.0000 scale=4.0000 norm=3.0300
[iter 300] loss=0.0033 val_loss=0.0000 scale=4.0000 norm=2.9490
[iter 400] loss=-0.3736 val_loss=0.0000 scale=4.0000 norm=2.8889
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0112 val_loss=0.0000 scale=2.0000 norm=1.5968
[iter 200] loss=0.7392 val_loss=0.0000 scale=4.0000 norm=3.1678
[iter 300] loss=0.4155 val_loss=0.0000 scale=4.0000 norm=3.1276
[iter 400] loss=0.2486 val_loss=0.0000 scale=4.0000 norm=3.1598
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5539 val_loss=0.0000 scale=2.0000 norm=1.1040
[iter 200] loss=0.1723 val_loss=0.0000 scale=2.0000 norm=1.1135
[iter 300] loss=-0.3882 val_loss=0.0000 scale=4.0000 norm=2.2321
[iter 400] loss=-0.8976 val_loss=0.0000 scale=4.0000 norm=2.2128
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0580 val_loss=0.0000 scale=2.0000 norm=1.6399
[iter 200] loss=0.8520 val_loss=0.0000 scale=2.0000 norm=1.6456
[iter 300] loss=0.6507 val_loss=0.0000 scale=4.0000 norm=3.2595
[iter 400] loss=0.4428 val_loss=0.0000 scale=4.0000 norm=3.2375
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0071 val_loss=0.0000 scale=2.0000 norm=1.6078
[iter 200] loss=0.7380 val_loss=0.0000 scale=4.0000 norm=3.1935
[iter 300] loss=0.4146 val_loss=0.0000 scale=4.0000 norm=3.1744
[iter 400] loss=0.2282 val_loss=0.0000 scale=4.0000 norm=3.1244
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0301 val_loss=0.0000 scale=2.0000 norm=1.6106
[iter 200] loss=0.7668 val_loss=0.0000 scale=4.0000 norm=3.1981
[iter 300] loss=0.4439 val_loss=0.0000 scale=4.0000 norm=3.1384
[iter 400] loss=0.2675 val_loss=0.0000 scale=4.0000 norm=3.1320
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9992 val_loss=0.0000 scale=2.0000 norm=1.5834
[iter 200] loss=0.7294 val_loss=0.0000 scale=4.0000 norm=3.1376
[iter 300] loss=0.4034 val_loss=0.0000 scale=2.0000 norm=1.5325
[iter 400] loss=0.2391 val_loss=0.0000 scale=4.0000 norm=3.0982
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=1.0317 val_loss=0.0000 scale=2.0000 norm=1.6446
[iter 200] loss=0.7648 val_loss=0.0000 scale=4.0000 norm=3.3176
[iter 300] loss=0.2648 val_loss=0.0000 scale=4.0000 norm=3.3251
[iter 400] loss=-0.6502 val_loss=0.0000 scale=8.0000 norm=6.6505
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0200 val_loss=0.0000 scale=2.0000 norm=1.6144
[iter 200] loss=0.7651 val_loss=0.0000 scale=2.0000 norm=1.6017
[iter 300] loss=0.4110 val_loss=0.0000 scale=4.0000 norm=3.1860
[iter 400] loss=0.2008 val_loss=0.0000 scale=4.0000 norm=3.0898
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0198 val_loss=0.0000 scale=2.0000 norm=1.5929
[iter 200] loss=0.7140 val_loss=0.0000 scale=4.0000 norm=3.1545
[iter 300] loss=0.3557 val_loss=0.0000 scale=4.0000 norm=3.0451
[iter 400] loss=0.1446 val_loss=0.0000 scale=4.0000 norm=2.9909
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0113 val_loss=0.0000 scale=2.0000 norm=1.5982
[iter 200] loss=0.7422 val_loss=0.0000 scale=4.0000 norm=3.1520
[iter 300] loss=0.4262 val_loss=0.0000 scale=4.0000 norm=3.0984
[iter 400] loss=0.2532 val_loss=0.0000 scale=4.0000 norm=3.1645
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0483 val_loss=0.0000 scale=2.0000 norm=1.6540
[iter 200] loss=0.7389 val_loss=0.0000 scale=4.0000 norm=3.3125
[iter 300] loss=0.3046 val_loss=0.0000 scale=4.0000 norm=3.2731
[iter 400] loss=-0.0069 val_loss=0.0000 scale=4.0000 norm=3.2200
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6957 val_loss=0.0000 scale=2.0000 norm=1.3640
[iter 200] loss=0.4211 val_loss=0.0000 scale=4.0000 norm=2.9320
[iter 300] loss=0.0761 val_loss=0.0000 scale=4.0000 norm=2.9362
[iter 400] loss=-0.2449 val_loss=0.0000 scale=8.0000 norm=5.7206
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0734 val_loss=0.0000 scale=2.0000 norm=1.6349
[iter 200] loss=0.8895 val_loss=0.0000 scale=4.0000 norm=3.2380
[iter 300] loss=0.6194 val_loss=0.0000 scale=4.0000 norm=3.1764
[iter 400] loss=0.4906 val_loss=0.0000 scale=4.0000 norm=3.1919
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9565 val_loss=0.0000 scale=2.0000 norm=1.5527
[iter 200] loss=0.6823 val_loss=0.0000 scale=4.0000 norm=3.0521
[iter 300] loss=0.2543 val_loss=0.0000 scale=4.0000 norm=2.9937
[iter 400] loss=0.0085 val_loss=0.0000 scale=2.0000 norm=1.4595
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0526 val_loss=0.0000 scale=2.0000 norm=1.6214
[iter 200] loss=0.8168 val_loss=0.0000 scale=4.0000 norm=3.2462
[iter 300] loss=0.4913 val_loss=0.0000 scale=4.0000 norm=3.2227
[iter 400] loss=0.2897 val_loss=0.0000 scale=4.0000 norm=3.1663
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0787 val_loss=0.0000 scale=2.0000 norm=1.6566
[iter 200] loss=0.7890 val_loss=0.0000 scale=4.0000 norm=3.2771
[iter 300] loss=0.4615 val_loss=0.0000 scale=4.0000 norm=3.1885
[iter 400] loss=0.2460 val_loss=0.0000 scale=8.0000 norm=6.2820
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9451 val_loss=0.0000 scale=2.0000 norm=1.5670
[iter 200] loss=0.5558 val_loss=0.0000 scale=4.0000 norm=3.0327
[iter 300] loss=0.1905 val_loss=0.0000 scale=4.0000 norm=2.9489
[iter 400] loss=-0.0203 val_loss=0.0000 scale=2.0000 norm=1.4484
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0140 val_loss=0.0000 scale=2.0000 norm=1.5810
[iter 200] loss=0.7874 val_loss=0.0000 scale=2.0000 norm=1.5619
[iter 300] loss=0.4939 val_loss=0.0000 scale=4.0000 norm=3.0904
[iter 400] loss=0.2634 val_loss=0.0000 scale=4.0000 norm=3.0266
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7349 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 200] loss=-0.1759 val_loss=0.0000 scale=4.0000 norm=3.0688
[iter 300] loss=-1.3658 val_loss=0.0000 scale=8.0000 norm=5.8055
[iter 400] loss=-3.6218 val_loss=0.0000 scale=16.0000 norm=11.6070
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9862 val_loss=0.0000 scale=2.0000 norm=1.5818
[iter 200] loss=0.7372 val_loss=0.0000 scale=4.0000 norm=3.0969
[iter 300] loss=0.4218 val_loss=0.0000 scale=4.0000 norm=3.0498
[iter 400] loss=0.2826 val_loss=0.0000 scale=4.0000 norm=3.0567
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9629 val_loss=0.0000 scale=2.0000 norm=1.5498
[iter 200] loss=0.6904 val_loss=0.0000 scale=2.0000 norm=1.5204
[iter 300] loss=0.3552 val_loss=0.0000 scale=4.0000 norm=3.0027
[iter 400] loss=0.1212 val_loss=0.0000 scale=4.0000 norm=2.9679
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8717 val_loss=0.0000 scale=2.0000 norm=1.5376
[iter 200] loss=0.4648 val_loss=0.0000 scale=4.0000 norm=3.0422
[iter 300] loss=-0.0286 val_loss=0.0000 scale=4.0000 norm=3.0142
[iter 400] loss=-0.3671 val_loss=0.0000 scale=4.0000 norm=2.9461
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5086 val_loss=0.0000 scale=2.0000 norm=1.0902
[iter 200] loss=0.0117 val_loss=0.0000 scale=2.0000 norm=1.1383
[iter 300] loss=-0.6654 val_loss=0.0000 scale=4.0000 norm=2.2548
[iter 400] loss=-1.7527 val_loss=0.0000 scale=8.0000 norm=4.5378
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0077 val_loss=0.0000 scale=2.0000 norm=1.6292
[iter 200] loss=0.6769 val_loss=0.0000 scale=4.0000 norm=3.2209
[iter 300] loss=0.2469 val_loss=0.0000 scale=4.0000 norm=3.1718
[iter 400] loss=-0.0972 val_loss=0.0000 scale=4.0000 norm=3.1110
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9994 val_loss=0.0000 scale=2.0000 norm=1.5666
[iter 200] loss=0.7758 val_loss=0.0000 scale=2.0000 norm=1.5463
[iter 300] loss=0.4628 val_loss=0.0000 scale=4.0000 norm=3.0632
[iter 400] loss=0.2710 val_loss=0.0000 scale=4.0000 norm=3.0151
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0420 val_loss=0.0000 scale=2.0000 norm=1.6198
[iter 200] loss=0.7688 val_loss=0.0000 scale=4.0000 norm=3.1944
[iter 300] loss=0.4804 val_loss=0.0000 scale=4.0000 norm=3.1157
[iter 400] loss=0.3489 val_loss=0.0000 scale=4.0000 norm=3.0881
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4169 val_loss=0.0000 scale=2.0000 norm=1.3175
[iter 200] loss=-0.2334 val_loss=0.0000 scale=2.0000 norm=1.2639
[iter 300] loss=-0.8684 val_loss=0.0000 scale=4.0000 norm=2.5376
[iter 400] loss=-1.7033 val_loss=0.0000 scale=8.0000 norm=4.9721
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0306 val_loss=0.0000 scale=2.0000 norm=1.6453
[iter 200] loss=0.7541 val_loss=0.0000 scale=2.0000 norm=1.6462
[iter 300] loss=0.2852 val_loss=0.0000 scale=4.0000 norm=3.2524
[iter 400] loss=-0.0378 val_loss=0.0000 scale=4.0000 norm=3.1878
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0651 val_loss=0.0000 scale=2.0000 norm=1.6710
[iter 200] loss=0.7351 val_loss=0.0000 scale=4.0000 norm=3.3114
[iter 300] loss=0.3693 val_loss=0.0000 scale=4.0000 norm=3.2829
[iter 400] loss=0.0563 val_loss=0.0000 scale=4.0000 norm=3.2271
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9878 val_loss=0.0000 scale=2.0000 norm=1.5697
[iter 200] loss=0.7405 val_loss=0.0000 scale=2.0000 norm=1.5535
[iter 300] loss=0.3855 val_loss=0.0000 scale=4.0000 norm=3.0581
[iter 400] loss=0.1845 val_loss=0.0000 scale=4.0000 norm=3.0593
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0060 val_loss=0.0000 scale=2.0000 norm=1.5759
[iter 200] loss=0.7614 val_loss=0.0000 scale=2.0000 norm=1.5591
[iter 300] loss=0.4198 val_loss=0.0000 scale=4.0000 norm=3.1033
[iter 400] loss=0.1742 val_loss=0.0000 scale=4.0000 norm=3.0054
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0592 val_loss=0.0000 scale=2.0000 norm=1.6577
[iter 200] loss=0.7319 val_loss=0.0000 scale=4.0000 norm=3.2833
[iter 300] loss=0.3285 val_loss=0.0000 scale=4.0000 norm=3.2076
[iter 400] loss=-0.0169 val_loss=0.0000 scale=8.0000 norm=6.3592
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8970 val_loss=0.0000 scale=2.0000 norm=1.5607
[iter 200] loss=0.5414 val_loss=0.0000 scale=4.0000 norm=3.0579
[iter 300] loss=-0.0644 val_loss=0.0000 scale=4.0000 norm=3.0232
[iter 400] loss=-1.1264 val_loss=0.0000 scale=8.0000 norm=6.0466
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0278 val_loss=0.0000 scale=2.0000 norm=1.6085
[iter 200] loss=0.8152 val_loss=0.0000 scale=2.0000 norm=1.6060
[iter 300] loss=0.4891 val_loss=0.0000 scale=4.0000 norm=3.1532
[iter 400] loss=0.2981 val_loss=0.0000 scale=4.0000 norm=3.0995
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0051 val_loss=0.0000 scale=2.0000 norm=1.5844
[iter 200] loss=0.7624 val_loss=0.0000 scale=4.0000 norm=3.1229
[iter 300] loss=0.4425 val_loss=0.0000 scale=4.0000 norm=3.1074
[iter 400] loss=0.2302 val_loss=0.0000 scale=4.0000 norm=3.0584
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9630 val_loss=0.0000 scale=2.0000 norm=1.6808
[iter 200] loss=0.5682 val_loss=0.0000 scale=4.0000 norm=3.3239
[iter 300] loss=-0.0770 val_loss=0.0000 scale=8.0000 norm=6.6579
[iter 400] loss=-1.3370 val_loss=0.0000 scale=16.0000 norm=13.3158
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5863 val_loss=0.0000 scale=2.0000 norm=1.1503
[iter 200] loss=0.2166 val_loss=0.0000 scale=2.0000 norm=1.1021
[iter 300] loss=-0.1534 val_loss=0.0000 scale=4.0000 norm=2.2001
[iter 400] loss=-0.4617 val_loss=0.0000 scale=4.0000 norm=2.1871
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0415 val_loss=0.0000 scale=2.0000 norm=1.6429
[iter 200] loss=0.8157 val_loss=0.0000 scale=4.0000 norm=3.2807
[iter 300] loss=0.4946 val_loss=0.0000 scale=4.0000 norm=3.2420
[iter 400] loss=0.3378 val_loss=0.0000 scale=4.0000 norm=3.1963
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0405 val_loss=0.0000 scale=2.0000 norm=1.6827
[iter 200] loss=0.6802 val_loss=0.0000 scale=4.0000 norm=3.2950
[iter 300] loss=0.2343 val_loss=0.0000 scale=8.0000 norm=6.4258
[iter 400] loss=-0.5536 val_loss=0.0000 scale=16.0000 norm=12.8330
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0123 val_loss=0.0000 scale=2.0000 norm=1.5881
[iter 200] loss=0.7695 val_loss=0.0000 scale=4.0000 norm=3.1345
[iter 300] loss=0.4484 val_loss=0.0000 scale=4.0000 norm=3.1003
[iter 400] loss=0.2737 val_loss=0.0000 scale=4.0000 norm=3.0809
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0014 val_loss=0.0000 scale=2.0000 norm=1.5813
[iter 200] loss=0.7596 val_loss=0.0000 scale=4.0000 norm=3.1223
[iter 300] loss=0.4224 val_loss=0.0000 scale=4.0000 norm=3.0762
[iter 400] loss=0.2728 val_loss=0.0000 scale=4.0000 norm=3.1233
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0044 val_loss=0.0000 scale=2.0000 norm=1.6009
[iter 200] loss=0.7449 val_loss=0.0000 scale=4.0000 norm=3.1895
[iter 300] loss=0.3501 val_loss=0.0000 scale=4.0000 norm=3.1559
[iter 400] loss=0.0912 val_loss=0.0000 scale=4.0000 norm=3.1456
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0153 val_loss=0.0000 scale=2.0000 norm=1.6032
[iter 200] loss=0.7560 val_loss=0.0000 scale=2.0000 norm=1.5929
[iter 300] loss=0.3864 val_loss=0.0000 scale=4.0000 norm=3.1423
[iter 400] loss=0.1233 val_loss=0.0000 scale=4.0000 norm=3.1121
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0442 val_loss=0.0000 scale=2.0000 norm=1.6685
[iter 200] loss=0.6848 val_loss=0.0000 scale=4.0000 norm=3.2652
[iter 300] loss=0.3238 val_loss=0.0000 scale=4.0000 norm=3.2091
[iter 400] loss=0.0511 val_loss=0.0000 scale=4.0000 norm=3.1165
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9607 val_loss=0.0000 scale=2.0000 norm=1.5530
[iter 200] loss=0.6763 val_loss=0.0000 scale=2.0000 norm=1.5268
[iter 300] loss=0.3489 val_loss=0.0000 scale=4.0000 norm=3.0002
[iter 400] loss=0.0330 val_loss=0.0000 scale=4.0000 norm=2.9189
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0238 val_loss=0.0000 scale=2.0000 norm=1.6109
[iter 200] loss=0.7539 val_loss=0.0000 scale=4.0000 norm=3.1787
[iter 300] loss=0.4102 val_loss=0.0000 scale=4.0000 norm=3.1384
[iter 400] loss=0.1753 val_loss=0.0000 scale=4.0000 norm=3.1216
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0313 val_loss=0.0000 scale=2.0000 norm=1.5970
[iter 200] loss=0.8115 val_loss=0.0000 scale=4.0000 norm=3.1535
[iter 300] loss=0.5284 val_loss=0.0000 scale=4.0000 norm=3.1170
[iter 400] loss=0.3786 val_loss=0.0000 scale=4.0000 norm=3.0837
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9876 val_loss=0.0000 scale=2.0000 norm=1.5637
[iter 200] loss=0.7551 val_loss=0.0000 scale=2.0000 norm=1.5404
[iter 300] loss=0.4365 val_loss=0.0000 scale=4.0000 norm=3.0572
[iter 400] loss=0.2254 val_loss=0.0000 scale=4.0000 norm=3.0395
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0066 val_loss=0.0000 scale=2.0000 norm=1.5947
[iter 200] loss=0.7542 val_loss=0.0000 scale=4.0000 norm=3.1402
[iter 300] loss=0.4481 val_loss=0.0000 scale=4.0000 norm=3.1095
[iter 400] loss=0.2245 val_loss=0.0000 scale=4.0000 norm=3.0666
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0338 val_loss=0.0000 scale=2.0000 norm=1.6107
[iter 200] loss=0.7902 val_loss=0.0000 scale=4.0000 norm=3.1980
[iter 300] loss=0.4468 val_loss=0.0000 scale=4.0000 norm=3.1596
[iter 400] loss=0.2364 val_loss=0.0000 scale=4.0000 norm=3.1384
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9709 val_loss=0.0000 scale=2.0000 norm=1.5808
[iter 200] loss=0.6001 val_loss=0.0000 scale=4.0000 norm=3.1596
[iter 300] loss=0.0859 val_loss=0.0000 scale=4.0000 norm=3.1627
[iter 400] loss=-0.3732 val_loss=0.0000 scale=4.0000 norm=3.0794
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0589 val_loss=0.0000 scale=2.0000 norm=1.6368
[iter 200] loss=0.8240 val_loss=0.0000 scale=4.0000 norm=3.2503
[iter 300] loss=0.5142 val_loss=0.0000 scale=4.0000 norm=3.2031
[iter 400] loss=0.3717 val_loss=0.0000 scale=2.0000 norm=1.5940
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0159 val_loss=0.0000 scale=2.0000 norm=1.5809
[iter 200] loss=0.7976 val_loss=0.0000 scale=4.0000 norm=3.1253
[iter 300] loss=0.4976 val_loss=0.0000 scale=4.0000 norm=3.1125
[iter 400] loss=0.3387 val_loss=0.0000 scale=4.0000 norm=3.1244
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9088 val_loss=0.0000 scale=2.0000 norm=1.5318
[iter 200] loss=0.5614 val_loss=0.0000 scale=4.0000 norm=2.9582
[iter 300] loss=0.1023 val_loss=0.0000 scale=4.0000 norm=2.7969
[iter 400] loss=-0.2252 val_loss=0.0000 scale=4.0000 norm=2.6875
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0149 val_loss=0.0000 scale=2.0000 norm=1.5969
[iter 200] loss=0.7638 val_loss=0.0000 scale=4.0000 norm=3.1639
[iter 300] loss=0.4248 val_loss=0.0000 scale=4.0000 norm=3.1247
[iter 400] loss=0.2511 val_loss=0.0000 scale=4.0000 norm=3.1693
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0420 val_loss=0.0000 scale=2.0000 norm=1.5935
[iter 200] loss=0.8447 val_loss=0.0000 scale=2.0000 norm=1.5833
[iter 300] loss=0.5764 val_loss=0.0000 scale=4.0000 norm=3.1318
[iter 400] loss=0.4482 val_loss=0.0000 scale=4.0000 norm=3.0472
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9675 val_loss=0.0000 scale=2.0000 norm=1.5361
[iter 200] loss=0.7072 val_loss=0.0000 scale=4.0000 norm=3.0460
[iter 300] loss=0.3627 val_loss=0.0000 scale=4.0000 norm=3.0074
[iter 400] loss=0.1788 val_loss=0.0000 scale=4.0000 norm=2.9419
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0627 val_loss=0.0000 scale=2.0000 norm=1.6558
[iter 200] loss=0.8208 val_loss=0.0000 scale=4.0000 norm=3.2932
[iter 300] loss=0.4540 val_loss=0.0000 scale=4.0000 norm=3.2459
[iter 400] loss=0.2461 val_loss=0.0000 scale=4.0000 norm=3.2017
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8182 val_loss=0.0000 scale=2.0000 norm=1.4217
[iter 200] loss=0.5882 val_loss=0.0000 scale=2.0000 norm=1.4642
[iter 300] loss=0.2895 val_loss=0.0000 scale=4.0000 norm=2.9019
[iter 400] loss=0.0357 val_loss=0.0000 scale=4.0000 norm=2.8345
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0395 val_loss=0.0000 scale=2.0000 norm=1.6202
[iter 200] loss=0.7556 val_loss=0.0000 scale=4.0000 norm=3.2120
[iter 300] loss=0.3864 val_loss=0.0000 scale=4.0000 norm=3.1564
[iter 400] loss=0.1193 val_loss=0.0000 scale=4.0000 norm=3.1011
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0338 val_loss=0.0000 scale=2.0000 norm=1.6114
[iter 200] loss=0.7883 val_loss=0.0000 scale=4.0000 norm=3.1816
[iter 300] loss=0.4730 val_loss=0.0000 scale=4.0000 norm=3.1223
[iter 400] loss=0.2698 val_loss=0.0000 scale=4.0000 norm=3.1049
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9903 val_loss=0.0000 scale=2.0000 norm=1.5684
[iter 200] loss=0.6877 val_loss=0.0000 scale=4.0000 norm=3.1181
[iter 300] loss=0.3279 val_loss=0.0000 scale=4.0000 norm=3.0786
[iter 400] loss=0.1212 val_loss=0.0000 scale=4.0000 norm=3.0107
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0109 val_loss=0.0000 scale=2.0000 norm=1.5914
[iter 200] loss=0.7620 val_loss=0.0000 scale=4.0000 norm=3.1554
[iter 300] loss=0.4440 val_loss=0.0000 scale=4.0000 norm=3.1118
[iter 400] loss=0.2790 val_loss=0.0000 scale=2.0000 norm=1.5480
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9409 val_loss=0.0000 scale=2.0000 norm=1.5946
[iter 200] loss=0.5891 val_loss=0.0000 scale=4.0000 norm=3.1183
[iter 300] loss=0.1412 val_loss=0.0000 scale=4.0000 norm=3.0932
[iter 400] loss=-0.1912 val_loss=0.0000 scale=4.0000 norm=3.0487
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9610 val_loss=0.0000 scale=2.0000 norm=1.5490
[iter 200] loss=0.6895 val_loss=0.0000 scale=2.0000 norm=1.5208
[iter 300] loss=0.3928 val_loss=0.0000 scale=4.0000 norm=3.0021
[iter 400] loss=0.1698 val_loss=0.0000 scale=4.0000 norm=2.9608
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9925 val_loss=0.0000 scale=2.0000 norm=1.5690
[iter 200] loss=0.6958 val_loss=0.0000 scale=4.0000 norm=3.1218
[iter 300] loss=0.2891 val_loss=0.0000 scale=4.0000 norm=3.0816
[iter 400] loss=0.0225 val_loss=0.0000 scale=4.0000 norm=3.0248
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0629 val_loss=0.0000 scale=2.0000 norm=1.6669
[iter 200] loss=0.8655 val_loss=0.0000 scale=2.0000 norm=1.6741
[iter 300] loss=0.6499 val_loss=0.0000 scale=2.0000 norm=1.6507
[iter 400] loss=0.5331 val_loss=0.0000 scale=4.0000 norm=3.2803
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1633 val_loss=0.0000 scale=2.0000 norm=1.8733
[iter 200] loss=0.8688 val_loss=0.0000 scale=4.0000 norm=3.7637
[iter 300] loss=0.3648 val_loss=0.0000 scale=8.0000 norm=7.5297
[iter 400] loss=-0.6112 val_loss=0.0000 scale=16.0000 norm=15.0595
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9513 val_loss=0.0000 scale=2.0000 norm=1.5687
[iter 200] loss=0.5827 val_loss=0.0000 scale=4.0000 norm=3.1378
[iter 300] loss=0.1058 val_loss=0.0000 scale=4.0000 norm=2.9998
[iter 400] loss=-0.1571 val_loss=0.0000 scale=4.0000 norm=3.0086
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.5676 val_loss=0.0000 scale=2.0000 norm=1.3047
[iter 200] loss=-0.0708 val_loss=0.0000 scale=4.0000 norm=2.7344
[iter 300] loss=-0.8612 val_loss=0.0000 scale=4.0000 norm=2.7331
[iter 400] loss=-1.7962 val_loss=0.0000 scale=8.0000 norm=5.1165
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0507 val_loss=0.0000 scale=2.0000 norm=1.6033
[iter 200] loss=0.8616 val_loss=0.0000 scale=2.0000 norm=1.5898
[iter 300] loss=0.5992 val_loss=0.0000 scale=4.0000 norm=3.1540
[iter 400] loss=0.4686 val_loss=0.0000 scale=4.0000 norm=3.0765
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9730 val_loss=0.0000 scale=2.0000 norm=1.5674
[iter 200] loss=0.6952 val_loss=0.0000 scale=4.0000 norm=3.1074
[iter 300] loss=0.3567 val_loss=0.0000 scale=4.0000 norm=3.0861
[iter 400] loss=0.1696 val_loss=0.0000 scale=4.0000 norm=3.0687
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0271 val_loss=0.0000 scale=2.0000 norm=1.6099
[iter 200] loss=0.7730 val_loss=0.0000 scale=4.0000 norm=3.1950
[iter 300] loss=0.4368 val_loss=0.0000 scale=4.0000 norm=3.1380
[iter 400] loss=0.2653 val_loss=0.0000 scale=4.0000 norm=3.1008
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0663 val_loss=0.0000 scale=2.0000 norm=1.6343
[iter 200] loss=0.8545 val_loss=0.0000 scale=2.0000 norm=1.6177
[iter 300] loss=0.5816 val_loss=0.0000 scale=4.0000 norm=3.1891
[iter 400] loss=0.3539 val_loss=0.0000 scale=2.0000 norm=1.5561
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9337 val_loss=0.0000 scale=2.0000 norm=1.5536
[iter 200] loss=0.5509 val_loss=0.0000 scale=4.0000 norm=3.0700
[iter 300] loss=0.0145 val_loss=0.0000 scale=4.0000 norm=3.0074
[iter 400] loss=-0.4682 val_loss=0.0000 scale=4.0000 norm=2.9519
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0590 val_loss=0.0000 scale=2.0000 norm=1.6306
[iter 200] loss=0.8309 val_loss=0.0000 scale=4.0000 norm=3.2367
[iter 300] loss=0.5189 val_loss=0.0000 scale=4.0000 norm=3.1864
[iter 400] loss=0.3113 val_loss=0.0000 scale=8.0000 norm=6.3445
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9858 val_loss=0.0000 scale=2.0000 norm=1.5716
[iter 200] loss=0.7012 val_loss=0.0000 scale=4.0000 norm=3.1044
[iter 300] loss=0.3476 val_loss=0.0000 scale=4.0000 norm=3.0634
[iter 400] loss=0.1456 val_loss=0.0000 scale=4.0000 norm=3.0459
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9508 val_loss=0.0000 scale=2.0000 norm=1.5560
[iter 200] loss=0.6848 val_loss=0.0000 scale=4.0000 norm=3.0405
[iter 300] loss=0.3347 val_loss=0.0000 scale=4.0000 norm=3.0041
[iter 400] loss=0.0804 val_loss=0.0000 scale=4.0000 norm=2.9571
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0293 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 200] loss=0.7727 val_loss=0.0000 scale=4.0000 norm=3.1787
[iter 300] loss=0.4415 val_loss=0.0000 scale=4.0000 norm=3.1377
[iter 400] loss=0.2120 val_loss=0.0000 scale=4.0000 norm=3.1465
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0102 val_loss=0.0000 scale=2.0000 norm=1.6209
[iter 200] loss=0.6859 val_loss=0.0000 scale=4.0000 norm=3.2029
[iter 300] loss=0.2747 val_loss=0.0000 scale=4.0000 norm=3.1708
[iter 400] loss=-0.0123 val_loss=0.0000 scale=4.0000 norm=3.1462
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9034 val_loss=0.0000 scale=2.0000 norm=1.5201
[iter 200] loss=0.5470 val_loss=0.0000 scale=4.0000 norm=3.0426
[iter 300] loss=0.0782 val_loss=0.0000 scale=4.0000 norm=3.0039
[iter 400] loss=-0.2257 val_loss=0.0000 scale=4.0000 norm=2.9320
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0620 val_loss=0.0000 scale=2.0000 norm=1.6498
[iter 200] loss=0.7988 val_loss=0.0000 scale=4.0000 norm=3.2913
[iter 300] loss=0.4683 val_loss=0.0000 scale=4.0000 norm=3.1918
[iter 400] loss=0.2652 val_loss=0.0000 scale=4.0000 norm=3.1411
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7953 val_loss=0.0000 scale=2.0000 norm=1.6092
[iter 200] loss=0.2738 val_loss=0.0000 scale=4.0000 norm=3.1503
[iter 300] loss=-0.5520 val_loss=0.0000 scale=8.0000 norm=6.2905
[iter 400] loss=-2.1820 val_loss=0.0000 scale=16.0000 norm=12.5810
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9707 val_loss=0.0000 scale=2.0000 norm=1.5521
[iter 200] loss=0.7268 val_loss=0.0000 scale=2.0000 norm=1.5236
[iter 300] loss=0.3945 val_loss=0.0000 scale=4.0000 norm=3.0044
[iter 400] loss=0.1823 val_loss=0.0000 scale=4.0000 norm=2.9357
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0212 val_loss=0.0000 scale=2.0000 norm=1.6019
[iter 200] loss=0.7585 val_loss=0.0000 scale=4.0000 norm=3.1747
[iter 300] loss=0.4566 val_loss=0.0000 scale=2.0000 norm=1.5511
[iter 400] loss=0.3064 val_loss=0.0000 scale=4.0000 norm=3.0798
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0659 val_loss=0.0000 scale=2.0000 norm=1.6226
[iter 200] loss=0.8771 val_loss=0.0000 scale=2.0000 norm=1.6111
[iter 300] loss=0.6225 val_loss=0.0000 scale=4.0000 norm=3.1848
[iter 400] loss=0.4562 val_loss=0.0000 scale=4.0000 norm=3.1232
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9748 val_loss=0.0000 scale=2.0000 norm=1.5568
[iter 200] loss=0.7192 val_loss=0.0000 scale=4.0000 norm=3.0734
[iter 300] loss=0.3840 val_loss=0.0000 scale=4.0000 norm=3.0482
[iter 400] loss=0.1923 val_loss=0.0000 scale=8.0000 norm=6.0352
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9329 val_loss=0.0000 scale=2.0000 norm=1.6486
[iter 200] loss=0.5627 val_loss=0.0000 scale=4.0000 norm=3.3267
[iter 300] loss=0.0066 val_loss=0.0000 scale=4.0000 norm=3.3264
[iter 400] loss=-0.4940 val_loss=0.0000 scale=4.0000 norm=3.2328
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9882 val_loss=0.0000 scale=2.0000 norm=1.5703
[iter 200] loss=0.7384 val_loss=0.0000 scale=4.0000 norm=3.1136
[iter 300] loss=0.3857 val_loss=0.0000 scale=4.0000 norm=3.0744
[iter 400] loss=0.2000 val_loss=0.0000 scale=4.0000 norm=3.0282
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7134 val_loss=0.0000 scale=2.0000 norm=1.3006
[iter 200] loss=0.3386 val_loss=0.0000 scale=4.0000 norm=2.6941
[iter 300] loss=-0.1875 val_loss=0.0000 scale=4.0000 norm=2.6583
[iter 400] loss=-0.6499 val_loss=0.0000 scale=4.0000 norm=2.6757
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9833 val_loss=0.0000 scale=2.0000 norm=1.5749
[iter 200] loss=0.7394 val_loss=0.0000 scale=4.0000 norm=3.1332
[iter 300] loss=0.3972 val_loss=0.0000 scale=4.0000 norm=3.1201
[iter 400] loss=0.1789 val_loss=0.0000 scale=4.0000 norm=3.0912
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0343 val_loss=0.0000 scale=2.0000 norm=1.6093
[iter 200] loss=0.7913 val_loss=0.0000 scale=4.0000 norm=3.1875
[iter 300] loss=0.4793 val_loss=0.0000 scale=4.0000 norm=3.1418
[iter 400] loss=0.3209 val_loss=0.0000 scale=4.0000 norm=3.0960
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7865 val_loss=0.0000 scale=2.0000 norm=1.4126
[iter 200] loss=0.5534 val_loss=0.0000 scale=4.0000 norm=2.9962
[iter 300] loss=0.2262 val_loss=0.0000 scale=4.0000 norm=3.0371
[iter 400] loss=-0.3599 val_loss=0.0000 scale=8.0000 norm=6.0770
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0349 val_loss=0.0000 scale=2.0000 norm=1.6178
[iter 200] loss=0.7856 val_loss=0.0000 scale=4.0000 norm=3.2193
[iter 300] loss=0.5070 val_loss=0.0000 scale=4.0000 norm=3.1565
[iter 400] loss=0.3707 val_loss=0.0000 scale=4.0000 norm=3.1391
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0655 val_loss=0.0000 scale=2.0000 norm=1.6299
[iter 200] loss=0.8559 val_loss=0.0000 scale=2.0000 norm=1.6083
[iter 300] loss=0.5617 val_loss=0.0000 scale=4.0000 norm=3.1644
[iter 400] loss=0.3130 val_loss=0.0000 scale=4.0000 norm=3.1561
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9846 val_loss=0.0000 scale=2.0000 norm=1.5628
[iter 200] loss=0.7354 val_loss=0.0000 scale=2.0000 norm=1.5465
[iter 300] loss=0.4014 val_loss=0.0000 scale=4.0000 norm=3.0318
[iter 400] loss=0.2213 val_loss=0.0000 scale=4.0000 norm=3.0017
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8945 val_loss=0.0000 scale=2.0000 norm=1.5232
[iter 200] loss=0.6372 val_loss=0.0000 scale=4.0000 norm=3.1432
[iter 300] loss=0.2177 val_loss=0.0000 scale=4.0000 norm=3.1682
[iter 400] loss=-0.5316 val_loss=0.0000 scale=8.0000 norm=6.3384
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0113 val_loss=0.0000 scale=2.0000 norm=1.5765
[iter 200] loss=0.7870 val_loss=0.0000 scale=2.0000 norm=1.5554
[iter 300] loss=0.4786 val_loss=0.0000 scale=4.0000 norm=3.0809
[iter 400] loss=0.2981 val_loss=0.0000 scale=4.0000 norm=3.0821
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9761 val_loss=0.0000 scale=2.0000 norm=1.5749
[iter 200] loss=0.7163 val_loss=0.0000 scale=2.0000 norm=1.5394
[iter 300] loss=0.3858 val_loss=0.0000 scale=4.0000 norm=3.0130
[iter 400] loss=0.2014 val_loss=0.0000 scale=4.0000 norm=2.9328
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9775 val_loss=0.0000 scale=2.0000 norm=1.6016
[iter 200] loss=0.6374 val_loss=0.0000 scale=4.0000 norm=3.1873
[iter 300] loss=0.2914 val_loss=0.0000 scale=4.0000 norm=3.1028
[iter 400] loss=0.1045 val_loss=0.0000 scale=4.0000 norm=3.0823
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9916 val_loss=0.0000 scale=2.0000 norm=1.5958
[iter 200] loss=0.7332 val_loss=0.0000 scale=2.0000 norm=1.5901
[iter 300] loss=0.3498 val_loss=0.0000 scale=4.0000 norm=3.1429
[iter 400] loss=0.1169 val_loss=0.0000 scale=4.0000 norm=3.1267
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0202 val_loss=0.0000 scale=2.0000 norm=1.6030
[iter 200] loss=0.7270 val_loss=0.0000 scale=4.0000 norm=3.1747
[iter 300] loss=0.3620 val_loss=0.0000 scale=4.0000 norm=3.0908
[iter 400] loss=0.0859 val_loss=0.0000 scale=8.0000 norm=6.1587
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8748 val_loss=0.0000 scale=2.0000 norm=1.6157
[iter 200] loss=0.4005 val_loss=0.0000 scale=4.0000 norm=3.2493
[iter 300] loss=-0.3972 val_loss=0.0000 scale=4.0000 norm=3.2484
[iter 400] loss=-1.1784 val_loss=0.0000 scale=8.0000 norm=6.2432
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0290 val_loss=0.0000 scale=2.0000 norm=1.6205
[iter 200] loss=0.7201 val_loss=0.0000 scale=4.0000 norm=3.2176
[iter 300] loss=0.3729 val_loss=0.0000 scale=4.0000 norm=3.1479
[iter 400] loss=0.1500 val_loss=0.0000 scale=2.0000 norm=1.5546
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0491 val_loss=0.0000 scale=2.0000 norm=1.6295
[iter 200] loss=0.8258 val_loss=0.0000 scale=4.0000 norm=3.2424
[iter 300] loss=0.5147 val_loss=0.0000 scale=4.0000 norm=3.1821
[iter 400] loss=0.3621 val_loss=0.0000 scale=4.0000 norm=3.1687
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9385 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 200] loss=0.7385 val_loss=0.0000 scale=4.0000 norm=3.1936
[iter 300] loss=0.4380 val_loss=0.0000 scale=4.0000 norm=3.2107
[iter 400] loss=-0.1290 val_loss=0.0000 scale=8.0000 norm=6.4220
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0029 val_loss=0.0000 scale=2.0000 norm=1.6188
[iter 200] loss=0.7310 val_loss=0.0000 scale=4.0000 norm=3.1298
[iter 300] loss=0.4601 val_loss=0.0000 scale=4.0000 norm=3.0387
[iter 400] loss=0.2511 val_loss=0.0000 scale=8.0000 norm=5.9118
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0616 val_loss=0.0000 scale=2.0000 norm=1.6411
[iter 200] loss=0.8432 val_loss=0.0000 scale=2.0000 norm=1.6446
[iter 300] loss=0.5290 val_loss=0.0000 scale=4.0000 norm=3.2374
[iter 400] loss=0.3194 val_loss=0.0000 scale=4.0000 norm=3.2345
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0019 val_loss=0.0000 scale=2.0000 norm=1.6062
[iter 200] loss=0.7724 val_loss=0.0000 scale=4.0000 norm=3.1686
[iter 300] loss=0.4691 val_loss=0.0000 scale=4.0000 norm=3.1732
[iter 400] loss=0.2392 val_loss=0.0000 scale=4.0000 norm=3.1036
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0397 val_loss=0.0000 scale=2.0000 norm=1.6165
[iter 200] loss=0.7889 val_loss=0.0000 scale=4.0000 norm=3.2095
[iter 300] loss=0.4477 val_loss=0.0000 scale=4.0000 norm=3.1814
[iter 400] loss=0.2285 val_loss=0.0000 scale=4.0000 norm=3.1928
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9943 val_loss=0.0000 scale=2.0000 norm=1.5933
[iter 200] loss=0.6818 val_loss=0.0000 scale=4.0000 norm=3.1508
[iter 300] loss=0.2424 val_loss=0.0000 scale=4.0000 norm=3.1192
[iter 400] loss=-0.0467 val_loss=0.0000 scale=2.0000 norm=1.5236
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9766 val_loss=0.0000 scale=2.0000 norm=1.5638
[iter 200] loss=0.6808 val_loss=0.0000 scale=4.0000 norm=3.1192
[iter 300] loss=0.2706 val_loss=0.0000 scale=4.0000 norm=3.0477
[iter 400] loss=0.0022 val_loss=0.0000 scale=4.0000 norm=3.0141
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0481 val_loss=0.0000 scale=2.0000 norm=1.6418
[iter 200] loss=0.7910 val_loss=0.0000 scale=4.0000 norm=3.2678
[iter 300] loss=0.4848 val_loss=0.0000 scale=4.0000 norm=3.2039
[iter 400] loss=0.3565 val_loss=0.0000 scale=4.0000 norm=3.2077
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0135 val_loss=0.0000 scale=2.0000 norm=1.5903
[iter 200] loss=0.7534 val_loss=0.0000 scale=4.0000 norm=3.1679
[iter 300] loss=0.4440 val_loss=0.0000 scale=4.0000 norm=3.0964
[iter 400] loss=0.3060 val_loss=0.0000 scale=4.0000 norm=3.1253
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0225 val_loss=0.0000 scale=2.0000 norm=1.6187
[iter 200] loss=0.7335 val_loss=0.0000 scale=4.0000 norm=3.2092
[iter 300] loss=0.4221 val_loss=0.0000 scale=4.0000 norm=3.1229
[iter 400] loss=0.2560 val_loss=0.0000 scale=4.0000 norm=3.1172
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9993 val_loss=0.0000 scale=2.0000 norm=1.5859
[iter 200] loss=0.7533 val_loss=0.0000 scale=2.0000 norm=1.5824
[iter 300] loss=0.4189 val_loss=0.0000 scale=4.0000 norm=3.1155
[iter 400] loss=0.2468 val_loss=0.0000 scale=4.0000 norm=3.1184
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6195 val_loss=0.0000 scale=2.0000 norm=1.1033
[iter 200] loss=0.2836 val_loss=0.0000 scale=4.0000 norm=2.2032
[iter 300] loss=-0.1897 val_loss=0.0000 scale=4.0000 norm=2.0853
[iter 400] loss=-0.9023 val_loss=0.0000 scale=8.0000 norm=4.1393
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0722 val_loss=0.0000 scale=2.0000 norm=1.6233
[iter 200] loss=0.8823 val_loss=0.0000 scale=2.0000 norm=1.6127
[iter 300] loss=0.5885 val_loss=0.0000 scale=4.0000 norm=3.1836
[iter 400] loss=0.3797 val_loss=0.0000 scale=4.0000 norm=3.1065
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0497 val_loss=0.0000 scale=2.0000 norm=1.6596
[iter 200] loss=0.7332 val_loss=0.0000 scale=4.0000 norm=3.2500
[iter 300] loss=0.4586 val_loss=0.0000 scale=4.0000 norm=3.2312
[iter 400] loss=0.2540 val_loss=0.0000 scale=4.0000 norm=3.1703
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4758 val_loss=0.0000 scale=2.0000 norm=1.0500
[iter 200] loss=0.0729 val_loss=0.0000 scale=2.0000 norm=1.1270
[iter 300] loss=-0.3956 val_loss=0.0000 scale=4.0000 norm=2.2556
[iter 400] loss=-1.1922 val_loss=0.0000 scale=8.0000 norm=4.5003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8212 val_loss=0.0000 scale=2.0000 norm=1.5044
[iter 200] loss=0.4886 val_loss=0.0000 scale=2.0000 norm=1.5240
[iter 300] loss=0.0116 val_loss=0.0000 scale=4.0000 norm=2.9516
[iter 400] loss=-0.3954 val_loss=0.0000 scale=4.0000 norm=2.9359
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0159 val_loss=0.0000 scale=2.0000 norm=1.5855
[iter 200] loss=0.7737 val_loss=0.0000 scale=2.0000 norm=1.5662
[iter 300] loss=0.4205 val_loss=0.0000 scale=4.0000 norm=3.1306
[iter 400] loss=0.1694 val_loss=0.0000 scale=4.0000 norm=3.0703
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0371 val_loss=0.0000 scale=2.0000 norm=1.6136
[iter 200] loss=0.7619 val_loss=0.0000 scale=4.0000 norm=3.1832
[iter 300] loss=0.3887 val_loss=0.0000 scale=4.0000 norm=3.1350
[iter 400] loss=0.1504 val_loss=0.0000 scale=4.0000 norm=3.1246
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9999 val_loss=0.0000 scale=2.0000 norm=1.5886
[iter 200] loss=0.6822 val_loss=0.0000 scale=4.0000 norm=3.1716
[iter 300] loss=0.3134 val_loss=0.0000 scale=4.0000 norm=3.1327
[iter 400] loss=0.1020 val_loss=0.0000 scale=4.0000 norm=3.1221
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9110 val_loss=0.0000 scale=2.0000 norm=1.5906
[iter 200] loss=0.5587 val_loss=0.0000 scale=4.0000 norm=3.0809
[iter 300] loss=0.1242 val_loss=0.0000 scale=8.0000 norm=6.1614
[iter 400] loss=-0.3098 val_loss=0.0000 scale=4.0000 norm=3.0544
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0242 val_loss=0.0000 scale=2.0000 norm=1.5860
[iter 200] loss=0.8119 val_loss=0.0000 scale=2.0000 norm=1.5681
[iter 300] loss=0.5252 val_loss=0.0000 scale=2.0000 norm=1.5418
[iter 400] loss=0.4307 val_loss=0.0000 scale=4.0000 norm=3.0901
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0104 val_loss=0.0000 scale=2.0000 norm=1.5856
[iter 200] loss=0.7708 val_loss=0.0000 scale=4.0000 norm=3.1304
[iter 300] loss=0.4254 val_loss=0.0000 scale=4.0000 norm=3.0763
[iter 400] loss=0.2570 val_loss=0.0000 scale=4.0000 norm=3.0626
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9601 val_loss=0.0000 scale=2.0000 norm=1.5471
[iter 200] loss=0.7246 val_loss=0.0000 scale=2.0000 norm=1.5681
[iter 300] loss=0.3367 val_loss=0.0000 scale=4.0000 norm=3.1565
[iter 400] loss=-0.0901 val_loss=0.0000 scale=4.0000 norm=3.1151
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0220 val_loss=0.0000 scale=2.0000 norm=1.5968
[iter 200] loss=0.8097 val_loss=0.0000 scale=2.0000 norm=1.5838
[iter 300] loss=0.5027 val_loss=0.0000 scale=4.0000 norm=3.1568
[iter 400] loss=0.3251 val_loss=0.0000 scale=4.0000 norm=3.1225
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0148 val_loss=0.0000 scale=2.0000 norm=1.5934
[iter 200] loss=0.7653 val_loss=0.0000 scale=4.0000 norm=3.1740
[iter 300] loss=0.4325 val_loss=0.0000 scale=4.0000 norm=3.1141
[iter 400] loss=0.2777 val_loss=0.0000 scale=2.0000 norm=1.5596
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0262 val_loss=0.0000 scale=2.0000 norm=1.6110
[iter 200] loss=0.7672 val_loss=0.0000 scale=4.0000 norm=3.1833
[iter 300] loss=0.4804 val_loss=0.0000 scale=2.0000 norm=1.5505
[iter 400] loss=0.3416 val_loss=0.0000 scale=4.0000 norm=3.0910
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1119 val_loss=0.0000 scale=2.0000 norm=1.6675
[iter 200] loss=0.9594 val_loss=0.0000 scale=4.0000 norm=3.3340
[iter 300] loss=0.7591 val_loss=0.0000 scale=4.0000 norm=3.3262
[iter 400] loss=0.5244 val_loss=0.0000 scale=8.0000 norm=6.6673
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0138 val_loss=0.0000 scale=2.0000 norm=1.5986
[iter 200] loss=0.7654 val_loss=0.0000 scale=2.0000 norm=1.5884
[iter 300] loss=0.4093 val_loss=0.0000 scale=4.0000 norm=3.1341
[iter 400] loss=0.2081 val_loss=0.0000 scale=4.0000 norm=3.1434
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0357 val_loss=0.0000 scale=2.0000 norm=1.6342
[iter 200] loss=0.7495 val_loss=0.0000 scale=4.0000 norm=3.2204
[iter 300] loss=0.3555 val_loss=0.0000 scale=4.0000 norm=3.1985
[iter 400] loss=0.0441 val_loss=0.0000 scale=4.0000 norm=3.1399
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6240 val_loss=0.0000 scale=2.0000 norm=1.1969
[iter 200] loss=0.2815 val_loss=0.0000 scale=2.0000 norm=1.2527
[iter 300] loss=-0.1276 val_loss=0.0000 scale=4.0000 norm=2.4826
[iter 400] loss=-0.4302 val_loss=0.0000 scale=4.0000 norm=2.4182
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9896 val_loss=0.0000 scale=2.0000 norm=1.5896
[iter 200] loss=0.7173 val_loss=0.0000 scale=4.0000 norm=3.1526
[iter 300] loss=0.3854 val_loss=0.0000 scale=4.0000 norm=3.1233
[iter 400] loss=0.1501 val_loss=0.0000 scale=4.0000 norm=3.0771
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0405 val_loss=0.0000 scale=2.0000 norm=1.6424
[iter 200] loss=0.8080 val_loss=0.0000 scale=4.0000 norm=3.2003
[iter 300] loss=0.5716 val_loss=0.0000 scale=4.0000 norm=3.1636
[iter 400] loss=0.3577 val_loss=0.0000 scale=8.0000 norm=6.2147
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0490 val_loss=0.0000 scale=2.0000 norm=1.6271
[iter 200] loss=0.8172 val_loss=0.0000 scale=4.0000 norm=3.2455
[iter 300] loss=0.5221 val_loss=0.0000 scale=4.0000 norm=3.1808
[iter 400] loss=0.4055 val_loss=0.0000 scale=4.0000 norm=3.1942
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8644 val_loss=0.0000 scale=2.0000 norm=1.5893
[iter 200] loss=0.4033 val_loss=0.0000 scale=4.0000 norm=3.1137
[iter 300] loss=-0.2404 val_loss=0.0000 scale=8.0000 norm=6.2680
[iter 400] loss=-0.8468 val_loss=0.0000 scale=4.0000 norm=3.0899
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8713 val_loss=0.0000 scale=2.0000 norm=1.5459
[iter 200] loss=0.5672 val_loss=0.0000 scale=4.0000 norm=3.0083
[iter 300] loss=0.1970 val_loss=0.0000 scale=4.0000 norm=3.0069
[iter 400] loss=-0.0961 val_loss=0.0000 scale=4.0000 norm=2.9939
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0288 val_loss=0.0000 scale=2.0000 norm=1.5990
[iter 200] loss=0.7642 val_loss=0.0000 scale=4.0000 norm=3.1982
[iter 300] loss=0.4123 val_loss=0.0000 scale=4.0000 norm=3.1501
[iter 400] loss=0.1853 val_loss=0.0000 scale=4.0000 norm=3.1338
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9375 val_loss=0.0000 scale=2.0000 norm=1.5883
[iter 200] loss=0.5475 val_loss=0.0000 scale=4.0000 norm=3.1485
[iter 300] loss=0.0734 val_loss=0.0000 scale=4.0000 norm=3.1129
[iter 400] loss=-0.2482 val_loss=0.0000 scale=4.0000 norm=3.0839
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0090 val_loss=0.0000 scale=2.0000 norm=1.6955
[iter 200] loss=0.6999 val_loss=0.0000 scale=4.0000 norm=3.3966
[iter 300] loss=0.3105 val_loss=0.0000 scale=8.0000 norm=6.6500
[iter 400] loss=-0.4275 val_loss=0.0000 scale=16.0000 norm=13.3003
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0337 val_loss=0.0000 scale=2.0000 norm=1.6308
[iter 200] loss=0.7995 val_loss=0.0000 scale=2.0000 norm=1.6282
[iter 300] loss=0.4506 val_loss=0.0000 scale=4.0000 norm=3.2399
[iter 400] loss=0.1444 val_loss=0.0000 scale=4.0000 norm=3.2069
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0506 val_loss=0.0000 scale=2.0000 norm=1.6723
[iter 200] loss=0.8244 val_loss=0.0000 scale=2.0000 norm=1.6625
[iter 300] loss=0.5528 val_loss=0.0000 scale=4.0000 norm=3.2523
[iter 400] loss=0.4137 val_loss=0.0000 scale=4.0000 norm=3.2004
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0148 val_loss=0.0000 scale=2.0000 norm=1.5983
[iter 200] loss=0.7679 val_loss=0.0000 scale=2.0000 norm=1.5925
[iter 300] loss=0.4130 val_loss=0.0000 scale=4.0000 norm=3.0991
[iter 400] loss=0.2484 val_loss=0.0000 scale=4.0000 norm=3.1096
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8630 val_loss=0.0000 scale=2.0000 norm=1.6621
[iter 200] loss=0.3981 val_loss=0.0000 scale=4.0000 norm=3.4013
[iter 300] loss=-0.1294 val_loss=0.0000 scale=4.0000 norm=3.3008
[iter 400] loss=-0.9508 val_loss=0.0000 scale=16.0000 norm=12.9102
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9150 val_loss=0.0000 scale=2.0000 norm=1.5395
[iter 200] loss=0.5951 val_loss=0.0000 scale=4.0000 norm=2.9948
[iter 300] loss=0.2313 val_loss=0.0000 scale=4.0000 norm=2.9680
[iter 400] loss=-0.0311 val_loss=0.0000 scale=4.0000 norm=2.9578
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6583 val_loss=0.0000 scale=2.0000 norm=1.2787
[iter 200] loss=0.4069 val_loss=0.0000 scale=2.0000 norm=1.3031
[iter 300] loss=0.1940 val_loss=0.0000 scale=4.0000 norm=2.6819
[iter 400] loss=-0.0517 val_loss=0.0000 scale=8.0000 norm=5.3463
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9694 val_loss=0.0000 scale=2.0000 norm=1.5781
[iter 200] loss=0.6742 val_loss=0.0000 scale=2.0000 norm=1.5610
[iter 300] loss=0.2181 val_loss=0.0000 scale=4.0000 norm=3.0736
[iter 400] loss=-0.0411 val_loss=0.0000 scale=4.0000 norm=2.9778
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0357 val_loss=0.0000 scale=2.0000 norm=1.6184
[iter 200] loss=0.7573 val_loss=0.0000 scale=4.0000 norm=3.2017
[iter 300] loss=0.3948 val_loss=0.0000 scale=4.0000 norm=3.1591
[iter 400] loss=0.2008 val_loss=0.0000 scale=2.0000 norm=1.5721
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8036 val_loss=0.0000 scale=2.0000 norm=1.3563
[iter 200] loss=0.5786 val_loss=0.0000 scale=2.0000 norm=1.4194
[iter 300] loss=0.2869 val_loss=0.0000 scale=4.0000 norm=2.8659
[iter 400] loss=0.0246 val_loss=0.0000 scale=4.0000 norm=2.8568
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9859 val_loss=0.0000 scale=2.0000 norm=1.5600
[iter 200] loss=0.7553 val_loss=0.0000 scale=2.0000 norm=1.5407
[iter 300] loss=0.4237 val_loss=0.0000 scale=4.0000 norm=3.0597
[iter 400] loss=0.2332 val_loss=0.0000 scale=4.0000 norm=3.0517
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9395 val_loss=0.0000 scale=2.0000 norm=1.5559
[iter 200] loss=0.7674 val_loss=0.0000 scale=2.0000 norm=1.5488
[iter 300] loss=0.5611 val_loss=0.0000 scale=4.0000 norm=3.0599
[iter 400] loss=0.2355 val_loss=0.0000 scale=8.0000 norm=6.1201
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8911 val_loss=0.0000 scale=2.0000 norm=1.4818
[iter 200] loss=0.6534 val_loss=0.0000 scale=2.0000 norm=1.5098
[iter 300] loss=0.3482 val_loss=0.0000 scale=2.0000 norm=1.4851
[iter 400] loss=0.1573 val_loss=0.0000 scale=4.0000 norm=2.9747
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8261 val_loss=0.0000 scale=2.0000 norm=1.6468
[iter 200] loss=0.3049 val_loss=0.0000 scale=4.0000 norm=3.2241
[iter 300] loss=-0.3025 val_loss=0.0000 scale=4.0000 norm=3.1619
[iter 400] loss=-0.8570 val_loss=0.0000 scale=4.0000 norm=3.0816
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9988 val_loss=0.0000 scale=2.0000 norm=1.5762
[iter 200] loss=0.7730 val_loss=0.0000 scale=4.0000 norm=3.0787
[iter 300] loss=0.4665 val_loss=0.0000 scale=4.0000 norm=3.0358
[iter 400] loss=0.3352 val_loss=0.0000 scale=4.0000 norm=2.9980
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.4415 val_loss=0.0000 scale=2.0000 norm=0.9961
[iter 200] loss=-0.0069 val_loss=0.0000 scale=2.0000 norm=1.0106
[iter 300] loss=-0.6007 val_loss=0.0000 scale=4.0000 norm=2.0618
[iter 400] loss=-1.6328 val_loss=0.0000 scale=8.0000 norm=4.1286
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0579 val_loss=0.0000 scale=2.0000 norm=1.5986
[iter 200] loss=0.8566 val_loss=0.0000 scale=2.0000 norm=1.5798
[iter 300] loss=0.5543 val_loss=0.0000 scale=4.0000 norm=3.1845
[iter 400] loss=0.2210 val_loss=0.0000 scale=8.0000 norm=6.2887
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0080 val_loss=0.0000 scale=2.0000 norm=1.5964
[iter 200] loss=0.7630 val_loss=0.0000 scale=4.0000 norm=3.1513
[iter 300] loss=0.4216 val_loss=0.0000 scale=4.0000 norm=3.1099
[iter 400] loss=0.1914 val_loss=0.0000 scale=4.0000 norm=3.0756
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0332 val_loss=0.0000 scale=2.0000 norm=1.7085
[iter 200] loss=0.4992 val_loss=0.0000 scale=4.0000 norm=3.3739
[iter 300] loss=-0.1150 val_loss=0.0000 scale=8.0000 norm=6.4115
[iter 400] loss=-1.2991 val_loss=0.0000 scale=16.0000 norm=12.8159
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0015 val_loss=0.0000 scale=2.0000 norm=1.6051
[iter 200] loss=0.7014 val_loss=0.0000 scale=4.0000 norm=3.1924
[iter 300] loss=0.3537 val_loss=0.0000 scale=4.0000 norm=3.1425
[iter 400] loss=0.1305 val_loss=0.0000 scale=4.0000 norm=3.0723
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0084 val_loss=0.0000 scale=2.0000 norm=1.5810
[iter 200] loss=0.7674 val_loss=0.0000 scale=4.0000 norm=3.1190
[iter 300] loss=0.4336 val_loss=0.0000 scale=4.0000 norm=3.0827
[iter 400] loss=0.2700 val_loss=0.0000 scale=4.0000 norm=3.0622
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7884 val_loss=0.0000 scale=2.0000 norm=1.5480
[iter 200] loss=0.2644 val_loss=0.0000 scale=4.0000 norm=2.9865
[iter 300] loss=-0.2800 val_loss=0.0000 scale=4.0000 norm=2.8759
[iter 400] loss=-0.6847 val_loss=0.0000 scale=4.0000 norm=2.8582
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9622 val_loss=0.0000 scale=2.0000 norm=1.5395
[iter 200] loss=0.7004 val_loss=0.0000 scale=2.0000 norm=1.5183
[iter 300] loss=0.3565 val_loss=0.0000 scale=4.0000 norm=2.9918
[iter 400] loss=0.0889 val_loss=0.0000 scale=4.0000 norm=2.9501
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9652 val_loss=0.0000 scale=2.0000 norm=1.5742
[iter 200] loss=0.6572 val_loss=0.0000 scale=2.0000 norm=1.5634
[iter 300] loss=0.1886 val_loss=0.0000 scale=4.0000 norm=3.1019
[iter 400] loss=-0.1035 val_loss=0.0000 scale=4.0000 norm=3.0360
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0302 val_loss=0.0000 scale=2.0000 norm=1.6208
[iter 200] loss=0.7652 val_loss=0.0000 scale=4.0000 norm=3.2276
[iter 300] loss=0.3696 val_loss=0.0000 scale=4.0000 norm=3.1851
[iter 400] loss=0.1193 val_loss=0.0000 scale=4.0000 norm=3.1432
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9622 val_loss=0.0000 scale=2.0000 norm=1.5465
[iter 200] loss=0.6991 val_loss=0.0000 scale=2.0000 norm=1.5269
[iter 300] loss=0.3103 val_loss=0.0000 scale=4.0000 norm=3.0007
[iter 400] loss=0.0803 val_loss=0.0000 scale=4.0000 norm=2.9688
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0420 val_loss=0.0000 scale=2.0000 norm=1.6099
[iter 200] loss=0.8334 val_loss=0.0000 scale=4.0000 norm=3.1967
[iter 300] loss=0.5304 val_loss=0.0000 scale=4.0000 norm=3.1739
[iter 400] loss=0.3468 val_loss=0.0000 scale=4.0000 norm=3.1181
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0263 val_loss=0.0000 scale=2.0000 norm=1.5985
[iter 200] loss=0.7653 val_loss=0.0000 scale=4.0000 norm=3.1725
[iter 300] loss=0.4161 val_loss=0.0000 scale=4.0000 norm=3.1281
[iter 400] loss=0.2389 val_loss=0.0000 scale=4.0000 norm=3.0861
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9605 val_loss=0.0000 scale=2.0000 norm=1.5387
[iter 200] loss=0.7498 val_loss=0.0000 scale=4.0000 norm=3.1875
[iter 300] loss=0.4137 val_loss=0.0000 scale=8.0000 norm=6.4023
[iter 400] loss=-0.2463 val_loss=0.0000 scale=16.0000 norm=12.8051
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8543 val_loss=0.0000 scale=2.0000 norm=1.6122
[iter 200] loss=0.4152 val_loss=0.0000 scale=4.0000 norm=3.2085
[iter 300] loss=-0.0602 val_loss=0.0000 scale=4.0000 norm=3.1636
[iter 400] loss=-0.3929 val_loss=0.0000 scale=4.0000 norm=3.0547
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9325 val_loss=0.0000 scale=2.0000 norm=1.5425
[iter 200] loss=0.6515 val_loss=0.0000 scale=4.0000 norm=3.0258
[iter 300] loss=0.2819 val_loss=0.0000 scale=4.0000 norm=2.9889
[iter 400] loss=0.0983 val_loss=0.0000 scale=4.0000 norm=2.9703
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0357 val_loss=0.0000 scale=2.0000 norm=1.5872
[iter 200] loss=0.8134 val_loss=0.0000 scale=2.0000 norm=1.5828
[iter 300] loss=0.4644 val_loss=0.0000 scale=4.0000 norm=3.1239
[iter 400] loss=0.2415 val_loss=0.0000 scale=4.0000 norm=3.1362
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0056 val_loss=0.0000 scale=2.0000 norm=1.5930
[iter 200] loss=0.7082 val_loss=0.0000 scale=4.0000 norm=3.1623
[iter 300] loss=0.4044 val_loss=0.0000 scale=2.0000 norm=1.5432
[iter 400] loss=0.2503 val_loss=0.0000 scale=4.0000 norm=3.1099
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9079 val_loss=0.0000 scale=2.0000 norm=1.5112
[iter 200] loss=0.6718 val_loss=0.0000 scale=2.0000 norm=1.5388
[iter 300] loss=0.2959 val_loss=0.0000 scale=4.0000 norm=3.0704
[iter 400] loss=-0.0343 val_loss=0.0000 scale=4.0000 norm=3.0147
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6275 val_loss=0.0000 scale=2.0000 norm=1.1953
[iter 200] loss=0.2696 val_loss=0.0000 scale=2.0000 norm=1.2845
[iter 300] loss=-0.1856 val_loss=0.0000 scale=4.0000 norm=2.5841
[iter 400] loss=-0.5901 val_loss=0.0000 scale=8.0000 norm=4.9538
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0140 val_loss=0.0000 scale=2.0000 norm=1.6017
[iter 200] loss=0.7416 val_loss=0.0000 scale=4.0000 norm=3.1689
[iter 300] loss=0.4165 val_loss=0.0000 scale=4.0000 norm=3.1152
[iter 400] loss=0.2246 val_loss=0.0000 scale=4.0000 norm=3.0557
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0277 val_loss=0.0000 scale=2.0000 norm=1.5991
[iter 200] loss=0.7803 val_loss=0.0000 scale=4.0000 norm=3.1515
[iter 300] loss=0.4607 val_loss=0.0000 scale=4.0000 norm=3.0968
[iter 400] loss=0.3115 val_loss=0.0000 scale=4.0000 norm=3.1585
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0290 val_loss=0.0000 scale=2.0000 norm=1.6019
[iter 200] loss=0.7873 val_loss=0.0000 scale=4.0000 norm=3.1973
[iter 300] loss=0.4499 val_loss=0.0000 scale=4.0000 norm=3.1697
[iter 400] loss=0.2562 val_loss=0.0000 scale=4.0000 norm=3.1478
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9853 val_loss=0.0000 scale=2.0000 norm=1.5682
[iter 200] loss=0.7242 val_loss=0.0000 scale=4.0000 norm=3.1078
[iter 300] loss=0.3931 val_loss=0.0000 scale=4.0000 norm=3.0747
[iter 400] loss=0.2031 val_loss=0.0000 scale=4.0000 norm=3.0361
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0426 val_loss=0.0000 scale=2.0000 norm=1.6204
[iter 200] loss=0.7845 val_loss=0.0000 scale=4.0000 norm=3.2062
[iter 300] loss=0.4394 val_loss=0.0000 scale=4.0000 norm=3.1504
[iter 400] loss=0.2368 val_loss=0.0000 scale=4.0000 norm=3.1574
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7653 val_loss=0.0000 scale=2.0000 norm=1.3383
[iter 200] loss=0.5083 val_loss=0.0000 scale=2.0000 norm=1.3845
[iter 300] loss=0.1646 val_loss=0.0000 scale=4.0000 norm=2.7568
[iter 400] loss=-0.1275 val_loss=0.0000 scale=4.0000 norm=2.6933
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9897 val_loss=0.0000 scale=2.0000 norm=1.6080
[iter 200] loss=0.6148 val_loss=0.0000 scale=4.0000 norm=3.2122
[iter 300] loss=0.1266 val_loss=0.0000 scale=4.0000 norm=3.1669
[iter 400] loss=-0.2521 val_loss=0.0000 scale=4.0000 norm=3.0924
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.6310 val_loss=0.0000 scale=2.0000 norm=1.3721
[iter 200] loss=0.2143 val_loss=0.0000 scale=4.0000 norm=2.6093
[iter 300] loss=-0.2981 val_loss=0.0000 scale=4.0000 norm=2.6618
[iter 400] loss=-0.7634 val_loss=0.0000 scale=8.0000 norm=4.9863
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8309 val_loss=0.0000 scale=2.0000 norm=1.4285
[iter 200] loss=0.5609 val_loss=0.0000 scale=2.0000 norm=1.4461
[iter 300] loss=0.2625 val_loss=0.0000 scale=4.0000 norm=2.8563
[iter 400] loss=0.0053 val_loss=0.0000 scale=4.0000 norm=2.7825
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0222 val_loss=0.0000 scale=2.0000 norm=1.6105
[iter 200] loss=0.8041 val_loss=0.0000 scale=2.0000 norm=1.6084
[iter 300] loss=0.4919 val_loss=0.0000 scale=4.0000 norm=3.1523
[iter 400] loss=0.2915 val_loss=0.0000 scale=4.0000 norm=3.0881
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0074 val_loss=0.0000 scale=2.0000 norm=1.5889
[iter 200] loss=0.7514 val_loss=0.0000 scale=4.0000 norm=3.1407
[iter 300] loss=0.4083 val_loss=0.0000 scale=4.0000 norm=3.1106
[iter 400] loss=0.2545 val_loss=0.0000 scale=4.0000 norm=3.1133
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0428 val_loss=0.0000 scale=2.0000 norm=1.6061
[iter 200] loss=0.7871 val_loss=0.0000 scale=4.0000 norm=3.2197
[iter 300] loss=0.4498 val_loss=0.0000 scale=4.0000 norm=3.1722
[iter 400] loss=0.2649 val_loss=0.0000 scale=4.0000 norm=3.1709
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0024 val_loss=0.0000 scale=2.0000 norm=1.5880
[iter 200] loss=0.7144 val_loss=0.0000 scale=4.0000 norm=3.1480
[iter 300] loss=0.3624 val_loss=0.0000 scale=4.0000 norm=3.1067
[iter 400] loss=0.1540 val_loss=0.0000 scale=4.0000 norm=3.0862
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9863 val_loss=0.0000 scale=2.0000 norm=1.5690
[iter 200] loss=0.7066 val_loss=0.0000 scale=4.0000 norm=3.1285
[iter 300] loss=0.3576 val_loss=0.0000 scale=4.0000 norm=3.0904
[iter 400] loss=0.1917 val_loss=0.0000 scale=4.0000 norm=3.1288
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0449 val_loss=0.0000 scale=2.0000 norm=1.6186
[iter 200] loss=0.8354 val_loss=0.0000 scale=2.0000 norm=1.6185
[iter 300] loss=0.5559 val_loss=0.0000 scale=4.0000 norm=3.2509
[iter 400] loss=0.3658 val_loss=0.0000 scale=4.0000 norm=3.1749
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0428 val_loss=0.0000 scale=2.0000 norm=1.6071
[iter 200] loss=0.8250 val_loss=0.0000 scale=2.0000 norm=1.5984
[iter 300] loss=0.5033 val_loss=0.0000 scale=4.0000 norm=3.1307
[iter 400] loss=0.3199 val_loss=0.0000 scale=4.0000 norm=3.0892
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9883 val_loss=0.0000 scale=2.0000 norm=1.6061
[iter 200] loss=0.6797 val_loss=0.0000 scale=4.0000 norm=3.1856
[iter 300] loss=0.2936 val_loss=0.0000 scale=4.0000 norm=3.1738
[iter 400] loss=-0.0818 val_loss=0.0000 scale=4.0000 norm=3.0782
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.9178 val_loss=0.0000 scale=2.0000 norm=1.6222
[iter 200] loss=0.6732 val_loss=0.0000 scale=4.0000 norm=3.3440
[iter 300] loss=0.2210 val_loss=0.0000 scale=4.0000 norm=3.3614
[iter 400] loss=-0.1670 val_loss=0.0000 scale=4.0000 norm=3.2580
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8419 val_loss=0.0000 scale=2.0000 norm=1.6270
[iter 200] loss=0.3599 val_loss=0.0000 scale=4.0000 norm=3.3314
[iter 300] loss=-0.4441 val_loss=0.0000 scale=8.0000 norm=6.6718
[iter 400] loss=-1.9801 val_loss=0.0000 scale=16.0000 norm=13.3436
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1226 val_loss=0.0000 scale=2.0000 norm=1.6699
[iter 200] loss=0.9594 val_loss=0.0000 scale=4.0000 norm=3.3122
[iter 300] loss=0.7196 val_loss=0.0000 scale=4.0000 norm=3.2622
[iter 400] loss=0.5638 val_loss=0.0000 scale=4.0000 norm=3.2200
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0041 val_loss=0.0000 scale=2.0000 norm=1.6031
[iter 200] loss=0.6980 val_loss=0.0000 scale=4.0000 norm=3.1856
[iter 300] loss=0.3495 val_loss=0.0000 scale=4.0000 norm=3.1285
[iter 400] loss=0.1120 val_loss=0.0000 scale=4.0000 norm=3.0812
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0477 val_loss=0.0000 scale=2.0000 norm=1.6122
[iter 200] loss=0.8218 val_loss=0.0000 scale=2.0000 norm=1.5886
[iter 300] loss=0.5355 val_loss=0.0000 scale=4.0000 norm=3.1178
[iter 400] loss=0.2726 val_loss=0.0000 scale=4.0000 norm=3.0751
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0544 val_loss=0.0000 scale=2.0000 norm=1.6244
[iter 200] loss=0.8076 val_loss=0.0000 scale=4.0000 norm=3.2323
[iter 300] loss=0.4831 val_loss=0.0000 scale=4.0000 norm=3.2077
[iter 400] loss=0.2970 val_loss=0.0000 scale=4.0000 norm=3.1594
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0387 val_loss=0.0000 scale=2.0000 norm=1.6321
[iter 200] loss=0.7043 val_loss=0.0000 scale=4.0000 norm=3.2172
[iter 300] loss=0.3181 val_loss=0.0000 scale=4.0000 norm=3.1638
[iter 400] loss=0.0854 val_loss=0.0000 scale=4.0000 norm=3.1295
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0542 val_loss=0.0000 scale=2.0000 norm=1.6344
[iter 200] loss=0.7394 val_loss=0.0000 scale=4.0000 norm=3.2618
[iter 300] loss=0.4656 val_loss=0.0000 scale=4.0000 norm=3.2134
[iter 400] loss=0.3771 val_loss=0.0000 scale=2.0000 norm=1.5812
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9571 val_loss=0.0000 scale=2.0000 norm=1.6015
[iter 200] loss=0.6840 val_loss=0.0000 scale=4.0000 norm=3.1576
[iter 300] loss=0.3623 val_loss=0.0000 scale=4.0000 norm=3.1561
[iter 400] loss=0.0995 val_loss=0.0000 scale=4.0000 norm=3.1059
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1173 val_loss=0.0000 scale=2.0000 norm=1.6861
[iter 200] loss=0.9350 val_loss=0.0000 scale=2.0000 norm=1.6542
[iter 300] loss=0.6342 val_loss=0.0000 scale=4.0000 norm=3.3170
[iter 400] loss=0.3428 val_loss=0.0000 scale=4.0000 norm=3.2952
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9883 val_loss=0.0000 scale=2.0000 norm=1.5876
[iter 200] loss=0.7311 val_loss=0.0000 scale=2.0000 norm=1.5674
[iter 300] loss=0.4009 val_loss=0.0000 scale=4.0000 norm=3.0896
[iter 400] loss=0.2092 val_loss=0.0000 scale=4.0000 norm=3.0499
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0018 val_loss=0.0000 scale=2.0000 norm=1.5884
[iter 200] loss=0.7278 val_loss=0.0000 scale=4.0000 norm=3.1708
[iter 300] loss=0.3368 val_loss=0.0000 scale=4.0000 norm=3.1072
[iter 400] loss=0.0934 val_loss=0.0000 scale=4.0000 norm=3.0742
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0374 val_loss=0.0000 scale=2.0000 norm=1.6304
[iter 200] loss=0.8065 val_loss=0.0000 scale=2.0000 norm=1.6258
[iter 300] loss=0.5161 val_loss=0.0000 scale=2.0000 norm=1.5969
[iter 400] loss=0.3419 val_loss=0.0000 scale=4.0000 norm=3.2113
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9081 val_loss=0.0000 scale=2.0000 norm=1.5574
[iter 200] loss=0.5580 val_loss=0.0000 scale=4.0000 norm=3.0540
[iter 300] loss=0.1327 val_loss=0.0000 scale=4.0000 norm=2.9405
[iter 400] loss=-0.1482 val_loss=0.0000 scale=4.0000 norm=2.8147
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8184 val_loss=0.0000 scale=2.0000 norm=1.5822
[iter 200] loss=0.1601 val_loss=0.0000 scale=4.0000 norm=3.1798
[iter 300] loss=-0.5545 val_loss=0.0000 scale=4.0000 norm=3.0729
[iter 400] loss=-1.0489 val_loss=0.0000 scale=4.0000 norm=2.8461
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9567 val_loss=0.0000 scale=2.0000 norm=1.5267
[iter 200] loss=0.7087 val_loss=0.0000 scale=2.0000 norm=1.5002
[iter 300] loss=0.3547 val_loss=0.0000 scale=4.0000 norm=2.9643
[iter 400] loss=0.0773 val_loss=0.0000 scale=4.0000 norm=2.9515
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0415 val_loss=0.0000 scale=2.0000 norm=1.5931
[iter 200] loss=0.8366 val_loss=0.0000 scale=2.0000 norm=1.5769
[iter 300] loss=0.5301 val_loss=0.0000 scale=4.0000 norm=3.1172
[iter 400] loss=0.3850 val_loss=0.0000 scale=4.0000 norm=3.1215
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0148 val_loss=0.0000 scale=2.0000 norm=1.5946
[iter 200] loss=0.7259 val_loss=0.0000 scale=4.0000 norm=3.1443
[iter 300] loss=0.3063 val_loss=0.0000 scale=4.0000 norm=3.1505
[iter 400] loss=-0.4514 val_loss=0.0000 scale=8.0000 norm=6.2995
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.1074 val_loss=0.0000 scale=2.0000 norm=1.6492
[iter 200] loss=0.9517 val_loss=0.0000 scale=2.0000 norm=1.6342
[iter 300] loss=0.7325 val_loss=0.0000 scale=4.0000 norm=3.2228
[iter 400] loss=0.5887 val_loss=0.0000 scale=4.0000 norm=3.2047
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9999 val_loss=0.0000 scale=2.0000 norm=1.5865
[iter 200] loss=0.7472 val_loss=0.0000 scale=2.0000 norm=1.5577
[iter 300] loss=0.4523 val_loss=0.0000 scale=4.0000 norm=3.0712
[iter 400] loss=0.2136 val_loss=0.0000 scale=2.0000 norm=1.4906
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0380 val_loss=0.0000 scale=2.0000 norm=1.6205
[iter 200] loss=0.7755 val_loss=0.0000 scale=4.0000 norm=3.2140
[iter 300] loss=0.4173 val_loss=0.0000 scale=4.0000 norm=3.1653
[iter 400] loss=0.2135 val_loss=0.0000 scale=4.0000 norm=3.0976
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0475 val_loss=0.0000 scale=2.0000 norm=1.6961
[iter 200] loss=0.8308 val_loss=0.0000 scale=4.0000 norm=3.4606
[iter 300] loss=0.4707 val_loss=0.0000 scale=8.0000 norm=6.9339
[iter 400] loss=-0.2373 val_loss=0.0000 scale=16.0000 norm=13.8680
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6625 val_loss=0.0000 scale=2.0000 norm=1.2795
[iter 200] loss=0.3158 val_loss=0.0000 scale=2.0000 norm=1.2823
[iter 300] loss=-0.1569 val_loss=0.0000 scale=4.0000 norm=2.5225
[iter 400] loss=-0.5529 val_loss=0.0000 scale=4.0000 norm=2.5065
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0259 val_loss=0.0000 scale=2.0000 norm=1.5882
[iter 200] loss=0.8080 val_loss=0.0000 scale=2.0000 norm=1.5791
[iter 300] loss=0.4985 val_loss=0.0000 scale=4.0000 norm=3.1230
[iter 400] loss=0.2284 val_loss=0.0000 scale=4.0000 norm=3.0713
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9818 val_loss=0.0000 scale=2.0000 norm=1.5688
[iter 200] loss=0.6973 val_loss=0.0000 scale=4.0000 norm=3.0644
[iter 300] loss=0.3579 val_loss=0.0000 scale=4.0000 norm=3.0094
[iter 400] loss=0.1070 val_loss=0.0000 scale=4.0000 norm=2.9508
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0276 val_loss=0.0000 scale=2.0000 norm=1.6086
[iter 200] loss=0.7764 val_loss=0.0000 scale=4.0000 norm=3.1539
[iter 300] loss=0.4357 val_loss=0.0000 scale=4.0000 norm=3.1022
[iter 400] loss=0.2062 val_loss=0.0000 scale=8.0000 norm=6.2115
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0159 val_loss=0.0000 scale=2.0000 norm=1.5865
[iter 200] loss=0.7850 val_loss=0.0000 scale=2.0000 norm=1.5761
[iter 300] loss=0.4967 val_loss=0.0000 scale=4.0000 norm=3.1151
[iter 400] loss=0.2909 val_loss=0.0000 scale=4.0000 norm=3.0950
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9949 val_loss=0.0000 scale=2.0000 norm=1.5778
[iter 200] loss=0.7307 val_loss=0.0000 scale=4.0000 norm=3.1457
[iter 300] loss=0.3536 val_loss=0.0000 scale=4.0000 norm=3.1023
[iter 400] loss=0.1392 val_loss=0.0000 scale=4.0000 norm=3.0743
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0019 val_loss=0.0000 scale=2.0000 norm=1.5950
[iter 200] loss=0.7371 val_loss=0.0000 scale=2.0000 norm=1.5880
[iter 300] loss=0.3942 val_loss=0.0000 scale=4.0000 norm=3.1684
[iter 400] loss=0.1543 val_loss=0.0000 scale=4.0000 norm=3.0485
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8964 val_loss=0.0000 scale=2.0000 norm=1.5809
[iter 200] loss=0.6371 val_loss=0.0000 scale=2.0000 norm=1.6244
[iter 300] loss=0.2988 val_loss=0.0000 scale=4.0000 norm=3.1301
[iter 400] loss=-0.0799 val_loss=0.0000 scale=8.0000 norm=6.3115
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9873 val_loss=0.0000 scale=2.0000 norm=1.5646
[iter 200] loss=0.7291 val_loss=0.0000 scale=4.0000 norm=3.0940
[iter 300] loss=0.4043 val_loss=0.0000 scale=4.0000 norm=3.0618
[iter 400] loss=0.2369 val_loss=0.0000 scale=4.0000 norm=3.0232
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8987 val_loss=0.0000 scale=2.0000 norm=1.6413
[iter 200] loss=0.2516 val_loss=0.0000 scale=4.0000 norm=3.1660
[iter 300] loss=-0.3210 val_loss=0.0000 scale=8.0000 norm=5.9007
[iter 400] loss=-1.3860 val_loss=0.0000 scale=16.0000 norm=11.6727
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9576 val_loss=0.0000 scale=2.0000 norm=1.5392
[iter 200] loss=0.7005 val_loss=0.0000 scale=2.0000 norm=1.5182
[iter 300] loss=0.3003 val_loss=0.0000 scale=4.0000 norm=2.9952
[iter 400] loss=0.0253 val_loss=0.0000 scale=4.0000 norm=2.9581
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.7906 val_loss=0.0000 scale=2.0000 norm=1.5033
[iter 200] loss=0.4903 val_loss=0.0000 scale=4.0000 norm=3.1652
[iter 300] loss=0.0575 val_loss=0.0000 scale=4.0000 norm=3.1419
[iter 400] loss=-0.6226 val_loss=0.0000 scale=8.0000 norm=6.2143
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0961 val_loss=0.0000 scale=2.0000 norm=1.6487
[iter 200] loss=0.9114 val_loss=0.0000 scale=4.0000 norm=3.2757
[iter 300] loss=0.6577 val_loss=0.0000 scale=4.0000 norm=3.2614
[iter 400] loss=0.4663 val_loss=0.0000 scale=4.0000 norm=3.2392
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.6129 val_loss=0.0000 scale=2.0000 norm=1.1755
[iter 200] loss=0.2344 val_loss=0.0000 scale=2.0000 norm=1.1735
[iter 300] loss=-0.3059 val_loss=0.0000 scale=4.0000 norm=2.2133
[iter 400] loss=-0.7052 val_loss=0.0000 scale=4.0000 norm=2.0840
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9395 val_loss=0.0000 scale=2.0000 norm=1.5689
[iter 200] loss=0.6499 val_loss=0.0000 scale=4.0000 norm=3.1269
[iter 300] loss=0.2817 val_loss=0.0000 scale=4.0000 norm=3.1144
[iter 400] loss=0.0002 val_loss=0.0000 scale=4.0000 norm=3.0498
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0337 val_loss=0.0000 scale=2.0000 norm=1.6122
[iter 200] loss=0.8191 val_loss=0.0000 scale=4.0000 norm=3.1727
[iter 300] loss=0.5329 val_loss=0.0000 scale=4.0000 norm=3.1218
[iter 400] loss=0.3160 val_loss=0.0000 scale=4.0000 norm=3.0620
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0344 val_loss=0.0000 scale=2.0000 norm=1.5824
[iter 200] loss=0.7723 val_loss=0.0000 scale=4.0000 norm=3.1413
[iter 300] loss=0.5020 val_loss=0.0000 scale=4.0000 norm=3.1007
[iter 400] loss=0.3303 val_loss=0.0000 scale=8.0000 norm=6.0561
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9473 val_loss=0.0000 scale=2.0000 norm=1.6438
[iter 200] loss=0.5493 val_loss=0.0000 scale=4.0000 norm=3.2419
[iter 300] loss=0.0870 val_loss=0.0000 scale=4.0000 norm=3.1952
[iter 400] loss=-0.2486 val_loss=0.0000 scale=4.0000 norm=3.0723
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0231 val_loss=0.0000 scale=2.0000 norm=1.5861
[iter 200] loss=0.8140 val_loss=0.0000 scale=4.0000 norm=3.1182
[iter 300] loss=0.5339 val_loss=0.0000 scale=4.0000 norm=3.0821
[iter 400] loss=0.4046 val_loss=0.0000 scale=4.0000 norm=3.0939
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0080 val_loss=0.0000 scale=2.0000 norm=1.5752
[iter 200] loss=0.7549 val_loss=0.0000 scale=4.0000 norm=3.1207
[iter 300] loss=0.3911 val_loss=0.0000 scale=4.0000 norm=3.0437
[iter 400] loss=0.1968 val_loss=0.0000 scale=4.0000 norm=3.0527
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0119 val_loss=0.0000 scale=2.0000 norm=1.6398
[iter 200] loss=0.7960 val_loss=0.0000 scale=4.0000 norm=3.2199
[iter 300] loss=0.5087 val_loss=0.0000 scale=8.0000 norm=6.5049
[iter 400] loss=-0.0513 val_loss=0.0000 scale=16.0000 norm=13.0103
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0811 val_loss=0.0000 scale=2.0000 norm=1.6177
[iter 200] loss=0.9029 val_loss=0.0000 scale=2.0000 norm=1.5978
[iter 300] loss=0.6323 val_loss=0.0000 scale=4.0000 norm=3.1586
[iter 400] loss=0.4366 val_loss=0.0000 scale=4.0000 norm=3.0711
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0290 val_loss=0.0000 scale=2.0000 norm=1.6058
[iter 200] loss=0.7785 val_loss=0.0000 scale=4.0000 norm=3.1764
[iter 300] loss=0.4626 val_loss=0.0000 scale=4.0000 norm=3.1211
[iter 400] loss=0.2801 val_loss=0.0000 scale=4.0000 norm=3.0821
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0995 val_loss=0.0000 scale=2.0000 norm=1.6765
[iter 200] loss=0.8763 val_loss=0.0000 scale=4.0000 norm=3.3023
[iter 300] loss=0.6019 val_loss=0.0000 scale=4.0000 norm=3.2700
[iter 400] loss=0.2952 val_loss=0.0000 scale=8.0000 norm=6.4098
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0151 val_loss=0.0000 scale=2.0000 norm=1.5869
[iter 200] loss=0.7843 val_loss=0.0000 scale=2.0000 norm=1.5657
[iter 300] loss=0.4599 val_loss=0.0000 scale=4.0000 norm=3.1023
[iter 400] loss=0.2740 val_loss=0.0000 scale=4.0000 norm=3.0685
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9990 val_loss=0.0000 scale=2.0000 norm=1.5963
[iter 200] loss=0.7247 val_loss=0.0000 scale=4.0000 norm=3.1647
[iter 300] loss=0.3723 val_loss=0.0000 scale=4.0000 norm=3.1249
[iter 400] loss=0.1708 val_loss=0.0000 scale=4.0000 norm=3.1437
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9806 val_loss=0.0000 scale=2.0000 norm=1.5510
[iter 200] loss=0.7304 val_loss=0.0000 scale=2.0000 norm=1.5314
[iter 300] loss=0.4144 val_loss=0.0000 scale=4.0000 norm=3.0209
[iter 400] loss=0.1670 val_loss=0.0000 scale=4.0000 norm=2.9777
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0014 val_loss=0.0000 scale=2.0000 norm=1.5941
[iter 200] loss=0.7343 val_loss=0.0000 scale=2.0000 norm=1.5818
[iter 300] loss=0.3242 val_loss=0.0000 scale=4.0000 norm=3.1269
[iter 400] loss=0.0659 val_loss=0.0000 scale=4.0000 norm=3.0419
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.4186 val_loss=0.0000 scale=2.0000 norm=1.1959
[iter 200] loss=-0.1390 val_loss=0.0000 scale=4.0000 norm=2.2464
[iter 300] loss=-0.8448 val_loss=0.0000 scale=4.0000 norm=2.2486
[iter 400] loss=-2.1748 val_loss=0.0000 scale=8.0000 norm=4.4986
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.7172 val_loss=0.0000 scale=2.0000 norm=1.2652
[iter 200] loss=0.4424 val_loss=0.0000 scale=2.0000 norm=1.2829
[iter 300] loss=0.1124 val_loss=0.0000 scale=4.0000 norm=2.5155
[iter 400] loss=-0.0606 val_loss=0.0000 scale=2.0000 norm=1.2142
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9398 val_loss=0.0000 scale=2.0000 norm=1.6125
[iter 200] loss=0.5065 val_loss=0.0000 scale=4.0000 norm=3.2317
[iter 300] loss=-0.0078 val_loss=0.0000 scale=4.0000 norm=3.1418
[iter 400] loss=-0.4283 val_loss=0.0000 scale=4.0000 norm=3.0658
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9883 val_loss=0.0000 scale=2.0000 norm=1.5708
[iter 200] loss=0.7254 val_loss=0.0000 scale=4.0000 norm=3.0997
[iter 300] loss=0.3497 val_loss=0.0000 scale=4.0000 norm=3.1042
[iter 400] loss=0.0890 val_loss=0.0000 scale=4.0000 norm=3.0554
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0338 val_loss=0.0000 scale=2.0000 norm=1.6009
[iter 200] loss=0.8154 val_loss=0.0000 scale=2.0000 norm=1.5866
[iter 300] loss=0.5127 val_loss=0.0000 scale=4.0000 norm=3.1208
[iter 400] loss=0.3524 val_loss=0.0000 scale=4.0000 norm=3.1298
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.8190 val_loss=0.0000 scale=2.0000 norm=1.4186
[iter 200] loss=0.6084 val_loss=0.0000 scale=2.0000 norm=1.4858
[iter 300] loss=0.2907 val_loss=0.0000 scale=4.0000 norm=2.9791
[iter 400] loss=0.0129 val_loss=0.0000 scale=4.0000 norm=2.9263
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.9251 val_loss=0.0000 scale=2.0000 norm=1.5646
[iter 200] loss=0.6083 val_loss=0.0000 scale=4.0000 norm=3.0305
[iter 300] loss=0.2300 val_loss=0.0000 scale=4.0000 norm=2.9876
[iter 400] loss=-0.0950 val_loss=0.0000 scale=4.0000 norm=2.9450
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0184 val_loss=0.0000 scale=2.0000 norm=1.5879
[iter 200] loss=0.7688 val_loss=0.0000 scale=4.0000 norm=3.1251
[iter 300] loss=0.4504 val_loss=0.0000 scale=4.0000 norm=3.0662
[iter 400] loss=0.3174 val_loss=0.0000 scale=4.0000 norm=3.0443
[iter 0] loss=1.4189 val_loss=0.0000 scale=1.0000 norm=1.0000
[iter 100] loss=0.5967 val_loss=0.0000 scale=2.0000 norm=1.1761
[iter 200] loss=0.2273 val_loss=0.0000 scale=2.0000 norm=1.1460
[iter 300] loss=-0.1470 val_loss=0.0000 scale=4.0000 norm=2.3398
[iter 400] loss=-0.4313 val_loss=0.0000 scale=4.0000 norm=2.2963
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=0.8195 val_loss=0.0000 scale=2.0000 norm=1.5549
[iter 200] loss=0.3314 val_loss=0.0000 scale=4.0000 norm=3.1034
[iter 300] loss=-0.2531 val_loss=0.0000 scale=4.0000 norm=3.0756
[iter 400] loss=-0.7045 val_loss=0.0000 scale=4.0000 norm=2.9970
[iter 0] loss=1.4189 val_loss=0.0000 scale=2.0000 norm=2.0000
[iter 100] loss=1.0114 val_loss=0.0000 scale=2.0000 norm=1.5849
[iter 200] loss=0.7898 val_loss=0.0000 scale=2.0000 norm=1.5672
[iter 300] loss=0.4927 val_loss=0.0000 scale=4.0000 norm=3.0928
[iter 400] loss=0.3243 val_loss=0.0000 scale=4.0000 norm=3.0892

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n12>
Subject: Job 853009: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:38 2024
Job was executed on host(s) <4*c207n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:44:40 2024
                            <4*c207n07>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:44:40 2024
Terminated at Tue Oct 22 18:03:04 2024
Results reported at Tue Oct 22 18:03:04 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "RRU Trimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4421.03 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.85 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               62.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   1133 sec.
    Turnaround time :                            1106 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1225), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007670294822702293), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008833697854541775), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1471), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 736), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1200), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1111), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005774750980742559), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 920), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 972), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006039763649282273), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007471332063750913), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1585), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1506), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009060802467823285), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1378), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1329), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004448322430226387), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1599), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1209), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005535429534297793), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1350), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006146286445618055), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008482949108218917), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1251), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006351603923212181), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007207810157942742), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1048), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007601414351972178), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1818), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1495), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006313885097659862), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008011644253423779), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1303), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005975581185788757), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1084), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000761779269598752), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1738), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000621045641708979), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006089063812286879), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1755), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006806243815261182), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005590069678230847), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007761821601640643), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1552), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008288933801119529), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1369), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005964376554310468), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1142), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1013), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008565563613336587), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1366), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005278222922749912), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000857563397113258), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1592), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006908126053798391), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1580), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009913954215551412), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1935), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 971), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009913954215551412), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1935), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1687), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1622), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1067), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004766036085439558), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1470), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 776), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009217250280849324), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1821), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008626303578811369), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000567610165459961), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1743), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005701634825982291), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1722), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007811437811729413), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1449), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1088), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007570079889395381), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1157), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008476283631522696), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1619), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1216), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005092772753778522), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006725989622175663), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1556), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006033694301539874), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005054446468659347), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1471), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006497264070963395), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1645), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007084423644447451), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1492), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005750183224627393), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007641231442643155), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1435), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006596997377503438), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008176968465039302), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1427), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008317845912980921), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1688), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1179), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007017430751878978), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1108), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00047937284315545696), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007818209188451587), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1694), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00035473005046680065), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008138840317700184), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1205), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0003679574924790477), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009530254515804563), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1947), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008616373018797132), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1593), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005153428858304325), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1757), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1256), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0003922456441795843), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009698775654796969), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1148), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005200150664283851), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1806), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009870948439326345), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1997), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


Monomer
Filename: (Mordred)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Monomer/(Mordred)_NGB_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c010n04>
Subject: Job 852974: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:56 2024
Job was executed on host(s) <4*c010n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:56 2024
                            <4*c004n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:19:56 2024
Terminated at Wed Oct 23 02:04:07 2024
Results reported at Wed Oct 23 02:04:07 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "Monomer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   148381.45 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.52 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   31477 sec.
    Turnaround time :                            31451 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009031869975994337), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1446), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005973168169665844), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00035352859618047145), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1642), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005830644553554342), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1615), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1710), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007888703115360922), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1475), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006545296527555449), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005693427370711547), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004920181629201088), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005627851359197724), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00082764568021368), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1529), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007453131636215007), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1504), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009014240130299916), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1194), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008439147522623667), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1377), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004920016033728123), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008468437686380083), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1445), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006548144165789429), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1517), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007194666513686725), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1490), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008166449707093197), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1772), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007785963810894216), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1382), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1093), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006636355967442379), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1574), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1333), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004916621715899191), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1508), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1705), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008900272601011674), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1812), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1753), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1290), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004755848824192268), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1624), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009991177171338626), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1883), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1563), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0005626126267295649), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1662), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1236), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004783095152352585), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007667259439196284), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1511), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 949), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1431), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


Trimer
Filename: (Mordred)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer/(Mordred)_NGB_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n13>
Subject: Job 852976: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:56 2024
Job was executed on host(s) <4*c202n13>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:58 2024
                            <4*c202n12>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:19:58 2024
Terminated at Wed Oct 23 04:34:44 2024
Results reported at Wed Oct 23 04:34:44 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "Trimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   189036.25 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.96 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   40504 sec.
    Turnaround time :                            40488 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007962255547642491), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1498), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000917996088183622), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1469), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 835), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008193449350465137), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1370), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 942), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1366), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


Dimer
Filename: (Mordred)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Dimer/(Mordred)_NGB_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c032n01>
Subject: Job 852975: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:56 2024
Job was executed on host(s) <4*c032n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:58 2024
                            <4*c027n04>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:19:58 2024
Terminated at Wed Oct 23 04:41:22 2024
Results reported at Wed Oct 23 04:41:22 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "Dimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   180154.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.66 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               60.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   40896 sec.
    Turnaround time :                            40886 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1753), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009999835663609621), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1998), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000850460489474196), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1826), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007841214862875694), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004414588809991), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1639), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1537), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 929), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008738031271080498), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1400), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0003965874775322101), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000897937558992772), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1805), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009388051340261275), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1927), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008542776007276315), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007215308193798231), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 778), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007201101417039859), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009938738319821344), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 895), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009923893373368759), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1507), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00039263208248704163), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1713), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 761), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007646570381694433), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1541), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004985682446136272), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1579), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1719), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1646), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006715256304090822), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0003533937215995918), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1772), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1457), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1062), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009994446703130541), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1816), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1402), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000866213605086797), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 873), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007107853135456076), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1659), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007474066955421419), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1152), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1543), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


RRU Trimer
Filename: (Mordred)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Trimer/(Mordred)_NGB_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n09>
Subject: Job 852979: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:56 2024
Job was executed on host(s) <4*c202n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:58 2024
                            <4*c202n06>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:19:58 2024
Terminated at Wed Oct 23 09:54:46 2024
Results reported at Wed Oct 23 09:54:46 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "RRU Trimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   279838.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.99 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   59701 sec.
    Turnaround time :                            59690 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007752006151936874), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1514), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0004741859272933493), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.000993035201562214), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1943), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


RRU Dimer
Filename: (Mordred)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Dimer/(Mordred)_NGB_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n05>
Subject: Job 852978: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:56 2024
Job was executed on host(s) <4*c202n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:58 2024
                            <4*c202n08>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:19:58 2024
Terminated at Wed Oct 23 10:00:43 2024
Results reported at Wed Oct 23 10:00:43 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "RRU Dimer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   278654.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.99 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   60072 sec.
    Turnaround time :                            60047 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1385), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1505), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0008498688348476532), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1731), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007076884923946406), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1375), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007655015502048451), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1530), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0006588720862182907), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1536), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__verbose', False)])


RRU Monomer
Filename: (Mordred)_NGB
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer/(Mordred)_NGB_generalizability_scores.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n03>
Subject: Job 852977: <structure_only_mordred_NGB_generalizibility> in cluster <Hazel> Done

Job <structure_only_mordred_NGB_generalizibility> was submitted from host <c009n04> by user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:56 2024
Job was executed on host(s) <4*c202n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Oct 22 17:19:58 2024
                            <4*c202n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_generalizibility> was used as the working directory.
Started at Tue Oct 22 17:19:58 2024
Terminated at Wed Oct 23 12:55:15 2024
Results reported at Wed Oct 23 12:55:15 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 30:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "structure_only_mordred_NGB_generalizibility"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/structure_only_mordred_NGB_generalizibility_shuffle.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../test_generalizability_structure_only.py mordred --regressor_type NGB --target "Rg1 (nm)" --oligo_type "RRU Monomer"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   283376.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.93 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               61.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   70529 sec.
    Turnaround time :                            70519 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/test_structure_only_mordred_NGB_generalizibility_shuffle.err> for stderr output of this job.

