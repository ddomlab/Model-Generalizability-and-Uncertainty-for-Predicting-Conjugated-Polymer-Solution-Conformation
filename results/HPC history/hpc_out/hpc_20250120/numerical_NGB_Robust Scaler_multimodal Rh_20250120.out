
------------------------------------------------------------
Sender: LSF System <lsfadmin@c040n04>
Subject: Job 292166: <numerical_NGB_polymer_size_feats_on_multimodal Rh_all_num_20250120> in cluster <Hazel> Exited

Job <numerical_NGB_polymer_size_feats_on_multimodal Rh_all_num_20250120> was submitted from host <c031n04> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 11:55:31 2025
Job was executed on host(s) <6*c040n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 11:55:33 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 11:55:33 2025
Terminated at Mon Jan 20 11:55:51 2025
Results reported at Mon Jan 20 11:55:51 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 25:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "numerical_NGB_polymer_size_feats_on_multimodal Rh_all_num_20250120"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_NGB_Robust Scaler_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_NGB_Robust Scaler_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "multimodal Rh"                                     --regressor_type "NGB"                                     --transform_type "Robust Scaler"                                     --numerical_feats 'Mn (g/mol)' 'PDI' "solvent dP" "solvent dD" "solvent dH" "polymer dP" "polymer dD" "polymer dH"                                     --columns_to_impute "PDI"
                                    --special_impute 'Mw (g/mol)' #                                   --imputer mean


conda deactivate


------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   1.79 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   26 sec.
    Turnaround time :                            20 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_NGB_Robust Scaler_multimodal Rh_20250120.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c040n04>
Subject: Job 292173: <numerical_NGB_polymer_size_feats_on_multimodal Rh_all_num_20250120> in cluster <Hazel> Done

Job <numerical_NGB_polymer_size_feats_on_multimodal Rh_all_num_20250120> was submitted from host <c031n04> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 11:56:56 2025
Job was executed on host(s) <6*c040n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 11:56:58 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 11:56:58 2025
Terminated at Mon Jan 20 11:57:47 2025
Results reported at Mon Jan 20 11:57:47 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 25:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "numerical_NGB_polymer_size_feats_on_multimodal Rh_all_num_20250120"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_NGB_Robust Scaler_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_NGB_Robust Scaler_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "multimodal Rh"                                     --regressor_type "NGB"                                     --transform_type "Robust Scaler"                                     --numerical_feats 'Mn (g/mol)' 'PDI' 'Mw (g/mol)' "solvent dP" "solvent dD" "solvent dH" "polymer dP" "polymer dD" "polymer dH"                                     --columns_to_impute "PDI"                                     --special_impute 'Mw (g/mol)' #                                   --imputer mean


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   6.68 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   52 sec.
    Turnaround time :                            51 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_NGB_Robust Scaler_multimodal Rh_20250120.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.022090997492208844), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 64), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0012373707661769272), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1422), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02613856989548374), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.000824086014861892), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.02477502536649419), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 70), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.006492178722724748), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 395), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.009396785459457286), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 54), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.04161149528600206), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.004375405782633583), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 300), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0015593589449876479), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1523), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 3.2579580899796083e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.034461442587387016), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 4.062729141163697e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.015694572340362253), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.017868544577002033), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 134), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0005250640881730065), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0058582872609685), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 426), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.011533999859559564), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 454), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 3.7081473573792404e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.09440768809885437), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 150), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 2.1911069426605047e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0013521738807478342), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.003383981776469461), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 252), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002372193188949905), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1007), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.006702584972556357), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 263), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.004203360304307557), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 368), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.019229752417615282), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 133), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0030196318903574298), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 246), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.001398190729727774), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0011130372502446094), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 1467), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.1), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 50), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0076899053520725295), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 128), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.002784677054436912), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 616), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.00037886726639670707), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.009055437051045036), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0006822232235353975), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 2000), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0022492906339114517), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 364), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.0024909215877546276), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 924), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 0.001), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.017557546003282187), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 85), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 4.2297265926445596e-05), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.05889790947231506), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 56), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.0374514253812812e-06), ('regressor__regressor__estimator__verbose', False)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__estimator__learning_rate', 0.003523081646090138), ('regressor__regressor__estimator__minibatch_frac', 1), ('regressor__regressor__estimator__n_estimators', 862), ('regressor__regressor__estimator__natural_gradient', True), ('regressor__regressor__estimator__tol', 1.832454833906162e-06), ('regressor__regressor__estimator__verbose', False)])


Average scores:	 r2: [ 0.127  0.233 -0.041]±[0.15  0.121 0.025]
[array([ 0.12745508,  0.23306538, -0.04115237]), array([8.09276539e+00, 1.38131562e+02, 9.38212520e+04]), array([5.49003808e+00, 8.98685096e+01, 1.40849831e+04])]
scaler
Filename: (PDI-Mw-solvent dP-solvent dD-solvent dH-polymer dP-polymer dD-polymer dH)_NGB_mean_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/scaler/(PDI-Mw-solvent dP-solvent dD-solvent dH-polymer dP-polymer dD-polymer dH)_NGB_mean_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/scaler/(PDI-Mw-solvent dP-solvent dD-solvent dH-polymer dP-polymer dD-polymer dH)_NGB_mean_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_multimodal Rh/scaler/(PDI-Mw-solvent dP-solvent dD-solvent dH-polymer dP-polymer dD-polymer dH)_NGB_mean_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c040n04>
Subject: Job 292194: <numerical_NGB_polymer_size_feats_on_multimodal Rh_all_num_20250120> in cluster <Hazel> Done

Job <numerical_NGB_polymer_size_feats_on_multimodal Rh_all_num_20250120> was submitted from host <c031n04> by user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:03:43 2025
Job was executed on host(s) <6*c040n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jan 20 12:03:45 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jan 20 12:03:45 2025
Terminated at Mon Jan 20 21:43:47 2025
Results reported at Mon Jan 20 21:43:47 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 25:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "numerical_NGB_polymer_size_feats_on_multimodal Rh_all_num_20250120"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_NGB_Robust Scaler_multimodal Rh_20250120.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_NGB_Robust Scaler_multimodal Rh_20250120.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "multimodal Rh"                                     --regressor_type "NGB"                                     --transform_type "Robust Scaler"                                     --numerical_feats 'Mn (g/mol)' 'PDI' 'Mw (g/mol)' "solvent dP" "solvent dD" "solvent dH" "polymer dP" "polymer dD" "polymer dH"                                     --columns_to_impute "PDI"                                     --special_impute 'Mw (g/mol)'                                     --imputer mean


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   160298.25 sec.
    Max Memory :                                 7 GB
    Average Memory :                             6.97 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               9.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   34822 sec.
    Turnaround time :                            34804 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/numerical_NGB_Robust Scaler_multimodal Rh_20250120.err> for stderr output of this job.

