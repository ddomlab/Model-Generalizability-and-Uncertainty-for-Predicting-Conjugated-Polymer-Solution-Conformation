
------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n03>
Subject: Job 257315: <NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309> in cluster <Hazel> Exited

Job <NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309> was submitted from host <c012n03> by user <sdehgha2> in cluster <Hazel> at Sun Mar  9 18:02:05 2025
Job was executed on host(s) <8*c200n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Mar  9 18:02:06 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Mar  9 18:02:06 2025
Terminated at Sun Mar  9 18:02:49 2025
Results reported at Sun Mar  9 18:02:49 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 8
#BSUB -W 72:05
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py --target_features "Rg1 (nm)"                                       --representation "ECFP"                                       --regressor_type "NGB"                                       --radius "3"                                       --vector "count"                                       --oligomer_representation "Trimer"                                       --numerical_feats 'Mn (g/mol)' 'PDI' 'Mw (g/mol)' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' "polymer dP" "polymer dD" "polymer dH" 'solvent dP' 'solvent dD' 'solvent dH'                                       --clustering_method "EG-Ionic-Based Cluster" 



------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   4.38 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   62 sec.
    Turnaround time :                            44 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n08>
Subject: Job 257348: <NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309> in cluster <Hazel> Exited

Job <NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309> was submitted from host <c012n03> by user <sdehgha2> in cluster <Hazel> at Sun Mar  9 18:06:01 2025
Job was executed on host(s) <8*c200n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Mar  9 18:06:02 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Mar  9 18:06:02 2025
Terminated at Sun Mar  9 18:06:19 2025
Results reported at Sun Mar  9 18:06:19 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 8
#BSUB -W 72:05
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py --target_features "Rg1 (nm)"                                       --representation "ECFP"                                       --regressor_type "NGB"                                       --radius "3"                                       --vector "count"                                       --oligomer_representation "Trimer"                                       --numerical_feats 'Mn (g/mol)' 'PDI' 'Mw (g/mol)' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' "polymer dP" "polymer dD" "polymer dH" 'solvent dP' 'solvent dD' 'solvent dH'                                       --clustering_method "EG-Ionic-Based Cluster" 



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   4.07 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   30 sec.
    Turnaround time :                            18 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.err> for stderr output of this job.




-------------------------------------------------- 
OOD TEST ON ionic-EG



-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6



-------------------------------------------------- 
OOD TEST ON ionic-EG



-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09998602757147512), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 436), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1.1420862479577863e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 332), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04200976253205515), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1401), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04985618229316576), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.037162760069820756), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 155), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08492485486124376), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 191), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0005793679854938016), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.012262652876735135), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 718), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07693173041362837), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 289), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 4.1200688201979395e-06), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 449), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09769138353846799), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 76), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1.4618556328765685e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.025041499136197735), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 349), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0005147024286785698), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0991428678550999), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 565), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0005064637034106048), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07352481813242628), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1246), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1.390574606467376e-05), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.038170382202866435), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06496836997888526), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1530), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06791330477469573), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 161), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 646), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009495219651973112), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09766422270753869), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 882), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0009877082613563713), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0824755917342211), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01849589380452444), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1959), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.0002601508988408948), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0656665003549608), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 83), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1.4251753162319205e-05), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007006781991451797), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1336), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02843490403659712), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1048), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 245), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.010336093445493438), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.00028821744245175347), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026546684773620308), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 494), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 2.165501042722766e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0562089045657799), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.041045136219780176), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])


{'CO_ionic-EG': {6: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 151.4780666637749, 'test_rmse': 187.40475058128087, 'test_r2': -1.0254161444426138, 'test_pearson_r': array([-0.84262301]), 'test_pearson_p_value': array([4.1841934e-05]), 'test_spearman_r': -0.7993572671255604, 'test_spearman_p_value': 0.00020270928700180966, 'test_kendall_r': -0.6530624749887988, 'test_kendall_p_value': 0.000702802496032528, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 332, 'regressor__learning_rate': 0.1, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler': None}}, 13: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 151.44374819878274, 'test_rmse': 187.36433233600647, 'test_r2': -1.0245425813303566, 'test_pearson_r': array([-0.83806175]), 'test_pearson_p_value': array([5.04563238e-05]), 'test_spearman_r': -0.7993572671255604, 'test_spearman_p_value': 0.00020270928700180966, 'test_kendall_r': -0.6530624749887988, 'test_kendall_p_value': 0.000702802496032528, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 2000, 'regressor__learning_rate': 0.1, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler': None}}, 42: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 151.3391727913277, 'test_rmse': 187.40206071975408, 'test_r2': -1.0253580023747424, 'test_pearson_r': array([-0.8389994]), 'test_pearson_p_value': array([4.85740612e-05]), 'test_spearman_r': -0.7993572671255604, 'test_spearman_p_value': 0.00020270928700180966, 'test_kendall_r': -0.6530624749887988, 'test_kendall_p_value': 0.000702802496032528, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 449, 'regressor__learning_rate': 0.1, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler': None}}, 69: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 148.41894209483786, 'test_rmse': 179.003114556406, 'test_r2': -0.8478820674952809, 'test_pearson_r': array([-0.82954339]), 'test_pearson_p_value': array([7.05287201e-05]), 'test_spearman_r': -0.8130924259135957, 'test_spearman_p_value': 0.00012831135436337882, 'test_kendall_r': -0.7023548044995571, 'test_kendall_p_value': 0.0003279876085745154, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 161, 'regressor__learning_rate': 0.06791330477469573, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler': None}}, 420: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 151.53076844513708, 'test_rmse': 187.58410728836571, 'test_r2': -1.0292948700853302, 'test_pearson_r': array([-0.84099097]), 'test_pearson_p_value': array([4.47702794e-05]), 'test_spearman_r': -0.7993572671255604, 'test_spearman_p_value': 0.00020270928700180966, 'test_kendall_r': -0.6530624749887988, 'test_kendall_p_value': 0.000702802496032528, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 2000, 'regressor__learning_rate': 0.1, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler': None}}, 1234567890: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 141.52157184087127, 'test_rmse': 180.91236556615502, 'test_r2': -0.8875113884762165, 'test_pearson_r': array([-0.86701481]), 'test_pearson_p_value': array([1.37773263e-05]), 'test_spearman_r': -0.7963351224483373, 'test_spearman_p_value': 0.00022313924500475487, 'test_kendall_r': -0.6709545975912317, 'test_kendall_p_value': 0.000498951441807686, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 2000, 'regressor__learning_rate': 0.02843490403659712, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler': None}}, 473129: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 151.5319594773315, 'test_rmse': 187.50738408138295, 'test_r2': -1.0276352180750488, 'test_pearson_r': array([-0.83807895]), 'test_pearson_p_value': array([5.04212537e-05]), 'test_spearman_r': -0.7993572671255604, 'test_spearman_p_value': 0.00020270928700180966, 'test_kendall_r': -0.6530624749887988, 'test_kendall_p_value': 0.000702802496032528, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 245, 'regressor__learning_rate': 0.1, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler': None}}}, 'overall data shape': {'targets_shape': (259, 1), 'training_features_shape': (259, 522)}}
Trimer_scaler
Filename: (ECFP3.count.512-Mw-PDI-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_transformerOFF
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/OOD_target_Rg/EG-Ionic-Based Cluster/Trimer_scaler/(ECFP3.count.512-Mw-PDI-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_transformerOFF_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/OOD_target_Rg/EG-Ionic-Based Cluster/Trimer_scaler/(ECFP3.count.512-Mw-PDI-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_transformerOFF_predictions.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/OOD_target_Rg/EG-Ionic-Based Cluster/Trimer_scaler/(ECFP3.count.512-Mw-PDI-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_transformerOFF_ClusterTruth.json
Done Saving scores!
---------------------------------------- 
Plotted Comparitive Cluster Scores!
____________________ 
Plotted Parity Plots!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c200n08>
Subject: Job 257403: <NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309> in cluster <Hazel> Done

Job <NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309> was submitted from host <c012n03> by user <sdehgha2> in cluster <Hazel> at Sun Mar  9 18:11:13 2025
Job was executed on host(s) <8*c200n08>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sun Mar  9 18:11:14 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sun Mar  9 18:11:14 2025
Terminated at Mon Mar 10 01:31:55 2025
Results reported at Mon Mar 10 01:31:55 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 8
#BSUB -W 72:05
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../make_ood_prediction.py --target_features "Rg1 (nm)"                                       --representation "ECFP"                                       --regressor_type "NGB"                                       --radius "3"                                       --vector "count"                                       --oligomer_representation "Trimer"                                       --numerical_feats 'Mw (g/mol)' 'PDI' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' "polymer dP" "polymer dD" "polymer dH" 'solvent dP' 'solvent dD' 'solvent dH'                                       --clustering_method "EG-Ionic-Based Cluster" 



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   114363.45 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.99 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               14.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   26466 sec.
    Turnaround time :                            26443 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09988370155478517), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 618), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1.3862656345949869e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 132), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 443), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 2.8489805845903663e-06), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09956176381420086), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 53), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 2.7393959826536427e-05), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1.3241140597337687e-05), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014115633136224532), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 662), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07693173041362837), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 289), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 4.1200688201979395e-06), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 315), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.016068542994568818), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1.8661930139290045e-05), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.097837011478468), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1877), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 5.05838284422925e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09896881347298955), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1873), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1.7292602379222276e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 7.857411758948257e-05), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0441516677903405), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06262804289230722), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 190), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07488635396120776), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0093194978926416), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006007864026733914), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 84), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1.4117456690138748e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08399327344650788), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 93), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09368683046837055), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 397), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 7.609344457164224e-06), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06449854041502759), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 82), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1.4103076091788285e-05), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0097664758942169), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1283), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 635), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])





-------------------------------------------------- 
OPTIMIZING HYPERPARAMETERS FOR REGRESSOR NGB 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 1203), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 0.001), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09674035817348825), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 55), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 7.237788462477598e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06508323312653085), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 760), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])




Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06698849169504878), ('regressor__regressor__minibatch_frac', 1), ('regressor__regressor__n_estimators', 358), ('regressor__regressor__natural_gradient', True), ('regressor__regressor__tol', 1e-06), ('regressor__regressor__verbose', False)])


{'CO_ionic-EG': {6: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 152.48422729219098, 'test_rmse': 188.9447810230196, 'test_r2': -1.0588413249545856, 'test_pearson_r': array([-0.84526708]), 'test_pearson_p_value': array([3.74383472e-05]), 'test_spearman_r': -0.7842465437394441, 'test_spearman_p_value': 0.00032272480603742976, 'test_kendall_r': -0.635170352386366, 'test_kendall_p_value': 0.0009819483841946098, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 2000, 'regressor__learning_rate': 0.1, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 13: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 152.78251073337339, 'test_rmse': 189.2055947964771, 'test_r2': -1.0645291749562324, 'test_pearson_r': array([-0.84774421]), 'test_pearson_p_value': array([3.36715203e-05]), 'test_spearman_r': -0.7842465437394441, 'test_spearman_p_value': 0.00032272480603742976, 'test_kendall_r': -0.635170352386366, 'test_kendall_p_value': 0.0009819483841946098, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 2000, 'regressor__learning_rate': 0.1, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 42: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 151.82255521740504, 'test_rmse': 187.89883083924872, 'test_r2': -1.0361099746863807, 'test_pearson_r': array([-0.84006674]), 'test_pearson_p_value': array([4.65032704e-05]), 'test_spearman_r': -0.7993572671255604, 'test_spearman_p_value': 0.00020270928700180966, 'test_kendall_r': -0.6530624749887988, 'test_kendall_p_value': 0.000702802496032528, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 315, 'regressor__learning_rate': 0.1, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 69: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 151.56111299082846, 'test_rmse': 199.2730310669655, 'test_r2': -1.2900772338455884, 'test_pearson_r': array([-0.86989717]), 'test_pearson_p_value': array([1.19129049e-05]), 'test_spearman_r': -0.8023794118027837, 'test_spearman_p_value': 0.00018385554013188897, 'test_kendall_r': -0.6709545975912317, 'test_kendall_p_value': 0.000498951441807686, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 2000, 'regressor__learning_rate': 0.07488635396120776, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 420: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 152.86966437606796, 'test_rmse': 189.25004729179616, 'test_r2': -1.065499381512161, 'test_pearson_r': array([-0.85133206]), 'test_pearson_p_value': array([2.87818599e-05]), 'test_spearman_r': -0.7842465437394441, 'test_spearman_p_value': 0.00032272480603742976, 'test_kendall_r': -0.635170352386366, 'test_kendall_p_value': 0.0009819483841946098, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 2000, 'regressor__learning_rate': 0.1, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 1234567890: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 152.99493610539588, 'test_rmse': 189.42634317694538, 'test_r2': -1.0693494058986044, 'test_pearson_r': array([-0.84825054]), 'test_pearson_p_value': array([3.2942046e-05]), 'test_spearman_r': -0.7842465437394441, 'test_spearman_p_value': 0.00032272480603742976, 'test_kendall_r': -0.635170352386366, 'test_kendall_p_value': 0.0009819483841946098, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 2000, 'regressor__learning_rate': 0.1, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}, 473129: {'test_mad': 124.73343749999998, 'test_ystd': 131.68110494082853, 'test_mae': 151.48061468491, 'test_rmse': 187.5445686225286, 'test_r2': -1.0284394974533027, 'test_pearson_r': array([-0.84132151]), 'test_pearson_p_value': array([4.41637635e-05]), 'test_spearman_r': -0.7993572671255604, 'test_spearman_p_value': 0.00020270928700180966, 'test_kendall_r': -0.6530624749887988, 'test_kendall_p_value': 0.000702802496032528, 'best_params': {'check_inverse': True, 'func': None, 'inverse_func': None, 'regressor__natural_gradient': True, 'regressor__n_estimators': 1203, 'regressor__learning_rate': 0.1, 'regressor__minibatch_frac': 1, 'regressor__col_sample': 1.0, 'regressor__verbose': False, 'transformer__memory': None, 'transformer__verbose': False, 'transformer__y scaler__copy': True, 'transformer__y scaler__with_mean': True, 'transformer__y scaler__with_std': True}}}, 'overall data shape': {'targets_shape': (259, 1), 'training_features_shape': (259, 522)}}
Trimer_scaler
Filename: (ECFP3.count.512-Mw-PDI-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_Standard
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/OOD_target_Rg/EG-Ionic-Based Cluster/Trimer_scaler/(ECFP3.count.512-Mw-PDI-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_Standard_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/OOD_target_Rg/EG-Ionic-Based Cluster/Trimer_scaler/(ECFP3.count.512-Mw-PDI-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_Standard_predictions.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/OOD_target_Rg/EG-Ionic-Based Cluster/Trimer_scaler/(ECFP3.count.512-Mw-PDI-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH)_NGB_Standard_ClusterTruth.json
Done Saving scores!
------------------------------ 
Plotted Comparitive Cluster Scores!
______________________________ 
Plotted Parity Plots!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n12>
Subject: Job 265156: <NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309> in cluster <Hazel> Done

Job <NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309> was submitted from host <c205n14> by user <sdehgha2> in cluster <Hazel> at Mon Mar 10 00:17:27 2025
Job was executed on host(s) <6*c203n12>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Mar 10 00:53:07 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Mar 10 00:53:07 2025
Terminated at Mon Mar 10 07:28:10 2025
Results reported at Mon Mar 10 07:28:10 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input



#BSUB -n 6
#BSUB -W 72:05
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=16GB]"
#BSUB -J "NGB_Rg1 (nm)_ECFP_Trimer_EG-Ionic-Based Cluster_20250309"  
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../make_ood_prediction.py --target_features "Rg1 (nm)"                                       --representation "ECFP"                                       --regressor_type "NGB"                                       --radius "3"                                       --vector "count"                                       --oligomer_representation "Trimer"                                       --numerical_feats 'Mw (g/mol)' 'PDI' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' "polymer dP" "polymer dD" "polymer dH" 'solvent dP' 'solvent dD' 'solvent dH'                                       --clustering_method "EG-Ionic-Based Cluster" 



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   103744.46 sec.
    Max Memory :                                 2 GB
    Average Memory :                             1.99 GB
    Total Requested Memory :                     16.00 GB
    Delta Memory :                               14.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   23727 sec.
    Turnaround time :                            25843 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_202503_09/NGB_Rg1 (nm)_ECFP_Trimer_3_count_EG-Ionic-Based Cluster_20250309.err> for stderr output of this job.

