polymer unit :Trimer with rep of ECFP6 and binary



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
polymer unit :RRU Monomer with rep of ECFP6 and binary



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0012321412672818095), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 785), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01671606449143059), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01847305086255508), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 114), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006596343829641937), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 136), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014170254738291593), ('regressor__regressor__max_depth', 5580), ('regressor__regressor__n_estimators', 160), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 445), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 225), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015729383726960796), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1462), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026574205341676562), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 70), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005208733082765943), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 268), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.003913871121172571), ('regressor__regressor__max_depth', 18), ('regressor__regressor__n_estimators', 544), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 105), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.050264850172466916), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.003842297813656947), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 628), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01935260766154183), ('regressor__regressor__max_depth', 2169), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1264), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 58), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1160), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009820834052570407), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 227), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08305949316374266), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.025243206469823323), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 61), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0021343145136141855), ('regressor__regressor__max_depth', 70), ('regressor__regressor__n_estimators', 544), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006620013620848667), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 376), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008226988589453896), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 267), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0013464657097237776), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 878), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0030811740287500183), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 301), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008048892683923024), ('regressor__regressor__max_depth', 13), ('regressor__regressor__n_estimators', 261), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07465059229224842), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 82), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00922024826132691), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 245), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05081051123942673), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001998541206582899), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 595), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015555193364877717), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0431567125917002), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005504274026053658), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 427), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02251642124856882), ('regressor__regressor__max_depth', 5322), ('regressor__regressor__n_estimators', 140), ('regressor__regressor__n_jobs', -2)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 4099)}
Average scores:	 r: 0.86±0.13	 r2: 0.64±0.5
RRU Monomer_scaler
Filename: (ECFP6.binary.4096-Mw-PDI)_XGBR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer_scaler/(ECFP6.binary.4096-Mw-PDI)_XGBR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer_scaler/(ECFP6.binary.4096-Mw-PDI)_XGBR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer_scaler/(ECFP6.binary.4096-Mw-PDI)_XGBR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c020n04>
Subject: Job 148880: <ecfp_radius6_vectorbinary_XGBR_Rg1 (nm)_polysize_with_RScaler> in cluster <Hazel> Done

Job <ecfp_radius6_vectorbinary_XGBR_Rg1 (nm)_polysize_with_RScaler> was submitted from host <c032n04> by user <sdehgha2> in cluster <Hazel> at Mon Nov  4 20:29:37 2024
Job was executed on host(s) <4*c020n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov  5 07:51:58 2024
                            <4*c016n02>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Nov  5 07:51:58 2024
Terminated at Tue Nov  5 13:28:10 2024
Results reported at Tue Nov  5 13:28:10 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius6_vectorbinary_XGBR_Rg1 (nm)_polysize_with_RScaler" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorbinary_Rg1 (nm)_XGBR_polysize_with_RScaler.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorbinary_Rg1 (nm)_XGBR_polysize_with_RScaler.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py ecfp --regressor_type XGBR --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   82795.00 sec.
    Max Memory :                                 9 GB
    Average Memory :                             8.01 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               55.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   20180 sec.
    Turnaround time :                            61113 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorbinary_Rg1 (nm)_XGBR_polysize_with_RScaler.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005098703751080629), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 154), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016352108055829623), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 514), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02873332657501033), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 75), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009935313354712965), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 88), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.043506124184151215), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.003333569639656913), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 133), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0787749672705762), ('regressor__regressor__max_depth', 25), ('regressor__regressor__n_estimators', 586), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0020110902680292066), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1463), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0018191016784326784), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1073), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.015085399756827574), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 92), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00118802856033661), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 203), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.051007396233877345), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02053835132771903), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 114), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 3047), ('regressor__regressor__n_estimators', 977), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0024510500756407703), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 512), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001557443991833396), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 736), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008216965092449798), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 275), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.023184786702165255), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.025440481003521833), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 61), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009755848961170847), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 120), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007961208084186242), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 320), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008065782339350496), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 273), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.018481284000546373), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 63), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0018309839134144576), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 499), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008932084845748749), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 261), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07465059229224842), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 82), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008991153053521118), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 249), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04289824120351953), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015962954478563446), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 724), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 157), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06058918913676751), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005954036404766505), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 366), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006895374178654082), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 471), ('regressor__regressor__n_jobs', -2)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 4099)}
Average scores:	 r: 0.86±0.14	 r2: 0.64±0.5
Trimer_scaler
Filename: (ECFP6.binary.4096-Mw-PDI)_XGBR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer_scaler/(ECFP6.binary.4096-Mw-PDI)_XGBR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer_scaler/(ECFP6.binary.4096-Mw-PDI)_XGBR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer_scaler/(ECFP6.binary.4096-Mw-PDI)_XGBR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c017n03>
Subject: Job 148881: <ecfp_radius6_vectorbinary_XGBR_Rg1 (nm)_polysize_with_RScaler> in cluster <Hazel> Done

Job <ecfp_radius6_vectorbinary_XGBR_Rg1 (nm)_polysize_with_RScaler> was submitted from host <c032n04> by user <sdehgha2> in cluster <Hazel> at Mon Nov  4 20:29:37 2024
Job was executed on host(s) <4*c017n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov  5 07:51:58 2024
                            <4*c020n03>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Nov  5 07:51:58 2024
Terminated at Tue Nov  5 13:54:20 2024
Results reported at Tue Nov  5 13:54:20 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius6_vectorbinary_XGBR_Rg1 (nm)_polysize_with_RScaler" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorbinary_Rg1 (nm)_XGBR_polysize_with_RScaler.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorbinary_Rg1 (nm)_XGBR_polysize_with_RScaler.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py ecfp --regressor_type XGBR --radius 6 --vector binary --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   89789.27 sec.
    Max Memory :                                 9 GB
    Average Memory :                             8.06 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               55.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   21744 sec.
    Turnaround time :                            62683 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorbinary_Rg1 (nm)_XGBR_polysize_with_RScaler.err> for stderr output of this job.

