polymer unit :RRU Monomer with rep of ECFP6 and count



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6
polymer unit :Trimer with rep of ECFP6 and count



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001230098440583811), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 787), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005231954072086485), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 160), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03110453615096799), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 68), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00659492804511861), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 136), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0433950422364143), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 445), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 318), ('regressor__regressor__n_estimators', 543), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.024426639494023156), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 92), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016976545790113526), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1130), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005176384401117114), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 269), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005626629196270377), ('regressor__regressor__max_depth', 9872), ('regressor__regressor__n_estimators', 374), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.029380287142759306), ('regressor__regressor__max_depth', 6578), ('regressor__regressor__n_estimators', 91), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0018519187627747183), ('regressor__regressor__max_depth', 3620), ('regressor__regressor__n_estimators', 1277), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 2501), ('regressor__regressor__n_estimators', 976), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0053183680740951375), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 240), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 61), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0011685678938101352), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 989), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008638225864984146), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 249), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.060550800587428295), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 147), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004747075787132998), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 325), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0021343145136141855), ('regressor__regressor__max_depth', 70), ('regressor__regressor__n_estimators', 544), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007660741110624209), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 318), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0014401554773554926), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1601), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.023184640199040797), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010742972352685693), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 861), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0524451853378337), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07465059229224842), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 82), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009218042670219459), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 245), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.051399197467045216), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009695739684691075), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 122), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.012820706856602674), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 261), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04206667837224477), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0014605885430084855), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1510), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015990786032705664), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 4099)}
Average scores:	 r: 0.86±0.14	 r2: 0.64±0.5
RRU Monomer_scaler
Filename: (ECFP6.count.4096-Mw-PDI)_XGBR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer_scaler/(ECFP6.count.4096-Mw-PDI)_XGBR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer_scaler/(ECFP6.count.4096-Mw-PDI)_XGBR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/RRU Monomer_scaler/(ECFP6.count.4096-Mw-PDI)_XGBR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n09>
Subject: Job 148878: <ecfp_radius6_vectorcount_XGBR_Rg1 (nm)_polysize_with_RScaler> in cluster <Hazel> Done

Job <ecfp_radius6_vectorcount_XGBR_Rg1 (nm)_polysize_with_RScaler> was submitted from host <c032n04> by user <sdehgha2> in cluster <Hazel> at Mon Nov  4 20:29:37 2024
Job was executed on host(s) <4*c202n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov  5 07:12:35 2024
                            <4*c202n13>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Nov  5 07:12:35 2024
Terminated at Tue Nov  5 15:54:19 2024
Results reported at Tue Nov  5 15:54:19 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius6_vectorcount_XGBR_Rg1 (nm)_polysize_with_RScaler" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorcount_Rg1 (nm)_XGBR_polysize_with_RScaler.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorcount_Rg1 (nm)_XGBR_polysize_with_RScaler.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py ecfp --regressor_type XGBR --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "RRU Monomer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   124536.00 sec.
    Max Memory :                                 7 GB
    Average Memory :                             6.19 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               57.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   31333 sec.
    Turnaround time :                            69882 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorcount_Rg1 (nm)_XGBR_polysize_with_RScaler.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0012476913548832018), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 766), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0032020646683872684), ('regressor__regressor__max_depth', 3138), ('regressor__regressor__n_estimators', 278), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.003985294378870709), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 522), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009888598612947101), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 88), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.020776809324710908), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 113), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00217497127868359), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 203), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0787749672705762), ('regressor__regressor__max_depth', 25), ('regressor__regressor__n_estimators', 586), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.038637029022812054), ('regressor__regressor__max_depth', 17), ('regressor__regressor__n_estimators', 69), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013050392915707197), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 108), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 13), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05027691349029887), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04750316624141403), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 3182), ('regressor__regressor__n_estimators', 977), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0018600710512816857), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 710), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0011686363423411747), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 981), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002466901750610921), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 926), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013121340277331396), ('regressor__regressor__max_depth', 3473), ('regressor__regressor__n_estimators', 98), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00562910130555884), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 269), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01724011028173883), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 66), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005482795600860764), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 446), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009706791758314478), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 237), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1287), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0018321614323202175), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 499), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009596848441991818), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 249), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07465059229224842), ('regressor__regressor__max_depth', 11), ('regressor__regressor__n_estimators', 82), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0494086694793396), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06039370705567035), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00973548398384146), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 121), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 169), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00957173918331003), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 294), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014592968568070514), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 153), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015507671558815288), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])


{'targets_shape': (257, 1), 'training_features_shape': (257, 4099)}
Average scores:	 r: 0.87±0.14	 r2: 0.65±0.51
Trimer_scaler
Filename: (ECFP6.count.4096-Mw-PDI)_XGBR_Robust Scaler
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer_scaler/(ECFP6.count.4096-Mw-PDI)_XGBR_Robust Scaler_scores.json
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer_scaler/(ECFP6.count.4096-Mw-PDI)_XGBR_Robust Scaler_predictions.csv
/gpfs_common/share03/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/target_Rg/Trimer_scaler/(ECFP6.count.4096-Mw-PDI)_XGBR_Robust Scaler_shape.json
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n09>
Subject: Job 148879: <ecfp_radius6_vectorcount_XGBR_Rg1 (nm)_polysize_with_RScaler> in cluster <Hazel> Done

Job <ecfp_radius6_vectorcount_XGBR_Rg1 (nm)_polysize_with_RScaler> was submitted from host <c032n04> by user <sdehgha2> in cluster <Hazel> at Mon Nov  4 20:29:37 2024
Job was executed on host(s) <4*c202n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Tue Nov  5 07:30:59 2024
                            <4*c202n01>
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training> was used as the working directory.
Started at Tue Nov  5 07:30:59 2024
Terminated at Tue Nov  5 16:09:41 2024
Results reported at Tue Nov  5 16:09:41 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -n 8
#BSUB -W 60:01
#BSUB -R span[ptile=4]
#BSUB -R "rusage[mem=32GB]"
#BSUB -J "ecfp_radius6_vectorcount_XGBR_Rg1 (nm)_polysize_with_RScaler" 
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorcount_Rg1 (nm)_XGBR_polysize_with_RScaler.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorcount_Rg1 (nm)_XGBR_polysize_with_RScaler.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_structure_numerical.py ecfp --regressor_type XGBR --radius 6 --vector count --target "Rg1 (nm)" --oligo_type "Trimer"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   127249.00 sec.
    Max Memory :                                 7 GB
    Average Memory :                             6.29 GB
    Total Requested Memory :                     64.00 GB
    Delta Memory :                               57.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   31126 sec.
    Turnaround time :                            70804 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/ecfp_radius6_vectorcount_Rg1 (nm)_XGBR_polysize_with_RScaler.err> for stderr output of this job.

