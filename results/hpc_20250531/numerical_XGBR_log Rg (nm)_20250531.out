


OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n11>
Subject: Job 634657: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Exited

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c201n09> by user <sdehgha2> in cluster <Hazel> at Sat May 31 10:51:13 2025
Job was executed on host(s) <6*c202n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sat May 31 10:51:15 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sat May 31 10:51:15 2025
Terminated at Sat May 31 10:51:23 2025
Results reported at Sat May 31 10:51:23 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 2:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' 'solvent dP' 'solvent dD' 'solvent dH' conda deactivate


------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   5.23 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   13 sec.
    Turnaround time :                            10 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04552630014634158), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004121597670386236), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 664), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.031199711227896356), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 94), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002935206761749984), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1038), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.053144629184664095), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005687956413555311), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 644), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007558371932524514), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 418), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.015870900684180102), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 200), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0014143856248783684), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0693054146454968), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001348638348873076), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007531526296317581), ('regressor__regressor__max_depth', 18), ('regressor__regressor__n_estimators', 556), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0599355916909144), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016994636371262762), ('regressor__regressor__max_depth', 1525), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0038508366333104664), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 657), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00922944834789857), ('regressor__regressor__max_depth', 3554), ('regressor__regressor__n_estimators', 309), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02256825001011882), ('regressor__regressor__max_depth', 522), ('regressor__regressor__n_estimators', 132), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0028303319036703622), ('regressor__regressor__max_depth', 140), ('regressor__regressor__n_estimators', 1216), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04647405790049598), ('regressor__regressor__max_depth', 14), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009020429110690243), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 326), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006085814553531904), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 429), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015354649493439286), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03679615976003491), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 67), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004833575924261605), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 640), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004742072024361489), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 600), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015263100055829602), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004124725760949138), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 692), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.046008115965963), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 80), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026120181926033298), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 101), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001591310911530367), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016970668750491606), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015017097421885636), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0534470844408209), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00834273432975416), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 362), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015268249749239096), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.88±0.04	 r2: 0.77±0.08
Filename: (Xn)_XGBR_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n02>
Subject: Job 634620: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Done

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c201n09> by user <sdehgha2> in cluster <Hazel> at Sat May 31 10:42:19 2025
Job was executed on host(s) <6*c202n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sat May 31 10:42:20 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sat May 31 10:42:20 2025
Terminated at Sat May 31 11:18:14 2025
Results reported at Sat May 31 11:18:14 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 2:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats "Xn"
conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4300.48 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.16 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               4.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   2160 sec.
    Turnaround time :                            2155 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n01>
Subject: Job 634732: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Exited

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c207n14> by user <sdehgha2> in cluster <Hazel> at Sat May 31 11:24:09 2025
Job was executed on host(s) <6*c205n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sat May 31 11:24:11 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sat May 31 11:24:11 2025
Terminated at Sat May 31 11:24:31 2025
Results reported at Sat May 31 11:24:31 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 2:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' 'solvent dP' 'solvent dD' 'solvent dH' "Dark/light" "Aging time (hour)" "To Aging Temperature (K)" "Sonication/Stirring/heating Temperature (K)" "Merged Stirring /sonication/heating time(min)" conda deactivate


------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   4.76 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   28 sec.
    Turnaround time :                            22 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n04>
Subject: Job 634765: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Exited

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c207n14> by user <sdehgha2> in cluster <Hazel> at Sat May 31 11:32:54 2025
Job was executed on host(s) <6*c203n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sat May 31 11:32:55 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sat May 31 11:32:55 2025
Terminated at Sat May 31 11:33:03 2025
Results reported at Sat May 31 11:33:03 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 2:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Xn' 'Mw (g/mol)' 'PDI' conda deactivate


------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   3.95 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   21 sec.
    Turnaround time :                            9 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n04>
Subject: Job 634797: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Exited

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c207n14> by user <sdehgha2> in cluster <Hazel> at Sat May 31 11:46:23 2025
Job was executed on host(s) <6*c203n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sat May 31 11:46:25 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sat May 31 11:46:25 2025
Terminated at Sat May 31 11:46:31 2025
Results reported at Sat May 31 11:46:31 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 2:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Xn' 'Mw (g/mol)' 'PDI' "polymer dP" "polymer dD" "polymer dH" conda deactivate


------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   3.97 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   27 sec.
    Turnaround time :                            8 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04552630014634158), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004121597670386236), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 664), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.031199711227896356), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 94), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002935206761749984), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1038), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.053144629184664095), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005687956413555311), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 644), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007558371932524514), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 418), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.015870900684180102), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 200), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0014143856248783684), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0693054146454968), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001348638348873076), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007531526296317581), ('regressor__regressor__max_depth', 18), ('regressor__regressor__n_estimators', 556), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0599355916909144), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016994636371262762), ('regressor__regressor__max_depth', 1525), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0038508366333104664), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 657), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00922944834789857), ('regressor__regressor__max_depth', 3554), ('regressor__regressor__n_estimators', 309), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02256825001011882), ('regressor__regressor__max_depth', 522), ('regressor__regressor__n_estimators', 132), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0028303319036703622), ('regressor__regressor__max_depth', 140), ('regressor__regressor__n_estimators', 1216), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04647405790049598), ('regressor__regressor__max_depth', 14), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009020429110690243), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 326), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006085814553531904), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 429), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015354649493439286), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03679615976003491), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 67), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004833575924261605), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 640), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004742072024361489), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 600), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015263100055829602), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004124725760949138), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 692), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.046008115965963), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 80), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026120181926033298), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 101), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001591310911530367), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016970668750491606), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015017097421885636), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0534470844408209), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00834273432975416), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 362), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015268249749239096), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.88±0.04	 r2: 0.77±0.08
Filename: (Xn)_XGBR_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c201n03>
Subject: Job 642619: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Done

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c201n10> by user <sdehgha2> in cluster <Hazel> at Mon Jun  2 14:48:04 2025
Job was executed on host(s) <6*c201n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jun  2 14:48:06 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jun  2 14:48:06 2025
Terminated at Mon Jun  2 15:23:10 2025
Results reported at Mon Jun  2 15:23:10 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 2:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Xn'

conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4278.43 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.03 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               4.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   2129 sec.
    Turnaround time :                            2106 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07376898770363757), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.011860758679786122), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 354), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03334043807222177), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.027414081619102602), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1440), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 8690), ('regressor__regressor__n_estimators', 441), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.027369634149137404), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07525340555067506), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1172), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05090816798496172), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09894412380619001), ('regressor__regressor__max_depth', 15), ('regressor__regressor__n_estimators', 85), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.010825449430464503), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 395), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02401855212775626), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.023246268160901192), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.024661741207087834), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1258), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06047852117326387), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 136), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00788935272759157), ('regressor__regressor__max_depth', 31), ('regressor__regressor__n_estimators', 319), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06586305110687425), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.050022025183280916), ('regressor__regressor__max_depth', 65), ('regressor__regressor__n_estimators', 1977), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0491585777145914), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 58), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09939793013929618), ('regressor__regressor__max_depth', 4800), ('regressor__regressor__n_estimators', 161), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 473), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0027256617415831303), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1222), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004886934049865876), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06403793051911429), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0605943656088878), ('regressor__regressor__max_depth', 8624), ('regressor__regressor__n_estimators', 344), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06403852320319396), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 410), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026048820551639047), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1292), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1412), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0021376625410487837), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004930270306320116), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006963241175072673), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 436), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07377902466117826), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 185), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.88±0.05	 r2: 0.77±0.1
Filename: (Xn-Mw-PDI-polymer dP-polymer dD-polymer dH)_XGBR_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c025n03>
Subject: Job 642712: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Done

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c200n01> by user <sdehgha2> in cluster <Hazel> at Mon Jun  2 15:01:37 2025
Job was executed on host(s) <6*c025n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jun  2 15:01:39 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jun  2 15:01:39 2025
Terminated at Mon Jun  2 15:23:28 2025
Results reported at Mon Jun  2 15:23:28 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 10:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Xn' 'Mw (g/mol)' 'PDI' "polymer dP" "polymer dD" "polymer dH"


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2696.00 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.86 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               4.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   1311 sec.
    Turnaround time :                            1311 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.042256883133469675), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 67), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.003066830040024029), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 930), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.08646479880356304), ('regressor__regressor__max_depth', 2326), ('regressor__regressor__n_estimators', 847), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05816587113967647), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 413), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.011811564363935186), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 222), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0522361112065981), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.013087809820695729), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 225), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07049617059993159), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 70), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.012593670815329304), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 176), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.011298158742698418), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 246), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016994636371262762), ('regressor__regressor__max_depth', 1525), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0035629270881042557), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 602), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0036849963283177483), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 577), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005459812882190875), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 457), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05360623243091646), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 58), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.023411504861488464), ('regressor__regressor__max_depth', 71), ('regressor__regressor__n_estimators', 67), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.022354568034465963), ('regressor__regressor__max_depth', 6070), ('regressor__regressor__n_estimators', 71), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0017431727157557876), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1329), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.034156978640684114), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 109), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009498169280394464), ('regressor__regressor__max_depth', 211), ('regressor__regressor__n_estimators', 1977), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02808287184621455), ('regressor__regressor__max_depth', 236), ('regressor__regressor__n_estimators', 102), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0012311120623438344), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0007902582034455777), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04120587797662674), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03283501980602062), ('regressor__regressor__max_depth', 2663), ('regressor__regressor__n_estimators', 1114), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06677496201427845), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005987406196078722), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 355), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006084544004702702), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 404), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0010585085139582812), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.011289660625142396), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 242), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.8±0.08	 r2: 0.63±0.14
Filename: (concentration-temperature-solvent dP-solvent dD-solvent dH)_XGBR_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c014n03>
Subject: Job 642723: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Done

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c200n01> by user <sdehgha2> in cluster <Hazel> at Mon Jun  2 15:05:56 2025
Job was executed on host(s) <4*c014n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jun  2 15:05:56 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jun  2 15:05:56 2025
Terminated at Mon Jun  2 15:29:40 2025
Results reported at Mon Jun  2 15:29:40 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 4
#BSUB -W 7:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats "Concentration (mg/ml)" "Temperature SANS/SLS/DLS/SEC (K)" "solvent dP" "solvent dD" "solvent dH"


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3039.17 sec.
    Max Memory :                                 5 GB
    Average Memory :                             4.83 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               3.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   1429 sec.
    Turnaround time :                            1424 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 74), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06896681311615943), ('regressor__regressor__max_depth', 74), ('regressor__regressor__n_estimators', 333), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05614723911912617), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06861009405972995), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 212), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07014704052613406), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1278), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 248), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004214231471058703), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 640), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007279031526970504), ('regressor__regressor__max_depth', 5675), ('regressor__regressor__n_estimators', 356), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04981465831113374), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007461726209598496), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 716), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06297008920036258), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.027364528220782454), ('regressor__regressor__max_depth', 33), ('regressor__regressor__n_estimators', 454), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.056278499916055734), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 83), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0992406908815049), ('regressor__regressor__max_depth', 5824), ('regressor__regressor__n_estimators', 192), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 84), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03237872819770682), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 128), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005251057585636431), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 505), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05167338671460219), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03558826683484989), ('regressor__regressor__max_depth', 113), ('regressor__regressor__n_estimators', 170), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02808287184621455), ('regressor__regressor__max_depth', 236), ('regressor__regressor__n_estimators', 102), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02808287184621455), ('regressor__regressor__max_depth', 236), ('regressor__regressor__n_estimators', 102), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05913781436124809), ('regressor__regressor__max_depth', 2125), ('regressor__regressor__n_estimators', 1123), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01059044438743521), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1322), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.056188398127543664), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 95), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.036814363071218256), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 105), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.011118656002458727), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0025157924722436172), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1687), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 105), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.022665962528244418), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 251), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06319478959758097), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 352), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.016453343046977122), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 552), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.039705570414978186), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 154), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.89±0.06	 r2: 0.78±0.14
Filename: (concentration-temperature-solvent dP-solvent dD-solvent dH-light exposure-aging time-aging temperature-prep temperature-prep time)_XGBR_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c020n04>
Subject: Job 642731: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Done

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c200n01> by user <sdehgha2> in cluster <Hazel> at Mon Jun  2 15:07:11 2025
Job was executed on host(s) <4*c020n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jun  2 15:07:13 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jun  2 15:07:13 2025
Terminated at Mon Jun  2 15:30:52 2025
Results reported at Mon Jun  2 15:30:52 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 4
#BSUB -W 7:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats "Concentration (mg/ml)" "Temperature SANS/SLS/DLS/SEC (K)" "solvent dP" "solvent dD" "solvent dH" "Dark/light" "Aging time (hour)" "To Aging Temperature (K)" "Sonication/Stirring/heating Temperature (K)" "Merged Stirring /sonication/heating time(min)"


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3132.44 sec.
    Max Memory :                                 5 GB
    Average Memory :                             4.78 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               3.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   1432 sec.
    Turnaround time :                            1421 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016198223532564117), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004655765134805081), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 539), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0408331098074548), ('regressor__regressor__max_depth', 385), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.06674453681451907), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 63), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002720071706043383), ('regressor__regressor__max_depth', 1461), ('regressor__regressor__n_estimators', 1066), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004639160622966188), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 526), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005282404667153844), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 452), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03570159656541738), ('regressor__regressor__max_depth', 1411), ('regressor__regressor__n_estimators', 78), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.014150136426184515), ('regressor__regressor__max_depth', 4277), ('regressor__regressor__n_estimators', 156), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.023862124550287914), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 119), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016994636371262762), ('regressor__regressor__max_depth', 1525), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03904620442558955), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 62), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02583194047450402), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 86), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.015040987147945783), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 138), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.028837979705653296), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 84), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006242693981360523), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 402), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03365723467233679), ('regressor__regressor__max_depth', 5052), ('regressor__regressor__n_estimators', 51), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006387677410887792), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 259), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002468746607105137), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1478), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.043718261110924035), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.05486531153168565), ('regressor__regressor__max_depth', 111), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0012145225070913932), ('regressor__regressor__max_depth', 14), ('regressor__regressor__n_estimators', 1987), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.003537937430115846), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 677), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.07446247491126995), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004632813841745114), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 426), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.01705192691226142), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 194), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00691472199702135), ('regressor__regressor__max_depth', 13), ('regressor__regressor__n_estimators', 372), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.022059897528545522), ('regressor__regressor__max_depth', 2290), ('regressor__regressor__n_estimators', 105), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0014260915645622294), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02071195172394759), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 109), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.09819569601196722), ('regressor__regressor__max_depth', 19), ('regressor__regressor__n_estimators', 87), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0009233703085713958), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.008288477666418697), ('regressor__regressor__max_depth', 7020), ('regressor__regressor__n_estimators', 219), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.028754950756777684), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 81), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.54±0.12	 r2: 0.27±0.14
Filename: (light exposure-aging time-aging temperature-prep temperature-prep time)_XGBR_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n09>
Subject: Job 642744: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Done

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c207n01> by user <sdehgha2> in cluster <Hazel> at Mon Jun  2 15:10:07 2025
Job was executed on host(s) <4*c205n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jun  2 15:10:08 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jun  2 15:10:08 2025
Terminated at Mon Jun  2 15:40:13 2025
Results reported at Mon Jun  2 15:40:13 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 4
#BSUB -W 7:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats "Dark/light" "Aging time (hour)" "To Aging Temperature (K)" "Sonication/Stirring/heating Temperature (K)" "Merged Stirring /sonication/heating time(min)"


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3569.00 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.92 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               5.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   1825 sec.
    Turnaround time :                            1806 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n09>
Subject: Job 702575: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Done

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c207n04> by user <sdehgha2> in cluster <Hazel> at Mon Jun  9 12:23:45 2025
Job was executed on host(s) <6*c207n09>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jun  9 12:23:45 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jun  9 12:23:45 2025
Terminated at Mon Jun  9 12:27:06 2025
Results reported at Mon Jun  9 12:27:06 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 7:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Xn' 'Mw (g/mol)' 'PDI' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' "polymer dP" "polymer dD" "polymer dH" 'solvent dP' 'solvent dD' 'solvent dH' "Dark/light" "Aging time (hour)" "To Aging Temperature (K)" "Sonication/Stirring/heating Temperature (K)" "Merged Stirring /sonication/heating time(min)"


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   14.60 sec.
    Max Memory :                                 1 GB
    Average Memory :                             0.33 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               7.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   204 sec.
    Turnaround time :                            201 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n05>
Subject: Job 702587: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Done

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c008n01> by user <sdehgha2> in cluster <Hazel> at Mon Jun  9 12:26:32 2025
Job was executed on host(s) <6*c205n05>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jun  9 12:26:59 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jun  9 12:26:59 2025
Terminated at Mon Jun  9 12:27:21 2025
Results reported at Mon Jun  9 12:27:21 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 7:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Xn' 


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   23.28 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   29 sec.
    Turnaround time :                            49 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.




OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6



OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.0022331375533686383), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.48807006581714873), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.0022875586357357087), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.5220934353527271), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.004015084622444675), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1255), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.4306693282059679), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.0015126872605803604), ('regressor__regressor__max_depth', 16), ('regressor__regressor__n_estimators', 1617), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.12486361110545949), ('regressor__regressor__reg_lambda', 1.9906022802611988e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.0015126872605803604), ('regressor__regressor__max_depth', 16), ('regressor__regressor__n_estimators', 1617), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.12486361110545949), ('regressor__regressor__reg_lambda', 1.9906022802611988e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.028356851109837586), ('regressor__regressor__max_depth', 3238), ('regressor__regressor__n_estimators', 537), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.16573806316556178), ('regressor__regressor__reg_lambda', 0.07095088922214145)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.07926022341961532), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1593), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.24964471771755073), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.01323727658408967), ('regressor__regressor__max_depth', 616), ('regressor__regressor__n_estimators', 195), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.0013463776021394652), ('regressor__regressor__reg_lambda', 0.010641580559535551)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.04830054566188945), ('regressor__regressor__max_depth', 1384), ('regressor__regressor__n_estimators', 329), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.5413274741111437), ('regressor__regressor__reg_lambda', 0.11723001974425547)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.007009403614972272), ('regressor__regressor__max_depth', 324), ('regressor__regressor__n_estimators', 837), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.10849812787022829), ('regressor__regressor__reg_lambda', 0.002835665964770575)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.00682922948980691), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-09), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.0022004851074222836), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 2.083573729433089e-08), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.007938574416704595), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.0005885022076382909), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.032475060854346816), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 61), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-09), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.011137448600473185), ('regressor__regressor__max_depth', 47), ('regressor__regressor__n_estimators', 175), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 5.314090198695772e-06), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.05448978165070584), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 143), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 2.004571951378643e-09), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.02303690322046876), ('regressor__regressor__max_depth', 90), ('regressor__regressor__n_estimators', 742), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.02947924482078345), ('regressor__regressor__reg_lambda', 4.722958225114915)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.011317615369815327), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 276), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-09), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.03812990177892988), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-09), ('regressor__regressor__reg_lambda', 0.19727065021392892)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.012059957457811535), ('regressor__regressor__max_depth', 35), ('regressor__regressor__n_estimators', 136), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.005644916329570915), ('regressor__regressor__reg_lambda', 0.12034829579341963)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.00811306138786181), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 256), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 3.140095513752865e-08), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 151), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 2.4545147286260696e-06), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.004901666588750529), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-09), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.005588648807755839), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.0023946552310993314), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.051207627658433584), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-09), ('regressor__regressor__reg_lambda', 0.5527885285240438)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.043126241540747005), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.00962412655588022), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.0268429727432754), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 92), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 2.876088842498251e-05), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.03888203235394876), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 139), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.13488168493590708), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.051992025042584114), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 206), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 7.578851706622258e-05), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.07340145586688099), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.3351671600838862), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.0014851878357563868), ('regressor__regressor__max_depth', 177), ('regressor__regressor__n_estimators', 1909), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 9.562090224681722e-07), ('regressor__regressor__reg_lambda', 0.22717893946542086)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.007184296584821575), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 6.2987633053472426e-09), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 2163), ('regressor__regressor__n_estimators', 170), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 2.889525442895774e-07), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.0013142986648107312), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.004552222000646987), ('regressor__regressor__reg_lambda', 1e-09)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 139), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.0023994237934520796), ('regressor__regressor__reg_lambda', 10.0)])


Average scores:	 r: 0.88±0.05	 r2: 0.76±0.09
Filename: (Xn)_XGBR_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c207n04>
Subject: Job 702622: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Done

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c024n04> by user <sdehgha2> in cluster <Hazel> at Mon Jun  9 12:31:56 2025
Job was executed on host(s) <6*c207n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jun  9 12:31:57 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jun  9 12:31:57 2025
Terminated at Mon Jun  9 13:12:18 2025
Results reported at Mon Jun  9 13:12:18 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 7:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Xn' 


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3973.54 sec.
    Max Memory :                                 3 GB
    Average Memory :                             2.94 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               5.00 GB
    Max Swap :                                   -
    Max Processes :                              26
    Max Threads :                                29
    Run time :                                   2421 sec.
    Turnaround time :                            2422 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 761), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1.4987679510954065e-05), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1540), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 3.483463740973261e-05), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 464), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.11831118947174352), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.04092140246752557), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-06), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.01107064899157147), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-06), ('regressor__regressor__reg_lambda', 3.773165405668525e-06)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.03600289259388458), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-06), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.019498675746902828), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-06), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 661), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.016011639164671732), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 682), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.21300148283792575), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.02467247611688341), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1084), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.7900574018411574), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.009954610217748152), ('regressor__regressor__max_depth', 101), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.003165994191521801), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 675), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 2.5293126124099378e-05), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 351), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.0010908817502256604), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.4640438651297997), ('regressor__regressor__reg_lambda', 1e-06)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 976), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.0011107874361343066), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.03227572808960953), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.0005945808722948349), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.027873855431029996), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 1507), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.34534889592535556), ('regressor__regressor__reg_lambda', 1.870546839822174)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.02591579842141472), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.04106034579832851), ('regressor__regressor__reg_lambda', 2.7151824064543195)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.01225851031942951), ('regressor__regressor__max_depth', 25), ('regressor__regressor__n_estimators', 1002), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.5386037765228187), ('regressor__regressor__reg_lambda', 0.0015406589240324264)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.05917044642939697), ('regressor__regressor__max_depth', 22), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 4.8286166281392696e-06), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.0349787460994975), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1144), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-06), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 721), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.0007929099497918813), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.024793734917123403), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-06), ('regressor__regressor__reg_lambda', 6.586217472635933)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.030380557111036453), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.0046291679997783166), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 154), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 2.0325938451756368e-05), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.007151127422237184), ('regressor__regressor__reg_lambda', 0.904374707998628)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.06548224530201616), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 426), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-06), ('regressor__regressor__reg_lambda', 0.824122669121939)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.0765851765184266), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-06), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 0.0032015759825855215), ('regressor__regressor__reg_lambda', 0.5919626595002444)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 73), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-06), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.06076175299843846), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 665), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 6.7648431401330225e-06), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.020349628961098053), ('regressor__regressor__max_depth', 19), ('regressor__regressor__n_estimators', 478), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 5.543419365400276e-06), ('regressor__regressor__reg_lambda', 2.3557296858637)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.025476798277849464), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-06), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.1), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 2.2330931305031094e-05), ('regressor__regressor__reg_lambda', 10.0)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__eval_metric', 'rmse'), ('regressor__regressor__learning_rate', 0.06876778777789454), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 713), ('regressor__regressor__n_jobs', -1), ('regressor__regressor__reg_alpha', 1e-06), ('regressor__regressor__reg_lambda', 10.0)])


Average scores:	 r: 0.93±0.05	 r2: 0.85±0.11
Filename: (Xn-Mw-PDI-concentration-temperature-polymer dP-polymer dD-polymer dH-solvent dP-solvent dD-solvent dH-light exposure-aging time-aging temperature-prep temperature-prep time)_XGBR_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c019n03>
Subject: Job 702640: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Done

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c202n08> by user <sdehgha2> in cluster <Hazel> at Mon Jun  9 12:37:42 2025
Job was executed on host(s) <6*c019n03>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Mon Jun  9 12:37:42 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Mon Jun  9 12:37:42 2025
Terminated at Mon Jun  9 13:19:26 2025
Results reported at Mon Jun  9 13:19:26 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 7:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Xn' 'Mw (g/mol)' 'PDI' 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' "polymer dP" "polymer dD" "polymer dH" 'solvent dP' 'solvent dD' 'solvent dH' "Dark/light" "Aging time (hour)" "To Aging Temperature (K)" "Sonication/Stirring/heating Temperature (K)" "Merged Stirring /sonication/heating time(min)"


conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   5257.57 sec.
    Max Memory :                                 6 GB
    Average Memory :                             5.41 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               2.00 GB
    Max Swap :                                   -
    Max Processes :                              38
    Max Threads :                                41
    Run time :                                   2510 sec.
    Turnaround time :                            2504 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.

