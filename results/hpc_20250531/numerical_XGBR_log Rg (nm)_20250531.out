


OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n11>
Subject: Job 634657: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Exited

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c201n09> by user <sdehgha2> in cluster <Hazel> at Sat May 31 10:51:13 2025
Job was executed on host(s) <6*c202n11>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sat May 31 10:51:15 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sat May 31 10:51:15 2025
Terminated at Sat May 31 10:51:23 2025
Results reported at Sat May 31 10:51:23 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 2:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' 'solvent dP' 'solvent dD' 'solvent dH' conda deactivate


------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   5.23 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   13 sec.
    Turnaround time :                            10 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.



Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04552630014634158), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004121597670386236), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 664), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.031199711227896356), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 94), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.002935206761749984), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 1038), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 6


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.053144629184664095), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.005687956413555311), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 644), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007558371932524514), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 418), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.015870900684180102), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 200), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0014143856248783684), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 13


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0693054146454968), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001348638348873076), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.007531526296317581), ('regressor__regressor__max_depth', 18), ('regressor__regressor__n_estimators', 556), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0599355916909144), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016994636371262762), ('regressor__regressor__max_depth', 1525), ('regressor__regressor__n_estimators', 1561), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 42


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0038508366333104664), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 657), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00922944834789857), ('regressor__regressor__max_depth', 3554), ('regressor__regressor__n_estimators', 309), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.02256825001011882), ('regressor__regressor__max_depth', 522), ('regressor__regressor__n_estimators', 132), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0028303319036703622), ('regressor__regressor__max_depth', 140), ('regressor__regressor__n_estimators', 1216), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.04647405790049598), ('regressor__regressor__max_depth', 14), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 69


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.009020429110690243), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 326), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.006085814553531904), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 429), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015354649493439286), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.03679615976003491), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 67), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004833575924261605), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 640), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 420


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004742072024361489), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 600), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015263100055829602), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.004124725760949138), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 692), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.046008115965963), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 80), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.026120181926033298), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 101), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 1234567890


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.001591310911530367), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0016970668750491606), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015017097421885636), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0534470844408209), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 50), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.00834273432975416), ('regressor__regressor__max_depth', 10000), ('regressor__regressor__n_estimators', 362), ('regressor__regressor__n_jobs', -2)])





OPTIMIZING HYPERPARAMETERS FOR REGRESSOR XGBR 	SEED: 473129


Best parameters: OrderedDict([('regressor__regressor__learning_rate', 0.0015268249749239096), ('regressor__regressor__max_depth', 10), ('regressor__regressor__n_estimators', 2000), ('regressor__regressor__n_jobs', -2)])


Average scores:	 r: 0.88±0.04	 r2: 0.77±0.08
Filename: (Xn)_XGBR_Standard
Done Saving scores!

------------------------------------------------------------
Sender: LSF System <lsfadmin@c202n02>
Subject: Job 634620: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Done

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c201n09> by user <sdehgha2> in cluster <Hazel> at Sat May 31 10:42:19 2025
Job was executed on host(s) <6*c202n02>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sat May 31 10:42:20 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sat May 31 10:42:20 2025
Terminated at Sat May 31 11:18:14 2025
Results reported at Sat May 31 11:18:14 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 2:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats "Xn"
conda deactivate


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4300.48 sec.
    Max Memory :                                 4 GB
    Average Memory :                             3.16 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               4.00 GB
    Max Swap :                                   -
    Max Processes :                              30
    Max Threads :                                33
    Run time :                                   2160 sec.
    Turnaround time :                            2155 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c205n01>
Subject: Job 634732: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Exited

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c207n14> by user <sdehgha2> in cluster <Hazel> at Sat May 31 11:24:09 2025
Job was executed on host(s) <6*c205n01>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sat May 31 11:24:11 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sat May 31 11:24:11 2025
Terminated at Sat May 31 11:24:31 2025
Results reported at Sat May 31 11:24:31 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 2:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Concentration (mg/ml)' 'Temperature SANS/SLS/DLS/SEC (K)' 'solvent dP' 'solvent dD' 'solvent dH' "Dark/light" "Aging time (hour)" "To Aging Temperature (K)" "Sonication/Stirring/heating Temperature (K)" "Merged Stirring /sonication/heating time(min)" conda deactivate


------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   4.76 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   28 sec.
    Turnaround time :                            22 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n04>
Subject: Job 634765: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Exited

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c207n14> by user <sdehgha2> in cluster <Hazel> at Sat May 31 11:32:54 2025
Job was executed on host(s) <6*c203n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sat May 31 11:32:55 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sat May 31 11:32:55 2025
Terminated at Sat May 31 11:33:03 2025
Results reported at Sat May 31 11:33:03 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 2:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Xn' 'Mw (g/mol)' 'PDI' conda deactivate


------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   3.95 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   21 sec.
    Turnaround time :                            9 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.


------------------------------------------------------------
Sender: LSF System <lsfadmin@c203n04>
Subject: Job 634797: <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> in cluster <Hazel> Exited

Job <numerical_XGBR_with_feats_on_log Rg (nm)_20250531> was submitted from host <c207n14> by user <sdehgha2> in cluster <Hazel> at Sat May 31 11:46:23 2025
Job was executed on host(s) <6*c203n04>, in queue <single_chassis>, as user <sdehgha2> in cluster <Hazel> at Sat May 31 11:46:25 2025
</home/sdehgha2> was used as the home directory.
</share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/code_/training/hpc_submit_training_Rh> was used as the working directory.
Started at Sat May 31 11:46:25 2025
Terminated at Sat May 31 11:46:31 2025
Results reported at Sat May 31 11:46:31 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#BSUB -n 6
#BSUB -W 2:01
#BSUB -R span[hosts=1]
#BSUB -R "rusage[mem=8GB]"
#BSUB -J "numerical_XGBR_with_feats_on_log Rg (nm)_20250531"
#BSUB -o "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.out"
#BSUB -e "/share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err"

source ~/.bashrc
conda activate /usr/local/usrapps/ddomlab/sdehgha2/pls-dataset-env
python ../train_numerical_only.py --target_features "log Rg (nm)"                                   --regressor_type "XGBR"                                   --numerical_feats 'Xn' 'Mw (g/mol)' 'PDI' "polymer dP" "polymer dD" "polymer dH" conda deactivate


------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   3.97 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   27 sec.
    Turnaround time :                            8 sec.

The output (if any) is above this job summary.



PS:

Read file </share/ddomlab/sdehgha2/working-space/main/P1_pls-dataset/pls-dataset-space/PLS-Dataset/results/hpc_20250531/numerical_XGBR_log Rg (nm)_20250531.err> for stderr output of this job.

